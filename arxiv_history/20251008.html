<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>arXiv Daily: 2025-10-08</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto:400,700&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Roboto', Arial, sans-serif; background: #f8f9fa; transition: background 0.3s; }
        .arxiv-card { margin: 2em auto; max-width: 900px; box-shadow: 0 2px 8px rgba(0,0,0,0.08); border-radius: 16px; transition: box-shadow 0.3s, transform 0.3s; }
        .arxiv-card:hover { box-shadow: 0 4px 12px rgba(0,0,0,0.1); transform: translateY(-1px); }
        .arxiv-title { background: linear-gradient(90deg, #0d6efd 60%, #6610f2 100%); color: #fff; border-radius: 16px 16px 0 0; padding: 1.5em; font-size: 1.5em; font-weight: 700; letter-spacing: 0.5px;}
        .arxiv-meta { font-size: 1em; color: #555; margin-bottom: 1em; }
        .arxiv-abstract { background: #fff; padding: 1.5em; border-radius: 0 0 16px 16px; font-size: 1.1em; line-height: 1.7;}
        .arxiv-links a { margin-right: 1em; }
        .arxiv-scores { margin-top: 1em; font-size: 1em; }
        .navbar { background: #fff; box-shadow: 0 2px 8px rgba(0,0,0,0.04);}
        .footer { text-align: center; color: #888; padding: 2em 0 1em 0; font-size: 0.95em;}
        @media (prefers-color-scheme: dark) {
            body { background: #181a1b; color: #e4e6eb; }
            .arxiv-card { background: #23272b; box-shadow: 0 2px 8px rgba(0,0,0,0.28);}
            .arxiv-card:hover { box-shadow: 0 5px 15px rgba(0,0,0,0.25); transform: translateY(-1px); }
            .arxiv-title { background: linear-gradient(90deg, #375a7f 60%, #6f42c1 100%);}
            .arxiv-abstract { background: #23272b; color: #e4e6eb;}
            .navbar { background: #23272b; color: #e4e6eb;}
            .text-muted { color: #adb5bd !important; }
        }
        /* Improved abstract readability */
        .arxiv-abstract-text {
            font-size: 1.1em;
            line-height: 1.7;
        }
        p.big {
            line-height: 1.6;
            font-size: large;
        }
    </style>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'] ],
          processEscapes: true
        }
      });
    </script>
    <script>
    function toggleAuthors(element) {
        const fullList = element.querySelector('.author-full');
        const shortList = element.querySelector('.author-short');
        const toggleText = element.querySelector('.author-toggle');
        
        fullList.style.display = fullList.style.display === 'none' ? 'inline' : 'none';
        shortList.style.display = shortList.style.display === 'none' ? 'inline' : 'none';
        toggleText.textContent = fullList.style.display === 'none' ? ' (show all)' : ' (show less)';
    }
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>
<body>
    <nav class="navbar navbar-expand-lg mb-4">
        <div class="container">
            <a class="navbar-brand fw-bold text-primary" href="#">arXiv Daily</a>
            <span class="navbar-text">Modern arXiv Reader</span>
        </div>
    </nav>

    
    <div class="container py-4">
        <header class="mb-4"><h2 class="display-6 text-center">Analysis of Your Favorites</h2></header>
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Word Cloud</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/word_cloud.png" class="img-fluid rounded" alt="Word Cloud of favorite paper topics">
                    </div>
                </div>
            </div>
        </div>
        
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Clusters</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/daily_cluster_map.png" class="img-fluid rounded" alt="2D Cluster plot of favorite papers">
                    </div>
                </div>
            </div>
        </div>
        
    </div>
    

    <div class="container py-4">
        <header class="mb-4"><h1 class="display-5 text-center text-primary">arXiv: 2025-10-08</h1></header>

        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Score Distribution of All Papers Found Today</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/score_distribution.png" class="img-fluid rounded" alt="Score distribution of all papers found today">
                    </div>
                </div>
            </div>
        </div>
        

        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">1. Euclid preparation: Towards a DR1 application of higher-order weak lensing statistics
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Euclid Collaboration, S. Vinciguerra, F. Bouchè, N. Martinet, L. Castiblanco, C. Uhlemann, S. Pires, J. Harnois-Déraps, C. Giocoli, M. Baldi, et al.</span>
                                <span class="author-full" style="display: none;">Euclid Collaboration, S. Vinciguerra, F. Bouchè, N. Martinet, L. Castiblanco, C. Uhlemann, S. Pires, J. Harnois-Déraps, C. Giocoli, M. Baldi, V. F. Cardone, A. Vadalà, N. Dagoneau, L. Linke, E. Sellentin, P. L. Taylor, J. C. Broxterman, S. Heydenreich, V. Tinnaneri Sreekanth, N. Porqueres, L. Porth, M. Gatti, D. Grandón, A. Barthelemy, F. Bernardeau, A. Tersenov, H. Hoekstra, J. -L. Starck, S. Cheng, P. A. Burger, I. Tereno, R. Scaramella, B. Altieri, S. Andreon, N. Auricchio, C. Baccigalupi, S. Bardelli, A. Biviano, E. Branchini, M. Brescia, S. Camera, G. Cañas-Herrera, V. Capobianco, C. Carbone, J. Carretero, M. Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C. Colodro-Conde, G. Congedo, L. Conversi, Y. Copin, F. Courbin, H. M. Courtois, M. Cropper, A. Da Silva, H. Degaudenzi, S. de la Torre, G. De Lucia, H. Dole, F. Dubath, X. Dupac, S. Dusini, S. Escoffier, M. Farina, R. Farinelli, S. Farrens, F. Faustini, S. Ferriol, F. Finelli, M. Frailis, E. Franceschi, M. Fumana, S. Galeotta, K. George, B. Gillis, J. Gracia-Carpio, A. Grazian, F. Grupp, S. V. H. Haugan, W. Holmes, F. Hormuth, A. Hornstrup, P. Hudelot, K. Jahnke, M. Jhabvala, B. Joachimi, E. Keihänen, S. Kermiche, A. Kiessling, M. Kilbinger, B. Kubik, M. Kunz, H. Kurki-Suonio, A. M. C. Le Brun, S. Ligori, P. B. Lilje, V. Lindholm, I. Lloro, G. Mainetti, D. Maino, O. Mansutti, O. Marggraf, M. Martinelli, F. Marulli, R. J. Massey, E. Medinaceli, S. Mei, M. Melchior, Y. Mellier, M. Meneghetti, G. Meylan, A. Mora, M. Moresco, L. Moscardini, C. Neissner, S. -M. Niemi, C. Padilla, S. Paltani, F. Pasian, K. Pedersen, V. Pettorino, G. Polenta, M. Poncet, L. A. Popa, F. Raison, A. Renzi, J. Rhodes, G. Riccio, E. Romelli, M. Roncarelli, R. Saglia, Z. Sakr, A. G. Sánchez, D. Sapone, B. Sartoris, P. Schneider, T. Schrabback, A. Secroun, G. Seidel, S. Serrano, C. Sirignano, G. Sirri, A. Spurio Mancini, L. Stanco, J. Steinwagner, P. Tallada-Crespí, A. N. Taylor, N. Tessore, S. Toft, R. Toledo-Moreo, F. Torradeflot, I. Tutusaus, J. Valiviita, T. Vassallo, Y. Wang, J. Weller, A. Zacchei, G. Zamorani, F. M. Zerbi, E. Zucca, M. Ballardini, M. Bolzonella, A. Boucaud, E. Bozzo, C. Burigana, R. Cabanac, M. Calabrese, A. Cappi, J. A. Escartin Vigo, L. Gabarra, W. G. Hartley, R. Maoli, J. Martín-Fleitas, S. Matthew, N. Mauri, R. B. Metcalf, A. Pezzotta, M. Pöntinen, I. Risso, V. Scottez, M. Sereno, M. Tenti, M. Viel, M. Wiesmann, Y. Akrami, I. T. Andika, R. E. Angulo, S. Anselmi, M. Archidiacono, F. Atrio-Barandela, E. Aubourg, D. Bertacca, M. Bethermin, A. Blanchard, L. Blot, M. Bonici, S. Borgani, M. L. Brown, S. Bruton, A. Calabro, B. Camacho Quevedo, F. Caro, C. S. Carvalho, T. Castro, F. Cogato, S. Conseil, A. R. Cooray, G. Desprez, A. Díaz-Sánchez, J. J. Diaz, S. Di Domizio, J. M. Diego, M. Y. Elkhashab, Y. Fang, P. G. Ferreira, A. Finoguenov, A. Franco, K. Ganga, J. García-Bellido, T. Gasparetto, V. Gautard, R. Gavazzi, E. Gaztanaga, F. Giacomini, F. Gianotti, G. Gozaliasl, M. Guidi, C. M. Gutierrez, A. Hall, S. Hemmati, H. Hildebrandt, J. Hjorth, J. J. E. Kajava, Y. Kang, D. Karagiannis, K. Kiiveri, J. Kim, C. C. Kirkpatrick, S. Kruk, L. Legrand, M. Lembo, F. Lepori, G. Leroy, G. F. Lesci, J. Lesgourgues, T. I. Liaudat, J. Macias-Perez, M. Magliocchetti, F. Mannucci, C. J. A. P. Martins, L. Maurin, M. Miluzio, P. Monaco, C. Moretti, G. Morgante, S. Nadathur, K. Naidoo, A. Navarro-Alsina, S. Nesseris, D. Paoletti, F. Passalacqua, K. Paterson, L. Patrizii, A. Pisani, D. Potter, S. Quai, M. Radovich, S. Sacquegna, M. Sahlén, D. B. Sanders, E. Sarpa, A. Schneider, D. Sciotti, L. C. Smith, K. Tanidis, C. Tao, G. Testera, R. Teyssier, S. Tosi, A. Troja, M. Tucci, D. Vergani, G. Verza, N. A. Walton</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> With respect to our first paper, we develop a full tomographic analysis based on realistic photometric redshifts which allows us to derive Fisher forecasts in the ($\sigma_8$, $w_0$) plane for a \textit{Euclid}-like data release 1 (DR1) setup. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">This is the second paper in the HOWLS (higher-order weak lensing statistics) series exploring the usage of non-Gaussian statistics for cosmology inference within \textit{Euclid}. With respect to our first paper, we develop a full tomographic analysis based on realistic photometric redshifts which allows us to derive Fisher forecasts in the ($\sigma_8$, $w_0$) plane for a \textit{Euclid}-like data release 1 (DR1) setup. We find that the 5 higher-order statistics (HOSs) that satisfy the Gaussian likelihood assumption of the Fisher formalism (1-point probability distribution function, $\ell$1-norm, peak counts, Minkowski functionals, and Betti numbers) each outperform the shear 2-point correlation functions by a factor $2.5$ on the $w_0$ forecasts, with only marginal improvement when used in combination with 2-point estimators, suggesting that every HOS is able to retrieve both the non-Gaussian and Gaussian information of the matter density field. The similar performance of the different estimators\inlinecomment{, with a slight preference for Minkowski functionals and 1-point probability distribution function,} is explained by a homogeneous use of multi-scale and tomographic information, optimized to lower computational costs. These results hold for the $3$ mass mapping techniques of the \textit{Euclid} pipeline: aperture mass, Kaiser--Squires, and Kaiser--Squires plus, and are unaffected by the application of realistic star masks. Finally, we explore the use of HOSs with the Bernardeau--Nishimichi--Taruya (BNT) nulling scheme approach, finding promising results towards applying physical scale cuts to HOSs.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04953" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04953" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04953" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 14.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.99</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">2. On the sensitivity of different galaxy properties to warm dark matter
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Belén Costanza, Bonny Y. Wang, Francisco Villaescusa-Navarro, Alex M. Garcia, Jonah C. Rose, Mark Vogelsberger, Paul Torrey, Arya Farahi, Xuejian Shen, Ilem Leisher</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We study the impact of warm dark matter (WDM) particle mass on galaxy properties using 1,024 state-of-the-art cosmological hydrodynamical simulations from the DREAMS project. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We study the impact of warm dark matter (WDM) particle mass on galaxy properties using 1,024 state-of-the-art cosmological hydrodynamical simulations from the DREAMS project. We begin by using a Multilayer Perceptron (MLP) coupled with a normalizing flow to explore global statistical descriptors of galaxy populations, such as the mean, standard deviation, and histograms of 14 galaxy properties. We find that subhalo gas mass is the most informative feature for constraining the WDM mass, achieving a determination coefficient of R^2 = 0.9. We employ symbolic regression to extract simple, interpretable relations with the WDM particle mass. Finally, we adopt a more localized approach by selecting individual dark matter halos and using a Graph Neural Network (GNN) with a normalizing flow to infer the WDM mass, incorporating subhalo properties as node features and global simulation statistics as graph-level features. The GNN approach yields only a residual improvement over MLP models based solely on global features, indicating that most of the predictive power resides in the global descriptors, with only marginal gains from halo-level information.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.05037" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.05037" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.05037" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.7</span>
                        <span class="badge bg-info text-dark">Author Score: 0.07</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">3. Analysis of Galaxies at the Extremes: Failed Galaxy Progenitors in the MAGNETICUM Simulations
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jonah S. Gannon, Lucas C. Kimmig, Duncan A. Forbes, Jean P. Brodie, Lucas M. Valenzuela, Rhea-Silvia Rhemus, Joel L. Pfeffer, Klaus Dolag</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We build a toy model of passive galaxy evolution within the stellar mass-halo mass relation to trace z = 0 observations of UDGs back to their z = 2 locations. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">There is increasing observational evidence for a failed galaxy formation pathway for some ultradiffuse galaxies (UDGs) at low redshift however they currently lack simulated counterparts. We attempt to identify dark matter halos at high redshift within the MAGNETICUM cosmological simulations that could plausibly be their progenitors. We build a toy model of passive galaxy evolution within the stellar mass-halo mass relation to trace z = 0 observations of UDGs back to their z = 2 locations. We identify a population of 443 galaxies that match these parameter space positions within the simulation. We build two comparison samples within the simulation that follow the stellar mass-halo mass relationship at z = 2, one of which is stellar mass matched (with varying smaller halo masses) and the other is halo mass matched (with varying larger stellar masses) to our sample. We identify that our failed galaxy progenitor candidates have 1) flatter, cored dark matter halos; 2) more extended stellar bodies; 3) a larger fraction of their gas in the outskirts of their halos; 4) lower metallicities and 5) higher star formation rates than the control samples. Findings 1) and 2) are similar to low redshift observations of UDGs. Finding 3) will aid the removal of gas and permanent quenching of star formation which is a requirement of the failed galaxy formation scenario. The low metallicities of finding 4) match those observed in low redshift failed galaxy UDGs. Comparing the high star formation rates of finding 5) to recent JWST observations suggests that a starburst would naturally explain the high globular cluster richness of the UDGs. Many of the properties we find for these failed galaxy progenitors can be explained by an assembly bias of their dark matter halo to later formation times. We conclude by proposing that the fraction of failed galaxy UDGs is expected to increase with environmental density.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04416" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04416" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04416" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.03</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">4. Colibri: A new tool for fast-flying PDF fits
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Mark N. Costantini, Luca Mantani, James M. Moore, Valentina Schutze Sanchez, Maria Ubiali</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The code is built so that users can implement their own PDF model, and use the built-in functionalities of Colibri for a fast computation of observables. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present Colibri, an open-source Python code that provides a general and flexible tool for PDF fits. The code is built so that users can implement their own PDF model, and use the built-in functionalities of Colibri for a fast computation of observables. It grants easy access to experimental data, several error propagation methodologies, including the Hessian method, the Monte Carlo replica method, and an efficient numerical Bayesian sampling algorithm. To demonstrate the capabilities of Colibri, we consider its simplest application: a polynomial PDF parametrisation. We perform closure tests using a full set of DIS data and compare the results of Hessian and Monte Carlo fits with those from a Bayesian fit. We further discuss how the functionalities illustrated in this example can be extended to more complex PDF parametrisations. In particular, the Bayesian framework in Colibri provides a principled approach to model selection and model averaging, making it a valuable tool for benchmarking and combining different PDF parametrisations on solid statistical grounds.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.03391" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.03391" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.03391" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">hep-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">5. Inferring the dark matter distribution of massive galaxy clusters from deep optical observations: insights from the TNG300 simulation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Aditya Manuwal, Vladimir Avila-Reese, Daniel Montenegro-Taborda, Vicente Rodriguez-Gomez, Bernardo Cervantes Sodi</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Extragalactic stars within galaxy clusters contribute to the intracluster light (ICL), which is thought to be a promising tracer of the underlying dark matter (DM) distribution. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Extragalactic stars within galaxy clusters contribute to the intracluster light (ICL), which is thought to be a promising tracer of the underlying dark matter (DM) distribution. In this study, we employ the TNG300 simulation to investigate the prospect of recovering the dark matter distribution of galaxy clusters from deep, wide-field optical images. For this, we generate mock observations of 40 massive clusters ($M_{200}\gtrsim 10^{14.5}\,{\rm M}_\odot$) at $z=0.06$ for the $g&#39;$ band of the Wendelstein Wide-Field Imager (WWFI), and isolate the emission from the brightest cluster galaxy (BCG) and the ICL by masking the satellite galaxies, following observational procedures. By comparing $\Sigma_{\rm BCG+ICL}$ profiles from these images against $\Sigma_{\rm DM}$ profiles for the central subhaloes, we find that $\Sigma_{\rm cen-DM}/\Sigma_{\rm BCG+ICL}$ exhibits a quasi-linear scaling relation in log space with the normalised distance $r/R_{\Delta}$, for both $R_{\Delta}=R_{200}$ and $R_{500}$. The scatter in the scaling is predominantly stochastic, showing a weak dependence on formation time and dynamical state. We recover the DM concentration and mass within $\approx 23$ and $\approx 15$ per cent of their true values (for $R_{200}$), respectively, and with $\approx 3$ per cent larger uncertainties for $R_{500}$. Alternatively, we find that the concentration can be estimated using the BCG+ICL fraction, the central&#39;s DM mass using the BCG+ICL flux, and the total DM mass using the bolometric flux. These results demonstrate the feasibility of deriving dark matter characteristics of galaxy clusters to be observed with facilities like the Vera C. Rubin Observatory in the near future.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.03424" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.03424" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.03424" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">6. Self-Speculative Masked Diffusions
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Andrew Campbell, Valentin De Bortoli, Jiaxin Shi, Arnaud Doucet</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> This is achieved by modifying the final transformer attention mask from non-causal to causal, enabling draft token generation and parallel validation via a novel, model-integrated speculative sampling mechanism. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present self-speculative masked diffusions, a new class of masked diffusion generative models for discrete data that require significantly fewer function evaluations to generate samples. Standard masked diffusion models predict factorized logits over currently masked positions. A number of masked positions are then sampled, however, the factorization approximation means that sampling too many positions in one go leads to poor sample quality. As a result, many simulation steps and therefore neural network function evaluations are required to generate high-quality data. We reduce the computational burden by generating non-factorized predictions over masked positions. This is achieved by modifying the final transformer attention mask from non-causal to causal, enabling draft token generation and parallel validation via a novel, model-integrated speculative sampling mechanism. This results in a non-factorized predictive distribution over masked positions in a single forward pass. We apply our method to GPT2 scale text modelling and protein sequences generation, finding that we can achieve a ~2x reduction in the required number of network forward passes relative to standard masked diffusion models.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.03929" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.03929" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.03929" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">7. Revisiting Gaussian Process Reconstruction for Cosmological Inference: The Generalised GP (Gen GP) Framework
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Ruchika, Purba Mukherjee, Arianna Favale</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> As machine learning becomes central to cosmological discovery, the Gen GP framework provides a principled approach to model-independent inference that properly accounts for methodological uncertainties while maintaining necessary flexibility for reliable parameter estimation. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We investigate uncertainties in the estimation of the Hubble constant ($H_0$) arising from Gaussian Process (GP) reconstruction, demonstrating that the choice of kernel introduces systematic variations comparable to those arising from different cosmological models. To address this limitation, we introduce the Generalized Gaussian Process (Gen GP) framework, in which the Mat\&#39;ern smoothness parameter $\nu$ is treated as a free parameter, allowing for data-driven kernel optimization. Using the cosmic chronometer Hubble data, we find that while standard GP with $\Lambda$CDM mean function exhibits noticeable reconstruction differences between optimized and marginalized approaches, particularly at $z &gt; 1$, Gen GP maintains methodological consistency. In Gen GP, slight increases in $\chi^2$ per degree of freedom relative to standard GP, for both the zero-mean and $\Lambda$CDM prior mean cases, reflect added flexibility rather than performance degradation. Our results emphasize that robust cosmological inference requires treating kernel parameters as free variables and implementing full Bayesian marginalization to avoid artificial precision from fixed hyperparameters. As machine learning becomes central to cosmological discovery, the Gen GP framework provides a principled approach to model-independent inference that properly accounts for methodological uncertainties while maintaining necessary flexibility for reliable parameter estimation.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.03742" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.03742" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.03742" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">8. The Milky Way - Large Magellanic Cloud Interaction with Simulation Based Inference
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Richard A. N. Brooks, Jason L. Sanders, Vedant Chandra, Nicolás Garavito-Camargo, Adam M. Dillamore, Adrian M. Price-Whelan, Yuan-Sen Ting</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We train neural networks to estimate parameter posterior distributions using a set of $128,000$ rigid MW--LMC simulations conditioned upon velocity data from the Dark Energy Spectroscopic Instrument (DESI) and the combined H3+SEGUE+MagE outer halo surveys. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The infall of the Large Magellanic Cloud (LMC) into the Milky Way (MW) has displaced the MW&#39;s centre of mass, manifesting as an observed reflex motion in the velocities of outer halo stars. We use a Simulation Based Inference framework to constrain properties of the MW, LMC and the induced reflex motion using the dynamics of outer MW halo stars. Specifically, we use the mean radial and tangential velocities of outer halo stars calculated in a set of distance and on-sky bins. We train neural networks to estimate parameter posterior distributions using a set of $128,000$ rigid MW--LMC simulations conditioned upon velocity data from the Dark Energy Spectroscopic Instrument (DESI) and the combined H3+SEGUE+MagE outer halo surveys. We constrain the reflex motion velocity and the enclosed MW and LMC masses within $50 \, \rm kpc$ using the DESI or H3+SEGUE+MagE dataset while varying the survey sky coverage and depth. We find the most precise constraints by using the radial and tangential velocity data from the H3+SEGUE+MagE survey and on-sky quadrant sky coverages. We report a reflex motion velocity, the speed at which the MW lurches towards the LMC, of $v_{\rm{travel}} = 26.4^{+5.5}_{-4.4} \, \rm km \, \rm s^{-1}$, while simultaneously finding an enclosed LMC mass of $M_{\rm LMC}(&lt; 50 \, \rm kpc) = 9.2^{+1.9}_{-2.3} \times 10^{10}\, \rm M_{\odot}$ and enclosed MW mass of $M_{\rm MW}(&lt; 50 \, \rm kpc) = 4.4^{+0.7}_{-0.7} \times 10^{11}\, \rm M_{\odot}$. Our results suggest that the LMC&#39;s total mass is at least $\approx 10-15 \%$ of that of the MW. This inference framework is flexible such that it can provide rapid and reliable constraints when applied to any future survey measuring the velocities of outer halo stars.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04735" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04735" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04735" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">9. Optimal estimation of a factorizable density using diffusion models with ReLU neural networks
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jianqing Fan, Yihong Gu, Ximing Li</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> This is, to the best of our knowledge, the first in the literature showing that diffusion models with standard configurations can adapt to the low-dimensional factorizable structures. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">This paper investigates the score-based diffusion models for density estimation when the target density admits a factorizable low-dimensional nonparametric structure. To be specific, we show that when the log density admits a $d^*$-way interaction model with $\beta$-smooth components, the vanilla diffusion model, which uses a fully connected ReLU neural network for score matching, can attain optimal $n^{-\beta/(2\beta+d^*)}$ statistical rate of convergence in total variation distance. This is, to the best of our knowledge, the first in the literature showing that diffusion models with standard configurations can adapt to the low-dimensional factorizable structures. The main challenge is that the low-dimensional factorizable structure no longer holds for most of the diffused timesteps, and it is very challenging to show that these diffused score functions can be well approximated without a significant increase in the number of network parameters. Our key insight is to demonstrate that the diffused score functions can be decomposed into a composition of either super-smooth or low-dimensional components, leading to a new approximation error analysis of ReLU neural networks with respect to the diffused score function. The rate of convergence under the 1-Wasserstein distance is also derived with a slight modification of the method.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.03994" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.03994" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.03994" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">math.ST</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">10. Overview of the latest developments in understanding the initial state and thermalization
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Kirill Boguslavski</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A proper description of the non-equilibrium matter preceding the quark-gluon plasma (QGP) in heavy-ion collisions and its observable consequences remain a major theoretical challenge, while at the same time offering new opportunities for experimental exploration. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">A proper description of the non-equilibrium matter preceding the quark-gluon plasma (QGP) in heavy-ion collisions and its observable consequences remain a major theoretical challenge, while at the same time offering new opportunities for experimental exploration. In these proceedings, I provide an overview of studies presented in talks and posters at Quark Matter 2025 on this topic. We will focus on the latest developments regarding the features and the numerical description of the non-equilibrium pre-QGP matter, as well as the potential to use hard probes as a means to study the hydrodynamization dynamics of the QCD plasma.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04661" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04661" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04661" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">hep-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">11. Theoretical Modelling of Early Massive Black Holes
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Marta Volonteri</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The current view on massive black holes and active galactic nuclei at high redshift is then summarized, highlighting the JWST ``revolution&#39;&#39; and the questions raised by the recent observations. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">These notes review theoretical models of massive black hole formation, growth and observables. They start with a brief summary of basic properties of massive black hole properties. The current view on massive black holes and active galactic nuclei at high redshift is then summarized, highlighting the JWST ``revolution&#39;&#39; and the questions raised by the recent observations. The notes then touch on massive black hole formation and growth mechanisms, emphasizing the processes at play at early cosmic times. Then techniques for modeling the cosmic massive black hole evolution, are reviewed with an emphasis on cosmological simulations, before approaching how observables are derived from models. They conclude with a section reflecting on the main questions on the JWST-discovered population in light of the material presented in the earlier sections.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04599" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04599" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04599" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">12. Understanding the Role of Training Data in Test-Time Scaling
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Adel Javanmard, Baharan Mirzasoleiman, Vahab Mirrokni</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Our analysis provides a theoretical explanation for several intriguing observations: First, at any fixed test error, increasing test-time compute allows us to reduce the number of in-context examples (context length) in training prompts. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Test-time scaling improves the reasoning capabilities of large language models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts (CoTs). This enables models to tackle more complex problem by breaking them down into additional steps, backtracking, and correcting mistakes. Despite its strong performance--demonstrated by OpenAI&#39;s o1 and DeepSeek R1, the conditions in the training data under which long CoTs emerge, and when such long CoTs improve the performance, remain unclear. In this paper, we study the performance of test-time scaling for transformers trained on an in-context weight prediction task for linear regression. Our analysis provides a theoretical explanation for several intriguing observations: First, at any fixed test error, increasing test-time compute allows us to reduce the number of in-context examples (context length) in training prompts. Second, if the skills required to solve a downstream task are not sufficiently present in the training data, increasing test-time compute can harm performance. Finally, we characterize task hardness via the smallest eigenvalue of its feature covariance matrix and show that training on a diverse, relevant, and hard set of tasks results in best performance for test-time scaling. We confirm our findings with experiments on large, nonlinear transformer architectures.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.03605" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.03605" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.03605" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">13. NeoPDF: A fast interpolation library for collinear and transverse momentum-dependent parton distributions
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Tanjona R. Rabemananjara</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> It aims to address the shortcomings of existing interpolation libraries while providing additional features to support generic non-perturbative functions. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present NeoPDF, an interpolation library that supports both collinear and transverse momentum-dependent parton distribution functions. NeoPDF is designed to be fast and reliable, with modern functionalities that target both current and future hadron collider experiments. It aims to address the shortcomings of existing interpolation libraries while providing additional features to support generic non-perturbative functions. Some of the features include a new interpolation based on Chebyshev polynomials, as well as the ability to interpolate along the nucleon number $A$, the reference strong coupling $\alpha_s(M_Z)$, and the parton&#39;s intrinsic transverse momentum $k_T$. NeoPDF implements its own file format using binary serialisation and lossless compression, prioritising speed and efficiency. A no-code migration design is provided for LHAPDF in order to remove the frictions associated with transitioning to NeoPDF. The library is written in Rust with interfaces for various programming languages such as Fortran, C, C++, Python, and Mathematica. We benchmark NeoPDF against LHAPDF and TMDlib for various sets and show that it is both fast and accurate.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.05079" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.05079" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.05079" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">hep-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">14. Computing Wasserstein Barycenters through Gradient Flows
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Eduardo Fernandes Montesuma, Yassir Bendou, Mike Gartrell</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Existing discrete methods suffer from poor scalability, as they require access to the complete set of samples from input measures. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Wasserstein barycenters provide a powerful tool for aggregating probability measures, while leveraging the geometry of their ambient space. Existing discrete methods suffer from poor scalability, as they require access to the complete set of samples from input measures. We address this issue by recasting the original barycenter problem as a gradient flow in the Wasserstein space. Our approach offers two advantages. First, we achieve scalability by sampling mini-batches from the input measures. Second, we incorporate functionals over probability measures, which regularize the barycenter problem through internal, potential, and interaction energies. We present two algorithms for empirical and Gaussian mixture measures, providing convergence guarantees under the Polyak-{\L}ojasiewicz inequality. Experimental validation on toy datasets and domain adaptation benchmarks show that our methods outperform previous discrete and neural net-based methods for computing Wasserstein barycenters.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04602" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04602" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04602" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">15. Fisher-Bingham-like normalizing flows on the sphere
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Thorsten Glüsenkamp</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Furthermore, they can naturally handle conditional density estimation with target distributions that vary by orders of magnitude in scale - a setting that is important in astronomical applications but that existing flows often struggle with. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">A generic D-dimensional Gaussian can be conditioned or projected onto the D-1 unit sphere, thereby leading to the well-known Fisher-Bingham (FB) or Angular Gaussian (AG) distribution families, respectively. These are some of the most fundamental distributions on the sphere, yet cannot straightforwardly be written as a normalizing flow except in two special cases: the von-Mises Fisher in D=3 and the central angular Gaussian in any D. In this paper, we describe how to generalize these special cases to a family of normalizing flows that behave similarly to the full FB or AG family in any D. We call them &#34;zoom-linear-project&#34; (ZLP)-Fisher flows. Unlike a normal Fisher-Bingham distribution, their composition allows to gradually add complexity as needed. Furthermore, they can naturally handle conditional density estimation with target distributions that vary by orders of magnitude in scale - a setting that is important in astronomical applications but that existing flows often struggle with. A particularly useful member of the new family is the Kent analogue that can cheaply upgrade any flow in this situation to yield better performance.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04762" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04762" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04762" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">16. Suzaku observations of outskirts of nearby clusters and groups: I. electron density and gas fraction to the virial radius
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Kyoko Matsushita, Marie Kondo, Kosuke Sato, Toru Sasaki, Nobuhiro Okabe, Kotaro Fukushima</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The observed mass dependence of gas fractions suggests that feedback and related processes play an increasingly important role toward the group scale, shaping the connection between baryons and dark matter halos. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present an analysis of Suzaku observations of 14 nearby galaxy clusters and groups (z &lt; 0.06), extending radial coverage out to the virial radius (approximately r200). The sample spans a wide mass range, from M500 about 2x10^13 to 7x10^14 solar masses, and includes well-studied systems such as Coma, Perseus, and Virgo. We carefully modeled all background components, including the soft X-ray foregrounds (the Local Hot Bubble, Milky Way Halo, and super-virial temperature components), the cosmic X-ray background, and the non-X-ray background, and assessed their effects on the derived properties of the intracluster medium (ICM). We constructed radial profiles of emission measure, electron density, and temperature. Temperatures decrease smoothly with radius, typically dropping to about one-third to half of their peak values near r200. For relaxed clusters, the emission measure profiles outside the core regions are well described by a beta model with beta around 0.6-0.7, while groups show slightly flatter slopes of beta around 0.4-0.65. Beyond r2500, electron density profiles follow a power-law decline with a slope close to 2. At r500 and r200, the electron density and the gas mass fraction show a tight correlation with the system mass, except for three clusters with bright subclusters. In massive clusters, the gas fraction increases with radius and approaches the cosmic baryon fraction near r200. In contrast, lower-mass systems exhibit gas fractions of around 0.1 at r200. The observed mass dependence of gas fractions suggests that feedback and related processes play an increasingly important role toward the group scale, shaping the connection between baryons and dark matter halos.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04523" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04523" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04523" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">17. GuidedSampling: Steering LLMs Towards Diverse Candidate Solutions at Inference-Time
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Divij Handa, Mihir Parmar, Aswin RRV, Md Nayem Uddin, Hamid Palangi, Chitta Baral</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Although it is an effective way of scaling inference time, it often struggles to generate diverse solution candidates, frequently relying on the same underlying approach to solve the problem and thus producing redundant samples. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Repeated Sampling (RS) is a simple inference-time algorithm that has been shown to improve model performance on complex tasks. Although it is an effective way of scaling inference time, it often struggles to generate diverse solution candidates, frequently relying on the same underlying approach to solve the problem and thus producing redundant samples. To address this limitation, we propose a new inference algorithm, GuidedSampling, which decouples the exploration and generation phases during inference, increasing diversity of generated candidate solutions. The exploration phase identifies multiple concepts that can be utilized to solve the problem, while the generation phase applies a specific concept to provide final solution candidates. We first define the theoretical bounds of GuidedSampling and then empirically demonstrate that it improves the performance of base model at pass@50 by on an average ~21.6% across various benchmarks compared to RS. Furthermore, models trained on trajectories of GuidedSampling exhibit substantial performance improvements at pass@5 by on an average ~9.7%, compared to models trained on traditional RS. Additionally, models trained with GuidedSampling increases the average number of concepts per instance (1.67 -&gt; 3.03), yielding a diverse set of candidates than traditional RS.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.03777" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.03777" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.03777" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">18. The analogy theorem in Hoare logic
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Nikitin Nikita</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Practical verification of the analogy theorem on model data obtained using the Monte Carlo method, as well as on MNIST and USPS data, allows us to achieving F1 scores of 0.84 and 0.88 for convolutional neural networks and random forests, respectively.The proposed approach not only allows us to justify the correctness of transfer between domains but also provides tools for comparing the applicability of models to different types of data.The main contribution of the work is a rigorous formalization of analogy at the level of program logic, providing verifiable guarantees of the correctness of knowledge transfer, which opens new opportunities for both theoretical research and the practical use of machine learning models in previously inaccessible areas. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The introduction of machine learning methods has led to significant advances in automation, optimization, and discoveries in various fields of science and technology. However, their widespread application faces a fundamental limitation: the transfer of models between data domains generally lacks a rigorous mathematical justification. The key problem is the lack of formal criteria to guarantee that a model trained on one type of data will retain its properties on another.This paper proposes a solution to this problem by formalizing the concept of analogy between data sets and models using first-order logic and Hoare logic.We formulate and rigorously prove a theorem that sets out the necessary and sufficient conditions for analogy in the task of knowledge transfer between machine learning models. Practical verification of the analogy theorem on model data obtained using the Monte Carlo method, as well as on MNIST and USPS data, allows us to achieving F1 scores of 0.84 and 0.88 for convolutional neural networks and random forests, respectively.The proposed approach not only allows us to justify the correctness of transfer between domains but also provides tools for comparing the applicability of models to different types of data.The main contribution of the work is a rigorous formalization of analogy at the level of program logic, providing verifiable guarantees of the correctness of knowledge transfer, which opens new opportunities for both theoretical research and the practical use of machine learning models in previously inaccessible areas.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.03685" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.03685" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.03685" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">19. Perspectives on Stochastic Localization
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Bobby Shi, Kevin Tian, Matthew S. Zhang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We survey different perspectives on the stochastic localization process of [Eld13], a powerful construction that has had many exciting recent applications in high-dimensional probability and algorithm design. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We survey different perspectives on the stochastic localization process of [Eld13], a powerful construction that has had many exciting recent applications in high-dimensional probability and algorithm design. Unlike prior surveys on this topic, our focus is on giving a self-contained presentation of all known alternative constructions of Eldan&#39;s stochastic localization, with an emphasis on connections between different constructions. Our hope is that by collecting these perspectives, some of which had primarily arisen within a particular community (e.g., probability theory, theoretical computer science, information theory, or machine learning), we can broaden the accessibility of stochastic localization, and ease its future use.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04460" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04460" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04460" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">math.PR</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">20. Is CPL dark energy a mirage?
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Mikel Artola, Ismael Ayuso, Ruth Lazkoz, Vincenzo Salzano</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> To address this, we design two alternative families of two-parameter dark energy parameterizations which remain close to the original CPL but aim to soften its phantom character. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Recent observations from the Dark Energy Spectroscopic Instrument (DESI) raise doubts about the standard cosmological model, $\Lambda$CDM, suggesting a preference for an inherently dynamical dark energy component. The Chevallier-Polarski-Linder (CPL) parameterization -- a widely used two-parameter model for the dark energy equation of state -- displays marked early-time phantom behavior and a recent crossing of the phantom divide. These features suggest the convenience to check observationally the robustness of such evolution. To address this, we design two alternative families of two-parameter dark energy parameterizations which remain close to the original CPL but aim to soften its phantom character. Specifically, these models reproduce CPL-like behavior at low redshift but mitigate early phantom behavior through the use of smooth sigmoid transitions, yielding a more gradual evolution. By combining recent DESI data with constraints from the cosmic microwave background and Type Ia supernovae, we assess the viability of these models. Our analysis shows that CPL remains a strong and competitive parameterization, with the proposed alternatives only marginally favored or disfavored. We conclude that current observational data lack the statistical precision to decisively distinguish between CPL and similarly constructed parameterizations across the redshift range probed by late-time observables.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04191" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04191" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04191" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">21. Adaptive Coverage Policies in Conformal Prediction
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Etienne Gauthier, Francis Bach, Michael I. Jordan</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> In this work, we leverage recent advances in e-values and post-hoc conformal inference, which allow the use of data-dependent coverage levels while maintaining valid statistical guarantees. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Traditional conformal prediction methods construct prediction sets such that the true label falls within the set with a user-specified coverage level. However, poorly chosen coverage levels can result in uninformative predictions, either producing overly conservative sets when the coverage level is too high, or empty sets when it is too low. Moreover, the fixed coverage level cannot adapt to the specific characteristics of each individual example, limiting the flexibility and efficiency of these methods. In this work, we leverage recent advances in e-values and post-hoc conformal inference, which allow the use of data-dependent coverage levels while maintaining valid statistical guarantees. We propose to optimize an adaptive coverage policy by training a neural network using a leave-one-out procedure on the calibration set, allowing the coverage level and the resulting prediction set size to vary with the difficulty of each individual example. We support our approach with theoretical coverage guarantees and demonstrate its practical benefits through a series of experiments.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04318" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04318" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04318" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">22. Searching for New Physics with the Large Hadron Collider
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Michael Spannowsky</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The chapter introduces event selection strategies, object identification, and multivariate classification methods, before presenting the statistical framework underpinning modern collider analyses, from likelihood construction to hypothesis testing and uncertainty treatment. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">This chapter provides an introduction to collider phenomenology, explaining how theoretical concepts are translated into experimental analyses at the Large Hadron Collider (LHC). Beginning with the principles of collider operation and detector design, it outlines how collisions of protons are modelled through parton distribution functions, hard matrix elements, parton showers, and hadronisation. The discussion then turns to the reconstruction of physical objects and the definition of kinematic observables that expose the quantum numbers and dynamics of the underlying interactions. Special emphasis is placed on jet physics, including infrared- and collinear-safe algorithms, grooming and tagging techniques, and modern reconstruction approaches to jet substructure. The chapter introduces event selection strategies, object identification, and multivariate classification methods, before presenting the statistical framework underpinning modern collider analyses, from likelihood construction to hypothesis testing and uncertainty treatment. Three representative case studies, the Higgs discovery in the diphoton channel, high-mass dilepton resonance searches, and constraints on new physics through the Standard Model Effective Field Theory, demonstrate how these ingredients combine in end-to-end analyses. The chapter concludes with a perspective on future colliders and the growing role of open data and simplified likelihoods in enabling reinterpretation and global fits.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.03704" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.03704" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.03704" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">hep-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">23. DRPO: Efficient Reasoning via Decoupled Reward Policy Optimization
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Gang Li, Yan Chen, Ming Lin, Tianbao Yang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> However, these models suffer from overthinking, generating unnecessarily long and redundant reasoning even for simple questions, which substantially increases computational cost and response latency. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Recent large reasoning models (LRMs) driven by reinforcement learning algorithms (e.g., GRPO) have achieved remarkable performance on challenging reasoning tasks. However, these models suffer from overthinking, generating unnecessarily long and redundant reasoning even for simple questions, which substantially increases computational cost and response latency. While existing methods incorporate length rewards to GRPO to promote concise reasoning, they incur significant performance degradation. We identify the root cause: when rewards for correct but long rollouts are penalized, GRPO&#39;s group-relative advantage function can assign them negative advantages, actively discouraging valid reasoning. To overcome this, we propose Decoupled Reward Policy Optimization (DRPO), a novel framework that decouples the length-based learning signal of correct rollouts from incorrect ones. DRPO ensures that reward signals for correct rollouts are normalized solely within the positive group, shielding them from interference by negative samples. The DRPO&#39;s objective is grounded in integrating an optimized positive data distribution, which maximizes length-based rewards under a KL regularization, into a discriminative objective. We derive a closed-form solution for this distribution, enabling efficient computation of the objective and its gradients using only on-policy data and importance weighting. Of independent interest, this formulation is general and can incorporate other preference rewards of positive data beyond length. Experiments on mathematical reasoning tasks demonstrate DRPO&#39;s significant superiority over six efficient reasoning baselines. Notably, with a 1.5B model, our method achieves 77\% length reduction with only 1.1\% performance loss on simple questions like GSM8k dataset, while the follow-up baseline sacrifices 4.3\% for 68\% length reduction.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04474" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04474" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04474" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">24. A quantum information method for early universe with non-trivial sound speed
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Shi-Cheng Liu, Lei-Hua Liu, Bichu Li, Hai-Qing Zhang, Peng-Zhang He</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Under the non-trivial sound speed, we employ the method of open quantum systems combined with Arnoldi iterations to study the Krylov complexity throughout the early universe, including the inflationary, radiation-dominated, and matter-dominated epochs. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Many quantum gravitational frameworks, such as DBI inflation, k-essence, and effective field theories obtained by integrating out heavy modes, can lead to a non-trivial sound speed. Meanwhile, our universe can be described as an open system. Under the non-trivial sound speed, we employ the method of open quantum systems combined with Arnoldi iterations to study the Krylov complexity throughout the early universe, including the inflationary, radiation-dominated, and matter-dominated epochs. A key ingredient in our analysis is the open two-mode squeezed state formalism and the generalized Lanczos algorithm. To numerically compute the Krylov complexity, we are the first time to derive the evolution equations for the parameters $r_k$ and $\phi_k$ within an open two-mode squeezed state. Our results indicate that the Krylov complexity exhibits a similar trend in both the standard case and the case with non-trivial sound speed. To distinguish between these two scenarios, we also investigate the Krylov entropy for completeness. The evolution of the Krylov entropy shows a clear difference between the standard case and the non-trivial sound speed case. Furthermore, based on the behavior of the Lanczos coefficients, we find that the case of non-trivial sound speed behaves as a maximally chaotic system. However, our numerical results suggest that the Krylov complexity does not saturate to a constant value due to the huge expansion of spacetime background. This study offers a new perspective for exploring the early universe through the quantum information.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04011" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04011" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04011" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">25. Matching JWST UV Luminosity Functions with Refined $Λ$CDM Halo Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Saeed Fakhry, Maryam Shiravand, Antonino Del Popolo</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Our results suggest that the JWST overabundance problem stems not from new physics beyond $\Lambda$CDM, but from oversimplified treatments of gravitational collapse, highlighting the critical role of small-scale dissipative dynamics in early structure formation. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The James Webb Space Telescope (JWST) has unveiled a population of unexpectedly massive and luminous galaxies at redshifts $z \gtrsim 7$, posing a significant challenge to the standard $\Lambda$CDM cosmological paradigm. In this work, we address the tension between early JWST observations of luminous high-redshift galaxies and predictions of the standard $\Lambda$CDM model by revisiting the physics of dark matter halo formation. Employing refined halo mass functions derived by Del Popolo \textit{et al.} (DP1 and DP2) that incorporate angular momentum, dynamical friction, and redshift-dependent collapse barriers, we demonstrate a significant enhancement in the abundance of massive halos at $z \gtrsim 7$ compared to the conventional Sheth-Tormen (ST) formalism. Using a semi-empirical framework linking halo mass to UV luminosity, we show that the DP2 model reproduces the observed UV luminosity functions from $z = 7$ to $14$ with moderate star formation efficiencies, whereas the ST model requires implausibly high efficiencies. Our results suggest that the JWST overabundance problem stems not from new physics beyond $\Lambda$CDM, but from oversimplified treatments of gravitational collapse, highlighting the critical role of small-scale dissipative dynamics in early structure formation.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04709" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04709" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04709" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">26. Foundation models for equation discovery in high energy physics
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Manuel Morales-Alvarado</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We apply the LLM-SR methodology both to benchmark problems of equation recovery in lepton angular distributions and to the discovery of functional forms for angular coefficients in electroweak boson production at the Large Hadron Collider, observables of high phenomenological relevance for which no closed-form expressions are known from first principles. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Foundation models, large machine learning models trained on broad, multimodal datasets, have been gaining increasing attention in scientific applications due to their strong performance on diverse downstream tasks. Large Language Models (LLMs), a prominent instance of foundation models, have achieved remarkable success in tasks such as text and image generation. In this work, we investigate their potential for equation discovery in high energy physics, focusing on symbolic regression. We apply the LLM-SR methodology both to benchmark problems of equation recovery in lepton angular distributions and to the discovery of functional forms for angular coefficients in electroweak boson production at the Large Hadron Collider, observables of high phenomenological relevance for which no closed-form expressions are known from first principles. Our results demonstrate that LLM-SR can uncover compact, accurate, and interpretable equations across in-domain and out-of-domain kinematic regions, effectively incorporating embedded scientific knowledge and offering a promising new approach to equation discovery in high energy physics.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.03397" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.03397" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.03397" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">hep-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">27. Signatures of Galactic Expansion in Gaia DR3: Implications for the JWST Early-Galaxy Puzzle
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>G. S. Karapetian, A. P. Mahtessian, L. E. Byzalov, M. A. Hovhannisyan, L. A. Mahtessian</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We find that 21 of 27 sectors exhibit statistically significant outward motions of 3-50 km/s, while one quadrant shows negative velocities, likely related to the configuration of an activity zone and/or the Galactic bar. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Recent observations with the James Webb Space Telescope (JWST) of massive galaxies at ages below 1 Gyr pose a challenge to standard models of galaxy formation, which predict significantly longer assembly timescales. One possible explanation is that active galactic nuclei (AGN) drive large scale outflows that accelerate galaxy growth. To test this scenario in the local Universe, we analyzed Gaia DR3 data for stars within 5 kpc of the Galactic center, computing galactocentric radial velocities (v_radial_gc) in 27 spatial sectors covering the entire Galaxy, with radial binning of 0.25 kpc. Coordinate transformations and velocity calculations were performed using the Astropy library. We find that 21 of 27 sectors exhibit statistically significant outward motions of 3-50 km/s, while one quadrant shows negative velocities, likely related to the configuration of an activity zone and/or the Galactic bar. Both disk and halo populations also display small but significant mean expansion of 3-9 km/s (p&lt;0.01). These results are consistent with our previous studies, where globular clusters showed outward velocities of 17-31 km/s up to 12 kpc, and axisymmetric analyses of Gaia DR3 stars indicated expansion of ~19 km/s to 5 kpc. Taken together, the evidence suggests that the Milky Way exhibits measurable central expansion, potentially reflecting AGN-driven feedback. This interpretation departs from standard theory and should be regarded as preliminary, requiring further study. However, if confirmed, such expansion could provide a natural explanation for the rapid appearance of massive galaxies observed by JWST.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.03688" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.03688" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.03688" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">28. Think Then Embed: Generative Context Improves Multimodal Embedding
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Xuanming Cui, Jianpeng Cheng, Hong-you Chen, Satya Narayan Shukla, Abhijeet Awasthi, Xichen Pan, Chaitanya Ahuja, Shlok Kumar Mishra, Qi Guo, Ser-Nam Lim, et al.</span>
                                <span class="author-full" style="display: none;">Xuanming Cui, Jianpeng Cheng, Hong-you Chen, Satya Narayan Shukla, Abhijeet Awasthi, Xichen Pan, Chaitanya Ahuja, Shlok Kumar Mishra, Qi Guo, Ser-Nam Lim, Aashu Singh, Xiangjun Fan</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> First, by leveraging a powerful MLLM reasoner, we achieve state-of-the-art performance on the MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house datasets. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">There is a growing interest in Universal Multimodal Embeddings (UME), where models are required to generate task-specific representations. While recent studies show that Multimodal Large Language Models (MLLMs) perform well on such tasks, they treat MLLMs solely as encoders, overlooking their generative capacity. However, such an encoding paradigm becomes less effective as instructions become more complex and require compositional reasoning. Inspired by the proven effectiveness of chain-of-thought reasoning, we propose a general Think-Then-Embed (TTE) framework for UME, composed of a reasoner and an embedder. The reasoner MLLM first generates reasoning traces that explain complex queries, followed by an embedder that produces representations conditioned on both the original query and the intermediate reasoning. This explicit reasoning step enables more nuanced understanding of complex multimodal instructions. Our contributions are threefold. First, by leveraging a powerful MLLM reasoner, we achieve state-of-the-art performance on the MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house datasets. Second, to reduce the dependency on large MLLM reasoners, we finetune a smaller MLLM reasoner using high-quality embedding-centric reasoning traces, achieving the best performance among open-source models with a 7% absolute gain over recently proposed models. Third, we investigate strategies for integrating the reasoner and embedder into a unified model for improved efficiency without sacrificing performance.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.05014" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.05014" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.05014" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">29. A reassessment of the role of high $x$ data on the MSHT global PDF fit
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>L. A. Harland-Lang, T. Cridge, M. Reader, R. S. Thorne</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We present updates within the MSHT global PDF fit that focus on the high $x$ region, and on improving our understanding of the interplay of various theoretical contributions and experimental constraints here. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present updates within the MSHT global PDF fit that focus on the high $x$ region, and on improving our understanding of the interplay of various theoretical contributions and experimental constraints here. We revisit the question of target mass and higher twist corrections, considering their impact for the first time at approximate N$^3$LO order in a global PDF analysis. Their inclusion is found to be moderate but not negligible on both the PDFs and preferred value of the strong coupling. Increased stability in these at aN$^3$LO is observed in comparison to lower orders. We also study the impact of an updated treatment of various fixed-target DIS data, the inclusion of Seaquest fixed-target Drell Yan data, and new ZEUS data that extends coverage into the high $x$ region. The Seaquest data have the largest effect of these, in particular on the light quark separation at high $x$, while the impact of the other updates is rather mild.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.03753" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.03753" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.03753" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">hep-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">30. H I Properties of Field Galaxies at $\boldsymbol{z\approx 0.2}$-0.6: Insights into Declining Cosmic Star Formation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">David DePalma, Neeraj Gupta, Hsiao-Wen Chen, Robert A. Simcoe, Sergei Balashev, Erin Boettcher, Sebastiano Cantalupo, Mandy C. Chen, Françoise Combes, Claude-André Faucher-Giguère, et al.</span>
                                <span class="author-full" style="display: none;">David DePalma, Neeraj Gupta, Hsiao-Wen Chen, Robert A. Simcoe, Sergei Balashev, Erin Boettcher, Sebastiano Cantalupo, Mandy C. Chen, Françoise Combes, Claude-André Faucher-Giguère, Sean D. Johnson, Hans-Rainer Klöckner, Jens-Kristian Krogager, Jennifer I-Hsiu Li, Sebastián López, Pasquier Noterdaeme, Patrick Petitjean, Zhijie Qu, Gwen C. Rudie, Joop Schaye, Fakhri Zahedy</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Additionally, the redshift evolution of H I mass in both low- and high-mass field galaxies, inferred from the stacked emission-line signal, aligns well with the expectation from the cosmic star formation history. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We report statistically significant detection of H I 21-cm emission from intermediate-redshift ($z\approx0.2$-0.6) galaxies. By leveraging multi-sightline galaxy survey data from the Cosmic Ultraviolet Baryon Survey (CUBS) and deep radio observations from the MeerKAT Absorption Line Survey (MALS), we have established a sample of $\approx6000$ spectroscopically identified galaxies in 11 distinct fields to constrain the neutral gas content at intermediate redshifts. The galaxies sample a broad range in stellar mass -- $8\lesssim\log{M_\rm{star}/\rm{M}_\odot}\lesssim11$ with a median of $\langle\log{M_\rm{star}/\rm{M}_\odot}\rangle_\rm{med}\approx10$ -- and a wide range in redshift -- $0.24\lesssim z\lesssim0.63$ with a median of $\langle z\rangle_\rm{med}=0.44$. Our detected emission-line signal exceeds $4\,\sigma$ significance in the stacked spectra of all subsamples, and the observed total H I 21-cm line flux translates to a H I mass $M_\rm{H\;I}\approx10^{10}\rm{M}_\odot$. We find a high H I-to-stellar mass ratio of $M_\mathrm{H\;I}/M_\rm{star}\approx6$ for low-mass galaxies with $\langle\log{M_\rm{star}/\rm{M}_\odot}\rangle \approx9.3$ ($&gt;3.7\,\sigma$). For galaxies with $\langle\log{M_\rm{star}/\rm{M}_\odot}\rangle\approx10.6$, we find $M_\mathrm{H\;I}/M_\rm{star}\approx0.3$ ($&gt;4.7\,\sigma$). Additionally, the redshift evolution of H I mass in both low- and high-mass field galaxies, inferred from the stacked emission-line signal, aligns well with the expectation from the cosmic star formation history. This suggests that the overall decline in the cosmic star formation activity across the general galaxy population may be connected to a decreasing supply of neutral hydrogen. Finally, our analysis has revealed significant 21-cm signals at distances greater than 75 kpc from these intermediate-redshift galaxies, indicating a substantial reservoir of H I gas in their extended surroundings.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.03400" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.03400" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.03400" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">31. Scalable Causal Discovery from Recursive Nonlinear Data via Truncated Basis Function Scores and Tests
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Joseph Ramsey, Bryan Andrews</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Extensive simulations and a real-data application to Canadian wildfire risk show that, when integrated into hybrid searches, BF-based methods enable interpretable and scalable causal discovery. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Learning graphical conditional independence structures from nonlinear, continuous or mixed data is a central challenge in machine learning and the sciences, and many existing methods struggle to scale to thousands of samples or hundreds of variables. We introduce two basis-expansion tools for scalable causal discovery. First, the Basis Function BIC (BF-BIC) score uses truncated additive expansions to approximate nonlinear dependencies. BF-BIC is theoretically consistent under additive models and extends to post-nonlinear (PNL) models via an invertible reparameterization. It remains robust under moderate interactions and supports mixed data through a degenerate-Gaussian embedding for discrete variables. In simulations with fully nonlinear neural causal models (NCMs), BF-BIC outperforms kernel- and constraint-based methods (e.g., KCI, RFCI) in both accuracy and runtime. Second, the Basis Function Likelihood Ratio Test (BF-LRT) provides an approximate conditional independence test that is substantially faster than kernel tests while retaining competitive accuracy. Extensive simulations and a real-data application to Canadian wildfire risk show that, when integrated into hybrid searches, BF-based methods enable interpretable and scalable causal discovery. Implementations are available in Python, R, and Java.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04276" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04276" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04276" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">32. Simulation-based inference via telescoping ratio estimation for trawl processes
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Dan Leonte, Raphaël Huser, Almut E. D. Veraart</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Simulation-based inference (SBI) offers a promising way forward, but existing methods typically require large training datasets or complex architectures and frequently yield confidence (credible) regions that fail to attain their nominal values, raising doubts on the reliability of estimates for the very features that motivate the use of these models. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The growing availability of large and complex datasets has increased interest in temporal stochastic processes that can capture stylized facts such as marginal skewness, non-Gaussian tails, long memory, and even non-Markovian dynamics. While such models are often easy to simulate from, parameter estimation remains challenging. Simulation-based inference (SBI) offers a promising way forward, but existing methods typically require large training datasets or complex architectures and frequently yield confidence (credible) regions that fail to attain their nominal values, raising doubts on the reliability of estimates for the very features that motivate the use of these models. To address these challenges, we propose a fast and accurate, sample-efficient SBI framework for amortized posterior inference applicable to intractable stochastic processes. The proposed approach relies on two main steps: first, we learn the posterior density by decomposing it sequentially across parameter dimensions. Then, we use Chebyshev polynomial approximations to efficiently generate independent posterior samples, enabling accurate inference even when Markov chain Monte Carlo methods mix poorly. We further develop novel diagnostic tools for SBI in this context, as well as post-hoc calibration techniques; the latter not only lead to performance improvements of the learned inferential tool, but also to the ability to reuse it directly with new time series of varying lengths, thus amortizing the training cost. We demonstrate the method&#39;s effectiveness on trawl processes, a class of flexible infinitely divisible models that generalize univariate Gaussian processes, applied to energy demand data.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04042" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04042" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04042" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">33. Toward a unified framework for data-efficient evaluation of large language models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Lele Liao, Qile Zhang, Ruofan Wu, Guanhua Fang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Through extensive experiments involving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves stable capability estimates using just $3\%$ of the total evaluation items. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Evaluating large language models (LLMs) on comprehensive benchmarks is a cornerstone of their development, yet it&#39;s often computationally and financially prohibitive. While Item Response Theory (IRT) offers a promising path toward data-efficient evaluation by disentangling model capability from item difficulty, existing IRT-based methods are hampered by significant limitations. They are typically restricted to binary correctness metrics, failing to natively handle the continuous scores used in generative tasks, and they operate on single benchmarks, ignoring valuable structural knowledge like correlations across different metrics or benchmarks. To overcome these challenges, we introduce LEGO-IRT, a unified and flexible framework for data-efficient LLM evaluation. LEGO-IRT&#39;s novel design natively supports both binary and continuous evaluation metrics. Moreover, it introduces a factorized architecture to explicitly model and leverage structural knowledge, decomposing model ability estimates into a general component and structure-specific (e.g., per-metric or per-benchmark) components. Through extensive experiments involving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves stable capability estimates using just $3\%$ of the total evaluation items. We demonstrate that incorporating structural knowledge reduces estimation error by up to $10\%$ and reveal that the latent abilities estimated by our framework may align more closely with human preferences.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04051" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04051" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04051" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">34. SPOGW: a Score-based Preference Optimization method via Group-Wise comparison for workflows
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yitong Cui, Liu Liu, Baosheng Yu, Jiayan Qiu, Xikai Zhang, Likang Xiao, Yixing Liu, Quan Chen</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> In five benchmark datasets covering mathematical reasoning, coding, and question answering, SPOGW matches or exceeds the performance of current state-of-the-art approaches, presenting a viable and forward-looking methodology for automated generation and optimization of agentic workflows. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Large language models (LLMs) have exhibited significant capabilities in addressing challenging problems throughout various fields, often through the use of agentic workflows that adhere to structured instructions and multi-step procedures. However, designing such workflows demands substantial manual effort, posing challenges to scalability and generalizability. Recent studies have aimed to minimize the human intervention needed for their construction, leading to advances in automated techniques for optimizing agentic workflows. However, current approaches are often constrained by their limited representational capacity, insufficient adaptability, weak scalability, and pairwise comparison paradigm -- issues that stem primarily from a dependence on discrete optimization techniques. To overcome these limitations, we introduce a new score-based preference approach, refereed as SPOGW, which operates directly on cardinal reward signals through group-wise comparison and enables more efficient and stable optimization in a continuous space. SPOGW incorporates Iterative offline GRPO (ioGRPO) with advantage-masked KL divergence (mKL), which regulates training update by placing greater emphasis on the advantageous regions of the policy response. In five benchmark datasets covering mathematical reasoning, coding, and question answering, SPOGW matches or exceeds the performance of current state-of-the-art approaches, presenting a viable and forward-looking methodology for automated generation and optimization of agentic workflows.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04089" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04089" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04089" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">35. Rapid event extraction and tensorial event adaption: Libraries for efficient access and generic reweighting of parton-level events and their implementation in the MadtRex module
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Stefan Roiser, Robert Schöfbeck, Zenny Wettersten</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> These libraries serve as the foundation for the MadtRex reweighting module for MadGraph5_aMC@NLO, extending the functionality of the CUDACPP plugin to allow for data-parallel model-generic leading order parameter reweighting on SIMD-enabled CPUs and SIMT GPUs, speeding up reweighting by more than two orders of magnitude compared to MadGraph5_aMC@NLO running on the exact same hardware while providing trivial scalability to larger and distributed systems. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present Rex and teaRex, C++17 libraries for efficient management of parton-level hard scattering event information and completely generic reweighting of such events, respectively. Rex is primarily an interfacing and I/O library for Les Houches Event format files and provides an internal event format designed with data parallelism in mind, and teaRex extends this format to provide full parton-level reweighting functionality with minimal code needing to be written by the end user. These libraries serve as the foundation for the MadtRex reweighting module for MadGraph5_aMC@NLO, extending the functionality of the CUDACPP plugin to allow for data-parallel model-generic leading order parameter reweighting on SIMD-enabled CPUs and SIMT GPUs, speeding up reweighting by more than two orders of magnitude compared to MadGraph5_aMC@NLO running on the exact same hardware while providing trivial scalability to larger and distributed systems.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.05100" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.05100" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.05100" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">hep-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">36. Internal states before wait modulate reasoning patterns
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Dmitrii Troitskii, Koyena Pal, Chris Wendler, Callum Stuart McDougall, Neel Nanda</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> In this work, we address the question whether model&#39;s latents preceding wait tokens contain relevant information for modulating the subsequent reasoning process. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Prior work has shown that a significant driver of performance in reasoning models is their ability to reason and self-correct. A distinctive marker in these reasoning traces is the token wait, which often signals reasoning behavior such as backtracking. Despite being such a complex behavior, little is understood of exactly why models do or do not decide to reason in this particular manner, which limits our understanding of what makes a reasoning model so effective. In this work, we address the question whether model&#39;s latents preceding wait tokens contain relevant information for modulating the subsequent reasoning process. We train crosscoders at multiple layers of DeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent attribution technique in the crosscoder setting. We locate a small set of features relevant for promoting/suppressing wait tokens&#39; probabilities. Finally, through a targeted series of experiments analyzing max activating examples and causal interventions, we show that many of our identified features indeed are relevant for the reasoning process and give rise to different types of reasoning patterns such as restarting from the beginning, recalling prior knowledge, expressing uncertainty, and double-checking.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04128" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04128" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04128" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">37. Embracing Discrete Search: A Reasonable Approach to Causal Structure Learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Marcel Wienöbst, Leonard Henckel, Sebastian Weichwald</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We present FLOP (Fast Learning of Order and Parents), a score-based causal discovery algorithm for linear models. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present FLOP (Fast Learning of Order and Parents), a score-based causal discovery algorithm for linear models. It pairs fast parent selection with iterative Cholesky-based score updates, cutting run-times over prior algorithms. This makes it feasible to fully embrace discrete search, enabling iterated local search with principled order initialization to find graphs with scores at or close to the global optimum. The resulting structures are highly accurate across benchmarks, with near-perfect recovery in standard settings. This performance calls for revisiting discrete search over graphs as a reasonable approach to causal discovery.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04970" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04970" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04970" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">38. Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zishang Jiang, Jinyi Han, Tingyun Li, Xinyi Wang, Sihang Jiang, Jiaqing Liang, Zhaoqian Dai, Shuguang Ma, Fei Yu, Yanghua Xiao</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely adopted technique for enhancing the reasoning ability of Large Language Models (LLMs). (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely adopted technique for enhancing the reasoning ability of Large Language Models (LLMs). However, the effectiveness of RLVR strongly depends on the capability of base models. This issue arises because it requires the model to have sufficient capability to perform high-quality exploration, which involves both effectiveness and diversity. Unfortunately, existing methods address this issue by imitating expert trajectories, which improve effectiveness but neglect diversity. To address this, we argue that the expert only needs to provide guidance only at critical decision points rather than the entire reasoning path. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation for Token-level Optimization of Reasoning, a framework that provides expert guidance only at critical decision points to perform effective and diverse exploration in RLVR. Extensive experiments show that MENTOR enables models capture the essence of expert strategies rather than surface imitation, thereby performing high-quality exploration and achieving superior overall performance. Our code is available online.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04140" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04140" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04140" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">39. Testing black hole metrics with binary black hole inspirals
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zhe Zhao, Swarnim Shashank, Debtroy Das, Cosimo Bambi</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We demonstrate that gravitational wave data provide evidence supporting the Kerr nature of black holes, showing no significant deviations from General Relativity, thereby affirming its validity within the current observational limits. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Gravitational wave astronomy has opened an unprecedented window onto tests of gravity and fundamental physics in the strong-field regime. In this study, we examine a series of well-motivated deviations from the classical Kerr solution of General Relativity and employ gravitational wave data to place constraints on possible deviations from the Kerr geometry. The method involves calculating the phase of gravitational waves using the effective one-body formalism and then applying the parameterized post-Einsteinian framework to constrain the parameters appearing in these scenarios beyond General Relativity. The effective one-body method, known for its capability to model complex gravitational waveforms, is used to compute the wave phase, and the post-Einsteinian framework allows for a flexible, model-independent approach to parameter estimation. We demonstrate that gravitational wave data provide evidence supporting the Kerr nature of black holes, showing no significant deviations from General Relativity, thereby affirming its validity within the current observational limits. This work bridges theoretical waveform modeling with observational constraints, providing a pathway to test the no-hair theorem and probe the astrophysical viability of modified black holes.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04703" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04703" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04703" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">40. MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Guoxin Chen, Zile Qiao, Wenqing Wang, Donglei Yu, Xuanzhong Chen, Hao Sun, Minpeng Liao, Kai Fan, Yong Jiang, Penguin Xie, et al.</span>
                                <span class="author-full" style="display: none;">Guoxin Chen, Zile Qiao, Wenqing Wang, Donglei Yu, Xuanzhong Chen, Hao Sun, Minpeng Liao, Kai Fan, Yong Jiang, Penguin Xie, Wayne Xin Zhao, Ruihua Song, Fei Huang</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Extensive experiments demonstrate MARS achieves substantial improvements of 3.86% on the challenging Humanity&#39;s Last Exam (HLE) benchmark and an average gain of 8.9% across 7 knowledge-intensive tasks, validating the effectiveness of our dual-system paradigm for complex reasoning in dynamic information environments. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in simple tasks, where the models excessively utilize System 2-type, deliberate reasoning, leading to inefficient token generation. Furthermore, these models face challenges in adapting their reasoning capabilities to rapidly changing environments due to the static nature of their pretraining data. To address these issues, advancing Large Language Models (LLMs) for complex reasoning tasks requires innovative approaches that bridge intuitive and deliberate cognitive processes, akin to human cognition&#39;s dual-system dynamic. This paper introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless integration of System 1&#39;s fast, intuitive thinking with System 2&#39;s deliberate reasoning within LLMs. MARS strategically integrates multiple external tools, such as Google Search, Google Scholar, and Python Interpreter, to access up-to-date information and execute complex computations, while creating a specialized division of labor where System 1 efficiently processes and summarizes high-volume external information, providing distilled insights that expand System 2&#39;s reasoning context without overwhelming its capacity. Furthermore, we propose a multi-agent reinforcement learning framework extending Group Relative Policy Optimization to simultaneously optimize both systems with multi-turn tool interactions, bin-packing optimization, and sample balancing strategies that enhance collaborative efficiency. Extensive experiments demonstrate MARS achieves substantial improvements of 3.86% on the challenging Humanity&#39;s Last Exam (HLE) benchmark and an average gain of 8.9% across 7 knowledge-intensive tasks, validating the effectiveness of our dual-system paradigm for complex reasoning in dynamic information environments.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04935" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04935" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04935" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">41. Staircase Streaming for Low-Latency Multi-Agent Inference
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Junlin Wang, Jue Wang, Zhen, Xu, Ben Athiwaratkun, Bhuwan Dhingra, Ce Zhang, James Zou</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> While multi-agent inference can enhance response quality, it can significantly increase the time to first token (TTFT), posing a challenge for latency-sensitive applications and hurting user experience. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Recent advances in large language models (LLMs) opened up new directions for leveraging the collective expertise of multiple LLMs. These methods, such as Mixture-of-Agents, typically employ additional inference steps to generate intermediate outputs, which are then used to produce the final response. While multi-agent inference can enhance response quality, it can significantly increase the time to first token (TTFT), posing a challenge for latency-sensitive applications and hurting user experience. To address this issue, we propose staircase streaming for low-latency multi-agent inference. Instead of waiting for the complete intermediate outputs from previous steps, we begin generating the final response as soon as we receive partial outputs from these steps. Experimental results demonstrate that staircase streaming reduces TTFT by up to 93% while maintaining response quality.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.05059" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.05059" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.05059" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">42. $I-$Love$-$Curvature: Exploring compact stars&#39; quasi-universal relation with curvature scalars
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>M. D. Danarianto, A. Sulaksono</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Using constraints from GW170817 and low-mass X-ray binaries, we demonstrate that $\Lambda$ measurements directly constrain both scalar curvature quantities and the interior properties of canonical-mass neutron stars. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We investigate quasi-universal relations in neutron stars linking standard observables, such as tidal deformability ($\Lambda$) and normalized moment of inertia ($\bar{I}$), with normalized curvature scalars in general relativity. These curvature scalars include the Ricci scalar ($\mathcal{R}$), the Ricci tensor contraction ($\mathcal{J}$), the Weyl scalar ($\mathcal{W}$), and the Kretschmann scalar ($\mathcal{K}$). We systematically examine both piecewise polytropic and color-flavor-locked equations of state, finding: (1) significant correlations between both local (central and surface) and global (volume-averaged) curvature scalars with $\bar{I}$ and $\Lambda$; (2) especially strong correlations between surface and volume-averaged curvature scalars and both $\bar{I}$ and $\Lambda$; (3) a near equation-of-state-independent maximum for the normalized Ricci scalar, suggesting a link to the trace anomaly; and (4) new universal relations involving normalized central and volume-averaged pressure and energy density, which also correlate strongly with $\bar{I}$ and $\Lambda$. Using constraints from GW170817 and low-mass X-ray binaries, we demonstrate that $\Lambda$ measurements directly constrain both scalar curvature quantities and the interior properties of canonical-mass neutron stars. These findings agree with the literature on equation-of-state-dependent Bayesian inference estimates. Our identified relations thus provide an equation-of-state-insensitive connection between stellar observables, spacetime geometry, and the microphysics of compact stars.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04260" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04260" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04260" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">43. AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Hanchen Zhang, Xiao Liu, Bowen Lv, Xueqiao Sun, Bohao Jing, Iat Long Iong, Zhenyu Hou, Zehan Qi, Hanyu Lai, Yifan Xu, et al.</span>
                                <span class="author-full" style="display: none;">Hanchen Zhang, Xiao Liu, Bowen Lv, Xueqiao Sun, Bohao Jing, Iat Long Iong, Zhenyu Hou, Zehan Qi, Hanyu Lai, Yifan Xu, Rui Lu, Hongning Wang, Jie Tang, Yuxiao Dong</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> On the algorithm side, we propose cross-policy sampling to encourage model exploration in multi-turn settings and task advantage normalization to stabilize multi-task training. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Recent advances in large language models (LLMs) have sparked growing interest in building generalist agents that can learn through online interactions. However, applying reinforcement learning (RL) to train LLM agents in multi-turn, multi-task settings remains challenging due to lack of scalable infrastructure and stable training algorithms. In this work, we present the AgentRL framework for scalable multi-turn, multi-task agentic RL training. On the infrastructure side, AgentRL features a fully-asynchronous generation-training pipeline for efficient multi-turn RL. To support heterogeneous environment development in multi-task RL, we design a unified function-call based API interface, containerized environment development, and a centralized controller. On the algorithm side, we propose cross-policy sampling to encourage model exploration in multi-turn settings and task advantage normalization to stabilize multi-task training. Experiments show that AgentRL, trained on open LLMs across five agentic tasks, significantly outperforms GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents. Multi-task training with AgentRL matches the best results among all task-specific models. AgentRL is open-sourced at https://github.com/THUDM/AgentRL. The algorithm and framework are adopted in building \textsc{\href{https://autoglm.zhipuai.cn}{AutoGLM}}.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04206" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04206" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04206" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">44. Rare Text Semantics Were Always There in Your Diffusion Transformer
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Seil Kang, Woojung Han, Dayun Ju, Seong Jae Hwang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> In this paper, we propose a simple yet effective intervention that surfaces rare semantics inside MM-DiTs without additional training steps, data, denoising-time optimization, or reliance on external modules (e.g., large language models). (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Starting from flow- and diffusion-based transformers, Multi-modal Diffusion Transformers (MM-DiTs) have reshaped text-to-vision generation, gaining acclaim for exceptional visual fidelity. As these models advance, users continually push the boundary with imaginative or rare prompts, which advanced models still falter in generating, since their concepts are often too scarce to leave a strong imprint during pre-training. In this paper, we propose a simple yet effective intervention that surfaces rare semantics inside MM-DiTs without additional training steps, data, denoising-time optimization, or reliance on external modules (e.g., large language models). In particular, the joint-attention mechanism intrinsic to MM-DiT sequentially updates text embeddings alongside image embeddings throughout transformer blocks. We find that by mathematically expanding representational basins around text token embeddings via variance scale-up before the joint-attention blocks, rare semantics clearly emerge in MM-DiT&#39;s outputs. Furthermore, our results generalize effectively across text-to-vision tasks, including text-to-image, text-to-video, and text-driven image editing. Our work invites generative models to reveal the semantics that users intend, once hidden yet ready to surface.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.03886" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.03886" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.03886" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">45. Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yiding Wang, Zhepei Wei, Xinyu Zhu, Yu Meng</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Enabling large language models (LLMs) to utilize search tools offers a promising path to overcoming fundamental limitations such as knowledge cutoffs and hallucinations. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Enabling large language models (LLMs) to utilize search tools offers a promising path to overcoming fundamental limitations such as knowledge cutoffs and hallucinations. Recent work has explored reinforcement learning (RL) for training search-augmented agents that interleave reasoning and retrieval before answering. These approaches usually rely on outcome-based rewards (e.g., exact match), implicitly assuming that optimizing for final answers will also yield effective intermediate search behaviors. Our analysis challenges this assumption: we uncover multiple systematic deficiencies in search that arise under outcome-only training and ultimately degrade final answer quality, including failure to invoke tools, invalid queries, and redundant searches. To address these shortcomings, we introduce DeSA (Decoupling Search-and-Answering), a simple two-stage training framework that explicitly separates search optimization from answer generation. In Stage 1, agents are trained to improve search effectiveness with retrieval recall-based rewards. In Stage 2, outcome rewards are employed to optimize final answer generation. Across seven QA benchmarks, DeSA-trained agents consistently improve search behaviors, delivering substantially higher search recall and answer accuracy than outcome-only baselines. Notably, DeSA outperforms single-stage training approaches that simultaneously optimize recall and outcome rewards, underscoring the necessity of explicitly decoupling the two objectives.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04695" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04695" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04695" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">46. Speculative Actions: A Lossless Framework for Faster Agentic Systems
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Naimeng Ye, Arnav Ahuja, Georgios Liargkovas, Yunan Lu, Kostis Kaffes, Tianyi Peng</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Moreover, performance can be further improved through stronger guessing models, top-K action prediction, multi-step speculation, and uncertainty-aware optimization, opening a promising path toward deploying low-latency agentic systems in the real world. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Despite growing interest in AI agents across industry and academia, their execution in an environment is often slow, hampering training, evaluation, and deployment. For example, a game of chess between two state-of-the-art agents may take hours. A critical bottleneck is that agent behavior unfolds sequentially: each action requires an API call, and these calls can be time-consuming. Inspired by speculative execution in microprocessors and speculative decoding in LLM inference, we propose speculative actions, a lossless framework for general agentic systems that predicts likely actions using faster models, enabling multiple steps to be executed in parallel. We evaluate this framework across three agentic environments: gaming, e-commerce, web search, and a &#34;lossy&#34; extension for an operating systems environment. In all cases, speculative actions achieve substantial accuracy in next-action prediction (up to 55%), translating into significant reductions in end-to-end latency. Moreover, performance can be further improved through stronger guessing models, top-K action prediction, multi-step speculation, and uncertainty-aware optimization, opening a promising path toward deploying low-latency agentic systems in the real world.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04371" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04371" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04371" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">47. WebRenderBench: Enhancing Web Interface Generation through Layout-Style Consistency and Reinforcement Learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Peichao Lai, Jinhui Zhuang, Kexuan Zhang, Ningchang Xiong, Shengjie Wang, Yanwei Xu, Chong Chen, Yilei Wang, Bin Cui</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Unlike vision-based methods that rely on costly LLM reasoning or structure-based comparisons vulnerable to noise and asymmetry, our approach enables more efficient, objective, and reliable UI quality assessment. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Automating the conversion of UI images into web code is a critical task for front-end development and rapid prototyping. Advances in multimodal large language models (MLLMs) have made WebUI-to-Code increasingly feasible, yet existing benchmarks remain limited in data diversity and evaluation reliability. To address these issues, we present WebRenderBench, a large-scale benchmark of 22.5k webpages collected from real-world portal sites, offering greater diversity, complexity, and realism than prior benchmarks. We further propose a novel evaluation metric that measures layout and style consistency from the final rendered pages. Unlike vision-based methods that rely on costly LLM reasoning or structure-based comparisons vulnerable to noise and asymmetry, our approach enables more efficient, objective, and reliable UI quality assessment. Finally, we introduce the Automated Layout and Style Inspection Agent (ALISA), which integrates this metric into reinforcement learning as a reward signal to enhance training on crawled asymmetric webpages. Experiments show that ALISA significantly boosts generation performance, achieving state-of-the-art results across multiple metrics.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04097" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04097" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04097" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.02</span>
                        <span class="badge bg-primary">Semantic Score: 0.91</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">48. Biased parameter inference of eccentric, spin-precessing binary black holes
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Divyajyoti, Isobel M. Romero-Shaw, Vaishak Prasad, Kaushik Paul, Chandra Kant Mishra, Prayush Kumar, Akash Maurya, Michael Boyle, Lawrence E. Kidder, Harald P. Pfeiffer, et al.</span>
                                <span class="author-full" style="display: none;">Divyajyoti, Isobel M. Romero-Shaw, Vaishak Prasad, Kaushik Paul, Chandra Kant Mishra, Prayush Kumar, Akash Maurya, Michael Boyle, Lawrence E. Kidder, Harald P. Pfeiffer, Mark A. Scheel</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We find significant biases in the source parameters, such as chirp mass and spin-precession ($\chi_p$), when signals from highly-eccentric BBHs are recovered with a quasi-circular waveform model. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">While the majority of gravitational wave (GW) events observed by the LIGO and Virgo detectors are consistent with mergers of binary black holes (BBHs) on quasi-circular orbits, some events are also consistent with non-zero orbital eccentricity, indicating that the binaries could have formed via dynamical interactions. Moreover, there may be GW events which show support for spin-precession, eccentricity, or both. In this work, we study the interplay of spins and eccentricity on the parameter estimation of GW signals from BBH mergers. We inject eccentric signals with no spins, aligned spins, and precessing spins using hybrids, TEOBResumS-DALI, and new Numerical Relativity (NR) simulations, respectively, and study the biases in the posteriors of source parameters when these signals are recovered with a quasi-circular precessing-spin waveform model, as opposed to an aligned-spin eccentric waveform model. We find significant biases in the source parameters, such as chirp mass and spin-precession ($\chi_p$), when signals from highly-eccentric BBHs are recovered with a quasi-circular waveform model. Moreover, we find that for signals with both eccentricity and spin-precession effects, Bayes factor calculations confirm that an eccentric, aligned-spin model is preferred over a quasi-circular precessing-spin model. Our study highlights the complex nature of GW signals from eccentric, precessing-spin binaries and the need for readily usable inspiral-merger-ringdown eccentric, spin-precessing waveform models for unbiased parameter estimation.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04332" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04332" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04332" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">49. Crosschecking Cosmic Distances from DESI BAO and DES SNe Points to Systematics
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Mauricio Lopez-Hernandez, Eoin Ó Colgáin, Saeed Pourojaghi, M. M. Sheikh-Jabbari</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Studying ratios of $D_H/r_d$ is a stronger test than the distance duality relation, and the rejection of a horizontal confirms systematics in either DESI BAO or DES SNe. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We perform a consistency check of DESI DR2 BAO constraints ($D_M/r_d, D_H/r_d)$ by reconstructing the same quantities from DES supernovae (SNe) in bins with the same effective redshift $z_{\textrm{eff}}$. We find that the ratio of $D_M/r_d$ values are consistent with a horizontal, thus confirming that the distance duality relation holds up to calibration. However, the $D_H/r_d$ ratio shows a decreasing trend with $z_{\textrm{eff}}$ at $2.3 \sigma$ to $2.5 \sigma$ that cannot be explained by physics. We demonstrate that the result does not depend on the choice of cosmological model, but the radius of the sound horizon $r_d$ has a much greater influence. Studying ratios of $D_H/r_d$ is a stronger test than the distance duality relation, and the rejection of a horizontal confirms systematics in either DESI BAO or DES SNe. Claims of new physics based on combined data still have rudimentary hurdles to clear.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04179" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04179" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04179" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">50. Inferring the spins of merging black holes in the presence of data-quality issues
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Rhiannon Udall, Sophie Bini, Katerina Chatziioannou, Derek Davis, Sophie Hourihane, Yannick Lecoeuche, Jess McIver, Simona Miller</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Gravitational waves from black hole binary mergers carry information about the component spins, but inference is sensitive to analysis assumptions, which may be broken by terrestrial noise transients known as glitches. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Gravitational waves from black hole binary mergers carry information about the component spins, but inference is sensitive to analysis assumptions, which may be broken by terrestrial noise transients known as glitches. Using a variety of simulated glitches and gravitational wave signals, we study the conditions under which glitches can bias spin measurements. We confirm the theoretical expectation that inference and subtraction of glitches invariably leaves behind residual power due to statistical uncertainty, no matter the strength (signal-to-noise ratio; SNR) of the original glitch. Next we show that low-SNR glitches - including those below the threshold for flagging data-quality issues - can still significantly bias spin inference. Such biases occur for a range of glitch morphologies, even in cases where glitches and signals are not precisely aligned in phase. Furthermore, we find that residuals of glitch subtraction can result in biases as well. Our results suggest that joint inference of the glitch and gravitational wave parameters, with appropriate models and priors, is required to address these uncertainties inherent in glitch mitigation via subtraction.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.05029" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.05029" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.05029" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
    </div>

    <footer class="footer">
        &copy; 2025 arXiv Daily · Powered by <a href="https://arxiv.org/" target="_blank">arXiv</a>
    </footer>
</body>
</html>