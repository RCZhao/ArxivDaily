<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>arXiv Daily: Backfill (2026-01-11 to 2026-01-18)</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto:400,700&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Roboto', Arial, sans-serif; background: #f8f9fa; transition: background 0.3s; }
        .arxiv-card { margin: 2em auto; max-width: 900px; box-shadow: 0 2px 8px rgba(0,0,0,0.08); border-radius: 16px; transition: box-shadow 0.3s, transform 0.3s; }
        .arxiv-card:hover { box-shadow: 0 4px 12px rgba(0,0,0,0.1); transform: translateY(-1px); }
        .arxiv-title { background: linear-gradient(90deg, #0d6efd 60%, #6610f2 100%); color: #fff; border-radius: 16px 16px 0 0; padding: 1.5em; font-size: 1.5em; font-weight: 700; letter-spacing: 0.5px;}
        .arxiv-meta { font-size: 1em; color: #555; margin-bottom: 1em; }
        .arxiv-abstract { background: #fff; padding: 1.5em; border-radius: 0 0 16px 16px; font-size: 1.1em; line-height: 1.7;}
        .arxiv-links a { margin-right: 1em; }
        .arxiv-scores { margin-top: 1em; font-size: 1em; }
        .navbar { background: #fff; box-shadow: 0 2px 8px rgba(0,0,0,0.04);}
        .footer { text-align: center; color: #888; padding: 2em 0 1em 0; font-size: 0.95em;}
        @media (prefers-color-scheme: dark) {
            body { background: #181a1b; color: #e4e6eb; }
            .arxiv-card { background: #23272b; box-shadow: 0 2px 8px rgba(0,0,0,0.28);}
            .arxiv-card:hover { box-shadow: 0 5px 15px rgba(0,0,0,0.25); transform: translateY(-1px); }
            .arxiv-title { background: linear-gradient(90deg, #375a7f 60%, #6f42c1 100%);}
            .arxiv-abstract { background: #23272b; color: #e4e6eb;}
            .navbar { background: #23272b; color: #e4e6eb;}
            .text-muted { color: #adb5bd !important; }
        }
        /* Improved abstract readability */
        .arxiv-abstract-text {
            font-size: 1.1em;
            line-height: 1.7;
        }
        p.big {
            line-height: 1.6;
            font-size: large;
        }
    </style>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'] ],
          processEscapes: true
        }
      });
    </script>
    <script>
    function toggleAuthors(element) {
        const fullList = element.querySelector('.author-full');
        const shortList = element.querySelector('.author-short');
        const toggleText = element.querySelector('.author-toggle');
        
        fullList.style.display = fullList.style.display === 'none' ? 'inline' : 'none';
        shortList.style.display = shortList.style.display === 'none' ? 'inline' : 'none';
        toggleText.textContent = fullList.style.display === 'none' ? ' (show all)' : ' (show less)';
    }
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>
<body>
    <nav class="navbar navbar-expand-lg mb-4">
        <div class="container">
            <a class="navbar-brand fw-bold text-primary" href="#">arXiv Daily</a>
            <span class="navbar-text">Modern arXiv Reader</span>
        </div>
    </nav>

    
    <div class="container py-4">
        <header class="mb-4"><h2 class="display-6 text-center">Analysis of Your Favorites</h2></header>
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Word Cloud</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/word_cloud.png" class="img-fluid rounded" alt="Word Cloud of favorite paper topics">
                    </div>
                </div>
            </div>
        </div>
        
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Clusters</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/backfill_cluster_map.png" class="img-fluid rounded" alt="2D Cluster plot of favorite papers">
                    </div>
                </div>
            </div>
        </div>
        
    </div>
    

    <div class="container py-4">
        <header class="mb-4"><h1 class="display-5 text-center text-primary">arXiv: Backfill (2026-01-11 to 2026-01-18)</h1></header>

        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Score Distribution of All Papers Found Today</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/score_distribution.png" class="img-fluid rounded" alt="Score distribution of all papers found today">
                    </div>
                </div>
            </div>
        </div>
        

        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">1. Euclid preparation. Calibrated intrinsic galaxy alignments in the Euclid Flagship simulation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Euclid Collaboration, K. Hoffmann, R. Paviot, B. Joachimi, N. Tessore, P. Tallada-Crespí, N. E. Chisari, E. J. Gonzalez, A. Loureiro, P. Fosalba, et al.</span>
                                <span class="author-full" style="display: none;">Euclid Collaboration, K. Hoffmann, R. Paviot, B. Joachimi, N. Tessore, P. Tallada-Crespí, N. E. Chisari, E. J. Gonzalez, A. Loureiro, P. Fosalba, J. Blazek, C. Laigle, Y. Dubois, C. Pichon, B. Altieri, S. Andreon, N. Auricchio, C. Baccigalupi, M. Baldi, S. Bardelli, F. Bernardeau, A. Biviano, E. Branchini, M. Brescia, S. Camera, G. Cañas-Herrera, V. Capobianco, C. Carbone, V. F. Cardone, J. Carretero, S. Casas, F. J. Castander, M. Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C. Colodro-Conde, G. Congedo, L. Conversi, Y. Copin, F. Courbin, H. M. Courtois, A. Da Silva, H. Degaudenzi, G. De Lucia, H. Dole, F. Dubath, C. A. J. Duncan, X. Dupac, S. Dusini, S. Escoffier, M. Farina, R. Farinelli, S. Farrens, S. Ferriol, F. Finelli, N. Fourmanoit, M. Frailis, E. Franceschi, M. Fumana, S. Galeotta, K. George, B. Gillis, C. Giocoli, J. Gracia-Carpio, A. Grazian, F. Grupp, S. V. H. Haugan, H. Hoekstra, W. Holmes, F. Hormuth, A. Hornstrup, K. Jahnke, M. Jhabvala, E. Keihänen, S. Kermiche, A. Kiessling, M. Kilbinger, B. Kubik, M. Kümmel, M. Kunz, H. Kurki-Suonio, A. M. C. Le Brun, S. Ligori, P. B. Lilje, V. Lindholm, I. Lloro, G. Mainetti, D. Maino, E. Maiorano, O. Mansutti, S. Marcin, O. Marggraf, M. Martinelli, N. Martinet, F. Marulli, R. J. Massey, E. Medinaceli, S. Mei, Y. Mellier, M. Meneghetti, E. Merlin, G. Meylan, A. Mora, M. Moresco, L. Moscardini, C. Neissner, S. -M. Niemi, C. Padilla, S. Paltani, F. Pasian, K. Pedersen, V. Pettorino, S. Pires, G. Polenta, M. Poncet, L. A. Popa, L. Pozzetti, F. Raison, R. Rebolo, A. Renzi, J. Rhodes, G. Riccio, E. Romelli, M. Roncarelli, R. Saglia, Z. Sakr, A. G. Sánchez, D. Sapone, B. Sartoris, P. Schneider, T. Schrabback, A. Secroun, E. Sefusatti, G. Seidel, S. Serrano, P. Simon, C. Sirignano, G. Sirri, A. Spurio Mancini, L. Stanco, J. Steinwagner, A. N. Taylor, I. Tereno, S. Toft, R. Toledo-Moreo, F. Torradeflot, I. Tutusaus, L. Valenziano, J. Valiviita, T. Vassallo, A. Veropalumbo, Y. Wang, J. Weller, G. Zamorani, F. M. Zerbi, E. Zucca, M. Ballardini, E. Bozzo, C. Burigana, R. Cabanac, M. Calabrese, A. Cappi, D. Di Ferdinando, J. A. Escartin Vigo, L. Gabarra, W. G. Hartley, S. Matthew, M. Maturi, N. Mauri, R. B. Metcalf, A. Pezzotta, M. Pöntinen, C. Porciani, I. Risso, V. Scottez, M. Sereno, M. Tenti, M. Viel, M. Wiesmann, Y. Akrami, S. Alvi, I. T. Andika, S. Anselmi, M. Archidiacono, F. Atrio-Barandela, D. Bertacca, M. Bethermin, A. Blanchard, L. Blot, M. Bonici, S. Borgani, M. L. Brown, S. Bruton, A. Calabro, B. Camacho Quevedo, F. Caro, C. S. Carvalho, T. Castro, F. Cogato, S. Conseil, A. R. Cooray, O. Cucciati, S. Davini, G. Desprez, A. Díaz-Sánchez, J. J. Diaz, S. Di Domizio, J. M. Diego, M. Y. Elkhashab, A. Enia, Y. Fang, A. G. Ferrari, A. Finoguenov, A. Fontana, A. Franco, K. Ganga, J. García-Bellido, T. Gasparetto, V. Gautard, R. Gavazzi, E. Gaztanaga, F. Giacomini, F. Gianotti, G. Gozaliasl, M. Guidi, C. M. Gutierrez, A. Hall, S. Hemmati, H. Hildebrandt, J. Hjorth, S. Joudaki, J. J. E. Kajava, Y. Kang, V. Kansal, D. Karagiannis, K. Kiiveri, J. Kim, C. C. Kirkpatrick, S. Kruk, J. Le Graet, L. Legrand, M. Lembo, F. Lepori, G. Leroy, G. F. Lesci, J. Lesgourgues, L. Leuzzi, T. I. Liaudat, J. Macias-Perez, G. Maggio, M. Magliocchetti, F. Mannucci, R. Maoli, C. J. A. P. Martins, L. Maurin, M. Miluzio, P. Monaco, A. Montoro, C. Moretti, G. Morgante, S. Nadathur, K. Naidoo, A. Navarro-Alsina, S. Nesseris, D. Paoletti, F. Passalacqua, K. Paterson, L. Patrizii, A. Pisani, D. Potter, S. Quai, M. Radovich, P. -F. Rocci, G. Rodighiero, S. Sacquegna, M. Sahlén, D. B. Sanders, E. Sarpa, J. Schaye, A. Schneider, M. Schultheis, D. Sciotti, E. Sellentin, L. C. Smith, J. G. Sorce, K. Tanidis, C. Tao, G. Testera, R. Teyssier, S. Tosi, A. Troja, M. Tucci, C. Valieri, A. Venhola, D. Vergani, F. Vernizzi, G. Verza, P. Vielzeuf, N. A. Walton</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Modeling intrinsic galaxy alignments within the Euclid Flagship simulation, researchers predict that this contamination source will modify Euclid&#39;s end-of-mission tomographic weak lensing two-point statistics by approximately 10%. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Intrinsic alignments of galaxies are potentially a major contaminant of cosmological analyses of weak gravitational lensing. We construct a semi-analytic model of galaxy ellipticities and alignments in the \Euclid Flagship simulation to predict this contamination in Euclid&#39;s weak lensing observations. Galaxy shapes and orientations are determined by the corresponding properties of the host haloes in the underlying $N$-body simulation, as well as the relative positions of galaxies within their halo. Alignment strengths are moderated via stochastic misalignments, separately for central and satellite galaxies and conditional on the galaxy&#39;s redshift, luminosity, and rest-frame colour. The resulting model is calibrated against galaxy ellipticity statistics from the COSMOS Survey, selected alignment measurements based on Sloan Digital Sky Survey samples, and galaxy orientations extracted from the Horizon-AGN hydrodynamic simulation at redshift $z=1$. The best-fit model has a total of 12 alignment parameters and generally reproduces the calibration data sets well within the $1σ$ statistical uncertainties of the observations and the \flagship simulation, with notable exceptions for the most luminous sub-samples on small physical scales. The statistical power of the calibration data and the volume of the single \flagship realisation are still too small to provide informative prior ranges for intrinsic alignment amplitudes in relevant galaxy samples. As a first application, we predict that \Euclid end-of-mission tomographic weak gravitational lensing two-point statistics are modified by up to order $10\,\%$ due to intrinsic alignments.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.07785" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.07785" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.07785" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 14.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.99</span>
                        <span class="badge bg-primary">Semantic Score: 0.96</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Survey Analysis, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">2. Euclid preparation. Testing analytic models of galaxy intrinsic alignments in the Euclid Flagship simulation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Euclid Collaboration, R. Paviot, B. Joachimi, K. Hoffmann, S. Codis, I. Tutusaus, D. Navarro-Gironés, J. Blazek, F. Hervas-Peters, B. Altieri, et al.</span>
                                <span class="author-full" style="display: none;">Euclid Collaboration, R. Paviot, B. Joachimi, K. Hoffmann, S. Codis, I. Tutusaus, D. Navarro-Gironés, J. Blazek, F. Hervas-Peters, B. Altieri, S. Andreon, N. Auricchio, C. Baccigalupi, M. Baldi, S. Bardelli, A. Biviano, E. Branchini, M. Brescia, S. Camera, G. Cañas-Herrera, V. Capobianco, C. Carbone, V. F. Cardone, J. Carretero, S. Casas, F. J. Castander, M. Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C. Colodro-Conde, G. Congedo, L. Conversi, Y. Copin, F. Courbin, H. M. Courtois, A. Da Silva, H. Degaudenzi, S. de la Torre, G. De Lucia, H. Dole, F. Dubath, C. A. J. Duncan, X. Dupac, S. Dusini, S. Escoffier, M. Farina, R. Farinelli, S. Farrens, S. Ferriol, F. Finelli, P. Fosalba, M. Frailis, E. Franceschi, S. Galeotta, K. George, B. Gillis, C. Giocoli, J. Gracia-Carpio, A. Grazian, F. Grupp, S. V. H. Haugan, H. Hoekstra, W. Holmes, F. Hormuth, A. Hornstrup, K. Jahnke, M. Jhabvala, E. Keihänen, S. Kermiche, A. Kiessling, M. Kilbinger, B. Kubik, M. Kümmel, M. Kunz, H. Kurki-Suonio, A. M. C. Le Brun, S. Ligori, P. B. Lilje, V. Lindholm, I. Lloro, G. Mainetti, D. Maino, E. Maiorano, O. Mansutti, S. Marcin, O. Marggraf, M. Martinelli, N. Martinet, F. Marulli, R. J. Massey, S. Maurogordato, E. Medinaceli, S. Mei, Y. Mellier, M. Meneghetti, E. Merlin, G. Meylan, A. Mora, M. Moresco, L. Moscardini, C. Neissner, S. -M. Niemi, C. Padilla, S. Paltani, F. Pasian, K. Pedersen, V. Pettorino, S. Pires, G. Polenta, M. Poncet, L. A. Popa, L. Pozzetti, F. Raison, R. Rebolo, A. Renzi, J. Rhodes, G. Riccio, E. Romelli, M. Roncarelli, R. Saglia, Z. Sakr, A. G. Sánchez, D. Sapone, B. Sartoris, P. Schneider, T. Schrabback, A. Secroun, E. Sefusatti, G. Seidel, S. Serrano, P. Simon, C. Sirignano, G. Sirri, A. Spurio Mancini, L. Stanco, J. Steinwagner, P. Tallada-Crespí, A. N. Taylor, I. Tereno, N. Tessore, S. Toft, R. Toledo-Moreo, F. Torradeflot, L. Valenziano, J. Valiviita, T. Vassallo, G. Verdoes Kleijn, A. Veropalumbo, Y. Wang, J. Weller, A. Zacchei, G. Zamorani, F. M. Zerbi, E. Zucca, E. Bozzo, C. Burigana, R. Cabanac, M. Calabrese, J. A. Escartin Vigo, L. Gabarra, W. G. Hartley, S. Matthew, M. Maturi, N. Mauri, R. B. Metcalf, A. Pezzotta, M. Pöntinen, C. Porciani, I. Risso, V. Scottez, M. Sereno, M. Tenti, M. Viel, M. Wiesmann, Y. Akrami, I. T. Andika, S. Anselmi, M. Archidiacono, F. Atrio-Barandela, D. Bertacca, M. Bethermin, A. Blanchard, L. Blot, H. Böhringer, M. Bonici, S. Borgani, M. L. Brown, S. Bruton, A. Calabro, B. Camacho Quevedo, F. Caro, C. S. Carvalho, T. Castro, F. Cogato, S. Conseil, A. R. Cooray, O. Cucciati, S. Davini, F. De Paolis, G. Desprez, A. Díaz-Sánchez, J. J. Diaz, S. Di Domizio, J. M. Diego, P. Dimauro, M. Y. Elkhashab, A. Enia, Y. Fang, A. G. Ferrari, A. Finoguenov, A. Fontana, A. Franco, K. Ganga, J. García-Bellido, T. Gasparetto, V. Gautard, R. Gavazzi, E. Gaztanaga, F. Giacomini, F. Gianotti, G. Gozaliasl, M. Guidi, C. M. Gutierrez, A. Hall, S. Hemmati, H. Hildebrandt, J. Hjorth, S. Joudaki, J. J. E. Kajava, Y. Kang, V. Kansal, D. Karagiannis, K. Kiiveri, J. Kim, C. C. Kirkpatrick, S. Kruk, J. Le Graet, L. Legrand, M. Lembo, F. Lepori, G. Leroy, G. F. Lesci, J. Lesgourgues, T. I. Liaudat, A. Loureiro, J. Macias-Perez, G. Maggio, M. Magliocchetti, F. Mannucci, R. Maoli, C. J. A. P. Martins, L. Maurin, M. Miluzio, P. Monaco, C. Moretti, G. Morgante, S. Nadathur, K. Naidoo, P. Natoli, A. Navarro-Alsina, S. Nesseris, L. Pagano, D. Paoletti, F. Passalacqua, K. Paterson, L. Patrizii, A. Pisani, D. Potter, S. Quai, M. Radovich, S. Sacquegna, M. Sahlén, D. B. Sanders, E. Sarpa, A. Schneider, M. Schultheis, D. Sciotti, E. Sellentin, L. C. Smith, J. G. Sorce, K. Tanidis, C. Tao, G. Testera, R. Teyssier, S. Tosi, A. Troja, M. Tucci, C. Valieri, A. Venhola, D. Vergani, F. Vernizzi, G. Verza, P. Vielzeuf, N. A. Walton</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Analysis of intrinsic alignments in the Euclid Flagship simulation confirms that NLA and TATT models are accurate down to small scales, but demonstrates that modeling IA evolution above $z=1.1$ necessitates including luminosity dependence beyond the standard redshift power-law. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We model intrinsic alignments (IA) in Euclid&#39;s Flagship simulation to investigate its impact on Euclid&#39;s weak lensing signal. Our IA implementation in the Flagship simulation takes into account photometric properties of galaxies as well as their dark matter host halos. We compare simulations against theory predictions, determining the parameters of two of the most widely used IA models: the Non Linear Alignment (NLA) and the Tidal Alignment and Tidal Torquing (TATT) models. We measure the amplitude of the simulated IA signal as a function of galaxy magnitude and colour in the redshift range $0.1&lt;z&lt;2.1$. We find that both NLA and TATT can accurately describe the IA signal in the simulation down to scales of $6$-$7 \,h^{-1}\,$Mpc. We measure alignment amplitudes for red galaxies comparable to those of the observations, with samples not used in the calibration procedure. For blue galaxies, our constraints are consistent with zero alignments in our first redshift bin $0.1 &lt; z &lt; 0.3$, but we detect a non-negligible signal at higher redshift, which is, however, consistent with the upper limits set by observational constraints. Additionally, several hydrodynamical simulations predict alignment for spiral galaxies, in agreement with our findings. Finally, the evolution of alignment with redshift is realistic and comparable to that determined in the observations. However, we find that the commonly adopted redshift power-law for IA fails to reproduce the simulation alignments above $z=1.1$. A significantly better agreement is obtained when a luminosity dependence is included, capturing the intrinsic luminosity evolution with redshift in magnitude-limited surveys. We conclude that the Flagship IA simulation is a useful tool for translating current IA constraints into predictions for IA contamination of Euclid-like samples.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.07784" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.07784" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.07784" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 14.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.99</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Survey Analysis, Cosmology / Galaxy-halo connection, clustering, lensing</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">3. Euclid preparation. 3D reconstruction of the cosmic web with simulated Euclid Deep spectroscopic samples
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Euclid Collaboration, K. Kraljic, C. Laigle, M. Balogh, P. Jablonka, U. Kuchner, N. Malavasi, F. Sarron, C. Pichon, G. De Lucia, et al.</span>
                                <span class="author-full" style="display: none;">Euclid Collaboration, K. Kraljic, C. Laigle, M. Balogh, P. Jablonka, U. Kuchner, N. Malavasi, F. Sarron, C. Pichon, G. De Lucia, M. Bethermin, F. Durret, M. Fumagalli, C. Gouin, M. Magliocchetti, J. G. Sorce, O. Cucciati, F. Fontanot, M. Hirschmann, Y. Kang, M. Spinelli, N. Aghanim, A. Amara, S. Andreon, N. Auricchio, C. Baccigalupi, M. Baldi, S. Bardelli, A. Biviano, E. Branchini, M. Brescia, J. Brinchmann, S. Camera, G. Cañas-Herrera, V. Capobianco, C. Carbone, J. Carretero, R. Casas, S. Casas, F. J. Castander, M. Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C. Colodro-Conde, G. Congedo, C. J. Conselice, L. Conversi, Y. Copin, F. Courbin, H. M. Courtois, A. Da Silva, H. Degaudenzi, S. de la Torre, H. Dole, M. Douspis, F. Dubath, C. A. J. Duncan, X. Dupac, S. Dusini, S. Escoffier, M. Farina, R. Farinelli, S. Ferriol, F. Finelli, P. Fosalba, N. Fourmanoit, M. Frailis, E. Franceschi, M. Fumana, S. Galeotta, K. George, W. Gillard, B. Gillis, C. Giocoli, J. Gracia-Carpio, A. Grazian, F. Grupp, S. V. H. Haugan, W. Holmes, F. Hormuth, A. Hornstrup, K. Jahnke, M. Jhabvala, B. Joachimi, E. Keihänen, S. Kermiche, A. Kiessling, M. Kilbinger, B. Kubik, M. Kümmel, M. Kunz, H. Kurki-Suonio, A. M. C. Le Brun, S. Ligori, P. B. Lilje, V. Lindholm, I. Lloro, G. Mainetti, D. Maino, E. Maiorano, O. Mansutti, S. Marcin, O. Marggraf, M. Martinelli, N. Martinet, F. Marulli, R. Massey, S. Maurogordato, E. Medinaceli, S. Mei, Y. Mellier, M. Meneghetti, E. Merlin, G. Meylan, A. Mora, M. Moresco, L. Moscardini, R. Nakajima, C. Neissner, S. -M. Niemi, C. Padilla, S. Paltani, F. Pasian, K. Pedersen, W. J. Percival, V. Pettorino, S. Pires, G. Polenta, M. Poncet, L. A. Popa, L. Pozzetti, F. Raison, R. Rebolo, A. Renzi, J. Rhodes, G. Riccio, E. Romelli, M. Roncarelli, C. Rosset, E. Rossetti, R. Saglia, Z. Sakr, A. G. Sánchez, D. Sapone, B. Sartoris, P. Schneider, T. Schrabback, M. Scodeggio, A. Secroun, E. Sefusatti, G. Seidel, M. Seiffert, S. Serrano, P. Simon, C. Sirignano, G. Sirri, L. Stanco, J. Steinwagner, P. Tallada-Crespí, A. N. Taylor, H. I. Teplitz, I. Tereno, N. Tessore, S. Toft, R. Toledo-Moreo, F. Torradeflot, I. Tutusaus, L. Valenziano, J. Valiviita, T. Vassallo, G. Verdoes Kleijn, A. Veropalumbo, D. Vibert, Y. Wang, J. Weller, A. Zacchei, G. Zamorani, E. Zucca, V. Allevato, M. Ballardini, M. Bolzonella, E. Bozzo, C. Burigana, R. Cabanac, M. Calabrese, A. Cappi, D. Di Ferdinando, J. A. Escartin Vigo, L. Gabarra, W. G. Hartley, J. Martín-Fleitas, S. Matthew, N. Mauri, R. B. Metcalf, A. A. Nucita, A. Pezzotta, M. Pöntinen, C. Porciani, I. Risso, V. Scottez, M. Sereno, M. Tenti, M. Viel, M. Wiesmann, Y. Akrami, S. Alvi, I. T. Andika, S. Anselmi, M. Archidiacono, F. Atrio-Barandela, A. Balaguera-Antolinez, P. Bergamini, D. Bertacca, A. Blanchard, L. Blot, H. Böhringer, S. Borgani, M. L. Brown, S. Bruton, A. Calabro, B. Camacho Quevedo, F. Caro, C. S. Carvalho, T. Castro, R. Chary, F. Cogato, S. Conseil, T. Contini, A. R. Cooray, S. Davini, F. De Paolis, G. Desprez, A. Díaz-Sánchez, J. J. Diaz, S. Di Domizio, J. M. Diego, P. Dimauro, P. -A. Duc, A. Enia, Y. Fang, A. G. Ferrari, A. Finoguenov, A. Fontana, A. Franco, K. Ganga, J. García-Bellido, T. Gasparetto, R. Gavazzi, E. Gaztanaga, F. Giacomini, F. Gianotti, G. Gozaliasl, M. Guidi, C. M. Gutierrez, A. Hall, H. Hildebrandt, J. Hjorth, S. Joudaki, J. J. E. Kajava, V. Kansal, D. Karagiannis, K. Kiiveri, C. C. Kirkpatrick, S. Kruk, M. Lattanzi, V. Le Brun, J. Le Graet, L. Legrand, M. Lembo, F. Lepori, G. Leroy, G. F. Lesci, J. Lesgourgues, L. Leuzzi, T. I. Liaudat, S. J. Liu, A. Loureiro, J. Macias-Perez, G. Maggio, E. A. Magnier, F. Mannucci, R. Maoli, C. J. A. P. Martins, L. Maurin, M. Miluzio, P. Monaco, C. Moretti, G. Morgante, S. Nadathur, K. Naidoo, A. Navarro-Alsina, S. Nesseris, L. Pagano, F. Passalacqua, K. Paterson, L. Patrizii, A. Pisani, D. Potter, S. Quai, M. Radovich, P. -F. Rocci, G. Rodighiero, S. Sacquegna, M. Sahlén, D. B. Sanders, A. Schneider, D. Sciotti, E. Sellentin, L. C. Smith, K. Tanidis, C. Tao, G. Testera, R. Teyssier, S. Tosi, A. Troja, M. Tucci, C. Valieri, A. Venhola, D. Vergani, G. Verza, P. Vielzeuf, N. A. Walton</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Assessing the quality of cosmic web reconstruction using simulated Euclid H$\alpha$ spectroscopic data reveals that while stellar mass weighting can correct selection biases, the recovery of large-scale structure statistics is significantly challenged by small-scale redshift-space distortions and redshift uncertainties. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The ongoing Euclid mission aims to measure spectroscopic redshifts for approximately two million galaxies using the H $α$ line emission detected in near-infrared slitless spectroscopic data from the Euclid Deep Fields (EDFs). These measurements will reach a flux limit of $5\times 10^{-17}\,{\rm erg}\,{\rm cm}^{-2}\,{\rm s}^{-1}$ in the redshift range $0.4&lt;z&lt;1.8$, opening the door to numerous investigations involving galaxy evolution, extending well beyond the mission&#39;s core objectives. The achieved H $α$ luminosity depth will lead to a sufficiently high sampling, enabling the reconstruction of the large-scale galaxy environment. We assess the quality of the reconstruction of the galaxy cosmic web environment with the expected spectroscopic dataset in EDFs. The analysis is carried out on the Flagship and GAEA galaxy mock catalogues. The quality of the reconstruction is first evaluated using geometrical and topological statistics measured on the cosmic web, namely the length of filaments, the area of walls, the volume of voids, and its connectivity and multiplicity. We then quantify how accurately gradients in galaxy properties with distance from filaments can be recovered. As expected, the small-scale redshift-space distortions, have a strong impact on filament lengths and connectivity, but can be mitigated by compressing galaxy groups before skeleton extraction. The cosmic web reconstruction is biased when relying solely on H $α$ emitters. This limitation can be mitigated by applying stellar mass weighting during the reconstruction. However, this approach introduces non-trivial biases that need to be accounted for when comparing to theoretical predictions. Redshift uncertainties pose the greatest challenge in recovering the expected dependence of galaxy properties, though the well-established stellar mass transverse gradients towards filaments can still be observed.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.10709" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.10709" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.10709" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 14.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.99</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">4. Baryon Acoustic Oscillations from the C IV Forest with DESI DR2
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Abby Bault, Andrei Cuceu, Julien Guy, J. Aguilar, S. Ahlen, D. Bianchi, A. Brodzeller, D. Brooks, R. Canning, E. Chaussidon, et al.</span>
                                <span class="author-full" style="display: none;">Abby Bault, Andrei Cuceu, Julien Guy, J. Aguilar, S. Ahlen, D. Bianchi, A. Brodzeller, D. Brooks, R. Canning, E. Chaussidon, T. Claybaugh, R. de Belsunce, A. de la Macorra, Arjun Dey, P. Doel, S. Ferraro, A. Font-Ribera, J. E. Forero-Romero, E. Gaztañaga, S. Gontcho A Gontcho, C. Gordon, D. Green, G. Gutierrez, C. Hahn, H. K. Herrera-Alcantar, K. Honscheid, M. Ishak, R. Joyce, S. Juneau, D. Kirkby, A. Kremin, C. Lamman, M. Landriau, L. Le Guillou, M. E. Levi, M. Manera, P. Martini, A. Meisner, R. Miquel, J. Moustakas, A. Muñoz-Gutiérrez, S. Nadathur, N. Palanque-Delabrouille, W. J. Percival, Matthew M. Pieri, C. Poppett, F. Prada, I. Pérez-Ràfols, G. Rossi, E. Sanchez, D. Schlegel, H. Seo, J. Silber, D. Sprayberry, G. Tarlé, B. A. Weaver</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Using DESI DR2 data, the isotropic Baryon Acoustic Oscillation signal was measured with high precision (4.2$\sigma$) via the cross-correlation of C IV absorption with quasars, alongside the first 2.5$\sigma$ detection using C IV cross-correlation with Emission Line Galaxies. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present a measurement of Baryon Acoustic Oscillations (BAO) in the cross-correlation of triply ionized carbon C IV absorption with the positions of quasars (QSO) and Emission Line Galaxies (ELG). We use quasars and ELGs from the second data release (DR2) of the Dark Energy Spectroscopic Instrument (DESI) survey. Our data sample consists of 2.5 million quasars, 3.1 million ELGs, and the C IV absorption is measured along the line of sight of 1.5 million high redshift quasars with $z &gt; 1.3$. We measure the isotropic BAO signal at 4.2$σ$ for the CIV$\times$QSO cross-correlation. This translates into a 3.0% precision measurement of the ratio of the isotropic distance scale, $D_{\rm V}$, and the sound horizon at the drag epoch, $r_{\rm d}$, with $D_{\rm V}/r_{\rm d}(z_{\rm eff} = 1.92) = 30.3 \pm 0.9$. We make the first detection of the BAO feature in the CIV$\times$ELG cross-correlation at a significance of 2.5$σ$ and find $D_{\rm V}/r_{\rm d}(z_{\rm eff} = 1.47) = 24.6 \pm 1.0$.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.08103" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.08103" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.08103" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.20</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Survey Analysis, Cosmology / Galaxy-halo connection, clustering, lensing</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">5. Understanding and Preserving Safety in Fine-Tuned LLMs
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jiawen Zhang, Yangfan Hu, Kejia Chen, Lipeng He, Jiachen Ma, Jian Lou, Dan Li, Jian Liu, Xiaohu Yang, Ruoxi Jia</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Safety-Preserving Fine-tuning (SPF) resolves the safety-utility conflict during LLM adaptation by identifying and removing gradient components that conflict with the low-rank safety subspace, effectively recovering pre-trained safety alignment without sacrificing downstream task performance. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Fine-tuning is an essential and pervasive functionality for applying large language models (LLMs) to downstream tasks. However, it has the potential to substantially degrade safety alignment, e.g., by greatly increasing susceptibility to jailbreak attacks, even when the fine-tuning data is entirely harmless. Despite garnering growing attention in defense efforts during the fine-tuning stage, existing methods struggle with a persistent safety-utility dilemma: emphasizing safety compromises task performance, whereas prioritizing utility typically requires deep fine-tuning that inevitably leads to steep safety declination. In this work, we address this dilemma by shedding new light on the geometric interaction between safety- and utility-oriented gradients in safety-aligned LLMs. Through systematic empirical analysis, we uncover three key insights: (I) safety gradients lie in a low-rank subspace, while utility gradients span a broader high-dimensional space; (II) these subspaces are often negatively correlated, causing directional conflicts during fine-tuning; and (III) the dominant safety direction can be efficiently estimated from a single sample. Building upon these novel insights, we propose safety-preserving fine-tuning (SPF), a lightweight approach that explicitly removes gradient components conflicting with the low-rank safety subspace. Theoretically, we show that SPF guarantees utility convergence while bounding safety drift. Empirically, SPF consistently maintains downstream task performance and recovers nearly all pre-trained safety alignment, even under adversarial fine-tuning scenarios. Furthermore, SPF exhibits robust resistance to both deep fine-tuning and dynamic jailbreak attacks. Together, our findings provide new mechanistic understanding and practical guidance toward always-aligned LLM fine-tuning.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.10141" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.10141" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.10141" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.1</span>
                        <span class="badge bg-info text-dark">Author Score: 0.21</span>
                        <span class="badge bg-primary">Semantic Score: 0.91</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">6. Tidal alignment and tidal torquing modeling for the cosmic shear three-point correlation function and mass aperture skewness
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Rafael C. H. Gomes, Kyle Miller, Sunao Sugiyama, Jonathan Blazek, Thomas Bakx, Bhuvnesh Jain</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Utilizing the TATT formalism, a new model for intrinsic alignment contamination in the shear three-point correlation function (3PCF) shows that higher-order IA effects significantly impact skewed 3PCF configurations, advocating for a joint 2PCF+3PCF analysis to break parameter degeneracies. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present a model for the intrinsic alignment contamination of the shear three-point correlation function and skewness of the mass aperture statistic using the tidal alignment and tidal torquing (TATT) formalism. We compute the intrinsic alignment bispectra components in terms of the TATT model parameters. We consider two effective field theory approaches in the literature, relate them to the TATT model parameters and an extension to TATT that includes the velocity-shear (VS) parameter. We compare the impact of changing between NLA, TATT, and TATT+VS on the theoretical computation of the 3PCF using the best fit parameters and tomographic redshift distributions from Dark Energy Survey Year 3. We find that the TATT model significantly impacts the skewed triangle configurations of the 3PCF. Additionally, including the higher-order effects from TATT can introduce opposite effects on the two-point function and on the mass aperture skewness, damping the signal of the former while boosting the signal of the latter. We argue that a joint 2PCF+3PCF analysis with the TATT model can help break the degeneracy between its model parameters and provide more robust constraints on both cosmology and intrinsic alignment amplitude parameters. We show that typical values of order unity for the intrinsic alignment parameters introduce differences of around $10\%$ between NLA and TATT predictions.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.09133" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.09133" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.09133" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.0</span>
                        <span class="badge bg-info text-dark">Author Score: 0.11</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">7. Sahyadri: A simulation suite for the cosmology dependence of the Cosmic Web
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Saee Dhawalikar, Shadab Alam, Aseem Paranjape, Arka Banerjee</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Sahyadri, a new suite of high-resolution cosmological N-body simulations varying six parameters, achieves a mass resolution significantly better than existing suites, enabling robust identification of low-mass halos crucial for precision studies of low-redshift spectroscopic surveys like DESI BGS. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present Sahyadri, a suite of cosmological $N$-body simulations designed to enable precision studies of the low-redshift Universe with next-generation spectroscopic surveys. Sahyadri includes systematic variations of six cosmological parameters around Planck 2018 constraints, with seed-matched initial conditions enabling cosmological parameter derivatives. Each simulation evolves $2048^3$ particles in a periodic box of side length $200$ $h^{-1}$ Mpc, yielding a particle mass of $m_{\rm{p}} = 8.1 \times 10^{7}\,h^{-1}\,M_{\odot}$ in the fiducial Planck 2018 cosmology. This resolution enables robust identification of dark matter halos down to $M_{\rm min} = 3.2 \times 10^{9}$ $h^{-1}$ $M_\odot$, which represents a factor of $\sim$25 improvement over the AbacusSummit suite, and is over two orders of magnitude better than the Quijote and Aemulus suites. We estimate that approximately 40% of DESI BGS galaxies at redshift $z &lt; 0.15$ - roughly 1.6 million objects - reside in halos accessible to Sahyadri but beyond the reach of existing parameter-varying simulation suites. We demonstrate Sahyadri&#39;s capabilities through measurements of the matter power spectrum, halo mass function and power spectrum, and beyond 2-point statistics such as the Voronoi volume function and $k^{\rm th}$ nearest neighbour statistics, showing excellent agreement with theoretical predictions and significant sensitivity to $Ω_{\rm m}$ variations. We implement a custom compression scheme reducing storage requirements by a factor of $\sim$3 while maintaining sub-percent clustering accuracy. Key data products will be made publicly available.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.07924" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.07924" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.07924" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.04</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">8. New CDM Crisis Revealed by Multi-Scale Cluster Lensing
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Priyamvada Natarajan, Barry T. Chiang, Isaque Dutra</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Combining strong and weak lensing data on massive clusters reveals a tension in dark matter models, showing that while subhalo mass functions are consistent with CDM, their steep inner density profiles and radial distributions strongly suggest core-collapsed Self-Interacting Dark Matter or hybrid dark matter scenarios. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The properties of substructure in galaxy clusters, exquisitely probed by gravitational lensing, offer a stringent test of dark matter models. Combining strong and weak lensing data for massive clusters, we map their total mass--dominated by dark matter--over the dynamic range needed to confront small-scale predictions for collisionless cold dark matter (CDM). Using state-of-the-art lens models, we extract four key subhalo properties: the mass function, projected radial distribution, internal density profile, and tidal truncation radius. We find that the subhalo mass function and truncation radii are consistent with CDM expectations. In contrast, the inner density profiles and radial distribution of subhalos are strongly discrepant with CDM. The incidence of galaxy-galaxy strong lensing (GGSL) from subhalo cores exceeds CDM predictions by nearly an order of magnitude, requiring inner density slopes as steep as $γ\gtrsim 2.5$ within $r \lesssim 0.01\,R_{200}$ consistent with core-collapsed self-interacting dark matter (SIDM), while the same subhalos behave as collisionless in their outskirts. Additionally, the observed radial distribution of subhalos hosting bright cluster member galaxies, explicitly modeled in the lens reconstructions, remains incompatible with CDM. Together, these small-scale stress tests reveal an intriguing paradox and challenge the dark matter microphysics of purely collisionless CDM and motivate hybrid scenarios, such as a dual-component model with both CDM and SIDM, or entirely new classes of dark matter theories.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.07909" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.07909" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.07909" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.02</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Survey Analysis, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">9. Extracting intrinsic alignments in the Dark Energy Survey&#39;s year 1 data, using the self-calibration method and LSST-DESC tools
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Eske M. Pedersen, Leonel Medina-Varela, Emily Phillips Longley, Mustapha Ishak, Joe Zuntz, Chihway Chang, C. Danielle Leonard</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The implementation of Self-Calibration of Intrinsic Alignments (SCIA) in the LSST DESC weak lensing pipeline, tested on DES Y1 data, successfully indicated a non-zero IA signal in high-redshift bins but demonstrated the method&#39;s high sensitivity to accurate individual galaxy photometric redshift estimates. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present the implementation of a Self-Calibration of Intrinsic Alignments of galaxies as an extension to the Vera C. Rubin Observatory&#39;s Legacy Survey of Space and Time (LSST) Dark Energy Science Collaboration (DESC)&#39;s weak lensing 3x2pt pipeline (TXPipe). As a demonstration, we have run this pipeline on the Dark Energy Survey (DES) year one data set. We find indications of a non-zero intrinsic alignment signal in the higher redshift bins, while in the lower bins our results look more uncertain. We believe this is caused by known issues with the individual galaxies photo-z estimation. This effect is particularly harmful for the self-calibration method, since it has high requirements for reliable estimation of the photo-$z$s, and the need for individual galaxy point estimates and tomographic binning to match. We show how different methods of recreating the redshift probability distribution can affect the detection of intrinsic alignment.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.10314" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.10314" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.10314" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.04</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">10. Sliced-Wasserstein Distribution Alignment Loss Improves the Ultra-Low-Bit Quantization of Large Language Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Deyu Cao, Yixin Yin, Samin Aref</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A novel sliced Wasserstein loss function enhances ultra-low-bit post-training quantization for large language models by aligning the output distributions of full-precision and quantized models, significantly improving perplexity and downstream accuracy when integrated with state-of-the-art compression methods. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The benefits of most large language models come with steep and often hidden economic and environmental costs due to their resource usage inefficiency during deployment. Model quantization improves energy and memory efficiency through representing model parameters by lower-precision values. However, compression below 4-bits often distorts activation distributions and degrades performance. We address this challenge by introducing a sliced Wasserstein loss function for distribution-aware calibration in ultra-low-bit post-training quantization. The proposed loss aligns the output distributions of full-precision and quantized models under random linear projections, complementing standard mean-squared error loss without adding any computational overhead during inference. Our proposed loss function can be incorporated with any post-training quantization framework that has a retraining component. We demonstrate the performance gains of our proposed model by incorporating it with two frontier methods known as OmniQuant and TesseraQ. Compared to these two baselines, the proposed loss consistently improves both perplexity and downstream task accuracy across multiple ultra-low-bit settings. Our proposed loss function recovers 4.12-20.37% of the OmniQuant&#39;s lost accuracy on the language model LLaMA-2-7B, 0.93-7.65% on OPT-6.7B, and 2.26-6.20% on LLaMA-2-13B. TesseraQ&#39;s accuracy degradation is recovered by 3.63-7.63% in relative terms when augmented by our proposed loss function. Taken together, these results demonstrate that distributional alignment provides a simple yet effective performance boost that can push the limits of frontier quantization methods. Our method is available on GitHub to facilitate future progress in ultra-low-bit quantization.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.07878" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.07878" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.07878" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">11. Testing subhalo abundance matching with galaxy kinematics
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Fedir Boreiko, Tariq Yasin, Harry Desmond, Richard Stiskalek, Matt J. Jarvis</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Modeling disc galaxy rotation velocities within $\Lambda$CDM reveals a tension between kinematic constraints and independent clustering data, suggesting that the SPARC sample may preferentially occupy the lowest $V_{\max}$ dark matter halos at fixed virial mass. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The rotation velocities of disc galaxies trace dark matter halo structure, providing direct constraints on the galaxy--halo connection. We construct a Bayesian forward model to connect the dark matter halo population predicted by $Λ$CDM with an observed sample of disc galaxies (SPARC) through their maximum rotation velocities. Our approach combines a subhalo abundance matching scheme (accounting for assembly bias) with a parameterised halo response to galaxy formation. When assuming no correlation between selection in the SPARC survey and halo properties, reproducing the observed velocities requires strong halo expansion, low abundance matching scatter ($&lt;0.15$ dex at $1σ$) and a halo proxy that strongly suppresses the stellar masses in satellite haloes. This is in clear tension with independent clustering constraints. Allowing for SPARC-like galaxies to preferentially populate low $\Vmax$ haloes at fixed virial mass greatly improves the goodness-of-fit and resolves these tensions: the preferred halo response shifts to mild contraction, the abundance matching scatter increases to $\sint = 0.19^{+0.13}_{-0.11}$ dex and the proxy becomes consistent with clustering. However, the inferred selection threshold is extreme, implying that SPARC galaxies occupy the lowest ${\sim}16$ per cent of the $\Vmaxhalo$ distribution at fixed $\Mvir$. Moreover, even with selection, the inferred scatter remains in statistical disagreement with the low-mass clustering constraints, which are most representative of the SPARC galaxies in our sample. Our analysis highlights the advantage of augmenting clustering-based constraints on the galaxy--halo connection with kinematics and suggests a possible tension using current data.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.07799" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.07799" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.07799" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Survey Analysis, Cosmology / Galaxy-halo connection, clustering, lensing</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">12. Attention Consistency Regularization for Interpretable Early-Exit Neural Networks
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yanhua Zhao</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Explanation-Guided Training (EGT) enhances the interpretability and consistency of early-exit neural networks by using an attention consistency loss to align intermediate attention maps with the final output, achieving performance parity with speedup. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Early-exit neural networks enable adaptive inference by allowing predictions at intermediate layers, reducing computational cost. However, early exits often lack interpretability and may focus on different features than deeper layers, limiting trust and explainability. This paper presents Explanation-Guided Training (EGT), a multi-objective framework that improves interpretability and consistency in early-exit networks through attention-based regularization. EGT introduces an attention consistency loss that aligns early-exit attention maps with the final exit. The framework jointly optimizes classification accuracy and attention consistency through a weighted combination of losses. Experiments on a real-world image classification dataset demonstrate that EGT achieves up to 98.97% overall accuracy (matching baseline performance) with a 1.97x inference speedup through early exits, while improving attention consistency by up to 18.5% compared to baseline models. The proposed method provides more interpretable and consistent explanations across all exit points, making early-exit networks more suitable for explainable AI applications in resource-constrained environments.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.08891" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.08891" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.08891" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">13. Linear Complexity Self-Supervised Learning for Music Understanding with Random Quantizer
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Petros Vavaroutsos, Theodoros Palamas, Pantelis Vikatos</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A combination of the Branchformer architecture, SummaryMixing, and random quantization successfully reduces the size of foundation models by up to 12.3% for music information retrieval tasks while achieving state-of-the-art competitive performance. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">In recent years, foundation models have become very popular due to their exceptional performance, mainly in natural language (NLP) tasks where they were first introduced. These models usually consist of hundreds of millions, or even billions, of parameters, making them resource-intensive during training and in production systems, leading to increased costs. This paper focuses on the reduction of a foundation&#39;s model size when applied to music information retrieval (MIR) tasks. Our research combines the Branchformer architecture with SummaryMixing, which were first applied in speech recognition, along with a random quantization process. To facilitate reproducibility, we conduct pre-training on publicly available datasets, complemented by a proprietary dataset comparable in scale to other private datasets reported in the literature. We ensure robust evaluation by using a framework consisting of a variety of downstream MIR tasks. Our results show that our architecture achieves competitive performance when compared with other state-of-the-art models that use multi-head self-attention, while reducing the model size from 8.5% up to 12.3%.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.09603" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.09603" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.09603" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.SD</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">14. MoE-DisCo:Low Economy Cost Training Mixture-of-Experts Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Xin Ye, Daning Cheng, Boyang Zhang, Yunquan Zhang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The MoE-DisCo framework enables efficient training of large Mixture-of-Experts models on low-cost, memory-constrained hardware by decomposing the model into independently trained dense submodels and clustering the training data, resulting in significant cost reduction and competitive performance. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Training large-scale Mixture-of-Experts (MoE) models typically requires high-memory, high-bandwidth GPUs (e.g., A100), and their high cost has become a major barrier to large-model training. In contrast, affordable hardware is low-cost but constrained by memory capacity and bandwidth, making it unsuitable for direct LLM training. To address this, we propose MoE-DisCo (Mixture-of-Experts with Disentangled Clustering and Coordination), a staged training framework. MoE-DisCo decomposes the MoE model into multiple dense submodels, each consisting of a shared backbone and a single expert, and partitions the training data into subsets using unsupervised clustering. Each submodel is trained independently and in parallel on its assigned data subset using low-cost devices, without any inter-device communication. Subsequently, all experts are integrated into a complete MoE model and fine-tuned globally for a short period on high-memory, high-bandwidth GPUs. Experiments show that our method matches or even surpasses full-parameter training in performance across multiple downstream tasks, loss function, and perplexity (PPL), while reducing training cost by 47.6 percent to 69.5 percent on Qwen1.5-MoE-2.7B and Llama-MoE-3.5B across different datasets.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.06857" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.06857" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.06857" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">15. The NANOGrav 15 yr Data Set: Piecewise Power-Law Reconstruction of the Gravitational-Wave Background
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Gabriella Agazie, Akash Anumarlapudi, Anne M. Archibald, Zaven Arzoumanian, Jeremy G. Baier, Paul T. Baker, Bence Bécsy, Amit Bhoonah, Laura Blecha, Adam Brazier, et al.</span>
                                <span class="author-full" style="display: none;">Gabriella Agazie, Akash Anumarlapudi, Anne M. Archibald, Zaven Arzoumanian, Jeremy G. Baier, Paul T. Baker, Bence Bécsy, Amit Bhoonah, Laura Blecha, Adam Brazier, Paul R. Brook, Sarah Burke-Spolaor, Rand Burnette, Robin Case, J. Andrew Casey-Clyde, Maria Charisi, Shami Chatterjee, Tyler Cohen, James M. Cordes, Neil J. Cornish, Fronefield Crawford, Thankful Cromartie, Kathryn Crowter, Megan E. DeCesar, Paul B. Demorest, Heling Deng, Lankeswar Dey, Timothy Dolch, Elizabeth C. Ferrara, William Fiore, Emmanuel Fonseca, Gabriel E. Freedman, Emiko C. Gardiner, Nate Garver-Daniels, Peter A. Gentile, Kyle A. Gersbach, Joseph Glaser, Brenda D. Gómez-Cortes, Deborah C. Good, Kayhan Gültekin, C. J. Harris, Jeffrey S. Hazboun, Ross J. Jennings, Aaron D. Johnson, Megan L. Jones, David L. Kaplan, Luke Zoltan Kelley, Matthew Kerr, Joey S. Key, Nima Laal, Michael T. Lam, William G. Lamb, Bjorn Larsen, T. Joseph W. Lazio, Natalia Lewandowska, Monica Leys, Tingting Liu, Duncan R. Lorimer, Jing Luo, Ryan S. Lynch, Chung-Pei Ma, Dustin R. Madison, Cayenne Matt, Alexander McEwen, James W. McKee, Maura A. McLaughlin, Natasha McMann, Bradley W. Meyers, Patrick M. Meyers, Chiara M. F. Mingarelli, Andrea Mitridate, Cherry Ng, David J. Nice, Stella Koch Ocker, Ken D. Olum, Timothy T. Pennucci, Benetge B. P. Perera, Polina Petrov, Nihan S. Pol, Henri A. Radovan, Scott M. Ransom, Paul S. Ray, Joseph D. Romano, Jessie C. Runnoe, Alexander Saffer, Shashwat C. Sardesai, Ann Schmiedekamp, Carl Schmiedekamp, Kai Schmitz, Brent J. Shapiro-Albert, Xavier Siemens, Joseph Simon, Sophia V. Sosa Fiscella, Ingrid H. Stairs, Daniel R. Stinebring, Kevin Stovall, Abhimanyu Susobhanan, Joseph K. Swiggum, Jacob Taylor, Stephen R. Taylor, Mercedes S. Thompson, Jacob E. Turner, Michele Vallisneri, Rutger van Haasteren, Sarah J. Vigeland, Haley M. Wahl, Si Wang, Kevin P. Wilson, Caitlin A. Witt, David Wright, Olivia Young</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Spectral characterization of the NANOGrav 15-year gravitational-wave background signal is achieved using a Bayesian model average of piecewise power-law models, offering a flexible yet constrained approach superior to standard constant or free spectral models. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The NANOGrav 15-year (NG15) data set provides evidence for a gravitational-wave background (GWB) signal at nanohertz frequencies, which is expected to originate either from a cosmic population of inspiraling supermassive black-hole binaries or new particle physics in the early Universe. A firm identification of the source of the NG15 signal requires an accurate reconstruction of its frequency spectrum. In this paper, we provide such a spectral characterization of the NG15 signal based on a piecewise power-law (PPL) ansatz that strikes a balance between existing alternatives in the literature. Our PPL reconstruction is more flexible than the standard constant-power-law model, which describes the GWB spectrum in terms of only two parameters: an amplitude A and a spectral index gamma. Concurrently, it better approximates physically realistic GWB spectra -- especially those of cosmological origin -- than the free spectral model, since the latter allows for arbitrary variations in the GWB amplitude from one frequency bin to the next. Our PPL reconstruction of the NG15 signal relies on individual PPL models with a fixed number of internal nodes (i.e., constant power law, broken power law, doubly broken power law, etc.) that are ultimately combined in a Bayesian model average. The data products resulting from our analysis provide the basis for fast refits of spectral GWB models.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.09481" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.09481" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.09481" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.HE</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">16. A comparative test of different pressure profile models in clusters of galaxies using recent ACT data
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Denis Tramonte</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Analysis of stacked Sunyaev-Zel&#39;dovich data from thousands of ACT galaxy clusters demonstrates that while various functional forms fit the electron pressure profile equally well, residual mass and redshift dependencies undermine the assumption of a universal pressure model. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Context. The electron pressure profile is a convenient tool to characterize the thermodynamical state of a galaxy cluster, with several studies adopting a &#34;universal&#34; functional form. Aims. This study aims at using Sunyaev-Zel&#39;dovich (SZ) data to test four different functional forms for the cluster pressure profile: generalized Navarro-Frenk-White (gNFW), $β$-model, polytropic, and exponential. The goal is to assess to what level they are universal over a population-level cluster sample. Methods. A set of 3496 ACT-DR4 galaxy clusters, spanning the mass range $[10^{14},10^{15.1}]\,\text{M}_{\odot}$ and the redshift range $[0,2]$, is stacked on the ACT-DR6 Compton parameter $y$ map over $\sim13,000\,\text{deg}^2$. An angular Compton profile is then extracted and modeled using the theoretical pressure recipes, whose free parameters are constrained against the measurement via a multi-stage MCMC approach. The analysis is repeated over cluster subsamples spanning smaller mass and redshift ranges. Results. All functional forms are effective in reproducing the measured $y$ profiles within their error bars, without a clearly favored model. While best-fit estimates are in broad agreement with previous findings, hints of residual subsample dependency are detected favoring higher amplitudes and steeper profiles in high-mass, low-redshift clusters. Conclusions. Population-level cluster studies based on SZ data alone are likely unable to accurately constrain different pressure profile models. Residual trends at population level and scatter at individual cluster level undermine the universal pressure model assumption whenever high precision is required. Finally, functional forms different from the gNFW prove equally effective while being more physically motivated.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.08933" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.08933" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.08933" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">17. MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Kewei Zhang, Ye Huang, Yufan Deng, Jincheng Yu, Junsong Chen, Huan Ling, Enze Xie, Daquan Zhou</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Multi-Head Linear Attention (MHLA) resolves the global context collapse failure mode in linear attention by computing attention within divided heads along the token dimension, successfully recovering the expressive power of quadratic softmax attention while maintaining linear complexity. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6\% improvement on ImageNet classification, a 6.3\% gain on NLP, a 12.6\% improvement on image generation, and a 41\% enhancement on video generation under the same time complexity.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.07832" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.07832" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.07832" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">18. RotCurves: A PYTHON package for efficient modelling and fitting of galactic rotation curves at high-z
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">A. Nestor Shachar, A. Sternberg, S. H. Price, N. M. Förster Schreiber, R. Genzel, L. J. Tacconi, H. Übler, C. Barfety, A. Burkert, J. Chen, et al.</span>
                                <span class="author-full" style="display: none;">A. Nestor Shachar, A. Sternberg, S. H. Price, N. M. Förster Schreiber, R. Genzel, L. J. Tacconi, H. Übler, C. Barfety, A. Burkert, J. Chen, R. Davies, F. Eisenhauer, J. M. Espejo Salcedo, R. Herrera-Camus, J. B. Jolly, L. L. Lee, T. Naab, S. Pastras, C. Pulsoni, T. T. Shimizu, G. Tozzi</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> RotCurves is introduced as a highly efficient, parametric forward-modeling tool for analyzing high-redshift galaxy rotation curves from IFU surveys, providing accurate beam smearing correction and achieving significant speed improvements over existing methods. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Rotation curves are a fundamental tool in the study of galaxies across cosmic time, and with the advent of large integral field unit (IFU) kinematic surveys there is an increasing need for efficient and flexible modelling tools. We present RotCurves, a parametric forward-modeling tool designed for rotation curve analysis at high-z, correcting for ``beam smearing&#34; by projecting and convolving the beam PSF in the plane of the galaxy. We benchmark RotCurves against the established parametric code dysmalpy using synthetic observations. The typical runtime with RotCurves is a few ~10ms, a factor 250 faster than dysmalpy for a single realization. For well-resolved systems (PSF FWHM 1.5 Reff) discrepancies increase to up to 15%. Using a built-in MCMC fitting procedure, RotCurves recovers well the intrinsic model parameters across a wide range of galaxy properties and accounting for realistic noise patterns. Systematic biases emerge for the effective radius and for low disk masses (Mdisk 25), with increasing deviations in parameter recovery at lower S/N. RotCurves is best suited for inclinations of 10 &lt; i &lt; 80. RotCurves is built as an exploratory tool for rapid testing of mass model assumptions, parameter studies and for efficiently processing large samples of observational data from large IFU surveys. The code is publicly available on github.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.08348" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.08348" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.08348" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">19. When Models Know When They Do Not Know: Calibration, Cascading, and Cleaning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Chenjie Hao, Weyl Lu, Yuko Ishiwaka, Zengyi Li, Weier Wan, Yubei Chen</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A universal, training-free method utilizes the reliability of calibrated model confidence to implement practical applications, including efficient model cascading through advantage routing and effective ensemble-based data cleaning for both vision and language models. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">When a model knows when it does not know, many possibilities emerge. The first question is how to enable a model to recognize that it does not know. A promising approach is to use confidence, computed from the model&#39;s internal signals, to reflect its ignorance. Prior work in specific domains has shown that calibration can provide reliable confidence estimates. In this work, we propose a simple, effective, and universal training-free method that applies to both vision and language models, performing model calibration, cascading, and data cleaning to better exploit a model&#39;s ability to recognize when it does not know. We first highlight two key empirical observations: higher confidence corresponds to higher accuracy within a single model, and models calibrated on the validation set remain calibrated on a held-out test set. These findings empirically establish the reliability and comparability of calibrated confidence. Building on this, we introduce two applications: (1) model cascading with calibrated advantage routing and (2) data cleaning based on model ensemble. Using the routing signal derived from the comparability of calibrated confidences, we cascade large and small models to improve efficiency with almost no compromise in accuracy, and we further cascade two models of comparable scale to achieve performance beyond either model alone. Leveraging multiple experts and their calibrated confidences, we design a simple yet effective data-cleaning method that balances precision and detection rate to identify mislabeled samples in ImageNet and Massive Multitask Language Understanding (MMLU) datasets. Our results demonstrate that enabling models to recognize when they do not know is a practical step toward more efficient, reliable, and trustworthy AI.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.07965" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.07965" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.07965" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">20. PRL: Process Reward Learning Improves LLMs&#39; Reasoning Ability and Broadens the Reasoning Boundary
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jiarui Yao, Ruida Wang, Tong Zhang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Process Reward Learning (PRL) enhances the reasoning capabilities of Large Language Models by decomposing the reinforcement learning objective to assign theoretically rigorous process supervision signals to intermediate steps, effectively guiding exploration during optimization. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs&#39; reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.10201" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.10201" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.10201" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">21. A new magnitude--redshift relation based on SNe Ia
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Ósmar Rodríguez, Alejandro Clocchiatti</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A novel two-parameter empirical relation linking Type Ia supernova magnitude and redshift, $d_L(z)=c\,H_0^{-1}z(1+z)10^{bz/5}$, accurately models the Hubble diagram up to $z\simeq1.1$ and confirms consistent cosmic acceleration across eight deep-field regions. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present a new empirical relation between the standardized magnitude ($m$) of Type Ia supernovae (SNe Ia) and redshift ($z$). Using Pantheon+ and DES-SN5YR, we find a negative linear correlation between $m-5\log(z(1+z))$ and $z$, implying that their magnitude--redshift relation can be parametrized with just two parameters: an intercept $\mathcal{M}$ and a slope $b$. This relation corresponds to the luminosity distance $d_L(z)=c\,H_0^{-1}z(1+z)10^{bz/5}$ and is valid up to at least $z\simeq1.1$. It outperforms the $Λ$CDM and flat $w$CDM models and the (2,1) Padé approximant for $d_L(z)$, and performs comparably to the flat $Λ$CDM model and the (2,1) Padé($j_0=1$) model of Hu et al. Furthermore, the relation is stable in the absence of low-$z$ SNe, making it suitable for fitting Hubble diagrams of SNe Ia without the need to add a low-$z$ sample. In deep fields in particular, assuming that the large-scale density is independent of the comoving radial coordinate, $b\propto q_0+1$. We fit the empirical relation to SN data in eight deep-field regions and find that their fitted $\mathcal{M}$ and $b$ parameters are consistent within $1.6\,σ$, in agreement with isotropy. The inferred $q_0$ values, ranging from $-0.6$ to $-0.4$, are consistent within $1.5\,σ$ and significantly lower than zero, indicating statistically consistent cosmic acceleration across all eight regions. We apply the empirical relation to the DES-Dovekie and Amalgame SN samples, finding $b$ values consistent with those from DES-SN5YR and Pantheon+. Finally, using the empirical relation in the hemispheric comparison method applied to Pantheon+ up to $z=1.1$, we find no evidence for anisotropies in $\mathcal{M}$ and $b$.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.08505" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.08505" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.08505" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Survey Analysis, Cosmology / Galaxy-halo connection, clustering, lensing</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">22. RIFT: Repurposing Negative Samples via Reward-Informed Fine-Tuning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zehua Liu, Shuqi Liu, Tao Zhong, Mingxuan Yuan</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Reward Informed Fine-Tuning (RIFT) enhances LLM alignment efficiency by utilizing all self-generated samples, reweighting the loss based on scalar rewards via a stabilized formulation, and consistently achieving superior performance over standard rejection sampling methods. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">While Supervised Fine-Tuning (SFT) and Rejection Sampling Fine-Tuning (RFT) are standard for LLM alignment, they either rely on costly expert data or discard valuable negative samples, leading to data inefficiency. To address this, we propose Reward Informed Fine-Tuning (RIFT), a simple yet effective framework that utilizes all self-generated samples. Unlike the hard thresholding of RFT, RIFT repurposes negative trajectories, reweighting the loss with scalar rewards to learn from both the positive and negative trajectories from the model outputs. To overcome the training collapse caused by naive reward integration, where direct multiplication yields an unbounded loss, we introduce a stabilized loss formulation that ensures numerical robustness and optimization efficiency. Extensive experiments on mathematical benchmarks across various base models show that RIFT consistently outperforms RFT. Our results demonstrate that RIFT is a robust and data-efficient alternative for alignment using mixed-quality, self-generated data.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.09253" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.09253" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.09253" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">23. GIFT: Unlocking Global Optimality in Post-Training via Finite-Temperature Gibbs Initialization
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zhengyang Zhao, Lu Ma, Yizhen Jiang, Xiaochen Ma, Zimo Meng, Chengyu Shen, Lexiang Tang, Haoze Sun, Peng Pei, Wentao Zhang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Gibbs Initialization with Finite Temperature (GIFT) reformulates Supervised Fine-Tuning by incorporating supervision as a finite-temperature energy potential, successfully preventing the distributional collapse that hinders subsequent Reinforcement Learning optimization in Large Reasoning Models. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The prevailing post-training paradigm for Large Reasoning Models (LRMs)--Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL)--suffers from an intrinsic optimization mismatch: the rigid supervision inherent in SFT induces distributional collapse, thereby exhausting the exploration space necessary for subsequent RL. In this paper, we reformulate SFT within a unified post-training framework and propose Gibbs Initialization with Finite Temperature (GIFT). We characterize standard SFT as a degenerate zero-temperature limit that suppresses base priors. Conversely, GIFT incorporates supervision as a finite-temperature energy potential, establishing a distributional bridge that ensures objective consistency throughout the post-training pipeline. Our experiments demonstrate that GIFT significantly outperforms standard SFT and other competitive baselines when utilized for RL initialization, providing a mathematically principled pathway toward achieving global optimality in post-training. Our code is available at https://github.com/zzy1127/GIFT.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.09233" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.09233" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.09233" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">24. R$^2$BD: A Reconstruction-Based Method for Generalizable and Efficient Detection of Fake Images
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Qingyu Liu, Zhongjie Ba, Jianmin Guo, Qiu Wang, Zhibo Wang, Jie Shi, Kui Ren</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The R²BD framework achieves superior efficiency and generalization in AIGC image detection by employing G-LDM, a unified reconstruction model for various generative paradigms, and calculating residual bias in a single inference step, making it over 22 times faster than prior reconstruction methods. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Recently, reconstruction-based methods have gained attention for AIGC image detection. These methods leverage pre-trained diffusion models to reconstruct inputs and measure residuals for distinguishing real from fake images. Their key advantage lies in reducing reliance on dataset-specific artifacts and improving generalization under distribution shifts. However, they are limited by significant inefficiency due to multi-step inversion and reconstruction, and their reliance on diffusion backbones further limits generalization to other generative paradigms such as GANs. In this paper, we propose a novel fake image detection framework, called R$^2$BD, built upon two key designs: (1) G-LDM, a unified reconstruction model that simulates the generation behaviors of VAEs, GANs, and diffusion models, thereby broadening the detection scope beyond prior diffusion-only approaches; and (2) a residual bias calculation module that distinguishes real and fake images in a single inference step, which is a significant efficiency improvement over existing methods that typically require 20$+$ steps. Extensive experiments on the benchmark from 10 public datasets demonstrate that R$^2$BD is over 22$\times$ faster than existing reconstruction-based methods while achieving superior detection accuracy. In cross-dataset evaluations, it outperforms state-of-the-art methods by an average of 13.87\%, showing strong efficiency and generalization across diverse generative methods. The code and dataset used for evaluation are available at https://github.com/QingyuLiu/RRBD.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.08867" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.08867" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.08867" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">25. Learning the relations between neutron star and nuclear matter properties with symbolic regression
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>N. K. Patra, Tuhin Malik, Kai Zhou, Constança Providência</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Symbolic regression analysis of neutron star models confirms a robust correlation between the tidal deformability of a $1.4 M_\odot$ star and $\beta$-equilibrium pressure, demonstrating that isovector properties are overwhelmingly responsible (90%) for determining the radius and tidal deformability. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The equation of state (EOS) of dense matter in neutron stars (NSs) remains uncertain, particularly at supra-nuclear densities where complex nuclear interactions and the potential presence of exotic matter, like hyperons, come into play. The complex relationships existing between nuclear matter and neutron star properties are investigated. The focus is on their nonlinearities and interdependencies. In our analysis, we apply a machine learning algorithm known as symbolic regression, paired with principal component analysis, to datasets generated from Bayesian inference over relativistic mean-field models. A systematic Principal Component Analysis has allowed to break down the percentage contribution of each element or feature in the relationships obtained. This study examines two main models (datasets): the NL model, which includes nucleonic degrees of freedom; and the NL-hyp model, which includes hyperons in addition to nucleons. Our analysis confirms a robust correlation between the tidal deformability of a 1.4 \(M_\odot\) neutron star and $β$-equilibrium pressure at twice the nuclear saturation density. This correlation remains once hyperons are included. The contribution of the different nuclear matter properties at saturation to the radius and tidal deformability was calculated. It was shown that the isovector properties have the largest impact, with a contribution of about 90\%. We also studied the relationship between the proton fraction at different densities and various symmetry energy parameters defined at saturation density. For the hyperon data set, we took into account the effects of the negatively charged hyperon $Ξ$ in order to recover the relationships. Our study reveals the individual impact of various symmetry energy parameters on proton fractions at different densities.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.07727" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.07727" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.07727" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">nucl-th</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">26. The Second CHIME/FRB Catalog of Fast Radio Bursts
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">The CHIME/FRB Collaboration, :, Thomas Abbott, Bridget C. Andersen, Shion Andrew, Kevin Bandura, Mohit Bhardwaj, Yash Bhusare, Charanjot Brar, Tomas Cassanelli, et al.</span>
                                <span class="author-full" style="display: none;">The CHIME/FRB Collaboration, :, Thomas Abbott, Bridget C. Andersen, Shion Andrew, Kevin Bandura, Mohit Bhardwaj, Yash Bhusare, Charanjot Brar, Tomas Cassanelli, Shami Chatterjee, Jean-Francois Cliche, Amanda M. Cook, Alice Curtin, Matt Dobbs, Fengqiu Adam Dong, Gwendolyn Eadie, Tarraneh Eftekhari, Emmanuel Fonseca, B. M. Gaensler, Deborah Good, Mark Halpern, Jason W. T. Hessels, Adaeze Ibik, Naman Jain, Ronniy C. Joseph, Zarif Kader, Victoria M. Kaspi, Afrokk Khan, Bikash Kharel, Ajay Kumar, T. L. Landecker, Dustin Lang, Adam E. Lanman, Magnus L&#39;Argent, Mattias Lazda, Calvin Leung, Dong Zi Li, Chris J. Lintott, Robert Main, Kiyoshi W. Masui, Sujay Mate, Kyle McGregor, Ryan Mckinven, Juan Mena-Parra, Bradley W. Meyers, Daniele Michilli, Cherry Ng, Mason Ng, Kenzie Nimmo, Gavin Noble, Ayush Pandhi, Swarali S. Patil, Aaron B. Pearlman, Ue-Li Pen, Ziggy Pleunis, J. Xavier Prochaska, Masoud Rafiei-Ravandi, Scott Ransom, Andre Renard, Mawson W. Sammons, Ketan R. Sand, Paul Scholz, Vishwangi Shah, Kaitlyn Shin, Seth R. Siegel, Sloane Sirota, Kendrick Smith, Ingrid Stairs, David C. Stenning, Shriharsh P. Tendulkar, Keith Vanderlinde, Mike Walmsley, Haochen Wang, Dallas Wulf</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The second CHIME/FRB catalog details 4539 fast radio bursts originating from 3641 unique sources, providing refined measurements of key parameters like dispersion measure, scattering time, and flux density for ongoing cosmological and astrophysical studies. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present a catalog of 4539 fast radio bursts (FRBs) observed with the Canadian Hydrogen Intensity Mapping Experiment (CHIME) telescope between 25 July 2018 and 15 September 2023. These bursts originate from 3641 unique sources, including 981 bursts from 83 known repeating sources. For each FRB, the catalog provides a $O(10&#39;)$ estimate of sky location along with corresponding measurements of cumulative exposure time and survey sensitivity over the observing period. It includes a total-intensity dynamic spectrum between 400 and 800 MHz at 0.983 ms resolution. From this spectrum, we constrain a model of the burst morphology and measure key parameters such as arrival time, intrinsic temporal width, dispersion measure, scattering time, and flux density. This second catalog includes all FRBs from the first catalog, with every event reprocessed using a uniform and improved analysis framework. We show that previously published inferences remain valid under the updated measurements. We assess consistency of the detection rate across observational parameters, present initial distributions of burst properties, and outline ongoing and future studies that will use this catalog to investigate the nature of FRBs and their utility as astrophysical and cosmological probes.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.09399" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.09399" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.09399" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.HE</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">27. Local Group analogues in a cosmological context -- I. Relating velocity structure to the cosmic web
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Kai Wang, Peder Norberg, Azadeh Fattahi, Louis E. Strigari</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The coupling energy of the Milky Way-M31 orbit strongly correlates with the surrounding large-scale environment, where low-energy Local Group analogues exhibit pronounced alignment with cosmic filaments in low-density regions, consistent with the observed properties of our own Local Group. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Our Local Group, dominated in mass by the Milky Way (MW) and M31, provides a unique laboratory for testing $Λ$CDM cosmology on small scales owing to its proximity. However, its connection to the surrounding large-scale environment, which is essential for interpreting its properties, is inadequately understood. In this work, we explore the connection between Local Group analogues (LGAs) and their surrounding large-scale environments using the ABACUSSUMMIT simulation suite, highlighting the key role of the coupling energy of the MW-M31 orbit, $E_{\rm coupling}$. We find that LGAs with high $E_{\rm coupling}$ preferentially reside in denser regions, whereas those with low $E_{\rm coupling}$ tend to occupy low-density environments. Furthermore, LGAs with low $E_{\rm coupling}$ exhibit strong alignment with cosmic filaments, manifested as a pronounced polar anisotropy in the distribution of tracer haloes. By contrast, LGAs with high $E_{\rm coupling}$ show a weaker polar anisotropy but an enhanced azimuthal anisotropy, with large-scale tracer haloes preferentially lying in the plane spanned by the halo pair and the orbital spin vector. Within this framework, our Local Group is characterised by typical $E_{\rm coupling}$ residing in a relatively under-dense environment, yet it remains consistent with the 95\% range of analogue systems identified in the simulation.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.08419" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.08419" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.08419" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">28. EfficientFSL: Enhancing Few-Shot Classification via Query-Only Tuning in Vision Transformers
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Wenwen Liao, Hang Ruan, Jianbo Yu, Bing Song, YuansongWang, Xiaofeng Yang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> EfficientFSL introduces a query-only fine-tuning framework for Vision Transformers that leverages lightweight trainable blocks and attention mechanisms to achieve state-of-the-art few-shot classification performance while drastically minimizing the number of tunable parameters and computational cost. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Large models such as Vision Transformers (ViTs) have demonstrated remarkable superiority over smaller architectures like ResNet in few-shot classification, owing to their powerful representational capacity. However, fine-tuning such large models demands extensive GPU memory and prolonged training time, making them impractical for many real-world low-resource scenarios. To bridge this gap, we propose EfficientFSL, a query-only fine-tuning framework tailored specifically for few-shot classification with ViT, which achieves competitive performance while significantly reducing computational overhead. EfficientFSL fully leverages the knowledge embedded in the pre-trained model and its strong comprehension ability, achieving high classification accuracy with an extremely small number of tunable parameters. Specifically, we introduce a lightweight trainable Forward Block to synthesize task-specific queries that extract informative features from the intermediate representations of the pre-trained model in a query-only manner. We further propose a Combine Block to fuse multi-layer outputs, enhancing the depth and robustness of feature representations. Finally, a Support-Query Attention Block mitigates distribution shift by adjusting prototypes to align with the query set distribution. With minimal trainable parameters, EfficientFSL achieves state-of-the-art performance on four in-domain few-shot datasets and six cross-domain datasets, demonstrating its effectiveness in real-world applications.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.08499" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.08499" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.08499" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">29. Photometric Redshift Estimation Using Scaled Ensemble Learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Swagata Biswas, Shubhrangshu Ghosh, Avyarthana Ghosh, Yogesh Wadadekar, Abhishek Roy Choudhury, Arijit Mukherjee, Shailesh Deshpande, Arpan Pal</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A novel scaled ensemble machine learning framework, combining multiple algorithms with bagged input data, reliably predicts photometric redshifts for faint galaxies up to $z \sim 4$, achieving precision and reliability benchmarks required for large-scale surveys like LSST. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The development of the state-of-the-art telescopic systems capable of performing expansive sky surveys such as the Sloan Digital Sky Survey, Euclid, and the Rubin Observatory&#39;s Legacy Survey of Space and Time (LSST) has significantly advanced efforts to refine cosmological models. These advances offer deeper insight into persistent challenges in astrophysics and our understanding of the Universe&#39;s evolution. A critical component of this progress is the reliable estimation of photometric redshifts (Pz). To improve the precision and efficiency of such estimations, the application of machine learning (ML) techniques to large-scale astronomical datasets has become essential. This study presents a new ensemble-based ML framework aimed at predicting Pz for faint galaxies and higher redshift ranges, relying solely on optical (grizy) photometric data. The proposed architecture integrates several learning algorithms, including gradient boosting machine, extreme gradient boosting, k-nearest neighbors, and artificial neural networks, within a scaled ensemble structure. By using bagged input data, the ensemble approach delivers improved predictive performance compared to stand-alone models. The framework demonstrates consistent accuracy in estimating redshifts, maintaining strong performance up to z ~ 4. The model is validated using publicly available data from the Hyper Suprime-Cam Strategic Survey Program by the Subaru Telescope. Our results show marked improvements in the precision and reliability of Pz estimation. Furthermore, this approach closely adheres to-and in certain instances exceeds-the benchmarks specified in the LSST Science Requirements Document. Evaluation metrics include catastrophic outlier, bias, and rms.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.07292" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.07292" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.07292" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">30. PKI: Prior Knowledge-Infused Neural Network for Few-Shot Class-Incremental Learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Kexin Baoa, Fanzhao Lin, Zichen Wang, Yong Li, Dan Zeng, Shiming Ge</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The Prior Knowledge-Infused (PKI) neural network, which utilizes an ensemble of cascaded projectors and memory, effectively mitigates catastrophic forgetting and overfitting in few-shot class-incremental learning by flexibly integrating accumulated prior knowledge with newly acquired information. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Few-shot class-incremental learning (FSCIL) aims to continually adapt a model on a limited number of new-class examples, facing two well-known challenges: catastrophic forgetting and overfitting to new classes. Existing methods tend to freeze more parts of network components and finetune others with an extra memory during incremental sessions. These methods emphasize preserving prior knowledge to ensure proficiency in recognizing old classes, thereby mitigating catastrophic forgetting. Meanwhile, constraining fewer parameters can help in overcoming overfitting with the assistance of prior knowledge. Following previous methods, we retain more prior knowledge and propose a prior knowledge-infused neural network (PKI) to facilitate FSCIL. PKI consists of a backbone, an ensemble of projectors, a classifier, and an extra memory. In each incremental session, we build a new projector and add it to the ensemble. Subsequently, we finetune the new projector and the classifier jointly with other frozen network components, ensuring the rich prior knowledge is utilized effectively. By cascading projectors, PKI integrates prior knowledge accumulated from previous sessions and learns new knowledge flexibly, which helps to recognize old classes and efficiently learn new classes. Further, to reduce the resource consumption associated with keeping many projectors, we design two variants of the prior knowledge-infused neural network (PKIV-1 and PKIV-2) to trade off a balance between resource consumption and performance by reducing the number of projectors. Extensive experiments on three popular benchmarks demonstrate that our approach outperforms state-of-the-art methods.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.08493" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.08493" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.08493" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">31. d3LLM: Ultra-Fast Diffusion LLM using Pseudo-Trajectory Distillation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yu-Yang Qian, Junda Su, Lanxiang Hu, Peiyuan Zhang, Zhijie Deng, Peng Zhao, Hao Zhang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The d3LLM framework utilizes pseudo-trajectory distillation and entropy-based multi-block decoding to effectively balance accuracy and parallelism in Diffusion Large Language Models, resulting in significant speedups (up to 10x) compared to existing methods. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Diffusion large language models (dLLMs) offer capabilities beyond those of autoregressive (AR) LLMs, such as parallel decoding and random-order generation. However, realizing these benefits in practice is non-trivial, as dLLMs inherently face an accuracy-parallelism trade-off. Despite increasing interest, existing methods typically focus on only one-side of the coin, targeting either efficiency or performance. To address this limitation, we propose d3LLM (Pseudo-Distilled Diffusion Large Language Model), striking a balance between accuracy and parallelism: (i) during training, we introduce pseudo-trajectory distillation to teach the model which tokens can be decoded confidently at early steps, thereby improving parallelism; (ii) during inference, we employ entropy-based multi-block decoding with a KV-cache refresh mechanism to achieve high parallelism while maintaining accuracy. To better evaluate dLLMs, we also introduce AUP (Accuracy Under Parallelism), a new metric that jointly measures accuracy and parallelism. Experiments demonstrate that our d3LLM achieves up to 10$\times$ speedup over vanilla LLaDA/Dream and 5$\times$ speedup over AR models without much accuracy drop. Our code is available at https://github.com/hao-ai-lab/d3LLM.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.07568" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.07568" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.07568" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">32. The recipe for the degrees of freedom
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Anamaria Hell, Elisa G. M. Ferreira, Dieter Lust, Misao Sasaki</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A novel, systematic recipe based on the Lagrangian formulation is introduced to directly and simply determine the degrees of freedom in field and gravity theories, bypassing the technical complexity of standard Hamiltonian approaches. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We consider the question of counting the degrees of freedom in theoretical models, with an emphasis on theories of fields and gravity. Among the possible approaches, the Hamiltonian formulation remains one of the most systematic and robust tools. However, it can easily become long and technically involved. In this work, we present a broadly applicable recipe to find the degrees of freedom directly, based on the Lagrangian formulation. We compare it to the standard approaches, highlight the challenges that may arise in the latter, and demonstrate that the proposed method leads to transparent insights about the dynamical nature of theory in a quick, simple, and straight-forward way.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.10288" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.10288" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.10288" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">hep-th</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">33. Reconstructing Gamma Ray Burst Energy Relations with Observational H(z) data in Neural Network Framework
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Nilanjana Bagchi Aurpa, Abha Dev Habib, Nisha Rani</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Model-independent calibration of Gamma Ray Burst luminosity relations is achieved using H(z) data and a robust Bayesian Neural Network framework, successfully avoiding the circularity problem and providing reliable constraints on the Amati relation parameters. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Gamma ray bursts (GRBs) offer a powerful probe of the cosmic expansion history far beyond the redshift range accessible to Type Ia supernovae. However, the calibration of GRB luminosity correlations is hindered by the circularity problem, which arises from assuming a fiducial cosmological model during calibration. In this work, we perform a model independent calibration of GRB luminosity relations using observational Hubble parameter H(z) data from the A220 and J220 compilations, thereby avoiding explicit cosmological assumptions. We employ Artificial Neural Network (ANN) to reconstruct the calibration relation directly from the data. In addition, we implement a Bayesian Neural Network (BNN) framework as an alternative approach, enabling a data driven treatment of both statistical and systematic uncertainties. The calibrated GRB sample is used to constrain the Amati relation, and we systematically compare the outcomes obtained from different calibration techniques and datasets. While the Amati Parameters obtained from GRBs caibrated from the ANN and BNN results are consistent with previous low redshifts calibrations using model-independent methods, the BNN approach provides a more robust framework.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.08550" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.08550" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.08550" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">34. Deep Exploration of Epoch-wise Double Descent in Noisy Data: Signal Separation, Large Activation, and Benign Overfitting
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Tomoki Kubo, Ryuken Uda, Yusuke Iida</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Empirical investigation into epoch-wise double descent demonstrates that re-generalization after fitting noisy data (&#34;benign overfitting&#34;) correlates with the separation of internal activations corresponding to clean and noisy data, alongside the emergence of large, input-pattern-correlated activations in shallow layers. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Deep double descent is one of the key phenomena underlying the generalization capability of deep learning models. In this study, epoch-wise double descent, which is delayed generalization following overfitting, was empirically investigated by focusing on the evolution of internal structures. Fully connected neural networks of three different sizes were trained on the CIFAR-10 dataset with 30% label noise. By decomposing the loss curves into signal contributions from clean and noisy training data, the epoch-wise evolutions of internal signals were analyzed separately. Three main findings were obtained from this analysis. First, the model achieved strong re-generalization on test data even after perfectly fitting noisy training data during the double descent phase, corresponding to a &#34;benign overfitting&#34; state. Second, noisy data were learned after clean data, and as learning progressed, their corresponding internal activations became increasingly separated in outer layers; this enabled the model to overfit only noisy data. Third, a single, very large activation emerged in the shallow layer across all models; this phenomenon is referred as &#34;outliers,&#34; &#34;massive activa-tions,&#34; and &#34;super activations&#34; in recent large language models and evolves with re-generalization. The magnitude of large activation correlated with input patterns but not with output patterns. These empirical findings directly link the recent key phenomena of &#34;deep double descent,&#34; &#34;benign overfitting,&#34; and &#34;large activation&#34;, and support the proposal of a novel scenario for understanding deep double descent.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.08316" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.08316" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.08316" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">35. Position: Don&#39;t be Afraid of Over-Smoothing And Over-Squashing
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Niklas Kormann, Benjamin Doerr, Johannes F. Lutzeyer</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Performance decreases in Graph Neural Networks are argued to stem from uninformative receptive fields rather than the commonly cited issues of over-smoothing or over-squashing, necessitating a research paradigm shift toward analyzing the localization of label-relevant graph information. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Over-smoothing and over-squashing have been extensively studied in the literature on Graph Neural Networks (GNNs) over the past years. We challenge this prevailing focus in GNN research, arguing that these phenomena are less critical for practical applications than assumed. We suggest that performance decreases often stem from uninformative receptive fields rather than over-smoothing. We support this position with extensive experiments on several standard benchmark datasets, demonstrating that accuracy and over-smoothing are mostly uncorrelated and that optimal model depths remain small even with mitigation techniques, thus highlighting the negligible role of over-smoothing. Similarly, we challenge that over-squashing is always detrimental in practical applications. Instead, we posit that the distribution of relevant information over the graph frequently factorises and is often localised within a small k-hop neighbourhood, questioning the necessity of jointly observing entire receptive fields or engaging in an extensive search for long-range interactions. The results of our experiments show that architectural interventions designed to mitigate over-squashing fail to yield significant performance gains. This position paper advocates for a paradigm shift in theoretical research, urging a diligent analysis of learning tasks and datasets using statistics that measure the underlying distribution of label-relevant information to better understand their localisation and factorisation.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.07419" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.07419" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.07419" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">36. Popcorn in the sky: Identifying primordial black holes in the gravitational-wave background
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Eleni Bagui, Sébastien Clesse, Federico De Lillo, Alexander C. Jenkins, Mairi Sakellariadou</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The duty cycle of a realistic Primordial Black Hole binary population is quantified as a key observable metric, dependent on gravitational-wave frequency and amplitude, essential for distinguishing PBH contributions from other sources in the Gravitational-Wave Background. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Primordial black holes (PBHs) are possible sources of a gravitational-wave background (GWB), detectable with the next observing runs of LIGO--Virgo--KAGRA. In case of a detection, it will be crucial to distinguish the possible sources of this GWB. One under-explored possibility is to exploit the duty cycle that quantifies the number of sources present in the time domain signal, which can be very different depending on the nature and population of the sources. We compute the duty cycle for a realistic population of PBH binaries, isolating the shot-noise, popcorn and continuous contributions to the GWB. We identify the dependence of the duty cycle on the signal frequency, duration and amplitude as a crucial metric for distinguishing PBHs from other sources in the GWB and constraining PBH models. Our work motivates the development of specific analysis tools to extract these observables, in order to unlock new cosmological insights with upcoming GW data.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.07774" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.07774" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.07774" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">37. WaveFormer: Frequency-Time Decoupled Vision Modeling with Wave Equation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zishan Shu, Juntong Wu, Wei Yan, Xudong Liu, Hongyu Zhang, Chang Liu, Youdong Mao, Jie Chen</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The Wave Propagation Operator (WPO), which models feature map evolution using an underdamped wave equation to explicitly control spatial frequency propagation, enables the WaveFormer architecture to achieve state-of-the-art vision performance with superior efficiency and throughput compared to standard Transformers. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Vision modeling has advanced rapidly with Transformers, whose attention mechanisms capture visual dependencies but lack a principled account of how semantic information propagates spatially. We revisit this problem from a wave-based perspective: feature maps are treated as spatial signals whose evolution over an internal propagation time (aligned with network depth) is governed by an underdamped wave equation. In this formulation, spatial frequency-from low-frequency global layout to high-frequency edges and textures-is modeled explicitly, and its interaction with propagation time is controlled rather than implicitly fixed. We derive a closed-form, frequency-time decoupled solution and implement it as the Wave Propagation Operator (WPO), a lightweight module that models global interactions in O(N log N) time-far lower than attention. Building on WPO, we propose a family of WaveFormer models as drop-in replacements for standard ViTs and CNNs, achieving competitive accuracy across image classification, object detection, and semantic segmentation, while delivering up to 1.6x higher throughput and 30% fewer FLOPs than attention-based alternatives. Furthermore, our results demonstrate that wave propagation introduces a complementary modeling bias to heat-based methods, effectively capturing both global coherence and high-frequency details essential for rich visual semantics. Codes are available at: https://github.com/ZishanShu/WaveFormer.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.08602" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.08602" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.08602" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">38. Confronting eikonal and post-Kerr methods with numerical evolution of scalar field perturbations in spacetimes beyond Kerr
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Ciro De Simone, Sebastian H. Völkel, Kostas D. Kokkotas, Vittorio De Falco, Salvatore Capozziello</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Numerical time-evolution is employed to systematically quantify the theoretical uncertainties of eikonal and post-Kerr approximations when calculating quasinormal modes of scalar fields on deformed Kerr spacetimes, informing the domain of validity for high-precision black-hole spectroscopy. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The accurate computation of quasinormal modes from rotating black holes beyond general relativity is crucial for testing fundamental physics with gravitational waves. In this study, we assess the accuracy of the eikonal and post-Kerr approximations in predicting the quasinormal mode spectrum of a scalar field on a deformed Kerr spacetime. To obtain benchmark results and to analyze the ringdown dynamics from generic perturbations, we further employ a 2+1-dimensional numerical time-evolution framework. This approach enables a systematic quantification of theoretical uncertainties across multiple angular harmonics, a broad range of spin parameters, and progressively stronger deviations from the Kerr geometry. We then confront these modeling errors with simple projections of statistical uncertainties in quasinormal mode frequencies as a function of the signal-to-noise ratio, thereby exploring the domain of validity of approximate methods for prospective high-precision black-hole spectroscopy. We also report that near-horizon deformations can affect prograde and retrograde modes differently and provide a geometrical explanation.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.09607" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.09607" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.09607" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">39. NOVAK: Unified adaptive optimizer for deep neural networks
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Sergii Kavun</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> NOVAK is a unified, high-performance optimization algorithm that synergistically combines rectified adaptive learning rates, decoupled weight decay, and memory-efficient lookahead synchronization, consistently achieving state-of-the-art accuracy and robustness across diverse deep network architectures. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">This work introduces NOVAK, a modular gradient-based optimization algorithm that integrates adaptive moment estimation, rectified learning-rate scheduling, decoupled weight regularization, multiple variants of Nesterov momentum, and lookahead synchronization into a unified, performance-oriented framework. NOVAK adopts a dual-mode architecture consisting of a streamlined fast path designed for production. The optimizer employs custom CUDA kernels that deliver substantial speedups (3-5 for critical operations) while preserving numerical stability under standard stochastic-optimization assumptions. We provide fully developed mathematical formulations for rectified adaptive learning rates, a memory-efficient lookahead mechanism that reduces overhead from O(2p) to O(p + p/k), and the synergistic coupling of complementary optimization components. Theoretical analysis establishes convergence guarantees and elucidates the stability and variance-reduction properties of the method. Extensive empirical evaluation on CIFAR-10, CIFAR-100, ImageNet, and ImageNette demonstrates NOVAK superiority over 14 contemporary optimizers, including Adam, AdamW, RAdam, Lion, and Adan. Across architectures such as ResNet-50, VGG-16, and ViT, NOVAK consistently achieves state-of-the-art accuracy, and exceptional robustness, attaining very high accuracy on VGG-16/ImageNette demonstrating superior architectural robustness compared to contemporary optimizers. The results highlight that NOVAKs architectural contributions (particularly rectification, decoupled decay, and hybrid momentum) are crucial for reliable training of deep plain networks lacking skip connections, addressing a long-standing limitation of existing adaptive optimization methods.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.07876" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.07876" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.07876" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">40. Difficulty-guided Sampling: Bridging the Target Gap between Dataset Distillation and Downstream Tasks
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Mingzhuo Li, Guang Li, Linfeng Ye, Jiafeng Mao, Takahiro Ogawa, Konstantinos N. Plataniotis, Miki Haseyama</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Difficulty-Guided Sampling (DGS) is proposed as a post-stage sampling module for dataset distillation that leverages task-specific difficulty distributions to bridge the target gap between the distillation objective and the downstream task, enhancing overall model performance. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">In this paper, we propose difficulty-guided sampling (DGS) to bridge the target gap between the distillation objective and the downstream task, therefore improving the performance of dataset distillation. Deep neural networks achieve remarkable performance but have time and storage-consuming training processes. Dataset distillation is proposed to generate compact, high-quality distilled datasets, enabling effective model training while maintaining downstream performance. Existing approaches typically focus on features extracted from the original dataset, overlooking task-specific information, which leads to a target gap between the distillation objective and the downstream task. We propose leveraging characteristics that benefit the downstream training into data distillation to bridge this gap. Focusing on the downstream task of image classification, we introduce the concept of difficulty and propose DGS as a plug-in post-stage sampling module. Following the specific target difficulty distribution, the final distilled dataset is sampled from image pools generated by existing methods. We also propose difficulty-aware guidance (DAG) to explore the effect of difficulty in the generation process. Extensive experiments across multiple settings demonstrate the effectiveness of the proposed methods. It also highlights the broader potential of difficulty for diverse downstream tasks.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.10090" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.10090" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.10090" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">41. The Impact of Anisotropic Covariance Structure on the Training Dynamics and Generalization Error of Linear Networks
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Taishi Watanabe, Ryo Karakida, Jun-nosuke Teramae</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Analyzing two-layer linear networks, researchers found that data anisotropy, modeled by a spiked covariance structure, dictates a two-phase learning trajectory and analytically quantifies how alignment with the task improves generalization performance. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The success of deep neural networks largely depends on the statistical structure of the training data. While learning dynamics and generalization on isotropic data are well-established, the impact of pronounced anisotropy on these crucial aspects is not yet fully understood. We examine the impact of data anisotropy, represented by a spiked covariance structure, a canonical yet tractable model, on the learning dynamics and generalization error of a two-layer linear network in a linear regression setting. Our analysis reveals that the learning dynamics proceed in two distinct phases, governed initially by the input-output correlation and subsequently by other principal directions of the data structure. Furthermore, we derive an analytical expression for the generalization error, quantifying how the alignment of the spike structure of the data with the learning task improves performance. Our findings offer deep theoretical insights into how data anisotropy shapes the learning trajectory and final performance, providing a foundation for understanding complex interactions in more advanced network architectures.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.06961" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.06961" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.06961" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">42. Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs without Training
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Shezheng Song, Shasha Li, Jie Yu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The DualPD decoding refinement strategy addresses the &#34;seeing it right but saying it wrong&#34; inconsistency in Multimodal Large Language Models by using layer-wise contrastive logits and head-wise filtering to suppress noisy attention and enhance visual understanding without additional training. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Multimodal Large Language Models (MLLMs) have demonstrated strong capabilities across a variety of vision-language tasks. However, their internal reasoning often exhibits a critical inconsistency: although deeper layers may attend to the correct visual regions, final predictions are frequently misled by noisy attention from earlier layers. This results in a disconnect between what the model internally understands and what it ultimately expresses, a phenomenon we describe as seeing it right but saying it wrong. To address this issue, we propose DualPD, a dual-perspective decoding refinement strategy that enhances the visual understanding without any additional training. DualPD consists of two components. (1) The layer-wise attention-guided contrastive logits module captures how the belief in the correct answer evolves by comparing output logits between layers that exhibit the largest attention shift. (2) The head-wise information filtering module suppresses low-contribution attention heads that focus on irrelevant regions, thereby improving attention quality within each layer. Experiments conducted on both the LLaVA and Qwen-VL model families across multiple multimodal benchmarks demonstrate that DualPD consistently improves accuracy without training, confirming its effectiveness and generalizability. The code will be released upon publication.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.07359" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.07359" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.07359" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">43. Segmental Advantage Estimation: Enhancing PPO for Long-Context LLM Training
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Xue Gong, Qi Yi, Ziyuan Nan, Guanhua Huang, Kejiao Li, Yuhao Jiang, Ruibin Xiong, Zenan Xu, Jiaming Guo, Shaohui Peng, et al.</span>
                                <span class="author-full" style="display: none;">Xue Gong, Qi Yi, Ziyuan Nan, Guanhua Huang, Kejiao Li, Yuhao Jiang, Ruibin Xiong, Zenan Xu, Jiaming Guo, Shaohui Peng, Bo Zhou</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Segmental Advantage Estimation (SAE) improves Reinforcement Learning with Verifiable Rewards (RLVR) for LLMs by mitigating the bias of Generalized Advantage Estimation (GAE) through partitioning sequences and selectively computing advantages only at informative segment boundaries. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Training Large Language Models (LLMs) for reasoning tasks is increasingly driven by Reinforcement Learning with Verifiable Rewards (RLVR), where Proximal Policy Optimization (PPO) provides a principled framework for stable policy updates. However, the practical application of PPO is hindered by unreliable advantage estimation in the sparse-reward RLVR regime. This issue arises because the sparse rewards in RLVR lead to inaccurate intermediate value predictions, which in turn introduce significant bias when aggregated at every token by Generalized Advantage Estimation (GAE). To address this, we introduce Segmental Advantage Estimation (SAE), which mitigates the bias that GAE can incur in RLVR. Our key insight is that aggregating $n$-step advantages at every token(as in GAE) is unnecessary and often introduces excessive bias, since individual tokens carry minimal information. Instead, SAE first partitions the generated sequence into coherent sub-segments using low-probability tokens as heuristic boundaries. It then selectively computes variance-reduced advantage estimates only from these information-rich segment transitions, effectively filtering out noise from intermediate tokens. Our experiments demonstrate that SAE achieves superior performance, with marked improvements in final scores, training stability, and sample efficiency. These gains are shown to be consistent across multiple model sizes, and a correlation analysis confirms that our proposed advantage estimator achieves a higher correlation with an approximate ground-truth advantage, justifying its superior performance.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.07320" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.07320" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.07320" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">44. More power on large scales
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jeremy Mould</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Modeling Primordial Black Holes (PBHs) as early-clustering macroscopic dark matter suggests systematically larger galaxy bulk flow velocities than standard LCDM, while mass loss from Hawking radiation introduces a dynamic density term that potentially alleviates the Hubble tension. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The high value of the cosmic microwave dipole may be telling us that dark matter is macroscopic rather than a fundamental particle. The possible presence of a significant dark matter component in the form of primordial black holes suggests that dark halo formation simulations should be commenced well before redshift z = 100. Unlike standard CDM candidates, PBHs behave as dense, non-relativistic matter from their inception in the radiation-dominated era. This allows them to seed gravitational potential wells and begin clustering earlier. We find that starting N-body simulations at redshifts even before matter-radiation equality yield galaxy bulk flow velocities that are systematically larger than those predicted by standard LCDM models. The early, high-mass concentrations established by PBHs lead to a more rapid and efficient gravitational acceleration of surrounding baryonic and dark matter, generating larger peculiar velocities that remain coherent over scales of hundreds of Mpc. Furthermore, a sub-population of PBHs in the 10^-20 to 10^-17 solar mass range would lose a non-negligible fraction of their mass via Hawking radiation over cosmological timescales. This evaporation process converts matter into radiation, so a time-varying matter density parameter, Omega_m&#39;, is introduced, which behaves like a boosted radiation term in the Friedmann equation. This dynamic term acts to reduce the Hubble tension. A higher effective Omega_r in the early universe reduces the sound horizon at the epoch of recombination. PBH mass loss also influences fits to the equation of state parameter, w, at low redshift. The naive N-body modelling presented here suggests investigation with tried and tested cosmology codes should be carried out, by introducing mass losing PBHs and starting the evolution as early as practicable.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.07106" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.07106" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.07106" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Survey Analysis, Cosmology / Galaxy-halo connection, clustering, lensing</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">45. Annealed Relaxation of Speculative Decoding for Faster Autoregressive Image Generation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Xingyao Li, Fengzhuo Zhang, Cunxiao Du, Hui Ji</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> COOL-SD, an annealed relaxation of speculative decoding, establishes a theoretical foundation using total variation distance and perturbation analysis to optimize resampling distributions, achieving superior speed-quality trade-offs in autoregressive image generation. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Despite significant progress in autoregressive image generation, inference remains slow due to the sequential nature of AR models and the ambiguity of image tokens, even when using speculative decoding. Recent works attempt to address this with relaxed speculative decoding but lack theoretical grounding. In this paper, we establish the theoretical basis of relaxed SD and propose COOL-SD, an annealed relaxation of speculative decoding built on two key insights. The first analyzes the total variation (TV) distance between the target model and relaxed speculative decoding and yields an optimal resampling distribution that minimizes an upper bound of the distance. The second uses perturbation analysis to reveal an annealing behaviour in relaxed speculative decoding, motivating our annealed design. Together, these insights enable COOL-SD to generate images faster with comparable quality, or achieve better quality at similar latency. Experiments validate the effectiveness of COOL-SD, showing consistent improvements over prior methods in speed-quality trade-offs.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.09212" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.09212" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.09212" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">46. MMR-GRPO: Accelerating GRPO-Style Training through Diversity-Aware Reward Reweighting
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Kangda Wei, Ruihong Huang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Integrating Maximal Marginal Relevance (MMR) into Group Relative Policy Optimization (GRPO) accelerates mathematical reasoning model training by prioritizing diverse completions, thereby reducing training steps and wall-clock time by filtering out semantically redundant learning signals. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Group Relative Policy Optimization (GRPO) has become a standard approach for training mathematical reasoning models; however, its reliance on multiple completions per prompt makes training computationally expensive. Although recent work has reduced the number of training steps required to reach peak performance, the overall wall-clock training time often remains unchanged or even increases due to higher per-step cost. We propose MMR-GRPO, which integrates Maximal Marginal Relevance to reweigh rewards based on completion diversity. Our key insight is that semantically redundant completions contribute limited marginal learning signal; prioritizing diverse solutions yields more informative updates and accelerates convergence. Extensive evaluations across three model sizes (1.5B, 7B, 8B), three GRPO variants, and five mathematical reasoning benchmarks show that MMR-GRPO achieves comparable peak performance while requiring on average 47.9% fewer training steps and 70.2% less wall-clock time. These gains are consistent across models, methods, and benchmarks. We will release our code, trained models, and experimental protocols.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.09085" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.09085" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.09085" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">47. Few-shot Class-Incremental Learning via Generative Co-Memory Regularization
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Kexin Bao, Yong Li, Dan Zeng, Shiming Ge</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A generative co-memory regularization approach enhances Few-Shot Class-Incremental Learning (FSCIL) by using generative domain adaptation to learn robust representations and employing two collaboratively updated class-wise memories to stabilize incremental learning and prevent catastrophic forgetting. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Few-shot class-incremental learning (FSCIL) aims to incrementally learn models from a small amount of novel data, which requires strong representation and adaptation ability of models learned under few-example supervision to avoid catastrophic forgetting on old classes and overfitting to novel classes. This work proposes a generative co-memory regularization approach to facilitate FSCIL. In the approach, the base learning leverages generative domain adaptation finetuning to finetune a pretrained generative encoder on a few examples of base classes by jointly incorporating a masked autoencoder (MAE) decoder for feature reconstruction and a fully-connected classifier for feature classification, which enables the model to efficiently capture general and adaptable representations. Using the finetuned encoder and learned classifier, we construct two class-wise memories: representation memory for storing the mean features for each class, and weight memory for storing the classifier weights. After that, the memory-regularized incremental learning is performed to train the classifier dynamically on the examples of few-shot classes in each incremental session by simultaneously optimizing feature classification and co-memory regularization. The memories are updated in a class-incremental manner and they collaboratively regularize the incremental learning. In this way, the learned models improve recognition accuracy, while mitigating catastrophic forgetting over old classes and overfitting to novel classes. Extensive experiments on popular benchmarks clearly demonstrate that our approach outperforms the state-of-the-arts.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.07117" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.07117" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.07117" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">48. Analyzing intermittent stochastic gravitational wave background I:Effect of detector response
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Xiaolin Liu, Sachiko Kuroyanagi</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Accurate detection of the non-Gaussian gravitational-wave background necessitates incorporating the detector antenna pattern, which is achieved through a computationally feasible method using second-order corrections to eliminate biases introduced by sky-averaged response functions. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">With the growing number of gravitational-wave detections, particularly from binary black hole mergers, there is increasing anticipation that an astrophysical background, formed by an ensemble of faint, high-redshift events, will be observed in the near future by the ground-based detector network. This background is anticipated to exhibit non-Gaussian statistical properties. To develop a robust method for detecting such a non-Gaussian gravitational-wave background, we revisit optimal detection strategies based on the Gaussian-mixture likelihood model. In this work, we demonstrate that properly accounting for the detector antenna pattern is essential. Current approaches typically rely on the overlap reduction function averaged over the sky. Through simulations, we show that using such an averaged response introduces significant biases in parameter estimation. In addition, we propose a computationally feasible method that incorporates second-order corrections as an approximation of the full integral over the source distribution. Our results indicate that this approach effectively eliminates these biases. We also show that our method remains robust even when considering anisotropic backgrounds.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.10428" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.10428" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.10428" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">49. Divide and Conquer: Static-Dynamic Collaboration for Few-Shot Class-Incremental Learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Kexin Bao, Daichi Zhang, Yong Li, Dan Zeng, Shiming Ge</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The Static-Dynamic Collaboration (SDC) framework resolves the stability-plasticity trade-off in Few-Shot Class-Incremental Learning (FSCIL) by separating the process into a static stage for retaining fundamental knowledge and a dynamic stage utilizing an auxiliary projector for continuous adaptation. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Few-shot class-incremental learning (FSCIL) aims to continuously recognize novel classes under limited data, which suffers from the key stability-plasticity dilemma: balancing the retention of old knowledge with the acquisition of new knowledge. To address this issue, we divide the task into two different stages and propose a framework termed Static-Dynamic Collaboration (SDC) to achieve a better trade-off between stability and plasticity. Specifically, our method divides the normal pipeline of FSCIL into Static Retaining Stage (SRS) and Dynamic Learning Stage (DLS), which harnesses old static and incremental dynamic class information, respectively. During SRS, we train an initial model with sufficient data in the base session and preserve the key part as static memory to retain fundamental old knowledge. During DLS, we introduce an extra dynamic projector jointly trained with the previous static memory. By employing both stages, our method achieves improved retention of old knowledge while continuously adapting to new classes. Extensive experiments on three public benchmarks and a real-world application dataset demonstrate that our method achieves state-of-the-art performance against other competitors.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.08448" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.08448" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.08448" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">50. Towards A Unified PAC-Bayesian Framework for Norm-based Generalization Bounds
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Xinping Yi, Gaojie Jin, Xiaowei Huang, Shi Jin</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A unified PAC-Bayesian framework derives structure-aware generalization bounds by modeling the derivation as a stochastic optimization over anisotropic Gaussian posteriors, leveraging a sensitivity matrix to explicitly incorporate architectural properties and heterogeneous parameter sensitivities. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Understanding the generalization behavior of deep neural networks remains a fundamental challenge in modern statistical learning theory. Among existing approaches, PAC-Bayesian norm-based bounds have demonstrated particular promise due to their data-dependent nature and their ability to capture algorithmic and geometric properties of learned models. However, most existing results rely on isotropic Gaussian posteriors, heavy use of spectral-norm concentration for weight perturbations, and largely architecture-agnostic analyses, which together limit both the tightness and practical relevance of the resulting bounds. To address these limitations, in this work, we propose a unified framework for PAC-Bayesian norm-based generalization by reformulating the derivation of generalization bounds as a stochastic optimization problem over anisotropic Gaussian posteriors. The key to our approach is a sensitivity matrix that quantifies the network outputs with respect to structured weight perturbations, enabling the explicit incorporation of heterogeneous parameter sensitivities and architectural structures. By imposing different structural assumptions on this sensitivity matrix, we derive a family of generalization bounds that recover several existing PAC-Bayesian results as special cases, while yielding bounds that are comparable to or tighter than state-of-the-art approaches. Such a unified framework provides a principled and flexible way for geometry-/structure-aware and interpretable generalization analysis in deep learning.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.08100" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.08100" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.08100" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Statistical Inference, Cosmological Modeling / Representation, Generative Models, Inference</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
    </div>

    <footer class="footer">
        &copy; 2026 arXiv Daily · Powered by <a href="https://arxiv.org/" target="_blank">arXiv</a>
    </footer>
</body>
</html>