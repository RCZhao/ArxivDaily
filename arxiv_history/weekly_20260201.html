<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>arXiv Daily: Backfill (2026-01-25 to 2026-02-01)</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto:400,700&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Roboto', Arial, sans-serif; background: #f8f9fa; transition: background 0.3s; }
        .arxiv-card { margin: 2em auto; max-width: 900px; box-shadow: 0 2px 8px rgba(0,0,0,0.08); border-radius: 16px; transition: box-shadow 0.3s, transform 0.3s; }
        .arxiv-card:hover { box-shadow: 0 4px 12px rgba(0,0,0,0.1); transform: translateY(-1px); }
        .arxiv-title { background: linear-gradient(90deg, #0d6efd 60%, #6610f2 100%); color: #fff; border-radius: 16px 16px 0 0; padding: 1.5em; font-size: 1.5em; font-weight: 700; letter-spacing: 0.5px;}
        .arxiv-meta { font-size: 1em; color: #555; margin-bottom: 1em; }
        .arxiv-abstract { background: #fff; padding: 1.5em; border-radius: 0 0 16px 16px; font-size: 1.1em; line-height: 1.7;}
        .arxiv-links a { margin-right: 1em; }
        .arxiv-scores { margin-top: 1em; font-size: 1em; }
        .navbar { background: #fff; box-shadow: 0 2px 8px rgba(0,0,0,0.04);}
        .footer { text-align: center; color: #888; padding: 2em 0 1em 0; font-size: 0.95em;}
        @media (prefers-color-scheme: dark) {
            body { background: #181a1b; color: #e4e6eb; }
            .arxiv-card { background: #23272b; box-shadow: 0 2px 8px rgba(0,0,0,0.28);}
            .arxiv-card:hover { box-shadow: 0 5px 15px rgba(0,0,0,0.25); transform: translateY(-1px); }
            .arxiv-title { background: linear-gradient(90deg, #375a7f 60%, #6f42c1 100%);}
            .arxiv-abstract { background: #23272b; color: #e4e6eb;}
            .navbar { background: #23272b; color: #e4e6eb;}
            .text-muted { color: #adb5bd !important; }
        }
        /* Improved abstract readability */
        .arxiv-abstract-text {
            font-size: 1.1em;
            line-height: 1.7;
        }
        p.big {
            line-height: 1.6;
            font-size: large;
        }
    </style>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'] ],
          processEscapes: true
        }
      });
    </script>
    <script>
    function toggleAuthors(element) {
        const fullList = element.querySelector('.author-full');
        const shortList = element.querySelector('.author-short');
        const toggleText = element.querySelector('.author-toggle');
        
        fullList.style.display = fullList.style.display === 'none' ? 'inline' : 'none';
        shortList.style.display = shortList.style.display === 'none' ? 'inline' : 'none';
        toggleText.textContent = fullList.style.display === 'none' ? ' (show all)' : ' (show less)';
    }
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>
<body>
    <nav class="navbar navbar-expand-lg mb-4">
        <div class="container">
            <a class="navbar-brand fw-bold text-primary" href="#">arXiv Daily</a>
            <span class="navbar-text">Modern arXiv Reader</span>
        </div>
    </nav>

    
    <div class="container py-4">
        <header class="mb-4"><h2 class="display-6 text-center">Analysis of Your Favorites</h2></header>
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Word Cloud</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/word_cloud.png" class="img-fluid rounded" alt="Word Cloud of favorite paper topics">
                    </div>
                </div>
            </div>
        </div>
        
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Clusters</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/backfill_cluster_map.png" class="img-fluid rounded" alt="2D Cluster plot of favorite papers">
                    </div>
                </div>
            </div>
        </div>
        
    </div>
    

    <div class="container py-4">
        <header class="mb-4"><h1 class="display-5 text-center text-primary">arXiv: Backfill (2026-01-25 to 2026-02-01)</h1></header>

        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Score Distribution of All Papers Found Today</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/score_distribution.png" class="img-fluid rounded" alt="Score distribution of all papers found today">
                    </div>
                </div>
            </div>
        </div>
        

        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">1. Euclid preparation. Galaxy power spectrum modelling in redshift space
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Euclid Collaboration, B. Camacho Quevedo, M. Crocce, M. Pellejero Ibañez, R. E. Angulo, A. Pezzotta, A. Eggemeier, G. Gambardella, C. Moretti, E. Sefusatti, et al.</span>
                                <span class="author-full" style="display: none;">Euclid Collaboration, B. Camacho Quevedo, M. Crocce, M. Pellejero Ibañez, R. E. Angulo, A. Pezzotta, A. Eggemeier, G. Gambardella, C. Moretti, E. Sefusatti, A. Moradinezhad Dizgah, M. Zennaro, M. -A. Breton, A. Chudaykin, G. D&#39;Amico, V. Desjacques, S. de la Torre, M. Guidi, M. Kärcher, K. Pardede, C. Porciani, A. Pugno, J. Salvalaggio, E. Sarpa, A. Veropalumbo, B. Altieri, S. Andreon, N. Auricchio, C. Baccigalupi, M. Baldi, S. Bardelli, R. Bender, A. Biviano, E. Branchini, M. Brescia, S. Camera, V. Capobianco, C. Carbone, V. F. Cardone, J. Carretero, S. Casas, F. J. Castander, M. Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C. Colodro-Conde, G. Congedo, L. Conversi, Y. Copin, F. Courbin, H. M. Courtois, H. Degaudenzi, G. De Lucia, H. Dole, M. Douspis, F. Dubath, C. A. J. Duncan, X. Dupac, S. Dusini, S. Escoffier, M. Farina, R. Farinelli, S. Farrens, S. Ferriol, F. Finelli, S. Fotopoulou, N. Fourmanoit, M. Frailis, E. Franceschi, M. Fumana, S. Galeotta, K. George, B. Gillis, C. Giocoli, J. Gracia-Carpio, A. Grazian, F. Grupp, L. Guzzo, S. V. H. Haugan, W. Holmes, F. Hormuth, A. Hornstrup, K. Jahnke, B. Joachimi, S. Kermiche, A. Kiessling, B. Kubik, M. Kümmel, M. Kunz, H. Kurki-Suonio, A. M. C. Le Brun, S. Ligori, P. B. Lilje, V. Lindholm, I. Lloro, G. Mainetti, E. Maiorano, O. Mansutti, S. Marcin, O. Marggraf, M. Martinelli, N. Martinet, F. Marulli, R. J. Massey, E. Medinaceli, M. Melchior, M. Meneghetti, E. Merlin, G. Meylan, A. Mora, M. Moresco, L. Moscardini, C. Neissner, S. -M. Niemi, C. Padilla, S. Paltani, F. Pasian, K. Pedersen, W. J. Percival, V. Pettorino, S. Pires, G. Polenta, M. Poncet, L. A. Popa, F. Raison, J. Rhodes, G. Riccio, F. Rizzo, E. Romelli, M. Roncarelli, R. Saglia, Z. Sakr, A. G. Sánchez, D. Sapone, B. Sartoris, P. Schneider, A. Secroun, G. Seidel, E. Sihvola, P. Simon, C. Sirignano, G. Sirri, A. Spurio Mancini, L. Stanco, P. Tallada-Crespí, D. Tavagnacco, A. N. Taylor, I. Tereno, N. Tessore, S. Toft, R. Toledo-Moreo, F. Torradeflot, I. Tutusaus, J. Valiviita, T. Vassallo, Y. Wang, J. Weller, G. Zamorani, F. M. Zerbi, E. Zucca, V. Allevato, M. Ballardini, A. Boucaud, E. Bozzo, C. Burigana, R. Cabanac, M. Calabrese, A. Cappi, T. Castro, J. A. Escartin Vigo, L. Gabarra, J. Macias-Perez, R. Maoli, J. Martín-Fleitas, N. Mauri, R. B. Metcalf, P. Monaco, A. A. Nucita, M. Pöntinen, I. Risso, V. Scottez, M. Sereno, M. Tenti, M. Tucci, M. Viel, M. Wiesmann, Y. Akrami, I. T. Andika, G. Angora, M. Archidiacono, F. Atrio-Barandela, L. Bazzanini, J. Bel, D. Bertacca, M. Bethermin, A. Blanchard, L. Blot, H. Böhringer, S. Borgani, M. L. Brown, S. Bruton, A. Calabro, F. Caro, C. S. Carvalho, F. Cogato, A. R. Cooray, S. Davini, F. De Paolis, G. Desprez, A. Díaz-Sánchez, S. Di Domizio, J. M. Diego, V. Duret, M. Y. Elkhashab, A. Enia, Y. Fang, A. G. Ferrari, A. Finoguenov, A. Fontana, F. Fontanot, A. Franco, K. Ganga, T. Gasparetto, E. Gaztanaga, F. Giacomini, F. Gianotti, G. Gozaliasl, A. Gruppuso, C. M. Gutierrez, A. Hall, C. Hernández-Monteagudo, H. Hildebrandt, J. Hjorth, J. J. E. Kajava, Y. Kang, V. Kansal, D. Karagiannis, K. Kiiveri, J. Kim, C. C. Kirkpatrick, S. Kruk, L. Legrand, M. Lembo, F. Lepori, G. Leroy, G. F. Lesci, J. Lesgourgues, T. I. Liaudat, M. Magliocchetti, F. Mannucci, C. J. A. P. Martins, L. Maurin, M. Miluzio, A. Montoro, G. Morgante, S. Nadathur, K. Naidoo, A. Navarro-Alsina, S. Nesseris, L. Pagano, D. Paoletti, F. Passalacqua, K. Paterson, L. Patrizii, A. Pisani, D. Potter, G. W. Pratt, S. Quai, M. Radovich, K. Rojas, W. Roster, S. Sacquegna, M. Sahlén, D. B. Sanders, A. Schneider, D. Sciotti, E. Sellentin, L. C. Smith, K. Tanidis, C. Tao, F. Tarsitano, G. Testera, R. Teyssier, S. Tosi, A. Troja, D. Vergani, F. Vernizzi, G. Verza, P. Vielzeuf, S. Vinciguerra, N. A. Walton, A. H. Wright</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The velocity difference generator (${\rm VDG_ \infty}$) model significantly improves cosmological parameter constraints compared to effective field theory (EFT) by accurately treating small-scale redshift-space distortions, making it the most effective perturbation theory approach for analyzing Euclid H$\alpha$ galaxy samples. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Accurate modelling of redshift-space distortions (RSD) is essential for maximizing the cosmological information extracted from large galaxy redshift surveys. In preparation for the forthcoming analysis of the Euclid spectroscopic data, we investigate three approaches to modelling RSD effects on the power spectrum multipoles of mock H$α$ emission line galaxies. We focus on two one-loop perturbation theory models -- the effective field theory (EFT) and velocity difference generator (${\rm VDG_ \infty}$) -- which differ in their treatment of the real-to-redshift space mapping on small scales, and a third approach, the BACCO emulator, which adopts a hybrid strategy combining perturbation theory with high-resolution N-body simulations. We assess the ability of these models to recover key cosmological parameters, including the expansion rate $h$, the cold dark matter density parameter $ω_{\rm c}$, and the scalar amplitude $A_{\rm s}$, across four redshift bins spanning $0.9 \leq z \leq 1.8$. In each bin, we find that ${\rm VDG_ \infty}$ and BACCO outperform the EFT model across all scales up to $k_{max} \lesssim 0.35 h\,Mpc^{-1} $. While BACCO saturates in constraining power at intermediate scales and higher redshift, the ${\rm VDG_ \infty}$ model continues to improve parameter constraints beyond $k_{max} \gtrsim 0.30 h\,Mpc^{-1}$. The EFT model, although robust on large scales, exhibits significant parameter biases for $k_{max} \gtrsim 0.25 h\,Mpc^{-1}$, limiting its applicability to Euclid-like H$α$ samples. Among the full perturbation theory-based models, the enhanced treatment of small-scale RSD effects in ${\rm VDG_ \infty}$ improves cosmological parameter constraints by up to a factor of two.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.20826" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.20826" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.20826" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 14.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.98</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmological Constraints</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">2. Euclid preparation. Decomposing components of the extragalactic background light using multi-band intensity mapping cross-correlations
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Euclid Collaboration, Y. Cao, A. R. Cooray, T. Li, Y. -T. Cheng, K. Tanidis, S. H. Lim, D. Scott, B. Altieri, A. Amara, et al.</span>
                                <span class="author-full" style="display: none;">Euclid Collaboration, Y. Cao, A. R. Cooray, T. Li, Y. -T. Cheng, K. Tanidis, S. H. Lim, D. Scott, B. Altieri, A. Amara, S. Andreon, N. Auricchio, C. Baccigalupi, M. Baldi, S. Bardelli, A. Biviano, E. Branchini, M. Brescia, S. Camera, G. Cañas-Herrera, V. Capobianco, C. Carbone, J. Carretero, S. Casas, M. Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C. Colodro-Conde, G. Congedo, C. J. Conselice, L. Conversi, Y. Copin, F. Courbin, H. M. Courtois, J. -C. Cuillandre, H. Degaudenzi, G. De Lucia, H. Dole, M. Douspis, F. Dubath, X. Dupac, M. Farina, R. Farinelli, F. Faustini, S. Ferriol, F. Finelli, M. Frailis, E. Franceschi, M. Fumana, S. Galeotta, K. George, B. Gillis, C. Giocoli, J. Gracia-Carpio, A. Grazian, F. Grupp, S. V. H. Haugan, W. Holmes, F. Hormuth, A. Hornstrup, K. Jahnke, M. Jhabvala, B. Joachimi, S. Kermiche, A. Kiessling, B. Kubik, M. Kunz, H. Kurki-Suonio, A. M. C. Le Brun, S. Ligori, P. B. Lilje, V. Lindholm, I. Lloro, G. Mainetti, D. Maino, E. Maiorano, O. Mansutti, S. Marcin, O. Marggraf, M. Martinelli, N. Martinet, F. Marulli, R. J. Massey, E. Medinaceli, S. Mei, Y. Mellier, M. Meneghetti, E. Merlin, G. Meylan, A. Mora, M. Moresco, L. Moscardini, C. Neissner, S. -M. Niemi, C. Padilla, S. Paltani, F. Pasian, K. Pedersen, W. J. Percival, V. Pettorino, G. Polenta, M. Poncet, L. A. Popa, F. Raison, A. Renzi, J. Rhodes, G. Riccio, E. Romelli, M. Roncarelli, R. Saglia, Z. Sakr, D. Sapone, P. Schneider, T. Schrabback, A. Secroun, G. Seidel, S. Serrano, E. Sihvola, C. Sirignano, G. Sirri, L. Stanco, J. Steinwagner, P. Tallada-Crespí, I. Tereno, N. Tessore, S. Toft, R. Toledo-Moreo, F. Torradeflot, I. Tutusaus, L. Valenziano, J. Valiviita, T. Vassallo, Y. Wang, J. Weller, G. Zamorani, F. M. Zerbi, E. Zucca, M. Ballardini, E. Bozzo, C. Burigana, R. Cabanac, M. Calabrese, A. Cappi, T. Castro, J. A. Escartin Vigo, L. Gabarra, J. Macias-Perez, R. Maoli, J. Martín-Fleitas, N. Mauri, R. B. Metcalf, P. Monaco, A. A. Nucita, A. Pezzotta, M. Pöntinen, I. Risso, V. Scottez, M. Sereno, M. Tenti, M. Tucci, M. Viel, M. Wiesmann, Y. Akrami, I. T. Andika, G. Angora, S. Anselmi, M. Archidiacono, E. Aubourg, L. Bazzanini, D. Bertacca, M. Bethermin, A. Blanchard, L. Blot, M. Bonici, S. Borgani, M. L. Brown, S. Bruton, A. Calabro, B. Camacho Quevedo, F. Caro, C. S. Carvalho, F. Cogato, S. Conseil, O. Cucciati, S. Davini, G. Desprez, A. Díaz-Sánchez, S. Di Domizio, J. M. Diego, V. Duret, M. Y. Elkhashab, A. Enia, A. Finoguenov, A. Fontana, A. Franco, K. Ganga, T. Gasparetto, E. Gaztanaga, F. Giacomini, F. Gianotti, G. Gozaliasl, A. Gruppuso, M. Guidi, C. M. Gutierrez, A. Hall, C. Hernández-Monteagudo, H. Hildebrandt, J. Hjorth, J. J. E. Kajava, Y. Kang, V. Kansal, D. Karagiannis, K. Kiiveri, J. Kim, C. C. Kirkpatrick, S. Kruk, M. Lattanzi, L. Legrand, F. Lepori, G. Leroy, G. F. Lesci, J. Lesgourgues, T. I. Liaudat, S. J. Liu, M. Magliocchetti, F. Mannucci, C. J. A. P. Martins, L. Maurin, M. Miluzio, C. Moretti, G. Morgante, K. Naidoo, P. Natoli, A. Navarro-Alsina, S. Nesseris, L. Pagano, D. Paoletti, F. Passalacqua, K. Paterson, L. Patrizii, A. Pisani, D. Potter, G. W. Pratt, S. Quai, M. Radovich, G. Rodighiero, K. Rojas, W. Roster, S. Sacquegna, M. Sahlén, D. B. Sanders, E. Sarpa, C. Scarlata, A. Schneider, D. Sciotti, E. Sellentin, L. C. Smith, J. G. Sorce, F. Tarsitano, G. Testera, R. Teyssier, S. Tosi, A. Troja, A. Venhola, D. Vergani, G. Verza, S. Vinciguerra, N. A. Walton, J. R. Weaver, A. H. Wright</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Combining multi-band intensity mapping with cosmic shear and galaxy clustering within a joint halo-model framework allows for robust decomposition of Extragalactic Background Light components, significantly tightening constraints on intra-halo light properties and extending star-formation rate density measurements up to $z\sim 11$ compared to EBL-only analyses. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The extragalactic background light (EBL) fluctuations in the optical/near-IR encode the cumulative integrated galaxy light (IGL), diffuse intra-halo light (IHL), and high-$z$ sources from the epoch of reionisation (EoR), but they are difficult to disentangle with auto-spectra alone. We aim to decompose the EBL into its principal constituents using multi-band intensity mapping combined with cosmic shear and galaxy clustering. We develop a joint halo-model framework in which IHL follows a mass- and redshift-dependent luminosity scaling, IGL is set by an evolving Schechter luminosity function, and EoR emission is modelled with Pop II/III stellar emissivities and a binned star-formation efficiency. Using mock surveys in a flat $Λ$CDM cosmology with ten spectral bands spanning 0.75-5.0$\rm μm$ in the NEP deep fields over about 100$°^2$ with source detections down to AB=20.5 for masking, and six redshift bins to $z=2.5$, we fit auto- and cross-power spectra using a MCMC method. The combined SPHEREx$\times$Euclid analysis recovers all fiducial parameters within 1$σ$ and reduces 1$σ$ uncertainties on IHL parameters by 10-35% relative to SPHEREx EBL-only, while EoR star-formation efficiency parameters improve by 20-35%. Cross-correlations reveal a stronger coupling of IHL than IGL to the shear field, enhancing component separation; conversely, the EoR contribution shows negligible correlation with cosmic shear and galaxy clustering, aiding its isolation in the EBL. Relative to the SPHEREx EBL-only case, the inferred IHL fraction as a function of halo mass is significantly tightened over $10^{11}-10^{14} M_{\odot}$, with uncertainties reduced by 5-30%, and the resulting star-formation rate density constraints extend to $z\sim 11$, with uncertainty reductions of 22-31%.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21111" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21111" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21111" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 14.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.97</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmological Constraints</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">3. Euclid: Early Release Observations -- The star-formation history of massive early-type galaxies in the Perseus cluster
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">S. Martocchia, A. Boselli, J. -C. Cuillandre, M. Mondelin, M. Bolzonella, C. Tortora, M. Fossati, C. Maraston, P. Amram, M. Baes, et al.</span>
                                <span class="author-full" style="display: none;">S. Martocchia, A. Boselli, J. -C. Cuillandre, M. Mondelin, M. Bolzonella, C. Tortora, M. Fossati, C. Maraston, P. Amram, M. Baes, S. Boissier, M. Boquien, H. Bouy, F. Durret, C. M. Gutierrez, M. Kluge, Y. Roehlly, T. Saifollahi, M. A. Taylor, D. Thomas, T. E. Woods, G. Zamorani, B. Altieri, S. Andreon, N. Auricchio, C. Baccigalupi, M. Baldi, S. Bardelli, P. Battaglia, R. Bender, A. Biviano, E. Branchini, M. Brescia, S. Camera, G. Cañas-Herrera, V. Capobianco, C. Carbone, J. Carretero, M. Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C. Colodro-Conde, G. Congedo, C. J. Conselice, L. Conversi, Y. Copin, A. Costille, F. Courbin, H. M. Courtois, H. Degaudenzi, G. De Lucia, F. Dubath, X. Dupac, S. Escoffier, M. Fabricius, M. Farina, R. Farinelli, F. Faustini, S. Ferriol, F. Finelli, M. Frailis, E. Franceschi, P. Franzetti, M. Fumana, S. Galeotta, K. George, B. Gillis, C. Giocoli, P. Gómez-Alvarez, J. Gracia-Carpio, A. Grazian, F. Grupp, S. V. H. Haugan, W. Holmes, I. M. Hook, F. Hormuth, A. Hornstrup, K. Jahnke, M. Jhabvala, B. Joachimi, S. Kermiche, A. Kiessling, B. Kubik, M. Kümmel, H. Kurki-Suonio, A. M. C. Le Brun, D. Le Mignant, S. Ligori, P. B. Lilje, V. Lindholm, I. Lloro, G. Mainetti, D. Maino, O. Mansutti, O. Marggraf, M. Martinelli, N. Martinet, F. Marulli, R. J. Massey, E. Medinaceli, Y. Mellier, M. Meneghetti, E. Merlin, G. Meylan, A. Mora, L. Moscardini, C. Neissner, S. -M. Niemi, C. Padilla, S. Paltani, F. Pasian, K. Pedersen, W. J. Percival, V. Pettorino, S. Pires, G. Polenta, M. Poncet, L. A. Popa, L. Pozzetti, A. Renzi, J. Rhodes, G. Riccio, E. Romelli, M. Roncarelli, R. Saglia, Z. Sakr, A. G. Sánchez, D. Sapone, B. Sartoris, P. Schneider, A. Secroun, G. Seidel, S. Serrano, E. Sihvola, P. Simon, C. Sirignano, G. Sirri, J. Steinwagner, P. Tallada-Crespí, A. N. Taylor, I. Tereno, N. Tessore, S. Toft, R. Toledo-Moreo, F. Torradeflot, I. Tutusaus, L. Valenziano, J. Valiviita, T. Vassallo, A. Veropalumbo, Y. Wang, J. Weller, I. A. Zinchenko, E. Zucca, J. García-Bellido, J. Martín-Fleitas, M. Maturi, V. Scottez</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Analysis of early-type galaxies in the Euclid ERO-Perseus cluster reveals that their star-formation histories are characterized by rapid mass assembly and a necessary UV upturn component, indicating that the most massive local ETGs are likely the descendants of the red quiescent galaxies observed by JWST at $z&gt;4.6$. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The Euclid Early Release Observations (ERO) programme targeted the Perseus galaxy cluster in its central region over 0.7deg$^2$. We combined the exceptional image quality and depth of the ERO-Perseus with FUV and NUV observations from GALEX and AstroSat/UVIT, as well as $ugrizHα$ data from MegaCam at the CFHT, to deliver FUV-to-NIR magnitudes of the 87 brightest galaxies within the Perseus cluster. We reconstructed the star-formation history (SFH) of 59 early-type galaxies (ETGs) within the sample, through the spectral energy distribution (SED) fitting code CIGALE and state-of-the-art stellar population (SP) models to reproduce the galactic UV emission from hot, old, low-mass stars (i.e. the UV upturn). In addition, for the six most massive ETGs in Perseus [stellar masses $\log_{10}(M_{\ast}/M_{\odot}) \geq 10.3$], we analysed their spatially resolved SP through a radial SED fitting. In agreement with our previous work on Virgo ETGs, we found that (i) the majority of ETGs needs the presence of an UV upturn to explain their FUV emission, with temperatures $\langle T_{\rm UV}\rangle$~33800 K; (ii) ETGs have grown their stellar masses quickly, with SF timescales $τ\lesssim 1500$ Myr. We found that all ETGs in the sample have formed more than about 30% of their stellar masses at z~5, up to ~100%. At z~5, the stellar masses of the most massive nearby ETGs, which have present-day stellar masses $\log_{10}(M_{\ast}/M_{\odot})\gtrsim 10.8$, are then found to be comparable to those of the red quiescent galaxies observed by JWST at similar redshifts (z&gt;4.6). This study can be extended to ETGs in the 14000 deg$^2$ extragalactic sky that will soon be observed by Euclid, in combination with those from other major upcoming surveys (e.g. Rubin/LSST), and UV observations, to ultimately assess whether the nearby massive ETGs represent the progeny of the massive high-z JWST red quiescent galaxies.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.20948" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.20948" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.20948" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 13.9</span>
                        <span class="badge bg-info text-dark">Author Score: 0.94</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmological Constraints / Galaxy-Halo Connection, Lensing, Clustering</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">4. First Observational Evidence for Split Infall Flow of Cosmic Filaments into Clusters
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Ji Yao, Huanyuan Shan, Pengjie Zhang, Xiaohu Yang, Jiale Zhou, Jiaxin Han, Peng Wang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Observational analysis of spectroscopic galaxies in the Sloan Digital Sky Survey reveals that cosmic web filaments connecting galaxy clusters exhibit a significant split infall pattern, characterized by opposing velocity flows toward each cluster, demonstrating that these structures dynamically respond to competing gravitational potentials. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Velocity fields in the cosmic web are fundamental to structure formation but remain difficult to observe directly beyond the linear regime. Here we present observational evidence that galaxy filaments connecting pairs of galaxy clusters undergo a split infall, with opposite velocity flows toward the two clusters. Using spectroscopic galaxies from the Sloan Digital Sky Survey, we isolate the internal filament velocity field by subtracting its rigid-body background motion and Hubble flow, and detect this effect at greater than $5σ$ significance across a wide range of cluster and filament selections. The measured velocity profile exhibits a sign reversal near the filament midpoint and a maximum infall amplitude of $\sim30$ km/s ($\sim20$ km/s projected onto the line-of-sight) for clusters of mass $\sim10^{14.3}M_\odot$, substantially lower than expected for infall from an average cosmic environment. Multiple results on density-velocity correlation, mass-dependency, and validation with simulation indicate that filaments dynamically respond to competing gravitational potentials rather than acting as passive mass transport channels. Our results establish a new observational window on quasi-linear velocity fields in the cosmic web and provide a promising probe of mass measurement, testing gravity and velocity reconstruction with upcoming wide-field spectroscopic surveys.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.18434" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.18434" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.18434" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 11.1</span>
                        <span class="badge bg-info text-dark">Author Score: 0.37</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">5. Cosmological analysis of the DESI DR1 Lyman alpha 1D power spectrum
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">J. Chaves-Montero, A. Font-Ribera, P. McDonald, E. Armengaud, D. Chebat, C. Garcia-Quintero, N. G. Karaçaylı, C. Ravoux, S. Satyavolu, N. Schöneberg, et al.</span>
                                <span class="author-full" style="display: none;">J. Chaves-Montero, A. Font-Ribera, P. McDonald, E. Armengaud, D. Chebat, C. Garcia-Quintero, N. G. Karaçaylı, C. Ravoux, S. Satyavolu, N. Schöneberg, M. Walther, J. Aguilar, S. Ahlen, S. Bailey, D. Bianchi, D. Brooks, T. Claybaugh, A. Cuceu, A. de la Macorra, P. Doel, S. Ferraro, J. E. Forero-Romero, E. Gaztañaga, S. Gontcho A Gontcho, A. X. Gonzalez-Morales, G. Gutierrez, J. Guy, C. Hahn, H. K. Herrera-Alcantar, K. Honscheid, M. Ishak, R. Joyce, S. Juneau, D. Kirkby, A. Kremin, O. Lahav, C. Lamman, M. Landriau, J. M. Le Goff, L. Le Guillou, A. Leauthaud, M. E. Levi, M. Manera, P. Martini, A. Meisner, R. Miquel, J. Moustakas, S. Nadathur, G. Niz, N. Palanque-Delabrouille, W. J. Percival, F. Prada, I. Pérez-Ràfols, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, H. Seo, J. Silber, D. Sprayberry, T. Tan, G. Tarlé, B. A. Weaver, C. Yèche, R. Zhou, H. Zou</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Cosmological analysis utilizing the one-dimensional Lyman-$\alpha$ flux power spectrum from DESI DR1, combined with BAO and CMB measurements, yields precise constraints on the linear matter power spectrum at $z=3$ and significantly tightens limits on parameters like the effective number of relativistic species and the running of the spectral index in extended $\Lambda$CDM models. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present the cosmological analysis of the one-dimensional Lyman-$α$ flux power spectrum from the first data release of the Dark Energy Spectroscopic Instrument (DESI). We capture the dependence of the signal on cosmology and intergalactic medium physics using an emulator trained on a cosmological suite of hydrodynamical simulations, and we correct its predictions for the impact of astrophysical contaminants and systematics, many of these not considered in previous analyses. We employ this framework to constrain the amplitude and logarithmic slope of the linear matter power spectrum at $k_\star=0.009\,\mathrm{km^{-1}s}$ and redshift $z=3$, obtaining $Δ^2_\star=0.379\pm0.032$ and $n_\star=-2.309\pm0.019$. The robustness of these constraints is validated through the analysis of mocks and a large number of alternative data analysis variations, with cosmological parameters kept blinded throughout the validation process. We then combine our results with constraints from DESI BAO and temperature, polarization, and lensing measurements from Planck, ACT, and SPT-3G to set constraints on $Λ$CDM extensions. While our measurements do not significantly tighten the limits on the sum of neutrino masses from the combination of these probes, they sharpen the constraints on the effective number of relativistic species, $N_\mathrm{eff}=3.02\pm0.10$, the running of the spectral index, $α_\mathrm{s}=0.0014\pm0.0041$, and the running of the running, $β_\mathrm{s}=-0.0006\pm0.0048$, by a factor of 1.18, 1.27, and 1.90, respectively. We conclude by outlining the improvements needed to fully reach the level of confidence implied by these uncertainties.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21432" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21432" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21432" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.20</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmological Constraints</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">6. Three-point intrinsic alignments of galaxies and haloes in the FLAMINGO simulations
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Casper Vedder, Thomas Bakx, Nora Elisa Chisari, Henk Hoekstra, Matthieu Schaller</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Third-order statistics of intrinsic alignments (IA) measured in the FLAMINGO simulation are accurately modeled by the tree-level effective field theory (EFT) on large scales, demonstrating that a reduced EFT assuming linear Lagrangian bias provides robust constraints and is valuable for simplifying IA modeling in photometric shear surveys. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Third-order statistics provide information beyond two-point measures, but extracting this information requires accurate and consistent modelling. We measure and detect the three-point correlation function and third-order aperture-mass statistics of intrinsic alignments (IA) for galaxies and for haloes with $M_{\rm halo} &gt; 10^{13}\,{\rm M}_\odot$ in the $(2.8\,\mathrm{Gpc})^3$ simulation volume of the FLAMINGO hydrodynamical simulation suite. We model the third-order aperture-mass statistics and show that on large scales both the galaxy and halo samples are well described by the tree-level effective field theory (EFT) of IA across the three dark matter density-shape combinations and a wide range of triangle configurations, with the alignment amplitude consistent with that inferred from two-point statistics. We compare the full EFT to several other models: a version neglecting the velocity-shear term, the non-linear alignment model (NLA), and to a reduced EFT assuming co-evolution relations that follow from the assumption that alignment is linear in Lagrangian space. The first two models yield biased constraints on the alignment amplitude, but the reduced EFT performs remarkably well, achieving a low reduced chi-squared and minimal bias. We examine the redshift and mass dependence of the higher-order bias parameters, finding that the linear Lagrangian bias assumption is approximately satisfied across the explored halo mass and redshift ranges for both galaxies and haloes, suggesting that the galaxies broadly follow the alignment properties of their host haloes. These co-evolution relations can be valuable for photometric shear surveys, where limited constraining power on IA parameters favours models with fewer free parameters.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.17914" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.17914" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.17914" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.21</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">7. Almanac: HMC sampling with bounded velocity
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Javier Silva Lafaurie, Lorne Whiteway, Elena Sellentin, Kutay Nazli, Andrew H. Jaffe, Alan F. Heavens, Arthur Loureiro</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Exploring alternative kinetic energy formulations in Hamiltonian Monte Carlo sampling, such as relativistic and Student&#39;s t distributions, reveals that moderately heavy-tailed momentum distributions can modestly improve convergence and effective sample rates, particularly in complex cosmological posteriors, by limiting particle velocities. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">In Hamiltonian Monte Carlo sampling, the shape of the potential and the choice of the momentum distribution jointly give rise to the Hamiltonian dynamics of the sampler. An efficient sampler propagates quickly in all regions of the parameter space, so that the chain has a low autocorrelation length and the sampler has a high acceptance rate, with the goal of optimising the number of near-independent samples for given computational cost. Standard Gaussian momentum distributions allow arbitrarily large velocities, which can lead to inefficient exploration in posteriors with ridges or funnel-like geometries. We investigate alternative momentum distributions based on relativistic and Student&#39;s t kinetic energies, which naturally limit particle velocities and may improve robustness. Using Almanac, a sampler for cosmological posterior distributions of sky maps and power spectra on the sphere, we test these alternatives in both low- and high-dimensional settings. We find that the choice of parameterization and momentum distribution can improve convergence and effective sample rate, though the achievable gains are generally modest and strongly problem-dependent, reaching up to an order of magnitude in favorable cases. Among the momentum distributions that we tested, those with moderately heavy tails achieved the best balance between efficiency and stability. These results highlight the importance of sampler design and encourage future work on adaptive and self-tuning strategies for kinetic energy parameter optimization in high-dimensional settings.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.19390" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.19390" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.19390" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.9</span>
                        <span class="badge bg-info text-dark">Author Score: 0.12</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">8. Baryonification III: An accurate analytical model for the dispersion measure probability density function of fast radio bursts
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>MohammadReza Torkamani, Robert Reischke, Michael Kovač, Andrina Nicola, Jozef Bucko, Alexandre Refregier, Sambit K. Giri, Aurel Schneider, Steffen Hagstotz</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A new fully analytical framework based on the baryonification model accurately predicts the one-point probability distribution function of fast radio burst dispersion measures, demonstrating excellent consistency with hydrodynamical simulations and establishing a self-consistent link between halo gas density profiles and integrated DM statistics for constraining baryonic feedback processes. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We develop a fully analytical framework for predicting the one-point probability distribution function (PDF) of dispersion measures (DM) for fast radio bursts (FRBs) using the baryonification (BFC) model. BFC provides a computationally efficient alternative to expensive hydrodynamical simulations for modelling baryonic effects on cosmological scales. By applying the halo mass function and halo bias, we convolve contributions from individual halos across a range of masses and redshifts to derive the large-scale structure contribution to the DM PDF. We validate our analytical predictions against consistency-check simulations and compare them with the IllustrisTNG hydrodynamical simulation across a range of redshifts up to $z = 5$, demonstrating excellent agreement. We demonstrate that our model produces consistent results when fitting gas profiles and predicting the PDF, and vice versa. We show that the BFC parameters controlling the gas profile, particularly the halo mass scale ($M_\mathrm{c}$), mass-dependent slope ($μ$), and outer truncation ($δ$), are the primary drivers of the PDF shape. Additionally, we investigate the validity of the log-normal approximation commonly used for DM distributions, finding that it provides a sufficient description for a few hundred FRBs. Our work provides a self-consistent model that links gas density profiles to integrated DM statistics, enabling future constraints on baryonic feedback processes from FRB observations.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.18784" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.18784" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.18784" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.8</span>
                        <span class="badge bg-info text-dark">Author Score: 0.08</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">9. Cross-correlating galaxies and cosmic dispersion measures: Constraints on the gas-to-halo mass relation from 2MASS galaxies and 133 localized fast radio bursts
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Masato Shirasaki, Ryuichi Takahashi, Ken Osato, Kunihito Ioka</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Cross-correlation analysis between 2MASS galaxies and fast radio burst dispersion measures yields a null signal at small scales, contradicting IllustrisTNG-300 predictions and indicating that the hot-gas mass fraction in nearby $10^{12-13}\, M_\odot$ halos is significantly lower than simulated, thus requiring stronger feedback mechanisms in galaxy formation models. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We conduct a cross-correlation analysis between large-scale structures traced by the Two Micron All Sky Survey (2MASS) galaxy catalog and the cosmic dispersion measures of 133 localized fast radio bursts (FRBs). The cross-correlation signal is measured as a function of the comoving separation $R$ between 2MASS galaxies and background FRB sightlines, making full use of the available redshift information for both datasets. Our measurements are consistent with a null detection over the range $0.01 &lt; R\, [h^{-1}\mathrm{Mpc}] &lt; 1$. Using a halo-based model in which free-electron density profiles are drawn from the hydrodynamical simulation IllustrisTNG-300 (TNG300), we show that the null signal at $R \sim 0.01\, h^{-1}\mathrm{Mpc}$ is inconsistent with the TNG300 prediction. This discrepancy indicates that the hot-gas mass fraction in halos with masses of $10^{12-13}\, M_\odot$ hosting 2MASS galaxies must be lower than that predicted by TNG300. A simple phenomenological modification of the TNG300 model suggests that the hot-gas mass fraction in halos of $10^{12-13}\, M_\odot$ should be below $\sim 10\%$ of the global baryon fraction in the nearby universe, implying the need for stronger feedback in this mass range. Our constraints are consistent with those inferred from X-ray emission and Sunyaev-Zel&#39;dovich measurements in galaxies, while providing a direct estimate of the hot-gas mass fraction that does not rely on electron-temperature measurements. These results demonstrate that galaxy-FRB cross correlations offer a powerful probe of feedback processes in galaxy formation.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21336" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21336" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21336" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.8</span>
                        <span class="badge bg-info text-dark">Author Score: 0.06</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmological Constraints / Galaxy-Halo Connection, Lensing, Clustering</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">10. The AIDA-TNG project: gas distributions inside and around haloes
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Chi Zhang, Enrico Garaldi, Giulia Despali, Matteo Viel, Lauro Moscardini, Mark Vogelsberger</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Cosmological simulations investigating various dark matter models show that while AGN feedback dominates the halo-to-halo variation in gas and neutral hydrogen profiles, systematic differences in central HI retention are detectable through galaxy-Lyman-$\alpha$ cross-correlation functions, offering a potential observational discriminant for DM physics. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The nature of Dark Matter (DM) is one of the most outstanding mysteries of modern astrophysics. While the standard Cold DM (CDM) model successfully explains observations on most astrophysical scales, DM particles have not yet been detected, leaving room for a plethora of different models. In order to identify their observable signatures, we use the AIDA-TNG cosmological simulation suite to predict the distributions of gas and neutral hydrogen (HI) in the CDM, Self-Interacting DM (SIDM), velocity-dependent SIDM (vSIDM), and Warm DM (WDM) models. We find that the DM models investigated have very limited impact on the median gas and HI profile of haloes. In particular, for the most massive haloes ($M_{\rm vir}\sim10^{14}\,\mathrm{M}_\odot$), we find that DM self-interactions can shallow the central potential and thereby enhance gas cooling. We find that, in all models, the halo-to-halo variation in the HI profiles is explained by AGN feedback, and that the specific characteristics of DM model is largely subdominant. Nevertheless, we detect some systematic difference in the case of SIDM, with more HI surviving close to the centre with respect to other models. We provide fitting functions for the gas and HI profiles. We investigate the galaxy-Ly$α$ cross-correlation function (\galacc) for different halo masses, redshift and observation strategies. We find that at $z=0$ vSIDM can be distinguished from CDM in haloes with $10^{12}\lesssim M_{\rm vir}\lesssim10^{13}\,{\rm M}_\odot$, while SIDM1 can be distinguished from CDM in haloes with $M_{\rm vir}\gtrsim10^{13}\,{\rm M}_\odot$. We estimate that statistically-robust detection requires sampling $\sim160$ haloes with $\sim20$ sightlines each, a task that can be achieved with current and future facilities like WEAVE, 4MOST, PFS, ELT and WST.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.18578" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.18578" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.18578" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.08</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">11. Self-Improving Pretraining: using post-trained models to pretrain better models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Ellen Xiaoqing Tan, Shehzaad Dhuliawala, Jing Xu, Ping Yu, Sainbayar Sukhbaatar, Jason Weston, Olga Golovneva</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A novel reinforcement learning-based pretraining method, which uses a strong judge model to evaluate token rollouts, significantly enhances the factuality, safety, and overall generation quality of large language models by addressing alignment issues from the ground up. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model&#39;s core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21343" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21343" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21343" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.04</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CL</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology / Neural Architectures, Representation Learning, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">12. Local environmental dependence on weak-lensing shear statistics
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Sonia Akter Ema, Md Rasel Hossen, Krzysztof Bolejko, Geraint F. Lewis</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Relativistic N-body simulations demonstrate that while the local large-scale structure minimally affects weak lensing shear constraints on $\Omega_m$ at higher redshifts, the observer&#39;s position introduces location-dependent biases that necessitate accounting for uncertainty in non-Gaussianity measurements like $f_{\rm NL}$. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Despite the assumption that an ideal FLRW observer is not dependent on the local environment, observations are biased by the positions of the observers due to the matter correlations in the large-scale structure (LSS) of the universe. The variation of the mass distribution of the LSS of the universe implies that observers residing in different locations may suffer bias in their measurements when they look at the images of distant galaxies. Here, we assess the influence of the local environment on weak gravitational lensing (WL) shear statistics in the context of relativistic $N$-body code, \texttt{gevolution}. We derive numerical constraints on the cosmological parameters from the WL shear angular power spectrum and comment on the local environment&#39;s influence on WL shear. We find tighter constraints on the parameter $Ω_\mathrm{m}$ above redshift $z$ = 0.2, which implies over this redshift the local environment&#39;s impact is minor. We also investigate the bispectrum and conclude that on average the impact of the local environment on $f_{\rm NL}$ (a measure of non-Gaussianities) is minimal and consistent with zero effect. However, we find that within the assembly of all possible observers/locations, there will also be a few that could infer the parameter $f_{\rm NL}$ of the order 10. These results could thus be used to estimate the uncertainty in the inference of cosmological parameters such as $f_{\rm NL}$ based on WL shear bispectrum and thus may have implications for future surveys requiring precision at the percent level.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.19268" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.19268" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.19268" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.96</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmological Constraints</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">13. Replicating weak-lensing summary-statistic covariances with normalizing flows
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Joaquin Armijo, Leander Thiele, Jia Liu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Normalizing flow generative models successfully reproduce the mean and variance of weak-lensing convergence statistics from cosmological simulations but require data augmentation and noise training to accurately model the off-diagonal elements of the covariance matrix, which are otherwise underestimated by up to 25%. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We explore the ability of normalizing flow (NF) generative models to reproduce weak-lensing summary statistics when trained on a set of cosmological simulations. Our analysis focuses on how accurately NF models recover the mean, standard deviation, and covariance of key statistics derived from convergence ($κ$) maps: The angular power spectrum $C_{\ell}$, probability density function, and Minkowski functionals of weak lensing convergence $κ$-maps. We test two scenarios for training: (1) on the data vectors and (2) on the full $κ$-maps. In both cases, the NF models reproduce the mean and variance of the target statistics within percent-level accuracy. However, the accuracy of the off-diagonal elements of the covariance matrix is underestimated by up to $\sim25\%$. We study several mitigation strategies and find that data augmentation and training with noisy fields help improve covariance recovery to $\mathcal{O}(10\%)$. Our study demonstrates that while the means and variances of weak lensing statistics can be well modeled by NF, covariances can be significantly underestimated if mitigation strategies are not applied.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.20669" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.20669" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.20669" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.03</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">14. Lensing without mixing: Probing Baryonic Acoustic Oscillations and other scale-dependent features in cosmic shear surveys
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>David Touzeau, Alexandre Barthélémy, Francis Bernardeau</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Applying the Bernardeau-Nishimichi-Taruya de-projection transform to tomographic weak-lensing data allows for the extraction of scale- and time-dependent features, such as Baryonic Acoustic Oscillations, which are usually obscured by the projection effect of gravitational lensing. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Weak-gravitational lensing tends to wash out scale and time-dependent features of the clustering of matter, such as the Baryonic Acoustic Oscillations (BAO) which appear in the form of wiggles in the matter power spectrum but that disappear in the analogous lensing $C_\ell$. This is a direct consequence of lensing being a projected effect. In this paper, we demonstrate how the noise complexity -- often deemed &#34;erasing the signal&#34; -- induced by a particular de-projection technique, the Bernardeau-Nishimichi-Taruya (BNT) transform arXiv:1312.0430, can be used to extract the BAO signal and non-gaussian aperture-mass-like properties at chosen physical scales. We take into account parts of the data vectors that should effectively be without cosmological signature and also introduce an additional re-weighting designed to specifically highlight clustering features -- both at the probe (summary statistics) or map (amplitude of the field) level. We thus demonstrate why weak-gravitational lensing by the large-scale structure of the Universe, though only in a tomographic setting, does not erase scale and time-dependent features of the dynamics of matter, while providing a tool to effectively extract them from actual galaxy-shapes measurements.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.19696" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.19696" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.19696" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.05</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">15. How well is the local Large Scale Structure of the Universe known? CosmicFlows vs. Biteau&#39;s Galaxy Catalog with Cloning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yifei Li, Glennys R. Farrar</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A comparison of the Biteau (2021) local matter density field with CosmicFlows models reveals significant regional differences, particularly in the Galactic zone of avoidance where Biteau&#39;s cloning method is unreliable, though the Biteau model is otherwise preferred for integrated structure mass outside that region. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Knowledge of the actual density distribution of matter in the local universe is needed for a variety of purposes -- for instance, as a baseline model for ultrahigh energy cosmic ray sources in the continuum limit and for predicting the diffuse Dark Matter annihilation signal. Determining the local mass density and velocity distribution is the aim of the CosmicFlows project. An alternate approach is based on catalogs of galaxies, supplemented with some scheme for filling in for unseen galaxies. Here, we compare the density field proposed by Biteau (2021) with the quasi-linear density field of CosmicFlows2 (Hoffman et al. 2018) and the mean posterior field of CosmicFlows4 (Valade 2026). We find factor-two level differences in some regions and even greater in regions toward the Galactic center zone of avoidance (ZoA) (|l| &lt; 30°, |b| &lt; 20°) as filled by Biteau using &#34;cloning&#34;. Within 11 Mpc the density field is well-determined by the Local Volume catalog (Karachentsev et al. 2018) which Biteau directly incorporates; at larger distances, Biteau (2021) should not be used in the ZoA where &#34;galaxies&#34; are entirely fictitious but otherwise is to be preferred over CosmicFlows emphasizing the direction and integrated mass of structures; the radial distribution of mass in Biteau (2021) is less robust due to line-of-sight peculiar velocities. The angular positions of structures in CosmicFlows are sometimes not congruent with evidence in the galaxy catalog.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.20808" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.20808" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.20808" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.02</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">16. Statistical Predictions of the Accreted Stellar Halos around Milky Way-Like Galaxies
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>J. Sebastian Monzon, Frank C. van den Bosch, Martin P. Rey</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Semi-analytic modeling within the SatGen framework reveals that the stochastic mass of accreted stellar halos in Milky Way-like galaxies is highly dependent on the fate of just a few massive progenitors, establishing a link between observable galaxy properties and early halo assembly history. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">In the $Λ$CDM paradigm, stellar halos form through the accretion and disruption of satellite galaxies. We introduce new semi-analytic modeling within the SatGen framework to track the ex-situ stellar components of Milky Way--like galaxies across large ensembles of merger trees, enabling a statistical study of the stochastic nature of galaxy assembly. We find that accreted stellar halos are typically built by only a few progenitors and are highly sensitive to the fate of the most massive satellite, producing order-of-magnitude variations in accreted stellar halo mass even at fixed host halo mass. Different stellar components trace distinct phases of host halo growth: central and accreted stellar mass correlate most strongly with early assembly, while surviving satellites trace more recent accretion. Finally, using Random Forest Regression, we quantify how well observable galaxy properties can recover halo assembly histories, providing a framework for interpreting upcoming low-surface-brightness observations of stellar halos.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.18954" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.18954" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.18954" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.08</span>
                        <span class="badge bg-primary">Semantic Score: 0.91</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">17. Impact of Rastall gravity on hydrostatic mass of galaxy clusters
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>M. Lawrence Pattersons, Feri Apryandi, Freddy P. Zen</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Implementing Rastall gravity in galaxy cluster dynamics significantly alleviates the hydrostatic mass bias, producing a statistically favorable framework where the derived hydrostatic mass aligns closely with either the observed baryonic mass (in the absence of dark matter) or the lensing mass (with dark matter). (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Galaxy clusters are the largest virialized structures in the Universe and are predominantly dominated by dark matter. The hydrostatic mass and the mass obtained from gravitational lensing measurements generally differ, a discrepancy known as the hydrostatic mass bias. In this work, we derive the hydrostatic mass of galaxy clusters within the framework of Rastall gravity and investigate its implications under two scenarios: (i) the absence of dark matter and (ii) the existence of dark matter. In the first scenario, Rastall gravity effectively reduces the hydrostatic mass, bringing it closer to the observed baryonic mass. The best linear fit yields a slope $\mathbf{M}=1.07\pm0.11$, indicating a near one-to-one correspondence between the two masses. In the second scenario, Rastall gravity helps to alleviate the hydrostatic mass bias. The linear fit between the Rastall hydrostatic mass and the observed lensing mass results in a best-fit slope $\mathbf{M}=1.01\pm0.16$, which is very close to unity. These results suggest that Rastall gravity provides a statistically favorable framework for addressing mass discrepancies in galaxy clusters.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.18652" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.18652" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.18652" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmological Constraints</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">18. Union3.1: Self-consistent Measurements of Host Galaxy Properties for 2000 Type Ia Supernovae
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Taylor J. Hoyt, David Rubin, Greg Aldering, Saul Perlmutter, Andrei Cuceu, Ravi Gupta</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Homogeneous determination of Type Ia supernova host galaxy properties using SED fitting resolves previous mass discrepancies, tightens standardization parameter uncertainties, and leads to updated cosmological constraints that show $3.4\sigma$ evidence against a cosmological constant when combined with BAO and CMB data. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The determination of distances using time-series photometry of Type Ia supernovae (SNe Ia) relies on a ~5% empirical correction related to the properties of their host galaxies, e.g., global stellar mass. It is therefore crucial for unbiased cosmology inference that host galaxy properties be self-consistently determined across the full range of redshifts probed, which we undertake in this study for approximately 2000 SNe in the Union3 compilation (now Union3.1). We use aperture-matched, homogeneously-reduced, optical-infrared photometry from the DESI Legacy Imaging Surveys to derive global galaxy properties using the stellar population synthesis and SED-fitting code Prospector. We find that the host masses of $z0.03$ mag discrepant, come into 0.01 mag agreement. We then update the UNITY SN analysis and find that the uncertainties on all standardization parameters shrink to 0.6-0.9x their previous sizes. For flat-$Λ$CDM, we find $Ω_m=0.344^{+0.026}_{-0.025}$, a -0.3$σ$ shift from Union3. We then combine with measurements of Baryon Acoustic Oscillations (BAO) and the Cosmic Microwave Background (CMB) exactly as done by DESI DR2 and find $w_0=-0.719\pm0.084$, $w_a=-0.95^{+0.29}_{-0.26}$, corresponding to 3.4$σ$ evidence against a cosmological constant (down from 3.8$σ$). We also update the DESI combined probe analysis using our correction to Pantheon+ and the recent DES-SN5YR Dovekie recalibration, finding $3.2σ$ (up from 2.8$σ$) and 3.4$σ$ (down from 4.2$σ$) evidence against a cosmological constant in the $w_0w_a$ plane, altogether marking a significantly improved consistency across SN analyses.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.19424" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.19424" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.19424" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmological Constraints / Galaxy-Halo Connection, Lensing, Clustering</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">19. Gradient Regularized Natural Gradients
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Satya Prakash Dash, Hossein Abdi, Wei Pan, Samuel Kaski, Mingfei Sun</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Gradient-Regularized Natural Gradients (GRNG) introduces a scalable family of second-order optimizers that integrate gradient regularization to enhance stability and convergence guarantees, empirically achieving superior optimization speed and generalization across large-scale deep learning tasks compared to standard methods. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Gradient regularization (GR) has been shown to improve the generalizability of trained models. While Natural Gradient Descent has been shown to accelerate optimization in the initial phase of training, little attention has been paid to how the training dynamics of second-order optimizers can benefit from GR. In this work, we propose Gradient-Regularized Natural Gradients (GRNG), a family of scalable second-order optimizers that integrate explicit gradient regularization with natural gradient updates. Our framework provides two complementary algorithms: a frequentist variant that avoids explicit inversion of the Fisher Information Matrix (FIM) via structured approximations, and a Bayesian variant based on a Regularized-Kalman formulation that eliminates the need for FIM inversion entirely. We establish convergence guarantees for GRNG, showing that gradient regularization improves stability and enables convergence to global minima. Empirically, we demonstrate that GRNG consistently enhances both optimization speed and generalization compared to first-order methods (SGD, AdamW) and second-order baselines (K-FAC, Sophia), with strong results on vision and language benchmarks. Our findings highlight gradient regularization as a principled and practical tool to unlock the robustness of natural gradient methods for large-scale deep learning.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.18420" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.18420" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.18420" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology / Neural Architectures, Representation Learning, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">20. CoFrGeNet: Continued Fraction Architectures for Language Generation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Amit Dhurandhar, Vijil Chenthamarakshan, Dennis Wei, Tejaswini Pedapati, Karthikeyan Natesan Ramamurthy, Rahul Nair</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Continued Fraction Generative Networks (CoFrGeNets) introduce novel architectural components based on continued fractions that efficiently replace standard Transformer blocks, achieving competitive performance on language tasks while requiring up to half the parameters and reduced pre-training time. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Transformers are arguably the preferred architecture for language generation. In this paper, inspired by continued fractions, we introduce a new function class for generative modeling. The architecture family implementing this function class is named CoFrGeNets - Continued Fraction Generative Networks. We design novel architectural components based on this function class that can replace Multi-head Attention and Feed-Forward Networks in Transformer blocks while requiring much fewer parameters. We derive custom gradient formulations to optimize the proposed components more accurately and efficiently than using standard PyTorch-based gradients. Our components are a plug-in replacement requiring little change in training or inference procedures that have already been put in place for Transformer-based models thus making our approach easy to incorporate in large industrial workflows. We experiment on two very different transformer architectures GPT2-xl (1.5B) and Llama3 (3.2B), where the former we pre-train on OpenWebText and GneissWeb, while the latter we pre-train on the docling data mix which consists of nine different datasets. Results show that the performance on downstream classification, Q\&amp; A, reasoning and text understanding tasks of our models is competitive and sometimes even superior to the original models with $\frac{2}{3}$ to $\frac{1}{2}$ the parameters and shorter pre-training time. We believe that future implementations customized to hardware will further bring out the true potential of our architectures.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21766" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21766" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21766" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.CL</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology / Neural Architectures, Representation Learning, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">21. Bayesian-LoRA: Probabilistic Low-Rank Adaptation of Large Language Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Moule Lin, Shuhao Guan, Andrea Patane, David Gregg, Goetz Botterweck</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Bayesian-LoRA reformulates deterministic LoRA updates as a probabilistic low-rank representation, leveraging Sparse Gaussian Processes to substantially improve LLM calibration and uncertainty quantification, achieving up to 84% ECE reduction with minimal overhead. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Large Language Models usually put more emphasis on accuracy and therefore, will guess even when not certain about the prediction, which is especially severe when fine-tuned on small datasets due to the inherent tendency toward miscalibration. In this work, we introduce Bayesian-LoRA, which reformulates the deterministic LoRA update as a probabilistic low-rank representation inspired by Sparse Gaussian Processes. We identify a structural isomorphism between LoRA&#39;s factorization and Kronecker-factored SGP posteriors, and show that LoRA emerges as a limiting case when posterior uncertainty collapses. We conduct extensive experiments on various LLM architectures across commonsense reasoning benchmarks. With only approximately 0.42M additional parameters and ${\approx}1.2{\times}$ training cost relative to standard LoRA, Bayesian-LoRA significantly improves calibration across models up to 30B, achieving up to 84% ECE reduction and 76% NLL reduction while maintaining competitive accuracy for both in-distribution and out-of-distribution (OoD) evaluations.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21003" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21003" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21003" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology / Neural Architectures, Representation Learning, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">22. Enhancing online estimation of CBC parameters with the low-latency MBTA analysis
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Florian Aubin, Inès Bentara, Damir Buskulic, Gianluca M Guidi, Vincent Juste, Morgan Lethuillier, Frédérique Marion, Lorenzo Mobilia, Benoît Mours, Amazigh Ouzriat, et al.</span>
                                <span class="author-full" style="display: none;">Florian Aubin, Inès Bentara, Damir Buskulic, Gianluca M Guidi, Vincent Juste, Morgan Lethuillier, Frédérique Marion, Lorenzo Mobilia, Benoît Mours, Amazigh Ouzriat, Thomas Sainrat, Viola Sordini</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The Multi-Band Template Analysis (MBTA) search pipeline utilizes an SNR optimizer technique and dense local template banks to efficiently produce online posterior distributions for compact binary coalescence gravitational-wave parameters. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">In this paper, we describe the procedure implemented in the Multi-Band Template Analysis (MBTA) search pipeline to produce online posterior distributions of compact binary coalescence (CBC) gravitational-wave parameters. This procedure relies on an SNR optimizer technique, which consists of filtering dense local template banks. We present how these banks are constructed using information from the initial detection and detail how the results of the filtering are used to estimate source parameters and provide posterior distributions. We demonstrate the performance of our procedure on simulations and compare our source parameter estimates with the results from the first part of the fourth observing run (O4a) recently released by the LIGO-Virgo-KAGRA (LVK) collaboration.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.20512" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.20512" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.20512" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">23. A Diffusive Classification Loss for Learning Energy-based Generative Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Louis Grenioux, RuiKang OuYang, José Miguel Hernández-Lobato</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Diffusive Classification (DiffCLF) reframes Energy-Based Model learning as a supervised classification task across noise levels, providing a computationally efficient objective that avoids mode blindness and enhances the fidelity and applicability of score-based generative models. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Score-based generative models have recently achieved remarkable success. While they are usually parameterized by the score, an alternative way is to use a series of time-dependent energy-based models (EBMs), where the score is obtained from the negative input-gradient of the energy. Crucially, EBMs can be leveraged not only for generation, but also for tasks such as compositional sampling or building Boltzmann Generators via Monte Carlo methods. However, training EBMs remains challenging. Direct maximum likelihood is computationally prohibitive due to the need for nested sampling, while score matching, though efficient, suffers from mode blindness. To address these issues, we introduce the Diffusive Classification (DiffCLF) objective, a simple method that avoids blindness while remaining computationally efficient. DiffCLF reframes EBM learning as a supervised classification problem across noise levels, and can be seamlessly combined with standard score-based objectives. We validate the effectiveness of DiffCLF by comparing the estimated energies against ground truth in analytical Gaussian mixture cases, and by applying the trained models to tasks such as model composition and Boltzmann Generator sampling. Our results show that DiffCLF enables EBMs with higher fidelity and broader applicability than existing approaches.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21025" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21025" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21025" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology / Neural Architectures, Representation Learning, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">24. Deep Neural Networks as Iterated Function Systems and a Generalization Bound
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jonathan Vacher</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Leveraging the theory of stochastic Iterated Function Systems (IFS), deep neural networks are analyzed as place-dependent IFS, enabling the derivation of a Wasserstein generalization bound and a new training objective that controls the collage-type approximation error for generative models. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Deep neural networks (DNNs) achieve remarkable performance on a wide range of tasks, yet their mathematical analysis remains fragmented: stability and generalization are typically studied in disparate frameworks and on a case-by-case basis. Architecturally, DNNs rely on the recursive application of parametrized functions, a mechanism that can be unstable and difficult to train, making stability a primary concern. Even when training succeeds, there are few rigorous results on how well such models generalize beyond the observed data, especially in the generative setting. In this work, we leverage the theory of stochastic Iterated Function Systems (IFS) and show that two important deep architectures can be viewed as, or canonically associated with, place-dependent IFS. This connection allows us to import results from random dynamical systems to (i) establish the existence and uniqueness of invariant measures under suitable contractivity assumptions, and (ii) derive a Wasserstein generalization bound for generative modeling. The bound naturally leads to a new training objective that directly controls the collage-type approximation error between the data distribution and its image under the learned transfer operator. We illustrate the theory on a controlled 2D example and empirically evaluate the proposed objective on standard image datasets (MNIST, CelebA, CIFAR-10).</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.19958" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.19958" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.19958" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology / Neural Architectures, Representation Learning, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">25. Leveraging rapid parameter estimates for efficient gravitational-wave Bayesian inference via posterior repartitioning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Metha Prathaban, Charlie Hoy, Michael J. Williams</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A novel, statistically rigorous approach combines fast constraints from the simple-pe algorithm with nested sampling acceleration via posterior repartitioning, achieving speedups up to 2.2x for high-SNR gravitational-wave Bayesian inference without compromising the final result&#39;s integrity. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Gravitational wave astronomy typically relies on rigorous, computationally expensive Bayesian analyses. Several methods have been developed to perform rapid Bayesian inference, but they are not yet used to inform our full analyses. We present a novel approach for doing this whilst ensuring that the Bayesian prior remains independent of the data, providing a statistically rigorous way to leverage low-latency information to accelerate the final inference. By combining the fast constraints from the simple-pe algorithm with the nested sampling acceleration technique of posterior repartitioning, we demonstrate that our method can guide the nested sampler towards the most probable regions of parameter space more efficiently for signal-to-noise ratios (SNR) greater than 20, while mathematically guaranteeing that the final inference is identical to that of a standard, uninformed analysis. We validate the method through an injection study, demonstrating that it produces statistically robust and unbiased results, whilst providing speedups of up to $2.2\times$ for binaries with SNRs $&lt; 150$. Importantly, we show that the performance gain provided by our method scales with SNR, establishing it as a powerful technique to mitigate the cost of analysing signals from current and future gravitational-wave observatories.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21630" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21630" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21630" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">26. Large-scale Modeling of the Observed Power Spectrum Multipoles
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Robin Y. Wen, Henry S. Grasshorn Gebhardt, Chen Heinrich, Olivier Doré</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A consistent theoretical framework utilizes the discrete spherical Fourier-Bessel (dSFB) basis to accurately model the galaxy power spectrum multipoles on large scales, robustly incorporating wide-angle effects, redshift evolution, and observational constraints necessary for upcoming all-sky surveys. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Current and upcoming large-scale structure surveys are pushing toward increasingly wide angular coverage, where wide-angle effects (arising from the varying line of sight across the curved sky) become critical for accurate modeling of the three-dimensional galaxy power spectrum. At the same time, these survey&#39;s broader redshift reach makes the effects of redshift evolution (beyond the effective-redshift approximation) non-negligible on large radial scales. Additional observational effects such as the survey window function and integral constraints also become significant on these large scales, necessitating a careful theoretical treatment to robustly constrain local primordial non-Gaussianities and relativistic effects. In this work, we present a consistent and accurate theoretical framework for modeling the commonly used power spectrum multipoles (PSM) on large scales using the discrete spherical Fourier-Bessel (dSFB) basis. This basis ensures numerical stability and allows an exact separation between angular and radial modes. Using the dSFB basis, we study the impact of wide-angle effects and redshift evolution on the PSM, and incorporate the effects of window function convolution and integral constraints. We validate our PSM modeling using lognormal mocks under radial integral constraints with realistic survey geometries, demonstrating the readiness of our framework for application to all-sky galaxy surveys.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.19438" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.19438" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.19438" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">27. Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Aníbal Silva, Moisés Santos, André Restivo, Carlos Soares</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Empirical investigation into integrating Transformers within Variational Autoencoders for tabular data reveals that placing them in the latent and decoder components introduces a trade-off between generative fidelity and diversity, alongside observing approximately linear relationships in consecutive decoder blocks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Tabular data remains a challenging domain for generative models. In particular, the standard Variational Autoencoder (VAE) architecture, typically composed of multilayer perceptrons, struggles to model relationships between features, especially when handling mixed data types. In contrast, Transformers, through their attention mechanism, are better suited for capturing complex feature interactions. In this paper, we empirically investigate the impact of integrating Transformers into different components of a VAE. We conduct experiments on 57 datasets from the OpenML CC18 suite and draw two main conclusions. First, results indicate that positioning Transformers to leverage latent and decoder representations leads to a trade-off between fidelity and diversity. Second, we observe a high similarity between consecutive blocks of a Transformer in all components. In particular, in the decoder, the relationship between the input and output of a Transformer is approximately linear.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.20854" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.20854" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.20854" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology / Neural Architectures, Representation Learning, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">28. Rethinking Cross-Modal Fine-Tuning: Optimizing the Interaction between Feature Alignment and Target Fitting
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Trong Khiem Tran, Manh Cuong Dao, Phi Le Nguyen, Thao Nguyen Truong, Trong Nghia Hoang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A principled framework establishes a provable generalization bound based on a novel feature-label distortion metric, offering actionable insights into optimizing the critical interaction between feature alignment and target fitting when adapting pre-trained models to new modalities. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Adapting pre-trained models to unseen feature modalities has become increasingly important due to the growing need for cross-disciplinary knowledge integration.~A key challenge here is how to align the representation of new modalities with the most relevant parts of the pre-trained model&#39;s representation space to enable accurate knowledge transfer.~This requires combining feature alignment with target fine-tuning, but uncalibrated combinations can exacerbate misalignment between the source and target feature-label structures and reduce target generalization.~Existing work however lacks a theoretical understanding of this critical interaction between feature alignment and target fitting.~To bridge this gap, we develop a principled framework that establishes a provable generalization bound on the target error, which explains the interaction between feature alignment and target fitting through a novel concept of feature-label distortion.~This bound offers actionable insights into how this interaction should be optimized for practical algorithm design. The resulting approach achieves significantly improved performance over state-of-the-art methods across a wide range of benchmark datasets.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.18231" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.18231" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.18231" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology / Neural Architectures, Representation Learning, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">29. Disk Wind Feedback from High-mass Protostars. V. Application of Multi-Modal Machine Learning to Characterize Outflow Properties
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Duo Xu, Ioana A. Stelea, Joshua S. Speagle, Yichen Zhang, Jonathan C. Tan</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A multi-modal deep learning framework, leveraging Vision Transformers and cross-attention fusion on CO spatial and spectral data, accurately infers protostellar outflow parameters (mass, inclination, position angle) from synthetic ALMA observations, demonstrating robustness against projection effects. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Characterizing protostellar outflows is fundamental to understanding star formation feedback, yet traditional methods are often hindered by projection effects and complex morphologies. We present a multi-modal deep learning framework that jointly leverages spatial and spectral information from CO observations to infer outflow mass, inclination, and position angle ($PA$). Our model, trained on synthetic ALMA observations generated from 3D magnetohydrodynamic simulations, utilizes a cross-attention fusion mechanism to integrate morphological and kinematic features with probabilistic uncertainty estimation. Our results demonstrate that Vision Transformer architectures significantly outperform convolutional networks, showing remarkable robustness to reduced spatial resolution. Interpretability analysis reveals a physically consistent hierarchy: spatial features dominate across all parameters, whereas spectral profiles provide secondary constraints for mass and inclination. Applied to observational ALMA data, the framework delivers stable mass and $PA$ estimates with exceptionally tightly constrained inclination angles. This study establishes multi-modal deep learning as a powerful, interpretable tool for overcoming projection biases in high-mass star formation studies.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21100" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21100" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21100" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.04</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">30. SAL: Selective Adaptive Learning for Backpropagation-Free Training with Sparsification
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Fanping Liu, Hua Yang, Jiasi Zou</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Selective Adaptive Learning (SAL) is introduced as a biologically plausible training method that partitions the parameter space into sample-dependent regions, effectively mitigating gradient interference and weight symmetry constraints to achieve competitive performance and scalability in deep neural networks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Standard deep learning relies on Backpropagation (BP), which is constrained by biologically implausible weight symmetry and suffers from significant gradient interference within dense representations. To mitigate these bottlenecks, we propose Selective Adaptive Learning (SAL), a training method that combines selective parameter activation with adaptive area partitioning. Specifically, SAL decomposes the parameter space into mutually exclusive, sample-dependent regions. This decoupling mitigates gradient interference across divergent semantic patterns and addresses explicit weight symmetry requirements through our refined feedback alignment. Empirically, SAL demonstrates competitive convergence rates, leading to improved classification performance across 10 standard benchmarks. Additionally, SAL achieves numerical consistency and competitive accuracy even in deep regimes (up to 128 layers) and large-scale models (up to 1B parameters). Our approach is loosely inspired by biological learning mechanisms, offering a plausible alternative that contributes to the study of scalable neural network training.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21561" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21561" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21561" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology / Neural Architectures, Representation Learning, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">31. Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yingfa Chen, Zhen Leng Thai, Zihan Zhou, Zhu Zhang, Xingyu Shen, Shuo Wang, Chaojun Xiao, Xu Han, Zhiyuan Liu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The HALO distillation pipeline and the HypeNet hybrid architecture, featuring the HyPE position encoding, efficiently convert large Transformer models into high-performing, long-context-efficient RNN-attention hybrids using minimal training data. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.22156" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.22156" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.22156" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CL</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology / Neural Architectures, Representation Learning, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">32. Do we really need Self-Attention for Streaming Automatic Speech Recognition?
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Youness Dkhissi, Valentin Vielzeuf, Elys Allesiardo, Anthony Larcher</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Transformer architectures are deemed unsuitable for constrained streaming applications due to high computational costs, prompting a demonstration that replacing or removing Self-Attention with deformable convolution in Streaming ASR significantly improves efficiency without sacrificing Word Error Rate. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Transformer-based architectures are the most used architectures in many deep learning fields like Natural Language Processing, Computer Vision or Speech processing. It may encourage the direct use of Transformers in the constrained tasks, without questioning whether it will yield the same benefits as in standard tasks. Given specific constraints, it is essential to evaluate the relevance of transformer models. This work questions the suitability of transformers for specific domains. We argue that the high computational requirements and latency issues associated with these models do not align well with streaming applications. Our study promotes the search for alternative strategies to improve efficiency without sacrificing performance. In light of this observation, our paper critically examines the usefulness of transformer architecture in such constrained environments. As a first attempt, we show that the computational cost for Streaming Automatic Speech Recognition (ASR) can be reduced using deformable convolution instead of Self-Attention. Furthermore, we show that Self-Attention mechanisms can be entirely removed and not replaced, without observing significant degradation in the Word Error Rate.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.19960" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.19960" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.19960" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">eess.AS</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology / Neural Architectures, Representation Learning, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">33. A Novel Lensed Point Source Modeling Pipeline using GIGA-Lens with Application to SN Zwicky and SN iPTF16geu
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Saul Baltasar, Nicolas Ratier-Werbin, Xiaosheng Huang, W. Sheu, C. J. Storfer, Y. -M. Hsu, Sean Xu, David J. Schlegel</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A new JAX-based GIGA-Lens pipeline accurately models strongly lensed point sources, enabling precise inference of lens mass distributions and achieving $\sim 3.6\%$ uncertainty on the Hubble constant ($H_0$) from a single system, yielding results consistent with or providing alternatives to prior lensed supernova analyses. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We introduce a novel modeling pipeline for strongly lensed point sources, using the GIGA-Lens framework, running on four A100 GPUs via the JAX platform. Using simulations, we demonstrate accurate and precise recovery of image positions, fluxes, and time delays, together with inference of complex lens mass distributions -- including the mass density slope, $γ$ -- from images of lensed point sources alone. We further show that we can achieve statistical uncertainty of $\sim 3.6\%$ ($\sim 2.5\, \mathrm{km\, s^{-1}/Mpc}$) on $H_0$ from a single system, with full forward modeling, i.e., simultaneous inference of all lens model parameters together with $H_0$. We apply our pipeline to two well-studied lensed SNe Ia, Zwicky and iPTF16geu. For SN iPTF16geu, unlike previous modeling efforts, we model only the images of the lensed point source (the SN) and do not use the lensed images of the extended host-galaxy. Nevertheless, we are able to infer all of the mass parameters modeled in earlier studies, and our best-fit values, including $γ$, are fully consistent with published results. In the case of SN Zwicky, taking the same approach, however, we obtain an alternative best-fit model compared to published results, underscoring the importance of fully exploring the model parameter space.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.18787" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.18787" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.18787" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">34. Forecasting the E_G measurements from the photometric and spectroscopic surveys of Chinese Space Station Survey Telescope (CSST)
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yu Song, Yi Zheng</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Forecasts indicate the Chinese Space Station Survey Telescope (CSST) will constrain the $E_G$ statistic to the few-percent level across $0 &lt; z &lt; 1.2$, significantly improving tests of gravity and enabling $\sim 5\%$ precision on the modified gravity parameter $\Sigma_0$, especially when combined with accurate spectroscopic measurements of the redshift space distortion parameter $\beta$. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present forecasts for the $E_G$ statistic using redshift distributions of realistic mock galaxy samples from the upcoming Chinese Space Station Survey Telescope (CSST). The dominant uncertainty in $E_G$ stems from the redshift space distortion parameter $β$, whose precision limits the overall constraining power. Our analysis shows that CSST will nevertheless achieve $E_G$ constraints at the few-percent level (3%-9%) over $0 &lt; z &lt; 1.2$, an improvement by a factor of several to an order of magnitude over current observations. Within the $μ-Σ$ modified gravity framework, the parameter $Σ_0$, associated with the effective gravitational constant of the Weyl potential, can be constrained to $\sim 5\%$ precision. In a plausible scenario where upcoming spectroscopic surveys determine $β$ to 1\% accuracy, $E_G$ constraints tighten to the percent level, and $Σ_0$ becomes measurable at $\sim 1\%$. These results demonstrate that CSST will serve as a powerful facility for testing gravity and underscore the essential synergy between photometric weak lensing and spectroscopic surveys in probing cosmic acceleration.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.17874" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.17874" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.17874" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmological Constraints</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">35. Multilevel and Sequential Monte Carlo for Training-Free Diffusion Guidance
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Aidan Gleich, Scott C. Schmidler</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A sequential Monte Carlo framework, utilizing Multi-Level Monte Carlo for variance reduction, provides an unbiased estimator for training-free guidance in conditional diffusion models, achieving state-of-the-art accuracy on CIFAR-10 and superior cost efficiency on ImageNet compared to existing point-estimate methods. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We address the problem of accurate, training-free guidance for conditional generation in trained diffusion models. Existing methods typically rely on point-estimates to approximate the posterior score, often resulting in biased approximations that fail to capture multimodality inherent to the reverse process of diffusion models. We propose a sequential Monte Carlo (SMC) framework that constructs an unbiased estimator of $p_θ(y|x_t)$ by integrating over the full denoising distribution via Monte Carlo approximation. To ensure computational tractability, we incorporate variance-reduction schemes based on Multi-Level Monte Carlo (MLMC). Our approach achieves new state-of-the-art results for training-free guidance on CIFAR-10 class-conditional generation, achieving $95.6\%$ accuracy with $3\times$ lower cost-per-success than baselines. On ImageNet, our algorithm achieves $1.5\times$ cost-per-success advantage over existing methods.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21104" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21104" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21104" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology / Neural Architectures, Representation Learning, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">36. Gravitational wave detectors from an experimental perspective
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Marina Trad-Nery, Margherita Turconi, Walid Chaibi</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The fundamental principles of gravitational wave detectors are introduced by detailing the critical noise sources, their coupling mechanisms, and mitigation strategies, culminating in a sensitivity curve calculation based on the Virgo detector framework. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">This chapter introduces the fundamental principles of gravitational wave detectors in a simple and comprehensive manner. Because these instruments aim for extremely high sensitivity, it is essential to understand their various noise sources, how such noise couples to the detector output, and the strategies used to mitigate them. These noises contributions are computed in the frame of the Virgo detector and a sensitivity curve is calculated. Although a simplified layout of a gravitational wave detector is considered, it takes into account the most dominant effects and yields in a sensitivity estimate close to the what is observed in real detectors.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.18501" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.18501" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.18501" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">physics.optics</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">37. Explicit Multi-head Attention for Inter-head Interaction in Large Language Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Runyu Peng, Yunhua Zhou, Demin Song, Kai Lv, Bo Wang, Qipeng Guo, Xipeng Qiu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Multi-head Explicit Attention (MEA), which uses Head-level Linear Composition (HLC) and Group Normalization to explicitly model cross-head interaction, improves pretraining robustness, accelerates convergence, and facilitates a 50% reduction in KV-cache memory usage with negligible performance degradation. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">In large language models built upon the Transformer architecture, recent studies have shown that inter-head interaction can enhance attention performance. Motivated by this, we propose Multi-head Explicit Attention (MEA), a simple yet effective attention variant that explicitly models cross-head interaction. MEA consists of two key components: a Head-level Linear Composition (HLC) module that separately applies learnable linear combinations to the key and value vectors across heads, thereby enabling rich inter-head communication; and a head-level Group Normalization layer that aligns the statistical properties of the recombined heads. MEA shows strong robustness in pretraining, which allows the use of larger learning rates that lead to faster convergence, ultimately resulting in lower validation loss and improved performance across a range of tasks. Furthermore, we explore the parameter efficiency of MEA by reducing the number of attention heads and leveraging HLC to reconstruct them using low-rank &#34;virtual heads&#34;. This enables a practical key-value cache compression strategy that reduces KV-cache memory usage by 50% with negligible performance loss on knowledge-intensive and scientific reasoning tasks, and only a 3.59% accuracy drop for Olympiad-level mathematical benchmarks.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.19611" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.19611" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.19611" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology / Neural Architectures, Representation Learning, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">38. Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Grzegorz Stefanski, Alberto Presta, Michal Byra</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Routing the Lottery (RTL) is an adaptive pruning framework that discovers specialized &#34;adaptive tickets&#34; tailored to data heterogeneity, significantly outperforming universal pruning baselines while using fewer parameters and introducing a similarity score to diagnose subnetwork collapse. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.22141" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.22141" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.22141" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology / Neural Architectures, Representation Learning, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">39. Provable Learning of Random Hierarchy Models and Hierarchical Shallow-to-Deep Chaining
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yunwei Ren, Yatin Dandi, Florent Krzakala, Jason D. Lee</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Theoretical analysis demonstrates that deep convolutional networks trained via gradient methods can efficiently learn complex hierarchical structures, such as those defined by Random Hierarchy Models, contingent upon clean signal propagation to intermediate layers and weak identifiability of relevant features. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The empirical success of deep learning is often attributed to deep networks&#39; ability to exploit hierarchical structure in data, constructing increasingly complex features across layers. Yet despite substantial progress in deep learning theory, most optimization results sill focus on networks with only two or three layers, leaving the theoretical understanding of hierarchical learning in genuinely deep models limited. This leads to a natural question: can we prove that deep networks, trained by gradient-based methods, can efficiently exploit hierarchical structure? In this work, we consider Random Hierarchy Models -- a hierarchical context-free grammar introduced by arXiv:2307.02129 and conjectured to separate deep and shallow networks. We prove that, under mild conditions, a deep convolutional network can be efficiently trained to learn this function class. Our proof builds on a general observation: if intermediate layers can receive clean signal from the labels and the relevant features are weakly identifiable, then layerwise training each individual layer suffices to hierarchically learn the target function.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.19756" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.19756" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.19756" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology / Neural Architectures, Representation Learning, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">40. Why Adam Works Better with $β_1 = β_2$: The Missing Gradient Scale Invariance Principle
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Alberto Fernández-Hernández, Cristian Pérez-Corral, Jose I. Mestre, Manuel F. Dolz, Enrique S. Quintana-Ortí</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The empirical success of setting Adam optimizer momentum parameters to $\beta_{1}=\beta_{2}$ is theoretically explained by proving that this condition is necessary and sufficient for Adam to achieve first-order gradient scale invariance, aligning it with modern scale-robust optimizer design principles. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Adam has been at the core of large-scale training for almost a decade, yet a simple empirical fact remains unaccounted for: both validation scores and the qualitative behaviour of the training runs improve when the momentum parameters satisfy $β_{1}=β_{2}$. Some recent studies have reported this pattern, but there is still no explanation for why this choice helps. We show that this choice is closely tied to a structural property that we refer to as \textit{gradient scale invariance}. We formalize this notion and prove that Adam becomes gradient scale invariant of first order if and only if $β_{1}=β_{2}$. This perspective places the balanced regime of Adam in direct alignment with the design principles underlying several recent optimizers that explicitly enforce scale-robust updates. The theory is supported by experiments across vision and language tasks, and across different architectural families, in which rescaling the gradient has a markedly smoother effect on the update when $β_{1}=β_{2}$. Overall, our results offer a coherent explanation for an open question in the behavior of Adam and provide a simple principle that helps guide the design of future optimizers.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21739" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21739" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21739" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology / Neural Architectures, Representation Learning, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">41. It&#39;s More Complicated Than You Think: A Forward Model to Infer the Recent Star Formation History, Bursty or Not, of Galaxy Populations
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Emilie Burnham, Bingjie Wang, Joel Leja, Owen Gonzales, Jenny E. Greene, Kartheik G. Iyer, Abby Mintz, David J. Setton, Sarah Wellons, Rachel Bezanson, et al.</span>
                                <span class="author-full" style="display: none;">Emilie Burnham, Bingjie Wang, Joel Leja, Owen Gonzales, Jenny E. Greene, Kartheik G. Iyer, Abby Mintz, David J. Setton, Sarah Wellons, Rachel Bezanson, Olivia Curtis, Robert Feldmann, Tim B. Miller, Themiya Nanayakkara, Joshua S. Speagle, Katherine A. Suess, Guochao Sun</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A novel simulation-based inference framework accurately measures the power and timescales of stochastic star formation rate fluctuations in early JWST galaxies, providing robust constraints on feedback physics by disentangling burstiness from secular trends. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Observations of the early Universe (z &gt; 4) with the James Webb Space Telescope reveal galaxy populations with a wide range of intrinsic luminosities and colors. Bursty star formation histories (SFHs), characterized by short-term fluctuations in the star formation rate (SFR), may explain this diversity, but constraining burst timescales and amplitudes in individual galaxies is challenging due to degeneracies and sensitivity limits. We introduce a population-level simulation-based inference framework that recovers the power and timescales of SFR fluctuations by forward-modeling galaxy populations and distributions of rest-UV to rest-optical spectral features sensitive to star formation timescales. We adopt a stochastic SFH model based on a power spectral density formalism spanning 1 Myr-10 Gyr. Using simulated samples of N=500 galaxies at z~4 with typical JWST/NIRSpec uncertainties, we demonstrate that: (i) the power of SFR fluctuations can be measured with sufficient precision to distinguish between simulations (e.g., FIRE-2-like vs. Illustris-like populations at &gt;99% confidence for timescales &lt; 100 Myr); (ii) simultaneously modeling stochastic fluctuations and the recent (t_L &lt; 500 Myr) average SFH slope is essential, as secular trends otherwise mimic burstiness in common diagnostics; (iii) frequent, intense bursts impose an outshining limit, and bias inference toward underestimating burstiness due to the obscuration of long-timescale power; and (iv) the power of SFR fluctuations can be inferred to 95% confidence across all timescales in both smooth and bursty populations. This framework establishes a novel and robust method for placing quantitative constraints on the feedback physics regulating star formation using large, uniformly selected spectroscopic samples.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.20930" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.20930" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.20930" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.04</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">42. ECO: Quantized Training without Full-Precision Master Weights
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Mahdi Nikdan, Amir Zandieh, Dan Alistarh, Vahab Mirrokni</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The Error-Compensating Optimizer (ECO) eliminates the memory-intensive master weights in quantized Large Language Model training by applying updates directly to quantized parameters and using an error-feedback loop, achieving near-lossless accuracy and improved memory efficiency, especially for Sparse Mixture of Experts models. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to a high-precision weight buffer, known as $\textit{master weights}$. This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage. To address this, we introduce the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters. ECO quantizes weights after each step and carefully injects the resulting quantization error into the optimizer momentum, forming an error-feedback loop with no additional memory. We prove that, under standard assumptions and a decaying learning rate, ECO converges to a constant-radius neighborhood of the optimum, while naive master-weight removal can incur an error that is inversely proportional to the learning rate. We show empirical results for pretraining small Transformers (30-800M), a Gemma-3 1B model, and a 2.1B parameter Sparse MoE model with FP8 quantization, and fine-tuning DeepSeek-MoE-16B in INT4 precision. Throughout, ECO matches baselines with master weights up to near-lossless accuracy, significantly shifting the static memory vs validation loss Pareto frontier.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.22101" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.22101" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.22101" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CL</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology / Neural Architectures, Representation Learning, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">43. From Observations to Events: Event-Aware World Model for Reinforcement Learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zhao-Han Peng, Shaohui Li, Zhi Li, Shulan Ruan, Yu Liu, You He</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The Event-Aware World Model (EAWM) framework enhances Model-Based Reinforcement Learning generalization by learning representations based on discrete, automatically generated event boundaries, leading to substantial performance gains across multiple complex control and gaming environments. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">While model-based reinforcement learning (MBRL) improves sample efficiency by learning world models from raw observations, existing methods struggle to generalize across structurally similar scenes and remain vulnerable to spurious variations such as textures or color shifts. From a cognitive science perspective, humans segment continuous sensory streams into discrete events and rely on these key events for decision-making. Motivated by this principle, we propose the Event-Aware World Model (EAWM), a general framework that learns event-aware representations to streamline policy learning without requiring handcrafted labels. EAWM employs an automated event generator to derive events from raw observations and introduces a Generic Event Segmentor (GES) to identify event boundaries, which mark the start and end time of event segments. Through event prediction, the representation space is shaped to capture meaningful spatio-temporal transitions. Beyond this, we present a unified formulation of seemingly distinct world model architectures and show the broad applicability of our methods. Experiments on Atari 100K, Craftax 1M, and DeepMind Control 500K, DMC-GB2 500K demonstrate that EAWM consistently boosts the performance of strong MBRL baselines by 10%-45%, setting new state-of-the-art results across benchmarks. Our code is released at https://github.com/MarquisDarwin/EAWM.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.19336" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.19336" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.19336" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology / Neural Architectures, Representation Learning, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">44. PTQ4ARVG: Post-Training Quantization for AutoRegressive Visual Generation Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Xuewen Liu, Zhikai Li, Jing Zhang, Mengjuan Chen, Qingyi Gu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> PTQ4ARVG, a novel post-training quantization framework, achieves competitive 6-bit and 8-bit precision for AutoRegressive Visual Generation models by employing specialized techniques—Gain-Projected Scaling, Static Token-Wise Quantization, and Distribution-Guided Calibration—to handle channel outliers and dynamic token activations. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">AutoRegressive Visual Generation (ARVG) models retain an architecture compatible with language models, while achieving performance comparable to diffusion-based models. Quantization is commonly employed in neural networks to reduce model size and computational latency. However, applying quantization to ARVG remains largely underexplored, and existing quantization methods fail to generalize effectively to ARVG models. In this paper, we explore this issue and identify three key challenges: (1) severe outliers at channel-wise level, (2) highly dynamic activations at token-wise level, and (3) mismatched distribution information at sample-wise level. To these ends, we propose PTQ4ARVG, a training-free post-training quantization (PTQ) framework consisting of: (1) Gain-Projected Scaling (GPS) mitigates the channel-wise outliers, which expands the quantization loss via a Taylor series to quantify the gain of scaling for activation-weight quantization, and derives the optimal scaling factor through differentiation.(2) Static Token-Wise Quantization (STWQ) leverages the inherent properties of ARVG, fixed token length and position-invariant distribution across samples, to address token-wise variance without incurring dynamic calibration overhead.(3) Distribution-Guided Calibration (DGC) selects samples that contribute most to distributional entropy, eliminating the sample-wise distribution mismatch. Extensive experiments show that PTQ4ARVG can effectively quantize the ARVG family models to 8-bit and 6-bit while maintaining competitive performance. Code is available at http://github.com/BienLuky/PTQ4ARVG .</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21238" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21238" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21238" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology / Neural Architectures, Representation Learning, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">45. Sparsity for isotropic spherical random fields
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Giacomo Greco, Domenico Marinucci</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A newly introduced simple representation for isotropic spherical random fields enables the discussion of sparsity under isotropy, facilitating computationally efficient simulations that accurately reproduce the angular power spectrum and polyspectra of complex non-Gaussian fields. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We introduce a simple representation for isotropic spherical random fields and we discuss how it allows to discuss different notions of sparsity under isotropy. We also show how a suitable construction of sparse fields can mimic well the angular power spectrum and the polyspectra of some popular non-Gaussian fields, at the same time allowing for computationally efficient simulation algorithms. Using related ideas we also show how it is possible to obtain sparse approximations of spherical random fields which preserve isotropy, thus addressing an issue which has been raised in the Cosmological literature.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21535" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21535" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21535" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">math.PR</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">46. L$^3$: Large Lookup Layers
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Albert Tseng, Christopher De Sa</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Introducing the Large Lookup Layer (L$^3$), a novel architecture that generalizes embedding tables using static token-based routing and context-dependent aggregation, provides a highly efficient, systems-friendly sparsity mechanism that outperforms dense and iso-sparse Mixture-of-Experts models in language tasks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Modern sparse language models typically achieve sparsity through Mixture-of-Experts (MoE) layers, which dynamically route tokens to dense MLP &#34;experts.&#34; However, dynamic hard routing has a number of drawbacks, such as potentially poor hardware efficiency and needing auxiliary losses for stable training. In contrast, the tokenizer embedding table, which is natively sparse, largely avoids these issues by selecting a single embedding per token at the cost of not having contextual information. In this work, we introduce the Large Lookup Layer (L$^3$), which unlocks a new axis of sparsity by generalizing embedding tables to model decoder layers. L$^3$ layers use static token-based routing to aggregate a set of learned embeddings per token in a context-dependent way, allowing the model to efficiently balance memory and compute by caching information in embeddings. L$^3$ has two main components: (1) a systems-friendly architecture that allows for fast training and CPU-offloaded inference with no overhead, and (2) an information-theoretic embedding allocation algorithm that effectively balances speed and quality. We empirically test L$^3$ by training transformers with up to 2.6B active parameters and find that L$^3$ strongly outperforms both dense models and iso-sparse MoEs in both language modeling and downstream tasks.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21461" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21461" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21461" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology / Neural Architectures, Representation Learning, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">47. Effect of noise characterization on the detection of mHz stochastic gravitational waves
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Nikolaos Karnesis, Quentin Baghi, Jean-Baptiste Bayle, Nikiforos Galanis</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Refined agnostic noise reconstruction methods, incorporating detailed instrumental simulations and flexible spline-fitting for power spectral density, improve the robustness and accuracy of detectability bounds for the stochastic gravitational-wave background signal sought by the LISA mission. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Pulsar timing arrays&#39; hint for a stochastic gravitational-wave background (SGWB) leverages the expectations of a future detection in the millihertz band, particularly with the LISA space mission. However, finding an SGWB with a single orbiting detector is challenging: It calls for cautious modelling of instrumental noise, which is also mainly stochastic. It was shown that agnostic noise reconstruction methods provide robustness in the detection process. We build on previous work to include more realistic instrumental simulations and additional degrees of freedom in the noise inference model and analyze the impact of LISA&#39;s sensitivity to SGWBs. Particularly, we model the two main types of noise sources with separate transfer functions and power spectral density spline fitting. We assess the detectability bounds and their dependence on the flexibility of the noise model and on the prior probability, allowing us to refine previously reported results.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.19741" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.19741" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.19741" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">48. PERTURB-c: Correlation Aware Perturbation Explainability for Regression Techniques to Understand Retrieval Black-boxes
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jools D. Clarke, Gordon Yip, Nikolaos Nikolaou</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> PERTURB-c introduces a correlation-aware, model-agnostic framework for interpreting black box regression models with structured inputs, efficiently leveraging physical knowledge of spectral correlation to verify predictions in resource-intensive applications like exoplanet transit spectroscopy retrievals. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">In this paper we introduce PERTURB-c, a correlation-aware framework for interpreting black box regression models with one-dimensional structured inputs. We demonstrate this framework on a simulated case study with machine learning based transit spectroscopy retrievals of exoplanet WASP-107b. Characterising many exoplanet atmospheres can answer important questions about planetary populations, but traditional retrievals are very resource intensive; machine learning based methods offer a fast alternative however (i) they require high volumes data (only obtainable through simulations) to train and (ii) their complexity renders them black-boxes. Better understanding how they reach predictions can allow us to inspect for biases, which is especially important with simulated data, and verify that predictions are made on the basis of physically plausible features. This ultimately improves the ease of adoption of machine learning techniques. The most used methods to explain machine learning model predictions (such as SHAP and other methods that rely on stochastic sample generation) suffer from high computational complexity and struggle to account for interactions between inputs. PERTURB-c addresses these issues by leveraging physical knowledge of the known spectral correlation. For visualisation of this analysis, we propose a heat-map-based representation which is better suited to large numbers of input features along a single dimension, and that is more intuitive to those who are already familiar with retrieval methods. Note that while we chose this exoplanet retrieval context to demonstrate our methodologies, the PERTURB-c framework is model agnostic and in a broader context has potential value across a plethora of adjacent regression problems.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21685" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21685" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21685" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.EP</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">49. Representation-Regularized Convolutional Audio Transformer for Audio Understanding
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Bing Han, Chushu Zhou, Yifan Yang, Wei Wang, Chenda Li, Wangyou Zhang, Yanmin Qian</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The Convolutional Audio Transformer (CAT) framework significantly outperforms baselines in audio understanding by integrating multi-resolution feature aggregation and a Representation Regularization objective, achieving competitive performance on AudioSet 20k with five times faster convergence. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Bootstrap-based Self-Supervised Learning (SSL) has achieved remarkable progress in audio understanding. However, existing methods typically operate at a single level of granularity, limiting their ability to model the diverse temporal and spectral structures inherent in complex audio signals. Furthermore, bootstrapping representations from scratch is computationally expensive, often requiring extensive training to converge. In this work, we propose the Convolutional Audio Transformer (CAT), a unified framework designed to address these challenges. First, to capture hierarchical audio features, CAT incorporates a Multi-resolution Block that aggregates information across varying granularities. Second, to enhance training efficiency, we introduce a Representation Regularization objective. Drawing inspiration from generative modeling, this auxiliary task guides the student model by aligning its predictions with high-quality semantic representations from frozen, pre-trained external encoders. Experimental results demonstrate that CAT significantly outperforms baselines on audio understanding benchmarks. Notably, it achieves competitive performance on the AudioSet 20k dataset with 5 times faster convergence than existing methods. Codes and checkpoints will be released soon at https://github.com/realzhouchushu/CAT.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21612" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21612" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21612" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">eess.AS</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology / Neural Architectures, Representation Learning, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">50. Minkowski Functionals of the 21 cm Signal as a Probe of Primordial Features
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Kanan Virkar, Suvedha Suresh Naik, Pravabati Chingangbam</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Minkowski Functionals are highly sensitive morphological statistics capable of robustly identifying and distinguishing primordial inflationary features from standard models and disentangling these signatures from Epoch of Reionization effects when combined across multiple redshifts in 21 cm surveys. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The redshifted 21 cm signal from the cosmic dawn and Epoch of Reionization (EoR) encodes important information about both astrophysical processes and primordial physics, such as inflation. In this work, we use morphological statistics to explore the sensitivity of the 21 cm signal to inflationary features and EoR dynamics simultaneously. Focusing on primordial features from particle production during inflation we generate semi-numerical simulations of the 21 cm signal across redshifts 5 &lt; z &lt; 35, incorporating these features. Using Minkowski Functionals (MFs), we analyze the morphology of 21 cm fields: density, neutral hydrogen fraction, spin temperature, and brightness temperature. We demonstrate that MFs are highly sensitive to both the amplitude and scale of primordial features, capturing rich morphological information. In particular, we show that MFs can robustly identify inflationary features and distinguish them from the standard model. We further explore various EoR scenarios, and demonstrate that combining MFs across redshifts can disentangle the signatures of primordial features from EoR effects. This approach opens new avenues for probing inflation with upcoming 21 cm surveys.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.19450" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.19450" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.19450" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Algorithms, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
    </div>

    <footer class="footer">
        &copy; 2026 arXiv Daily · Powered by <a href="https://arxiv.org/" target="_blank">arXiv</a>
    </footer>
</body>
</html>