<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>arXiv Daily: Backfill (2025-11-16 to 2025-11-23)</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto:400,700&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Roboto', Arial, sans-serif; background: #f8f9fa; transition: background 0.3s; }
        .arxiv-card { margin: 2em auto; max-width: 900px; box-shadow: 0 2px 8px rgba(0,0,0,0.08); border-radius: 16px; transition: box-shadow 0.3s, transform 0.3s; }
        .arxiv-card:hover { box-shadow: 0 4px 12px rgba(0,0,0,0.1); transform: translateY(-1px); }
        .arxiv-title { background: linear-gradient(90deg, #0d6efd 60%, #6610f2 100%); color: #fff; border-radius: 16px 16px 0 0; padding: 1.5em; font-size: 1.5em; font-weight: 700; letter-spacing: 0.5px;}
        .arxiv-meta { font-size: 1em; color: #555; margin-bottom: 1em; }
        .arxiv-abstract { background: #fff; padding: 1.5em; border-radius: 0 0 16px 16px; font-size: 1.1em; line-height: 1.7;}
        .arxiv-links a { margin-right: 1em; }
        .arxiv-scores { margin-top: 1em; font-size: 1em; }
        .navbar { background: #fff; box-shadow: 0 2px 8px rgba(0,0,0,0.04);}
        .footer { text-align: center; color: #888; padding: 2em 0 1em 0; font-size: 0.95em;}
        @media (prefers-color-scheme: dark) {
            body { background: #181a1b; color: #e4e6eb; }
            .arxiv-card { background: #23272b; box-shadow: 0 2px 8px rgba(0,0,0,0.28);}
            .arxiv-card:hover { box-shadow: 0 5px 15px rgba(0,0,0,0.25); transform: translateY(-1px); }
            .arxiv-title { background: linear-gradient(90deg, #375a7f 60%, #6f42c1 100%);}
            .arxiv-abstract { background: #23272b; color: #e4e6eb;}
            .navbar { background: #23272b; color: #e4e6eb;}
            .text-muted { color: #adb5bd !important; }
        }
        /* Improved abstract readability */
        .arxiv-abstract-text {
            font-size: 1.1em;
            line-height: 1.7;
        }
        p.big {
            line-height: 1.6;
            font-size: large;
        }
    </style>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'] ],
          processEscapes: true
        }
      });
    </script>
    <script>
    function toggleAuthors(element) {
        const fullList = element.querySelector('.author-full');
        const shortList = element.querySelector('.author-short');
        const toggleText = element.querySelector('.author-toggle');
        
        fullList.style.display = fullList.style.display === 'none' ? 'inline' : 'none';
        shortList.style.display = shortList.style.display === 'none' ? 'inline' : 'none';
        toggleText.textContent = fullList.style.display === 'none' ? ' (show all)' : ' (show less)';
    }
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>
<body>
    <nav class="navbar navbar-expand-lg mb-4">
        <div class="container">
            <a class="navbar-brand fw-bold text-primary" href="#">arXiv Daily</a>
            <span class="navbar-text">Modern arXiv Reader</span>
        </div>
    </nav>

    
    <div class="container py-4">
        <header class="mb-4"><h2 class="display-6 text-center">Analysis of Your Favorites</h2></header>
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Word Cloud</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/word_cloud.png" class="img-fluid rounded" alt="Word Cloud of favorite paper topics">
                    </div>
                </div>
            </div>
        </div>
        
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Clusters</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/backfill_cluster_map.png" class="img-fluid rounded" alt="2D Cluster plot of favorite papers">
                    </div>
                </div>
            </div>
        </div>
        
    </div>
    

    <div class="container py-4">
        <header class="mb-4"><h1 class="display-5 text-center text-primary">arXiv: Backfill (2025-11-16 to 2025-11-23)</h1></header>

        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Score Distribution of All Papers Found Today</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/score_distribution.png" class="img-fluid rounded" alt="Score distribution of all papers found today">
                    </div>
                </div>
            </div>
        </div>
        

        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">1. Galaxy-Multiplet Clustering from DESI DR2
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Hanyue Wang, Daniel J. Eisenstein, Jessica Nicole Aguilar, Steven Ahlen, Davide Bianchi, David Brooks, Todd Claybaugh, Axel de la Macorra, Arjun Dey, Biprateep Dey, et al.</span>
                                <span class="author-full" style="display: none;">Hanyue Wang, Daniel J. Eisenstein, Jessica Nicole Aguilar, Steven Ahlen, Davide Bianchi, David Brooks, Todd Claybaugh, Axel de la Macorra, Arjun Dey, Biprateep Dey, Peter Doel, Simone Ferraro, Andreu Font-Ribera, Jaime E. Forero-Romero, Enrique Gaztañaga, Gaston Gutierrez, Klaus Honscheid, Mustapha Ishak, Richard Joyce, Stephanie Juneau, David Kirkby, Theodore Kisner, Anthony Kremin, Ofer Lahav, Claire Lamman, Martin Landriau, Marc Manera, Aaron Meisner, Ramon Miquel, Eva-Maria Mueller, Seshadri Nadathur, Gustavo Niz, Nathalie Palanque-Delabrouille, Will J. Percival, Francisco Prada, Ignasi Pérez-Ràfols, Ashley J. Ross, Graziano Rossi, Eusebio Sanchez, David Schlegel, Michael Schubnell, Joseph Harry Silber, David Sprayberry, Gregory Tarlé, Benjamin Alan Weaver, Rongpu Zhou, Hu Zou</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Measuring the cross-correlation of DESI Luminous Red Galaxy multiplets demonstrates that standard Halo Occupation Distribution models require secondary biases, such as environmental density dependence, to accurately capture the observed, stronger clustering associated with massive dark matter halos. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present an efficient estimator for higher-order galaxy clustering using small groups of nearby galaxies, or multiplets. Using the Luminous Red Galaxy (LRG) sample from the Dark Energy Spectroscopic Instrument (DESI) Data Release 2, we identify galaxy multiplets as discrete objects and measure their cross-correlations with the general galaxy field. Our results show that the multiplets exhibit stronger clustering bias as they trace more massive dark matter halos than individual galaxies. When comparing the observed clustering statistics with the mock catalogs generated from the N-body simulation AbacusSummit, we find that the mocks underpredict multiplet clustering despite reproducing the galaxy two-point auto-correlation reasonably well. This discrepancy indicates that the standard Halo Occupation Distribution (HOD) model is insufficient to describe the properties of galaxy multiplets, revealing the greater constraining power of this higher-order statistic on galaxy-halo connection and the possibility that multiplets are specific to additional assembly bias. We demonstrate that incorporating secondary biases into the HOD model improves agreement with the observed multiplet statistics, specifically by allowing galaxies to preferentially occupy halos in denser environments. Our results highlight the potential of utilizing multiplet clustering, beyond traditional two-point correlation measurements, to break degeneracies in models describing the galaxy-dark matter connection.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.15354" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.15354" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.15354" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 11.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.42</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmological Constraints, Survey Analysis</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">2. Unveiling the Sources of X-ray Luminosity in DESI Galaxy Groups: Insights from the SRG/eROSITA All-Sky Survey
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">YunLiang Zheng, Xiaohu Yang, Teng Liu, Shijiang Chen, Esra Bulbul, Ang Liu, Yi Zhang, Dawei Li, Xi Kang, Yizhou Gu, et al.</span>
                                <span class="author-full" style="display: none;">YunLiang Zheng, Xiaohu Yang, Teng Liu, Shijiang Chen, Esra Bulbul, Ang Liu, Yi Zhang, Dawei Li, Xi Kang, Yizhou Gu, Yirong Wang, Qingyang Li, Jiaqi Wang</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Investigating eROSITA galaxy groups reveals that central Active Galactic Nuclei dominate X-ray luminosity in low-mass halos ($M_h \lesssim 10^{13}h^{-1}M_\odot$), while extended gas dominates in higher-mass systems, providing new context for feedback processes despite inconclusive evidence for a direct AGN-gas correlation. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We use the first eROSITA all-sky survey (eRASS1) to investigate the contributions of AGN and extended gas to the total X-ray luminosity ($L_X$) of galaxy groups with different halo masses ($M_h$) at different redshifts. The presence of AGN in their central galaxies is identified using multi-wavelength catalogs, including the X-ray counterparts, the ASKAP radio catalog, and the DESI spectroscopic measurements. We apply the stacking method to obtain sufficient statistics for the X-ray surface brightness profile and the $L_X$ for groups with different central AGN properties. We find that the X-ray groups exhibit the highest $L_X$, followed by groups with QSO, radio, BPT-AGN, and non-AGN centrals. Moreover, the $L_X$ of the $M_h \lesssim 10^{13}h^{-1}M_\odot$ groups is dominated by the central AGN, while the X-ray emission from extended gas tends to be more prominent in the $M_h \gtrsim 10^{13}h^{-1}M_\odot$ groups. In groups where the AGN play a major role in X-ray emission, the contribution from extended gas is minor, resulting in significant uncertainties concerning the extended X-ray emission. When the subset containing the X-ray detected counterparts is excluded, the extended gas component becomes easier to obtain. A correlation has been identified between the X-ray luminosity of the central AGN and extended gas. However, once we account for the positional offset, their correlation becomes less prominent. Currently, the results are not conclusive enough to confirm whether there is a connection between the AGN feedback and extended gas. However, they provide a new perspective on the feedback processes in the history of group assembly.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.14216" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.14216" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.14216" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.23</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmological Constraints, Survey Analysis / Galaxy-Dark Matter, Lensing, Clustering</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">3. The PAU Survey: The $i$-band galaxy luminosity function from the present-day to $z = 2$
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">S. Koonkor, C. M. Baugh, G. Manzoni, D. Navarro-Gironés, P. Renard, H. Hoekstra, H. Hildebrandt, E. Gaztañaga, J. García-Bellido, P. Tallada-Crespí, et al.</span>
                                <span class="author-full" style="display: none;">S. Koonkor, C. M. Baugh, G. Manzoni, D. Navarro-Gironés, P. Renard, H. Hoekstra, H. Hildebrandt, E. Gaztañaga, J. García-Bellido, P. Tallada-Crespí, F. J. Castander, J. De Vincente, R. Casas, R. Miquel, N. Sevilla-Noarbe, M. Eriksen</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The $i$-band galaxy luminosity function measured using 1.1 million PAUS galaxies up to $z=2$ demonstrates that while models agree well at $z&lt;1$, increasing photometric redshift errors at higher redshifts lead to discrepancies, particularly in the flattening of the bright end and the overprediction of faint galaxies. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present a measurement of the $i$-band galaxy luminosity function from the present-day to $z = 2$, using over 1.1 million galaxies from the Physics of the Accelerating Universe Survey (PAUS). PAUS combines broad-band imaging from the Canada-France-Hawaii Telescope Lensing Survey with narrow-band photometry from PAUCam, enabling high-precision photometric redshifts with an accuracy of $σ_{68} (Δz) = 0.019$ down to $i_{\textrm{AB}} = 23$. A synthetic lightcone mock catalogue built using the \texttt{GALFORM} semi-analytic model is used to simulate PAUS selection effects and photometric uncertainties, and to derive a machine-learning based estimate of the $k$-correction. We recover rest-frame $i$-band luminosities using a random forest regressor trained on simulated $ugriz$ photometry and redshifts. Luminosity functions are estimated using the $1/V_{\textrm{max}}$ method, accounting for photometric redshift and magnitude errors, and validated against mock data. We find good agreement between observations and models at $z &lt; 1$, with increasing discrepancies at higher redshifts due to photometric redshift outliers. The bright-end of the luminosity function becomes flatter at high redshift, primarily driven by redshift errors. We show that the faint-end of the luminosity function becomes more incomplete with increasing redshift, but is still useful for constraining models. We analyze the red and blue galaxy populations separately, observing distinct evolutionary trends. The model overpredicts the number of both faint red and blue galaxies. Our study highlights the importance of accurate redshift estimation and selection modeling for robust luminosity function recovery, and demonstrates that PAUS can characterise the galaxy population with photometric redshifts across a wide redshift baseline.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.16042" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.16042" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.16042" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.19</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmological Constraints, Survey Analysis / Galaxy-Dark Matter, Lensing, Clustering</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">4. The first AKRA mass map reconstruction from HSC Y1 data
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yuan Shi, Pengjie Zhang, Zhao Chen, Jian Qin, Li Cui, Furen Deng, Ji Yao</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The prior-free, maximum-likelihood Accurate Kappa Reconstruction Algorithm (AKRA) was validated using Hyper Suprime-Cam Year 1 mock data, demonstrating unbiased recovery of lensing statistics, before being applied to the real HSC Y1 shear catalog to produce reconstructed convergence maps. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Weak lensing mass-mapping from shear catalogs faces systematic challenges from survey masks and spatially varying noise. To overcome these issues and reconstruct unbiased convergence $κ$ maps, we have constructed the AKRA (Accurate Kappa Reconstruction Algorithm), a prior-free and maximum-likelihood based analytical method. It has been validated for mock shear catalogs with a variety of survey masks. In this work, we present the first real-data application of the AKRA on the Subaru Hyper Suprime-Cam Year 1 (HSC Y1) data. We first validate AKRA using mock shear catalogs from the \texttt{Kun} simulation suite, with masks corresponding to the six HSC Y1 regions (\texttt{GAMA09H}, \texttt{GAMA15H}, \texttt{HECTOMAP}, \texttt{VVDS}, \texttt{WIDE12H}, and \texttt{XMMLSS}). The investigated statistics, including the lensing power spectrum, $\langle κ^2\rangle$, $\langle κ^3\rangle$, and the one-point probability distribution function of $κ$, are all unbiased. We then apply AKRA to the HSC Y1 shear catalog and provide reconstructed $κ$ maps ready for subsequent scientific analyses.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.12488" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.12488" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.12488" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.18</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">5. High-resolution weak lensing mass mapping from DES-Y3 data using diffusion-based prior
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Supranta S. Boruah, Michael Jacob, Bhuvnesh Jain, Riya Maiya, Raghav Venkataramanan</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> High-resolution mass maps were successfully reconstructed from DES-Y3 weak lensing data using a generative diffusion model and Diffusion Posterior Sampling, provided that the log-likelihood score was scaled during the process to correct for inherent biases and ensure proper uncertainty quantification. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">High-resolution mapping of cosmic mass distribution is essential for a variety of astrophysical applications including understanding cosmic structure formation, and galaxy formation and evolution. However dark matter is not directly observed and therefore we need advanced methods for solving inverse problems to reconstruct the underlying cosmic matter distribution. Here, we train a generative diffusion model and use it in the Diffusion Posterior Sampling (DPS) framework to reconstruct mass maps from Dark Energy Survey-Year 3 (DES-Y3) weak gravitational lensing data at high (1 arcminute) resolution. We show that the standard DPS results are biased, but they can be easily corrected by scaling the log-likelihood score during the diffusion process, yielding unbiased results with proper uncertainty quantification. The resulting mass maps reveal cosmic structures with enhanced detail, opening the door for improved astrophysical studies using the obtained mass maps.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.14667" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.14667" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.14667" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.11</span>
                        <span class="badge bg-primary">Semantic Score: 0.96</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling / Cosmological Inference, Bayesian Sampling, Structure Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">6. Learning Cosmology from Nearest Neighbour Statistics
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Atrideb Chatterjee, Arka Banerjee, Francisco Villaescusa-Navarro, Tom Abel</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Achieving state-of-the-art accuracy in cosmological parameter inference, a hybrid neural network utilizes novel NN distance maps—a field-level representation derived from k-Nearest Neighbour statistics—applied to halo catalogs from the Quijote N-body simulations. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Extracting cosmological parameters from galaxy/halo catalogues with sub-percent level accuracy is an important aspect of modern cosmology, especially in view of ongoing and upcoming surveys such as Euclid, DESI, and LSST. While traditional two-point statistics have been known to be suboptimal for this task, recently proposed k-Nearest Neighbour (kNN) based summary statistics have demonstrated tighter constraining power. Building on the kNN statistics, we introduce a new field-level representation of discrete halo catalogues - NN distance maps. We employ this technique on the halo catalogues obtained from Quijote N-body simulation suites. By combining these maps with kNN-based summary statistics, we train a hybrid neural network to infer cosmological parameters, showing that the resulting constraints achieve state-of-the-art, if not the best, accuracy. In addition, our hybrid framework is 5-10 times more computationally efficient than some of the existing point-cloud-based ML methods.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.13393" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.13393" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.13393" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.7</span>
                        <span class="badge bg-info text-dark">Author Score: 0.04</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">7. The Chandra Strong Lens Sample: Measuring the Dynamical States and Relaxation Fraction of a Sample of 28 Strong Lensing Selected Galaxy Clusters
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Raven Gassis, Matthew B. Bayliss, Michael McDonald, Keren Sharon, Guillaume Mahler, Michael D. Gladders, Hakon Dahle, Michael K. Florian, Jane R. Rigby, Lauren A. Elicker, et al.</span>
                                <span class="author-full" style="display: none;">Raven Gassis, Matthew B. Bayliss, Michael McDonald, Keren Sharon, Guillaume Mahler, Michael D. Gladders, Hakon Dahle, Michael K. Florian, Jane R. Rigby, Lauren A. Elicker, M. Riley Owens, Prasanna Adhikari, Gourav Khullar</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> By combining four X-ray and optical morphological parameters into a single metric, the dynamical state classification of 28 strong lensing galaxy clusters reveals a relaxation fraction of $43\%$, which is consistent with clusters selected via mass-observables. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present the results of our dynamical state proxy measurements performed on 28 strong lensing galaxy clusters from the Sloan Giant Arcs Survey (SGAS). Using Chandra ACIS-I/S X-ray data supplemented with HST WFC3 imaging, we measure four morphological parameters: the concentration parameter (c), asymmetry parameter (A), centroid shift (log(w)), and the X-ray-BCG centroid separation (D [kpc]). Our goals are to (A) provide a robust classification of the dynamical state of the clusters in this strong lensing selected sample to enable studies that test various problems in cluster astrophysics and observational cosmology; (B) identify correlations, biases, or disagreements between different measurement proxies and cluster properties; and (C) measure the relaxation fraction (the fraction of clusters classified as relaxed based on X-ray morphology) and compare it to relaxation fractions from cluster samples selected using other methods. We combine the four morphological parameters into a single metric, the combined parameter M, which effectively separates the cluster sample into four dynamical state categories: relaxed; moderately relaxed; moderately disturbed; and disturbed. We find no significant trend in a cluster&#39;s dynamical state with its size, and only a weak, statistically limited dependence on mass and redshift. Based on our classification system, we find that $43\%^{+9}_{-9}$ of the clusters are relaxed, which is consistent with relaxation fractions measured for other cluster samples selected on mass-observables. This implies a strong lensing selected sample of clusters is on average dynamically similar to clusters selected via different methods.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.12707" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.12707" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.12707" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.7</span>
                        <span class="badge bg-info text-dark">Author Score: 0.05</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmological Constraints, Survey Analysis</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">8. The Atacama Cosmology Telescope: Cross-correlation of kSZ and continuity equation velocity reconstruction with photometric DESI LRGs
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Fiona McCarthy, Boryana Hadzhiyska, J. Richard Bond, William R. Coulton, Jo Dunkley, Carmen Embil Villagra, Matthew C. Johnson, Kavilan Moodley, Toshiya Namikawa, Bernardita Ried Guachalla, et al.</span>
                                <span class="author-full" style="display: none;">Fiona McCarthy, Boryana Hadzhiyska, J. Richard Bond, William R. Coulton, Jo Dunkley, Carmen Embil Villagra, Matthew C. Johnson, Kavilan Moodley, Toshiya Namikawa, Bernardita Ried Guachalla, Blake D. Sherwin, Cristóbal Sifón, Alexander van Engelen, Eve M. Vavagiakis, Edward J. Wollack</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Utilizing tomographic kinematic Sunyaev–Zel&#39;dovich velocity reconstruction with ACT DR6 and DESI Luminous Red Galaxies, a robust $11\sigma$ measurement of the velocity amplitude ($b_v=0.33\pm0.03$) was achieved, constraining local primordial non-Gaussianity to be consistent with zero at 95% confidence. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Over the last year, kinematic Sunyaev--Zel&#39;dovich (kSZ) velocity reconstruction -- the measurement of the large-scale velocity field using the anisotropic statistics of the small-scale kSZ-galaxy overdensity correlation -- has emerged as a statistically significant probe of the large-scale Universe. In this work, we perform a 2-dimensional tomographic reconstruction using ACT DR6 CMB data and DESI legacy luminous red galaxies (LRGs). We measure the cross-correlation of the kSZ-reconstructed velocity $v^{\mathrm{kSZ}}$ with the velocity inferred from the continuity equation applied to the DESI LRGs $v^{\mathrm{cont}}$ at the $\sim 10 σ$ level, detecting the signal with an amplitude with respect to our theory of $b_v = 0.339\pm 0.034$. We fit a scale-dependent galaxy bias model to our measurement in order to constrain local primordial non-Gaussianity $f_{\mathrm{NL}}^{\mathrm{loc}}$, finding {$f_{\mathrm{NL}}^{\mathrm{loc}}=-180^{+61}_{-86}$} at 67\% confidence, with $f_{\mathrm{NL}}^{\mathrm{loc}}$ consistent with zero at 95\% confidence. We also measure an auto spectrum at $2.1σ$ significance which provides a constraint on $b_v$ of $b_v=0.26_{-0.05}^{+0.11}$, which is consistent with the measurement from the cross spectrum. Our combined measurement is $b_v=0.33\pm0.03$, an $11σ$ measurement. We find a good fit of our model to the data in all cases. Finally, we use different ACT frequency combinations to explore foreground contamination, finding no evidence for foreground contamination in our velocity cross correlation. We compare to a similar measurement where $v^{\mathrm{kSZ}}$ is directly cross correlated with the large-scale galaxy field, and find signs of foreground contamination which is contained in the equal-redshift spectra.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.15701" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.15701" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.15701" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.7</span>
                        <span class="badge bg-info text-dark">Author Score: 0.02</span>
                        <span class="badge bg-primary">Semantic Score: 0.96</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmological Constraints, Survey Analysis</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">9. Constraining the Neutral Hydrogen Fraction from SKA Simulated Observation using a Double-Gaussian Decomposition Technique
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jiajun Zhang, Huanyuan Shan</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The evolution of the global neutral hydrogen fraction during the Epoch of Reionization was successfully extracted from noisy SKA 21 cm intensity maps using a statistically robust method that models the pixel histogram as a mixture of two Gaussian components representing ionized and neutral regions. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The Epoch of Reionization (EoR) is a unique phase in cosmic history, marked by the ionization of neutral hydrogen by the first luminous sources. The global neutral hydrogen fraction (x_HI) is a key observable for probing this era. This paper presents a novel, statistically robust method to extract the evolution of x_HI from the challenging noise-dominated data from the Square Kilometre Array (SKA) Data Challenge 3b. Our approach is based on a key physical insight: the pixel value distribution in SKA intensity maps is a mixture of signals from ionized and neutral regions. We model this distribution as a superposition of two Gaussian components-one fixed at zero representing noise and ionized bubbles, and a second, offset Gaussian tracing the neutral hydrogen signal. We perform this decomposition on data grouped into three redshift bins. The double-Gaussian model provides an excellent fit to the pixel histogram data. The derived x_HI values show a clear decreasing trend across the three redshift bins, consistent with a progressing reionization process. And the results are consistent with the provided simulation data. This method offers a powerful, model-independent, and fully interpretable way for measuring x_HI from 21 cm data, demonstrating significant potential for application to future SKA observations.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.13370" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.13370" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.13370" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.03</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">10. Aletheia: Emulating the non-linear matter power spectrum in the context of evolution mapping
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Ariel G. Sanchez, Andrés N. Ruiz, Facundo Rodriguez, Carlos Correa, Andrea Fiorilli, Matteo Esposito, Jenny Gonzalez-Jara, Nelson D. Padilla, Alejandro Pérez-Fernández, Sofia Contarini</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Leveraging the evolution mapping framework, the Aletheia emulator accurately predicts the non-linear matter power spectrum with sub-percent precision and high robustness (0.2% variance), successfully modeling dynamic dark energy cosmologies outside its primary training range. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present Aletheia, a new emulator of the non-linear matter power spectrum, $P(k)$, built upon the evolution mapping framework. This framework addresses the limitations of traditional emulation by focusing on $h$-independent cosmological parameters, which can be separated into those defining the linear power spectrum shape ($\mathbfΘ_{\mathrm{s}}$) and those affecting only its amplitude evolution ($\mathbfΘ_{\mathrm{e}}$). The combined impact of evolution parameters and redshift is compressed into a single amplitude parameter, $σ_{12}$. Aletheia uses a two-stage Gaussian Process emulation: a primary emulator predicts the non-linear boost factor as a function of ($\mathbfΘ_{\mathrm{s}}$) and $σ_{12}$ for fixed evolution parameters, while a second one applies a small linear correction based on the integrated growth history. The emulator is trained on shape parameters spanning $\pm$5$σ$ of Planck constraints and a wide clustering range $0.2 &lt; σ_{12} &lt; 1.0$, providing predictions for $0.006\,{\rm Mpc}^{-1} &lt; k &lt; 2\,{\rm Mpc}^{-1}$. We validate Aletheia against N-body simulations, demonstrating sub-percent accuracy. When tested on a suite of dynamic dark energy models, the full emulator&#39;s predictions show a variance of approximately 0.2%, a factor of five smaller than that of the state-of-the-art EuclidEmulator2 (around 1% variance). Furthermore, Aletheia maintains sub-percent accuracy for the best-fit dynamic dark energy cosmology from recent DESI data, a model whose parameters lie outside the training ranges of most conventional emulators. This demonstrates the power of the evolution mapping approach, providing a robust and extensible tool for precision cosmology.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.13826" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.13826" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.13826" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.02</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">11. The Eddington Ratio Distribution of Narrow Line Active Galactic Nuclei
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Michael R. Blanton, Arjun Suresh, Kyle B. Westfall, Dou Liu, John Moustakas</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Analyzing MaNGA integral field spectroscopy data while correcting for selection biases, the occurrence rate of narrow-line Active Galactic Nuclei (AGN) above an Eddington ratio of $10^{-3}$ is found to increase monotonically with specific star formation rate (sSFR) at high stellar masses, providing constraints on black hole growth models. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We measure the Eddington ratio distribution of local optical narrow line active galactic nuclei (AGN) as a function of host galaxy properties, as a potential test of supermassive black hole growth and feedback models in galaxy formation theory. We base our sample on integral field spectroscopy from the MaNGA data in SDSS-IV&#39;s DR17. Starting with MaNGA&#39;s calibrated row-stacked spectra, we produce new spectroscopic data cubes with minimal covariance between spaxels and higher resolution point spread functions (PSF), and then extract line fluxes for the central PSF. Using the line ratio diagnostic techniques of Ji &amp; Yan (2020), we identify AGN galaxies and determine their H$β$ and [O III] line luminosities. For all galaxies not identified as AGN, we determine the threshold line luminosity they would have needed to be identified as AGN. These luminosity thresholds are essential to determine, because many star forming galaxies likely host AGN of significant luminosity that are unidentified because they are outshone by star formation related emission. We show that ignoring these selection effects when measuring the Eddington ratio distribution would lead to biased results. From the H$β$ luminosities and luminosity detection thresholds, accounting for selection effects, we measure the luminosity and Eddington ratio distributions of Seyferts as a function of specific star formation rate (sSFR) and stellar mass. Defining $F_{\rm AGN}$ as the occurrence rate of AGN above a fixed Eddington ratio of $10^{-3}$, we find that $F_{\rm AGN}$ is constant or increasing with stellar mass for star forming galaxies and declines strongly with stellar mass for quiescent galaxies. At stellar masses $\log_{10} M_\ast &gt; 10.25$, the occurrence rate increases monotonically with sSFR. These patterns reveal a complicated dependence of AGN activity on galaxy properties for theoretical models to explain.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.14575" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.14575" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.14575" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.05</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmological Constraints, Survey Analysis / Galaxy-Dark Matter, Lensing, Clustering</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">12. Sensitivity to low-mass WIMPs with an improved liquid argon ionization response model within the DarkSide programme
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">F. Acerbi, P. Adhikari, P. Agnes, I. Ahmad, S. Albergo, I. F. Albuquerque, T. Alexander, A. K. Alton, P. Amaudruz, M. Angiolilli, et al.</span>
                                <span class="author-full" style="display: none;">F. Acerbi, P. Adhikari, P. Agnes, I. Ahmad, S. Albergo, I. F. Albuquerque, T. Alexander, A. K. Alton, P. Amaudruz, M. Angiolilli, E. Aprile, M. Atzori Corona, D. J. Auty, M. Ave, I. C. Avetisov, O. Azzolini, H. O. Back, Z. Balmforth, A. I. Barrado Olmedo, P. Barrillon, G. Batignani, S. Bharat, P. Bhowmick, S. Blua, V. Bocci, W. Bonivento, B. Bottino, M. G. Boulay, T. Braun, A. Buchowicz, S. Bussino, J. Busto, M. Cadeddu, R. Calabrese, V. Camillo, A. Caminata, N. Canci, M. Caravati, M. Cárdenas-Montes, N. Cargioli, M. Carlini, P. Cavalcante, S. Cebrian, S. Chashin, A. Chepurnov, S. Choudhary, L. Cifarelli, B. Cleveland, Y. Coadou, I. Coarasa, V. Cocco, E. Conde Vilda, L. Consiglio, A. F. V. Cortez, B. S. Costa, M. Czubak, S. D&#39;Auria, M. D. Da Rocha Rolo, A. Dainty, G. Darbo, S. Davini, R. de Asmundis, S. De Cecco, M. De Napoli, G. Dellacasa, A. V. Derbin, L. Di Noto, P. Di Stefano, L. K. Dias, D. Díaz Mairena, C. Dionisi, G. Dolganov, F. Dordei, V. Dronik, A. Elersich, T. Erjavec, N. Fearon, M. Fernández Díaz, L. Ferro, A. Ficorella, G. Fiorillo, D. Fleming, P. Franchini, D. Franco, H. Frandini Gatti, E. Frolov, F. Gabriele, D. Gahan, C. Galbiati, G. Galiński, G. Gallina, M. Garbini, P. Garcia Abia, A. Gawdzik, G. K. Giovanetti, V. Goicoechea Casanueva, A. Gola, L. Grandi, G. Grauso, G. Grilli di Cortona, A. Grobov, M. Gromov, J. Guerrero Cánovas, M. Gulino, B. R. Hackett, A. L. Hallin, M. Haranczyk, B. Harrop, T. Hessel, C. Hidalgo, J. Hollingham, J. Hu, F. Hubaut, D. Huff, T. Hugues, E. V. Hungerford, An. Ianni, V. Ippolito, A. Jamil, C. Jillings, R. Keloth, N. Kemmerich, M. Kimura, A. Klenin, K. Kondo, G. Korga, L. Kotsiopoulou, S. Koulosousas, A. Kubankin, P. Kunzé, M. Kuss, M. Kuźniak, M. Kuzwa, M. La Commara, M. Lai, E. Le Guirriec, E. Leason, A. Leoni, L. Lidey, J. Lipp, M. Lissia, L. Luzzi, O. Lychagina, O. Macfadyen, I. Machts, I. N. Machulin, S. Manecki, I. Manthos, L. Mapelli, A. Marasciulli, S. M. Mari, C. Mariani, J. Maricic, M. Martinez, C. J. Martoff, G. Matteucci, K. Mavrokoridis, A. B. McDonald, S. Merzi, A. Messina, R. Milincic, S. Minutoli, A. Mitra, J. Monroe, M. Morrocchi, A. Morsy, V. N. Muratova, M. Murra, P. Musico, R. Nania, M. Nessi, G. Nieradka, K. Nikolopoulos, E. Nikoloudaki, I. Nikulin, J. Nowak, K. Olchanski, A. Oleinik, V. Oleynikov, P. Organtini, A. Ortiz de Solórzano, A. Padmanabhan, M. Pallavicini, L. Pandola, E. Pantic, E. Paoloni, D. Papi, B. Park, G. Pastuszak, G. Paternoster, R. Pavarani, A. Peck, K. Pelczar, R. Perez, V. Pesudo, S. Piacentini, N. Pino, G. Plante, A. Pocar, S. Pordes, P. Pralavorio, E. Preosti, D. Price, M. Pronesti, S. Puglia, M. Queiroga Bazetto, F. Raffaelli, F. Ragusa, Y. Ramachers, A. Ramirez, S. Ravinthiran, M. Razeti, A. L. Renshaw, A. Repond, M. Rescigno, S. Resconi, F. Retiere, L. P. Rignanese, A. Ritchie-Yates, A. Rivetti, A. Roberts, C. Roberts, G. Rogers, L. Romero, M. Rossi, D. Rudik, J. Runge, M. A. Sabia, D. Sablone, P. Salomone, O. Samoylov, S. Sanfilippo, D. Santone, R. Santorelli, E. M. Santos, I. Sargeant, M. L. Sarsa, C. Savarese, E. Scapparone, F. G. Schuckman, D. A. Semenov, C. Seoane, M. Sestu, V. Shalamova, S. Sharma Poudel, A. Sheshukov, M. Simeone, P. Skensved, M. D. Skorokhvatov, O. Smirnov, T. Smirnova, B. Smith, F. Spadoni, M. Spangenberg, A. Steri, V. Stornelli, S. Stracka, A. Sung, C. Sunny, Y. Suvorov, A. M. Szelc, O. Taborda, R. Tartaglia, A. Taylor, J. Taylor, G. Testera, K. Thieme, A. Thompson, S. Torres-Lara, A. Tricomi, S. Tullio, E. V. Unzhakov, M. Van Uffelen, P. Ventura, G. Vera Díaz, S. Viel, A. Vishneva, R. B. Vogelaar, J. Vossebeld, B. Vyas, M. Wada, M. Walczak, Y. Wang, S. Westerdale, L. Williams, M. M. Wojcik, M. Wojcik, C. Yang, J. Yin, A. Zabihi, P. Zakhary, A. Zani, Y. Zhang, T. Zhu, A. Zichichi, G. Zuzel, M. P. Zykova</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> By combining and analyzing multiple liquid argon dark matter datasets, researchers refined the ionization response model by constraining atomic screening effects, leading to new world-leading exclusion limits on WIMP interactions in the 1-3 GeV/c$^2$ mass range and boosting the sensitivity projections for DarkSide-20k. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Dark matter detection experiments using liquid argon rely on a precise characterization of the ionization response to nuclear recoils, especially in the keV energy range relevant for light dark matter interactions. In this work, we present a comprehensive analysis that combines new measurements from the ReD setup, part of the DarkSide experimental program, with calibration data from DarkSide-50, as well as results from the ARIS and SCENE experiments. These combined datasets enable improved constraints on atomic screening effects in the modeling of the ionization response of liquid argon to nuclear recoils. By including the updated ionization model into the DarkSide-50 analysis framework, we obtain stronger exclusion limits on low-mass WIMP interactions, setting new world-leading constraints in the 1-3 GeV/c$^2$ WIMP mass range. Finally, we recast the sensitivity projections for the upcoming DarkSide-20k detector, demonstrating a significantly enhanced discovery potential for low-mass dark matter candidates.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.13629" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.13629" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.13629" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.05</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">hep-ex</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">13. Likelihood-guided Regularization in Attention Based Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Mohamed Salem, Inyoung Kim</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A novel likelihood-guided variational Ising-based regularization framework applies structured Bayesian sparsification to Vision Transformers (ViTs), dynamically pruning redundant parameters and enhancing generalization, interpretability, and uncertainty quantification across benchmark vision datasets. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The transformer architecture has demonstrated strong performance in classification tasks involving structured and high-dimensional data. However, its success often hinges on large- scale training data and careful regularization to prevent overfitting. In this paper, we intro- duce a novel likelihood-guided variational Ising-based regularization framework for Vision Transformers (ViTs), which simultaneously enhances model generalization and dynamically prunes redundant parameters. The proposed variational Ising-based regularization approach leverages Bayesian sparsification techniques to impose structured sparsity on model weights, allowing for adaptive architecture search during training. Unlike traditional dropout-based methods, which enforce fixed sparsity patterns, the variational Ising-based regularization method learns task-adaptive regularization, improving both efficiency and interpretability. We evaluate our approach on benchmark vision datasets, including MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100, demonstrating improved generalization under sparse, complex data and allowing for principled uncertainty quantification on both weights and selection parameters. Additionally, we show that the Ising regularizer leads to better-calibrated probability estimates and structured feature selection through uncertainty-aware attention mechanisms. Our results highlight the effectiveness of structured Bayesian sparsification in enhancing transformer-based architectures, offering a principled alternative to standard regularization techniques.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.13221" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.13221" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.13221" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">14. ODE-ViT: Plug &amp; Play Attention Layer from the Generalization of the ViT as an Ordinary Differential Equation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Carlos Boned Riera, David Romero Sanchez, Oriol Ramos Terrades</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Reformulating the Vision Transformer as ODE-ViT, a stable and well-posed Ordinary Differential Equation system, yields competitive classification performance with up to an order of magnitude fewer parameters, which is further improved using a plug-and-play teacher-student distillation framework. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">In recent years, increasingly large models have achieved outstanding performance across CV tasks. However, these models demand substantial computational resources and storage, and their growing complexity limits our understanding of how they make decisions. Most of these architectures rely on the attention mechanism within Transformer-based designs. Building upon the connection between residual neural networks and ordinary differential equations (ODEs), we introduce ODE-ViT, a Vision Transformer reformulated as an ODE system that satisfies the conditions for well-posed and stable dynamics. Experiments on CIFAR-10 and CIFAR-100 demonstrate that ODE-ViT achieves stable, interpretable, and competitive performance with up to one order of magnitude fewer parameters, surpassing prior ODE-based Transformer approaches in classification tasks. We further propose a plug-and-play teacher-student framework in which a discrete ViT guides the continuous trajectory of ODE-ViT by treating the intermediate representations of the teacher as solutions of the ODE. This strategy improves performance by more than 10% compared to training a free ODE-ViT from scratch.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.16501" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.16501" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.16501" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">15. Equilateral non-Gaussian Bias at the Field Level
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Divij Sharma, James M. Sullivan, Kazuyuki Akitsu, Mikhail M. Ivanov</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Precision measurements of the equilateral primordial non-Gaussianity bias coefficient $b_\psi$ for dark matter halos are achieved using field-level effective field theory, which successfully disentangles PNG from Gaussian bias effects and provides a new fitting formula for cosmological constraints. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Primordial non-Gaussianity (PNG) is a common prediction of a wide class of inflationary models. Equilateral-type PNG, generically predicted by single-field inflationary models with higher-derivative interactions, imprints subtle but measurable signatures on the large-scale distribution of matter. An important parameter of these imprints is the PNG-induced bias coefficient $b_ψ$, which quantifies how the abundance and clustering of dark matter halos and galaxies respond to mode coupling in the initial conditions. Measuring $b_ψ$ is important for constraining equilateral PNG, yet it is notoriously challenging due to its degeneracy with Gaussian scale-dependent bias contributions. In this work, we present the first precision measurements of equilateral $b_ψ$ for dark matter halos using effective field theory at the field level. We show that this approach disentangles PNG effects from those of the Gaussian bias by virtue of noise variance cancellation. We compare our results with the phenomenological predictions based on the Peak-Background Split model, finding some agreement at the qualitative level on the redshift and mass dependence, but poor agreement at the quantitative level. We present a fitting formula for $b_ψ$ as a function of the linear bias, which can be used to set priors in PNG searches with ongoing and future galaxy surveys.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.14677" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.14677" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.14677" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">16. Auto-encoder model for faster generation of effective one-body gravitational waveform approximations
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Suyog Garg, Feng-Li Lin, Kipp Cannon</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A conditional variational auto-encoder model is developed for the rapid generation of aligned-spin SEOBNRv4 inspiral-merger-ringdown gravitational waveforms, achieving generation speeds 2-3 orders of magnitude faster than standard implementations with high fidelity (median mismatch $\sim10^{-2}$). (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Upgrades to current gravitational wave detectors for the next observation run and upcoming third-generation observatories, like the Einstein telescope, are expected to have enormous improvements in detection sensitivities and compact object merger event rates. Estimation of source parameters for a wider parameter space that these detectable signals will lie in, will be a computational challenge. Thus, it is imperative to have methods to speed-up the likelihood calculations with theoretical waveform predictions, which can ultimately make the parameter estimation faster and aid in rapid multi-messenger follow-ups. Towards this end, we present a conditional variational auto-encoder model, based on the best performing architecture of Liao+2021, for faster generation of aligned-spin SEOBNRv4 inspiral-merger-ringdown waveforms. Our parameter space consists of four parameters, [$m_1$, $m_2$, $χ_1(z)$, $χ_2(z)$]. The masses are uniformly sampled in $[5,75]\,M_{\odot}$ with a mass ratio limit at $10\,M_{\odot}$, while the spins are uniform in $[-0.99,0.99]$. We train the model using $\sim10^5$ input waveforms data with a 70\%/10\% train/validation split, while 20\% data are reserved for testing. The median mismatch for the generated waveforms in the test dataset is $\sim10^{-2}$, with better performance in a restricted parameter space of $χ_{\rm eff}\in[-0.80,0.80]$. Our model is able to generate 100 waveforms in 0.1 second at an average speed of about 4.46 ms per waveform. This is 2-3 orders of magnitude faster than the native SEOBNRv4 implementation in lalsimulation. The latent sampling uncertainty of our model can be quantified with a mean mismatch deviation of $2\times10^{-1}$ for 1000 generations of the same waveform. Our work aims to be the first step towards developing a production-ready machine learning framework for the faster generation of gravitational waveform approximations.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.12642" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.12642" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.12642" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">17. Denoising weak lensing mass maps with diffusion model and generative adversarial network
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Shohei D. Aoyama, Ken Osato, Masato Shirasaki</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Diffusion models (DM) are shown to surpass Generative Adversarial Networks (GANs) for weak gravitational lensing denoising, offering superior training stability, more robust signal recovery through sample averaging, and higher accuracy in reproducing cosmological statistics from mock observations. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The matter distribution of the Universe can be mapped through the weak gravitational lensing (WL) effect: small distortions of the shapes of distant galaxies, which reflects the inhomogeneity of the cosmic density field. The most dominant contaminant in the WL effect is the shape noise; the signal is diluted due to the finite number of source galaxies. In order to explore the full potential of WL measurements, sharpening the signal by removing the shape noise from the observational data, i.e., WL denoising, is a pressing issue. Machine learning approaches, in particular, deep generative models, have proven effective at the WL denoising task. We implement a denoising model based on the diffusion model (DM) and conduct systematic in-depth comparisons with generative adversarial networks (GANs), which have been applied in previous works for WL denoising. Utilizing the large suite of mock simulations of WL observations, we demonstrate that DM surpasses GAN in the WL denosing task in multiple aspects: (1) the training process is more stable, (2) taking the average of multiple samples from DM can robustly reproduce the true signal, and (3) DM can recover various statistics with higher accuracy.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.16415" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.16415" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.16415" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.04</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">18. VENUS: A Strongly Lensed Clumpy Galaxy at $z\sim11-12$ behind the Galaxy Cluster MACS J0257.1-2325
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Minami Nakane, Vasily Kokorev, Seiji Fujimoto, Masami Ouchi, Derek J. McLeod, Miriam Golubchik, Masamune Oguri, Adi Zitrin, Cecilia Bondestam, Callum T. Donnan, et al.</span>
                                <span class="author-full" style="display: none;">Minami Nakane, Vasily Kokorev, Seiji Fujimoto, Masami Ouchi, Derek J. McLeod, Miriam Golubchik, Masamune Oguri, Adi Zitrin, Cecilia Bondestam, Callum T. Donnan, Gabriel Brammer, Steven L. Finkelstein, Chris Willott, Angela Adamo, Eros Vanzella, Marusa Bradač, Matteo Messa, Hiroto Yanagisawa, Fengwu Sun, Henry C. Ferguson, Ray A. Lucas, Dan Coe, Johan Richard, Abdurro&#39;uf, Hollis B. Akins, Joseph F. V. Allingham, Ricardo O. Amorín, Yoshihisa Asada, Hakim Atek, Rachel Bezanson, Larry D. Bradley, John Chisholm, Christopher J. Conselice, Pratika Dayal, Miroslava Dessauges-Zavadsky, Jose M. Diego, Andreas L. Faisst, Qinyue Fei, Brenda L. Frye, Yoshinobu Fudamoto, Lukas J. Furtak, Yuichi Harikane, Tiger Yu-Yang Hsiao, Yolanda Jiménez-Teja, Jeyhan S. Kartaltepe, Tomokazu Kiyota, Anton M. Koekemoer, Claudia del P. Lagos, Georgios E. Magdis, Ashish Kumar Meena, Lamiya Mowla, Gaël Noirot, Pascal A. Oesch, Yoshiaki Ono, Rafael Ortiz, Richard Pan, Casey Papovich, Justin D. Pierel, Massimo Ricotti, Luke Robbins, Daniel Schaerer, Raffaella Schneider, Tommaso Treu, Francesco Valentino, Rogier A. Windhorst, Franz E. Bauer, Volker Bromm, Eiichi Egami, Mauro González-Otero, Kotaro Kohno, Ivo Labbe, Jorryt Matthee, Marcie Mun, Rohan P. Naidu, Roberta Tripodi</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The strongly lensed &#34;Misty Moons&#34; galaxy, discovered by JWST at $z\sim11-12$, is resolved into multiple dense stellar clumps, indicating a highly clustered mode of star formation characteristic of typical faint galaxies in the early Universe. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present the discovery of a strongly lensed galaxy at $z\sim11-12$, dubbed the ``Misty Moons&#39;&#39;, identified in the JWST Treasury Survey, Vast Exploration for Nascent, Unexplored Sources (VENUS). The Misty Moons is gravitationally lensed by the galaxy cluster MACS J0257.1-2325 at $z=0.505$, and has five multiple images suggested by two independent lensing models. Two of the five images, ID1 and ID2 ($μ\sim 20-30$), are very bright (F200W$\sim26$ AB mag) and exhibit blue SEDs with prominent Ly$α$ breaks. In the source plane, the Misty Moons is a sub-$L^*$ galaxy ($M_{\rm UV}\sim-18.0$ mag) resolved into multiple stellar clumps, each of which has effective radius of $r_\mathrm{eff}\sim 10-70$ pc and stellar mass of $\sim10^7\ M_\odot$. These clumps dominate the stellar mass budget of the Misty Moons ($\gtrsim80\%$), similar to other high-$z$ clumps, which suggests a highly clustered mode of star formation in the early Universe, unlike seen in local dwarf galaxies. We convolve the source-plane image with the JWST/NIRCam point-spread function to produce a mock NIRCam image of the Misty Moons without lensing magnification, and find that the intrinsic galaxy has a radial surface-brightness profile comparable to those of $z\gtrsim10$ faint galaxies, such as JADES-GS-z13-0 and JADES-GS-z14-1, indicating that the Misty Moons represents a typical $z\gtrsim10$ faint galaxy. The Misty Moons, a lensed galaxy with resolved internal structures, provides an ideal laboratory for exploring the early stages of galaxy formation at $z\gtrsim10$.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.14483" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.14483" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.14483" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.03</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmological Constraints, Survey Analysis / Galaxy-Dark Matter, Lensing, Clustering</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">19. MFI-ResNet: Efficient ResNet Architecture Optimization via MeanFlow Compression and Selective Incubation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Nuolin Sun, Linyuan Wang, Haonan Wei, Lei Li, Bin Yan</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> MeanFlow-Incubated ResNet (MFI-ResNet) leverages generative flow matching to compress and then selectively incubate ResNet stages, resulting in a highly parameter-efficient architecture that maintains or improves discriminative performance on benchmark datasets. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">ResNet has achieved tremendous success in computer vision through its residual connection mechanism. ResNet can be viewed as a discretized form of ordinary differential equations (ODEs). From this perspective, the multiple residual blocks within a single ResNet stage essentially perform multi-step discrete iterations of the feature transformation for that stage. The recently proposed flow matching model, MeanFlow, enables one-step generative modeling by learning the mean velocity field to transform distributions. Inspired by this, we propose MeanFlow-Incubated ResNet (MFI-ResNet), which employs a compression-expansion strategy to jointly improve parameter efficiency and discriminative performance. In the compression phase, we simplify the multi-layer structure within each ResNet stage to one or two MeanFlow modules to construct a lightweight meta model. In the expansion phase, we apply a selective incubation strategy to the first three stages, expanding them to match the residual block configuration of the baseline ResNet model, while keeping the last stage in MeanFlow form, and fine-tune the incubated model. Experimental results show that on CIFAR-10 and CIFAR-100 datasets, MFI-ResNet achieves remarkable parameter efficiency, reducing parameters by 46.28% and 45.59% compared to ResNet-50, while still improving accuracy by 0.23% and 0.17%, respectively. This demonstrates that generative flow-fields can effectively characterize the feature transformation process in ResNet, providing a new perspective for understanding the relationship between generative modeling and discriminative learning.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.12422" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.12422" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.12422" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">20. The Scatter of the Many Outweighs the Scatter of the Few: Systematic Error Asymmetry in Steeply-Falling Mass Functions for High-Redshift JWST Galaxies
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jay R. Krishnan, Kevork N. Abazajian</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A new galaxy model framework designed to test $\Lambda$CDM against massive high-redshift JWST galaxies demonstrates that systematic uncertainties in stellar mass estimates dominate the error budget, largely mitigating the apparent conflict between inferred star formation efficiencies and standard cosmological structure formation models. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The discovery of massive, high redshift galaxies with JWST has been argued to challenge $Λ$CDM: such systems would require extremely rare halos and baryon-to-stellar-mass conversion efficiencies unphysically approaching--or exceeding--100%. If confirmed at galaxy formation forbidden efficiencies, these galaxies could signal new physics beyond standard cosmological structure formation. We develop a galaxy model framework that ties the linear power spectrum to the inferred efficiencies of galaxy growth in order to test the structure formation models. In addition, we incorporate multiple sources of error, including (i) observational sample variance, (ii) asymmetric scatter induced by the steepness of the high-mass halo tail, and (iii) systematic uncertainties in stellar mass estimates. We find that the inferred efficiency of star formation is dominated by systematic uncertainties on the spectral energy distribution inferred stellar mass of the JWST detected galaxies. The systematic uncertainty augments the asymmetry in scatter that largely brings the inferred efficiencies to be in line with that expected from early galaxy formation models. Our framework can be used to test $Λ$CDM as errors are reduced and further detections are made.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.13708" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.13708" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.13708" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmological Constraints, Survey Analysis / Galaxy-Dark Matter, Lensing, Clustering</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">21. ScoresActivation: A New Activation Function for Model Agnostic Global Explainability by Design
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Emanuel Covaci, Fabian Galis, Radu Balan, Daniela Zaharie, Darian Onchis</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Integrating the novel differentiable ScoresActivation function directly into model training provides a scalable framework for inherently explainable machine learning, yielding faithful feature rankings 150 times faster than SHAP while significantly boosting predictive performance and robustness to irrelevant inputs. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Understanding the decision of large deep learning models is a critical challenge for building transparent and trustworthy systems. Although the current post hoc explanation methods offer valuable insights into feature importance, they are inherently disconnected from the model training process, limiting their faithfulness and utility. In this work, we introduce a novel differentiable approach to global explainability by design, integrating feature importance estimation directly into model training. Central to our method is the ScoresActivation function, a feature-ranking mechanism embedded within the learning pipeline. This integration enables models to prioritize features according to their contribution to predictive performance in a differentiable and end-to-end trainable manner. Evaluations across benchmark datasets show that our approach yields globally faithful, stable feature rankings aligned with SHAP values and ground-truth feature importance, while maintaining high predictive performance. Moreover, feature scoring is 150 times faster than the classical SHAP method, requiring only 2 seconds during training compared to SHAP&#39;s 300 seconds for feature ranking in the same configuration. Our method also improves classification accuracy by 11.24% with 10 features (5 relevant) and 29.33% with 16 features (5 relevant, 11 irrelevant), demonstrating robustness to irrelevant inputs. This work bridges the gap between model accuracy and interpretability, offering a scalable framework for inherently explainable machine learning.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.13809" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.13809" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.13809" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">22. Weight-sparse transformers have interpretable circuits
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Leo Gao, Achyuta Rajaram, Jacob Coxon, Soham V. Govande, Bowen Baker, Dan Mossing</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Training language models with extreme weight sparsity facilitates the isolation and rigorous validation of fine-grained, human-understandable mechanistic circuits corresponding to natural concepts, validating the capability-interpretability trade-off and the benefits of scaling model size. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Finding human-understandable circuits in language models is a central goal of the field of mechanistic interpretability. We train models to have more understandable circuits by constraining most of their weights to be zeros, so that each neuron only has a few connections. To recover fine-grained circuits underlying each of several hand-crafted tasks, we prune the models to isolate the part responsible for the task. These circuits often contain neurons and residual channels that correspond to natural concepts, with a small number of straightforwardly interpretable connections between them. We study how these models scale and find that making weights sparser trades off capability for interpretability, and scaling model size improves the capability-interpretability frontier. However, scaling sparse models beyond tens of millions of nonzero parameters while preserving interpretability remains a challenge. In addition to training weight-sparse models de novo, we show preliminary results suggesting our method can also be adapted to explain existing dense models. Our work produces circuits that achieve an unprecedented level of human understandability and validates them with considerable rigor.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.13653" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.13653" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.13653" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">23. Data Whitening Improves Sparse Autoencoder Learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Ashwin Saraswatula, David Klindt</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Preprocessing Sparse Autoencoder (SAE) inputs with PCA Whitening improves interpretability metrics, such as feature disentanglement and probing accuracy, by transforming the optimization landscape to be more convex, challenging the assumption that optimal interpretability requires perfect reconstruction fidelity. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Sparse autoencoders (SAEs) have emerged as a promising approach for learning interpretable features from neural network activations. However, the optimization landscape for SAE training can be challenging due to correlations in the input data. We demonstrate that applying PCA Whitening to input activations -- a standard preprocessing technique in classical sparse coding -- improves SAE performance across multiple metrics. Through theoretical analysis and simulation, we show that whitening transforms the optimization landscape, making it more convex and easier to navigate. We evaluate both ReLU and Top-K SAEs across diverse model architectures, widths, and sparsity regimes. Empirical evaluation on SAEBench, a comprehensive benchmark for sparse autoencoders, reveals that whitening consistently improves interpretability metrics, including sparse probing accuracy and feature disentanglement, despite minor drops in reconstruction quality. Our results challenge the assumption that interpretability aligns with an optimal sparsity--fidelity trade-off and suggest that whitening should be considered as a default preprocessing step for SAE training, particularly when interpretability is prioritized over perfect reconstruction.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.13981" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.13981" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.13981" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">24. MeanFlow Transformers with Representation Autoencoders
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zheyuan Hu, Chieh-Hsin Lai, Ge Wu, Yuki Mitsufuji, Stefano Ermon</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> An efficient training and sampling scheme for MeanFlow (MF) in the Representation Autoencoder (RAE) latent space stabilizes training via Consistency Mid-Training and distillation, achieving superior 1-step FID performance (2.03) while reducing training cost by 83% and sampling GFLOPS by 38% compared to vanilla MF. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inference, the SD-VAE decoder dominates the generation cost, and MF depends on complex guidance hyperparameters for class-conditional generation. In this work, we develop an efficient training and sampling scheme for MF in the latent space of a Representation Autoencoder (RAE), where a pre-trained vision encoder (e.g., DINO) provides semantically rich latents paired with a lightweight decoder. We observe that naive MF training in the RAE latent space suffers from severe gradient explosion. To stabilize and accelerate training, we adopt Consistency Mid-Training for trajectory-aware initialization and use a two-stage scheme: distillation from a pre-trained flow matching teacher to speed convergence and reduce variance, followed by an optional bootstrapping stage with a one-point velocity estimator to further reduce deviation from the oracle mean flow. This design removes the need for guidance, simplifies training configurations, and reduces computation in both training and sampling. Empirically, our method achieves a 1-step FID of 2.03, outperforming vanilla MF&#39;s 3.43, while reducing sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. We further scale our approach to ImageNet 512, achieving a competitive 1-step FID of 3.23 with the lowest GFLOPS among all baselines. Code is available at https://github.com/sony/mf-rae.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.13019" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.13019" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.13019" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">25. Fossil group origins XIV: The radial orbits of A267
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>S. Zarattini, A. Biviano, I. Bartalucci, J. A. L. Aguerri, C. P. Haines, M. Girardi</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Solving the Jeans equation for the fossil cluster A267 confirms that its member galaxies follow more radial orbits in the external regions compared to typical clusters, suggesting this orbital distribution is a key factor in generating the large magnitude gap characteristic of fossil groups. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Fossil groups (FGs) are groups or clusters of galaxies with a single, massive, central galaxy and with a clear lack of L* galaxies. The physical reason for their large magnitude gap (dm12) may arise from early FG formation, which allowed all L galaxies to merge with the central one, and/or it could be related to the fact that galaxies accreting on the FGs move on radial orbits, shortening their merging timescales. The latter properties could be linked with the peculiar position of FGs within the cosmic web. We determine the velocity anisotropy profile beta(r) of the fossil cluster A267, which is related to the orbital distribution of cluster galaxies. This is the first individual FG for which the orbital distribution of galaxies is determined. We aim to confirm previous findings based on stack samples that indicate that FGs, on average, host galaxies on more radial orbits than normal clusters. We started with a sample of 2315 redshifts in the field of A267 and we determined the membership for 329 of them. Of these, 174 are located within r200. We used them as tracers of the gravitational potential of the cluster to solve the Jeans equation using the MAMPOSSt algorithm. We thus obtained the cluster mass M(r) and beta(r) profiles. We also estimated M(r) from the X-ray data. A comparison of the MAMPOSSt and X-ray-determined M(r)s allows us to estimate the cluster hydrostatic mass bias, that is consistent with previous findings. The anisotropy parameter beta(r) indicates tangential orbits for the galaxies near the cluster centre and increasingly radial orbits in the external regions. We therefore confirm that FGs are characterised by more radial orbits for their member galaxies than the average cluster population. We speculate that this different orbital distribution might be an important element in creating a large dm12.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.15786" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.15786" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.15786" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmological Constraints, Survey Analysis / Galaxy-Dark Matter, Lensing, Clustering</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">26. Selective Weak-to-Strong Generalization
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Hao Lang, Fei Huang, Yongbin Li</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A selective weak-to-strong generalization framework improves model alignment robustness by training a classifier to identify questions the strong model can answer, allowing the system to selectively use self-generated labels and refine weak labels via graph smoothing, thereby avoiding harmful weak supervision. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Future superhuman models will surpass the ability of humans and humans will only be able to \textit{weakly} supervise superhuman models. To alleviate the issue of lacking high-quality data for model alignment, some works on weak-to-strong generalization (W2SG) finetune a strong pretrained model with a weak supervisor so that it can generalize beyond weak supervision. However, the invariable use of weak supervision in existing methods exposes issues in robustness, with a proportion of weak labels proving harmful to models. In this paper, we propose a selective W2SG framework to avoid using weak supervision when unnecessary. We train a binary classifier P(IK) to identify questions that a strong model can answer and use its self-generated labels for alignment. We further refine weak labels with a graph smoothing method. Extensive experiments on three benchmarks show that our method consistently outperforms competitive baselines. Further analyses show that P(IK) can generalize across tasks and difficulties, which indicates selective W2SG can help superalignment.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.14166" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.14166" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.14166" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CL</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">27. TS-PEFT: Token-Selective Parameter-Efficient Fine-Tuning with Learnable Threshold Gating
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Dabiao Ma, Ziming Dai, Zhimin Xin, Shu Wang, Ye Wang, Haojun Fei</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The proposed Token-Selective PEFT (TS-PEFT) paradigm demonstrates that applying parameter-efficient modifications only to a subset of token positions, rather than all indices, is often sufficient and sometimes superior for optimizing large model performance on downstream tasks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">In the field of large models (LMs) for natural language processing (NLP) and computer vision (CV), Parameter-Efficient Fine-Tuning (PEFT) has emerged as a resource-efficient method that modifies a limited number of parameters while keeping the pretrained weights fixed. This paper investigates the traditional PEFT approach, which applies modifications to all position indices, and questions its necessity. We introduce a new paradigm called Token-Selective PEFT (TS-PEFT), in which a function S selectively applies PEFT modifications to a subset of position indices, potentially enhancing performance on downstream tasks. Our experimental results reveal that the indiscriminate application of PEFT to all indices is not only superfluous, but may also be counterproductive. This study offers a fresh perspective on PEFT, advocating for a more targeted approach to modifications and providing a framework for future research to optimize the fine-tuning process for large models.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.16147" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.16147" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.16147" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CL</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">28. Collaborative Management for Chronic Diseases and Depression: A Double Heterogeneity-based Multi-Task Learning Method
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yidong Chai, Haoxin Liu, Jiaheng Xie, Chaopeng Wang, Xiao Fang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The Advanced Double Heterogeneity-based Multi-Task Learning (ADH-MTL) method leverages group-level modeling and a Bayesian network to effectively address patient and disease heterogeneity, significantly improving the joint assessment accuracy of comorbid physical diseases and depression using wearable sensor data. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Wearable sensor technologies and deep learning are transforming healthcare management. Yet, most health sensing studies focus narrowly on physical chronic diseases. This overlooks the critical need for joint assessment of comorbid physical chronic diseases and depression, which is essential for collaborative chronic care. We conceptualize multi-disease assessment, including both physical diseases and depression, as a multi-task learning (MTL) problem, where each disease assessment is modeled as a task. This joint formulation leverages inter-disease relationships to improve accuracy, but it also introduces the challenge of double heterogeneity: chronic diseases differ in their manifestation (disease heterogeneity), and patients with the same disease show varied patterns (patient heterogeneity). To address these issues, we first adopt existing techniques and propose a base method. Given the limitations of the base method, we further propose an Advanced Double Heterogeneity-based Multi-Task Learning (ADH-MTL) method that improves the base method through three innovations: (1) group-level modeling to support new patient predictions, (2) a decomposition strategy to reduce model complexity, and (3) a Bayesian network that explicitly captures dependencies while balancing similarities and differences across model components. Empirical evaluations on real-world wearable sensor data demonstrate that ADH-MTL significantly outperforms existing baselines, and each of its innovations is shown to be effective. This study contributes to health information systems by offering a computational solution for integrated physical and mental healthcare and provides design principles for advancing collaborative chronic disease management across the pre-treatment, treatment, and post-treatment phases.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.16398" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.16398" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.16398" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.05</span>
                        <span class="badge bg-primary">Semantic Score: 0.91</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">29. UNSEEN: Enhancing Dataset Pruning from a Generalization Perspective
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Furui Xu, Shaobo Wang, Jiajun Zhang, Chenghao Sun, Haixiang Tang, Linfeng Zhang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The UNSEEN framework improves dataset pruning by shifting from fitting-centric to generalization-centric scoring, using models trained on varying coresets to dynamically select samples, resulting in significant performance gains, such as achieving lossless performance while reducing ImageNet-1K training data by 30%. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The growing scale of datasets in deep learning has introduced significant computational challenges. Dataset pruning addresses this challenge by constructing a compact but informative coreset from the full dataset with comparable performance. Previous approaches typically establish scoring metrics based on specific criteria to identify representative samples. However, these methods predominantly rely on sample scores obtained from the model&#39;s performance during the training (i.e., fitting) phase. As scoring models achieve near-optimal performance on training data, such fitting-centric approaches induce a dense distribution of sample scores within a narrow numerical range. This concentration reduces the distinction between samples and hinders effective selection. To address this challenge, we conduct dataset pruning from the perspective of generalization, i.e., scoring samples based on models not exposed to them during training. We propose a plug-and-play framework, UNSEEN, which can be integrated into existing dataset pruning methods. Additionally, conventional score-based methods are single-step and rely on models trained solely on the complete dataset, providing limited perspective on the importance of samples. To address this limitation, we scale UNSEEN to multi-step scenarios and propose an incremental selection technique through scoring models trained on varying coresets, and optimize the quality of the coreset dynamically. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art (SOTA) methods on CIFAR-10, CIFAR-100, and ImageNet-1K. Notably, on ImageNet-1K, UNSEEN achieves lossless performance while reducing training data by 30\%.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.12988" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.12988" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.12988" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">30. Constraining the Nature of Dark Matter from Tidal Radii of Cluster Galaxy Subhalos
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Barry T. Chiang, Isaque Dutra, Priyamvada Natarajan</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Analysis of subhalo truncation radii in eight strong lensing galaxy clusters confirms that the outer spatial extents of these subhalos are statistically consistent with Cold Dark Matter (CDM) simulations, providing complementary diagnostics that challenge the predictions of strongly collisional Self-Interacting Dark Matter (SIDM). (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Gravitational lensing by galaxy clusters provides a powerful probe of the spatial distribution of dark matter and its microphysical properties. Strong and weak lensing constraints on the density profiles of subhalos and their truncation radii offer key diagnostics for distinguishing between collisionless cold dark matter (CDM) and self-interacting dark matter (SIDM). Notably, in the strongly collisional SIDM regime, subhalo core collapse and enhanced mass loss from ram-pressure stripping predict steeper central density slopes and more compact truncation radii--features that are directly testable with current lensing data. We analyze subhalo truncation in eight lensing clusters (Abell 2218, 383, 963, 209, 2390, and MACS J0416.1, J1206.2, J1149.6) that span the redshift range $ \simeq 0.17$-$0.54$ with virial masses $M_{200} \simeq0.41$-$2.2\times 10^{15}$ M$_\odot$ to constrain SIDM versus CDM. Our results indicate that the outer spatial extents of subhalos are statistically consistent with CDM, corroborated by redshift- and mass-matched analogs from the Illustris-TNG simulations. We conclude that the tidal radii of cluster galaxy subhalos serve as an important and complementary diagnostic of the nature of dark matter in these violent, dense environments.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.14726" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.14726" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.14726" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.02</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmological Constraints, Survey Analysis / Galaxy-Dark Matter, Lensing, Clustering</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">31. MSLoRA: Multi-Scale Low-Rank Adaptation via Attention Reweighting
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Xu Yang, Gady Agam</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> MSLoRA introduces a backbone-agnostic, parameter-efficient adapter that enhances transfer performance across CNNs and Vision Transformers by reweighting feature responses via a multi-scale nonlinear transformation, rather than re-tuning the underlying backbone weights. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We introduce MSLoRA, a backbone-agnostic, parameter-efficient adapter that reweights feature responses rather than re-tuning the underlying backbone. Existing low-rank adaptation methods are mostly confined to vision transformers (ViTs) and struggle to generalize across architectures. MSLoRA unifies adaptation for both convolutional neural networks (CNNs) and ViTs by combining a low-rank linear projection with a multi-scale nonlinear transformation that jointly modulates spatial and channel attention. The two components are fused through pointwise multiplication and a residual connection, yielding a lightweight module that shifts feature attention while keeping pretrained weights frozen. Extensive experiments demonstrate that MSLoRA consistently improves transfer performance on classification, detection, and segmentation tasks with roughly less than 5\% of backbone parameters. The design further enables stable optimization, fast convergence, and strong cross-architecture generalization. By reweighting rather than re-tuning, MSLoRA provides a simple and universal approach for efficient adaptation of frozen vision backbones.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.12400" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.12400" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.12400" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">32. Primordial non-Gaussianity in noncanonical warm inflation with nonminimal derivative coupling
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Xiao-Min Zhang, Run-Qing Zhao, Yun-Cai Feng, Peng-Cheng Chu, Zhi-Peng Peng, Xi-Bin Li</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Non-Gaussian perturbations in the warm k-inflation model are quantified by separately computing the intrinsic ($f_{NL}^{int}$) and $\delta N$ ($f_{NL}^{\delta N}$) components, which arise from the three-point and four-point correlations of the inflaton field, respectively, to constrain theoretical parameters using experimental observations. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">This paper presents and investigates non-Gaussian perturbations for the warm k-inflation model that is driven by pure kinetic energy. The two complementary components of the overall non-Gaussianity are the three-point and four-point correlations. The intrinsic non-Gaussian component, denoted as the nonlinear parameter f_{NL}^{int}, is rooted in the three-point correlation for the inflaton field. Meanwhile, the δN part non-Gaussianity, denoted as f_{NL}^{δN}, is the contribution attributed to the four-point correlation function of the inflaton field. In this paper, the above two components in warm k-inflation are individually computed and analyzed. Then, comparisons and discussions between them are conducted, and the non-Gaussian theoretical results are compared with experimental observations to determine the range of model parameters within the allowable range of observation.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.16312" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.16312" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.16312" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">33. Deep Learning Framework for Enhanced Neutrino Reconstruction of Single-line Events in the ANTARES Telescope
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">A. Albert, S. Alves, M. André, M. Ardid, S. Ardid, J. -J. Aubert, J. Aublin, B. Baret, S. Basa, Y. Becherini, et al.</span>
                                <span class="author-full" style="display: none;">A. Albert, S. Alves, M. André, M. Ardid, S. Ardid, J. -J. Aubert, J. Aublin, B. Baret, S. Basa, Y. Becherini, B. Belhorma, F. Benfenati, V. Bertin, S. Biagi, J. Boumaaza, M. Bouta, M. C. Bouwhuis, H. Brânzaş, R. Bruijn, J. Brunner, J. Busto, B. Caiffi, D. Calvo, S. Campion, A. Capone, F. Carenini, J. Carr, V. Carretero, T. Cartraud, S. Celli, L. Cerisy, M. Chabab, R. Cherkaoui El Moursli, T. Chiarusi, M. Circella, J. A. B. Coelho, A. Coleiro, R. Coniglione, P. Coyle, A. Creusot, A. F. Díaz, B. De Martino, C. Distefano, I. Di Palma, C. Donzaud, D. Dornic, D. Drouhin, T. Eberl, A. Eddymaoui, T. van Eeden, D. van Eijk, S. El Hedri, N. El Khayati, A. Enzenhöfer, P. Fermani, G. Ferrara, F. Filippini, L. Fusco, S. Gagliardini, J. García-Méndez, C. Gatius Oliver, P. Gay, N. Geißelbrecht, H. Glotin, R. Gozzini, R. Gracia Ruiz, K. Graf, C. Guidi, L. Haegel, H. van Haren, A. J. Heijboer, Y. Hello, L. Hennig, J. J. Hernández-Rey, J. Hößl, F. Huang, G. Illuminati, B. Jisse-Jung, M. de Jong, P. de Jong, M. Kadler, O. Kalekin, U. Katz, A. Kouchner, I. Kreykenbohm, V. Kulikovskiy, R. Lahmann, M. Lamoureux, A. Lazo, D. Lefèvre, E. Leonora, G. Levi, S. Le Stum, S. Loucatos, J. Manczak, M. Marcelin, A. Margiotta, A. Marinelli, J. A. Martínez-Mora, P. Migliozzi, A. Moussa, R. Muller, S. Navas, E. Nezri, B. Ó Fearraigh, E. Oukacha, A. M. Păun, G. E. Păvălaş, S. Peña-Martínez, M. Perrin-Terrin, P. Piattelli, C. Poiré, V. Popa, T. Pradier, N. Randazzo, D. Real, G. Riccobene, A. Romanov, A. Sánchez Losa, A. Saina, F. Salesa Greus, D. F. E. Samtleben, M. Sanguineti, P. Sapienza, F. Schüssler, J. Seneca, M. Spurio, Th. Stolarczyk, M. Taiuti, Y. Tayalati, B. Vallage, G. Vannoye, V. Van Elewyck, S. Viola, D. Vivolo, J. Wilms, S. Zavatarelli, A. Zegarelli, J. D. Zornoza, J. Zúñiga</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The $N$-Fit deep learning framework, utilizing convolutional layers and transfer learning, substantially improves the reconstruction of low-energy neutrino events from single ANTARES lines, yielding significant reductions in error for spatial parameters, including previously unattainable reliable azimuthal angle predictions. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present the $N$-fit algorithm designed to improve the reconstruction of neutrino events detected by a single line of the ANTARES underwater telescope, usually associated with low energy neutrino events ($\sim$ 100 GeV). $N$-Fit is a neural network model that relies on deep learning and combines several advanced techniques in machine learning --deep convolutional layers, mixture density output layers, and transfer learning. This framework divides the reconstruction process into two dedicated branches for each neutrino event topology --tracks and showers-- composed of sub-models for spatial estimation --direction and position-- and energy inference, which later on are combined for event classification. Regarding the direction of single-line events, the $N$-Fit algorithm significantly refines the estimation of the zenithal angle, and delivers reliable azimuthal angle predictions that were previously unattainable with traditional $χ^2$-fit methods. Improving on energy estimation of single-line events is a tall order; $N$-Fit benefits from transfer learning to efficiently integrate key characteristics, such as the estimation of the closest distance from the event to the detector. $N$-Fit also takes advantage from transfer learning in event topology classification by freezing convolutional layers of the pretrained branches. Tests on Monte Carlo simulations and data demonstrate a significant reduction in mean and median absolute errors across all reconstructed parameters. The improvements achieved by $N$-Fit highlight its potential for advancing multimessenger astrophysics and enhancing our ability to probe fundamental physics beyond the Standard Model using single-line events from ANTARES data.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.16614" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.16614" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.16614" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">physics.comp-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">34. Symmetry-Aware Graph Metanetwork Autoencoders: Model Merging through Parameter Canonicalization
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Odysseas Boufalis, Jorge Carrasco-Pollo, Joshua Rosenthal, Eduardo Terres-Caballero, Alejandro García-Castellanos</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Leveraging an autoencoder framework with Scale Graph Metanetworks as invariant encoders, a method is introduced to align Implicit Neural Representations and CNNs by explicitly incorporating both permutation and parameter scaling symmetries, ensuring similar networks converge in the same loss basin for effective model merging. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Neural network parameterizations exhibit inherent symmetries that yield multiple equivalent minima within the loss landscape. Scale Graph Metanetworks (ScaleGMNs) explicitly leverage these symmetries by proposing an architecture equivariant to both permutation and parameter scaling transformations. Previous work by Ainsworth et al. (2023) addressed permutation symmetries through a computationally intensive combinatorial assignment problem, demonstrating that leveraging permutation symmetries alone can map networks into a shared loss basin. In this work, we extend their approach by also incorporating scaling symmetries, presenting an autoencoder framework utilizing ScaleGMNs as invariant encoders. Experimental results demonstrate that our method aligns Implicit Neural Representations (INRs) and Convolutional Neural Networks (CNNs) under both permutation and scaling symmetries without explicitly solving the assignment problem. This approach ensures that similar networks naturally converge within the same basin, facilitating model merging, i.e., smooth linear interpolation while avoiding regions of high loss. The code is publicly available on our GitHub repository.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.12601" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.12601" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.12601" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">35. Deep-learning mitigation of foregrounds and beam effects in 21-cm intensity mapping using hybrid frequency differencing and PCA
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zitong Wang, Feng Shi, Le Zhang, Yanming Liu, Xiaoping Li, Ming Jiang, Xiaofan Ma</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> To robustly recover the 21-cm cosmological signal from foregrounds and realistic beam effects, a hybrid UNet architecture utilizing both frequency-differencing and PCA cleaning inputs achieves superior performance, maintaining accurate large-scale cross-correlation power spectra where single-input methods fail. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">21-cm intensity mapping (IM) is a powerful technique to probe the large-scale distribution of neutral hydrogen (HI) and extract cosmological information such as the baryon acoustic oscillation feature. A key challenge lies in recovering the faint HI signal from bright foregrounds and frequency-dependent beam effects, which can compromise traditional cleaning methods like principal component analysis (PCA) by removing part of the cosmological signal. Deep-learning approaches have recently been proposed to mitigate these effects by learning mappings between contaminated and true cosmological signals. Building upon our previous work~\citep{2024PhRvD.109f3509S} on the frequency-differencing (FD) method, this study extends the framework to systematically compare FD-based and PCA-based UNet reconstructions using realistic simulations that include foregrounds and beam convolution. We find that both approaches perform comparably without beam or with a Gaussian beam, but under a realistic cosine beam they systematically underestimate the large-scale cross-correlation power spectrum, particularly for $k&lt;0.1 h~\mathrm{Mpc}^{-1}$. To address this limitation, we explore a hybrid approach in which the UNet is trained with two input channels, one constructed from FD and the other from PCA cleaning, allowing the network to simultaneously exploit the strengths of both inputs. This two-channel strategy achieves superior performance, maintaining the cross-correlation power spectrum close to unity on large scales under a cosine beam, improving by 5-8% relative to either FD-based or PCA-based UNet alone. These results demonstrate that providing complementary FD and PCA information to a single deep network is an effective route to robust HI reconstruction, laying the groundwork for precision BAO measurements with future low-redshift 21 cm IM surveys.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.15072" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.15072" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.15072" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">36. The SRG/eROSITA all-sky survey: X-ray scaling relations of galaxy groups and clusters in the western Galactic hemisphere
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">M. E. Ramos-Ceja, L. Fiorino, E. Bulbul, V. Ghirardini, N. Clerc, A. Liu, J. S. Sanders, Y. E. Bahar, J. Dietl, M. Kluge, et al.</span>
                                <span class="author-full" style="display: none;">M. E. Ramos-Ceja, L. Fiorino, E. Bulbul, V. Ghirardini, N. Clerc, A. Liu, J. S. Sanders, Y. E. Bahar, J. Dietl, M. Kluge, F. Pacaud, E. Artis, F. Balzer, J. Comparat, Z. Ding, N. Malavasi, A. Merloni, T. Mistele, K. Nandra, R. Seppi, S. Zelmer, X. Zhang</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Scaling relations derived from the largest sample of galaxy groups and clusters detected by eROSITA (eRASS1) reveal that the slopes relating X-ray luminosity, temperature, gas mass, and $Y_{\rm X}$ significantly deviate from self-similar predictions, although the redshift evolution aligns with the self-similar model. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The soft X-ray telescope on board the Spectrum-Roentgen-Gamma (SRG) mission, eROSITA (extended ROentgen Survey with an Imaging Telescope Array), has produced the largest sample to date of galaxy groups and clusters detected via their intracluster/intragroup medium (ICM/IGrM) emission. Scaling relations between the intrinsic properties of these systems provide valuable insight into their formation and evolution. In this work, we investigate the scaling relations between key physical properties, such as soft band X-ray luminosity, temperature, gas mass, and the low-scatter mass proxy $Y_{\rm X}$, for the galaxy groups and clusters detected in the first eROSITA All-Sky Survey (eRASS1). Our analysis fully accounts for selection effects and the redshift evolution of the observable distributions. We construct a high-purity sample of $3061$ galaxy groups and clusters spanning the redshift range $0.05&lt;z&lt;1.07$ and mass range of $1.1\times10^{13}&lt;M_{500}/$M$_{\odot}&lt;1.6\times10^{15}$. This represents the largest sample to date used for scaling relation analysis. The selection function, derived from state-of-the-art simulations of the eROSITA sky, is rigorously incorporated into our modeling. We report best-fit parameters - normalization, slope, redshift evolution, and intrinsic scatter - for a set of scaling relations: $L_{\mathrm{X}}-T$, $L_{\mathrm{X}}-M_{\rm gas}$, $L_{\mathrm{X}}-Y_{\rm X}$, as well as the $M_{\rm gas}-T$ relation. Our best-fit models indicate that the slopes of the scaling relations deviate significantly from self-similar expectations, while the redshift evolution remains consistent with the self-similar model. The fits exhibit small statistical uncertainties, likely owing to the large sample size. Our results are in good agreement with previous observational studies that account for selection effects, as well as with simulations that incorporate non-gravitational physics.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.14356" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.14356" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.14356" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmological Constraints, Survey Analysis / Galaxy-Dark Matter, Lensing, Clustering</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">37. Foreground removal in HI 21 cm intensity mapping under frequency-dependent beam distortions
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Athanasia Gkogkou, Victor Bonjean, Jean-Luc Starck, Marta Spinelli, Panagiotis Tsakalides</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Combining sparse component separation and beam deconvolution, the SDecGMCA method proves highly effective and robust for neutral hydrogen intensity mapping, significantly outperforming other foreground removal techniques in recovering the cosmological power spectrum under realistic frequency-dependent beam distortions. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Neutral hydrogen (HI) intensity mapping with single-dish experiments is a powerful approach for probing cosmology in the post-reionization epoch. However, the presence of bright foregrounds over four orders of magnitude stronger than the HI signal makes its extraction highly challenging. While all methods perform well when assuming a Gaussian beam degraded to the worst resolution, most of them degrade significantly in the presence of a more realistic beam model. In this work, we investigate the performance of SDecGMCA. This method extends DecGMCA to spherical data, combining sparse component separation with beam deconvolution. Our goal is to evaluate this method in comparison with established foreground removal techniques, assessing its ability to recover the cosmological HI signal from single-dish intensity mapping observations under varying beam conditions. We use simulated HI signal and foregrounds, covering the frequency ranges relevant to MeerKAT and SKA-Mid. The foreground removal techniques tested fall into two main categories: model-fitting methods (polynomial and parametric) and blind source separation methods (PCA, ICA, GMCA, and SDecGMCA). Their effectiveness is evaluated based on the recovery of the HI angular and frequency power spectra under progressively more realistic beam conditions. While all methods perform adequately under a uniform degraded beam, SDecGMCA remains robust when frequency-dependent beam distortions are introduced. In the oscillating beam case, SDecGMCA suppresses the spurious spectral peak at $k_ν\sim 0.3$ and achieves $\lesssim 5\%$ accuracy at intermediate angular scales ($10 &lt; \ell &lt; 200$), outperforming other methods. Beam inversion, however, remains intrinsically unstable beyond $\ell \sim 200$, setting a practical limit on the method.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.13328" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.13328" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.13328" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">38. Derivative of the truncated singular value and eigen decomposition
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jan Naumann</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Detailed derivations are provided for the derivative of the truncated singular and eigenvalue decompositions, focusing on expressing the result solely in terms of the truncated components, which is essential for stable and efficient linear algebra gradient computations in automatic differentiation applications. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Recently developed applications in the field of machine learning and computational physics rely on automatic differentiation techniques, that require stable and efficient linear algebra gradient computations. This technical note provides a comprehensive and detailed discussion of the derivative of the truncated singular and eigenvalue decomposition. It summarizes previous work and builds on them with an extensive description of how to derive the relevant terms. A main focus is correctly expressing the derivative in terms of the truncated part, despite lacking knowledge of the full decomposition.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.14651" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.14651" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.14651" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">math.NA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">39. FusionFM: All-in-One Multi-Modal Image Fusion with Flow Matching
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Huayi Zhu, Xiu Shu, Youqiang Xiong, Qiao Liu, Rui Chen, Di Yuan, Xiaojun Chang, Zhenyu He</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Leveraging the flow matching paradigm, a novel approach formulates multi-modal image fusion as a direct probabilistic transport, utilizing a task-aware selection function to generate reliable pseudo-labels and integrating a Fusion Refiner module to achieve competitive performance, high sampling efficiency, and multi-task stability. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Current multi-modal image fusion methods typically rely on task-specific models, leading to high training costs and limited scalability. While generative methods provide a unified modeling perspective, they often suffer from slow inference due to the complex sampling trajectories from noise to image. To address this, we formulate image fusion as a direct probabilistic transport from source modalities to the fused image distribution, leveraging the flow matching paradigm to improve sampling efficiency and structural consistency. To mitigate the lack of high-quality fused images for supervision, we collect fusion results from multiple state-of-the-art models as priors, and employ a task-aware selection function to select the most reliable pseudo-labels for each task. We further introduce a Fusion Refiner module that employs a divide-and-conquer strategy to systematically identify, decompose, and enhance degraded components in selected pseudo-labels. For multi-task scenarios, we integrate elastic weight consolidation and experience replay mechanisms to preserve cross-task performance and enhance continual learning ability from both parameter stability and memory retention perspectives. Our approach achieves competitive performance across diverse fusion tasks, while significantly improving sampling efficiency and maintaining a lightweight model design. The code will be available at: https://github.com/Ist-Zhy/FusionFM.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.13794" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.13794" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.13794" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">40. Time dependent loss reweighting for flow matching and diffusion models is theoretically justified
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Lukas Billera, Hedwig Nora Nordlinder, Ben Murrell</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The theoretical foundation of Generator Matching is extended to confirm that the Bregman divergence loss and generator parameterization can depend on both state and time, thereby justifying the practical use of time-dependent loss weighting schemes for stabilizing training in diffusion and flow models. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">This brief note clarifies that, in Generator Matching (which subsumes a large family of flow matching and diffusion models over continuous, manifold, and discrete spaces), both the Bregman divergence loss and the linear parameterization of the generator can depend on both the current state $X_t$ and the time $t$, and we show that the expectation over time in the loss can be taken with respect to a broad class of time distributions. We also show this for Edit Flows, which falls outside of Generator Matching. That the loss can depend on $t$ clarifies that time-dependent loss weighting schemes, often used in practice to stabilize training, are theoretically justified when the specific flow or diffusion scheme is a special case of Generator Matching (or Edit Flows). It also often simplifies the construction of $X_1$-predictor schemes, which are sometimes preferred for model-related reasons. We show examples that rely upon the dependence of linear parameterizations, and of the Bregman divergence loss, on $t$ and $X_t$.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.16599" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.16599" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.16599" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">41. The abundance and properties of the lowest luminosity dwarf galaxies around the Milky Way: Insights from Semi-Analytic Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Niusha Ahvazi, Andrew B. Pace, Christopher T. Garling, Xiaowei Ou, Nitya Kallivayalil, Paul Torrey, Andrew Benson, Aklant Bhowmick, Núria Torres-Albà, Alex M. Garcia, et al.</span>
                                <span class="author-full" style="display: none;">Niusha Ahvazi, Andrew B. Pace, Christopher T. Garling, Xiaowei Ou, Nitya Kallivayalil, Paul Torrey, Andrew Benson, Aklant Bhowmick, Núria Torres-Albà, Alex M. Garcia, Alejandro Saravia, Jonathan Kho, Jack T. Warfield, Kaia R. Atzberger</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Using the Galacticus semi-analytic model, simulations incorporating molecular hydrogen cooling predict a substantially larger population of hyper-faint Milky Way satellites characterized by high velocity dispersions ($\sim 1-3$ km/s), providing a kinematic diagnostic to distinguish them from star clusters. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We investigate the formation and observable properties of faint satellite galaxies (M$_\rm V &gt; -3$) in Milky Way-like halos using the semi-analytic galaxy formation model Galacticus. The ability of the smallest dark matter halos to form stars depends sensitively on the balance between gas cooling and reionization heating. To quantify how this balance shapes the abundance and properties of the faintest galaxies, we compare two model variants: a fiducial model that includes molecular hydrogen (H$_2$) cooling and UV background radiation, and a No-H$_2$ model with atomic cooling only. Both models reproduce the structural properties of brighter Milky Way satellites, but they diverge at the lowest luminosities in the hyper-faint regime. The fiducial model predicts a substantially larger population of such systems that are on average hosted in halos with lower peak masses and quenched earlier. Many of these predicted systems lie below current observational thresholds but are within reach of next-generation deep imaging surveys. The predicted size-luminosity distributions of both models overlap with the region occupied by recently discovered &#34;ambiguous&#34; systems, whose classification as galaxies or star clusters remains uncertain. Specifically, we find that hyper-faint satellites have line-of-sight velocity dispersions of $σ_{\rm los} \sim 1-3$ km/s in the fiducial model, nearly an order of magnitude higher than expected for purely self-gravitating stellar systems of the same stellar mass. This distinction underscores the diagnostic power of precise kinematic measurements for determining whether ambiguous objects are dark matter dominated dwarf galaxies or star clusters, and highlights the importance of upcoming spectroscopic campaigns in resolving the nature of the faintest satellites.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.15808" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.15808" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.15808" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmological Constraints, Survey Analysis / Galaxy-Dark Matter, Lensing, Clustering</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">42. Parameter Importance-Driven Continual Learning for Foundation Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Lingxiang Wang, Hainan Zhang, Zhiming Zheng</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The Parameter Importance Estimation-based Continual Enhancement (PIECE) method mitigates catastrophic forgetting in foundation models by selectively updating only 0.1% of parameters identified as core via importance estimators, achieving state-of-the-art continual learning performance across language and multimodal tasks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.15375" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.15375" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.15375" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">43. Deep Andromeda JCMT-SCUBA2 Observations. The Submillimeter Maps and Giant Molecular Clouds
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Sihan Jiao, Jingwen Wu, Hauyu Baobab Liu, Chao-Wei Tsai, Yuxin Lin, Di Li, Zhi-Yu Zhang, Yu Cheng, Linjing Feng, Henrik Beuther, et al.</span>
                                <span class="author-full" style="display: none;">Sihan Jiao, Jingwen Wu, Hauyu Baobab Liu, Chao-Wei Tsai, Yuxin Lin, Di Li, Zhi-Yu Zhang, Yu Cheng, Linjing Feng, Henrik Beuther, Junzhi Wang, Lihwai Lin, Jakob den Brok, Ludan Zhang, Fengwei Xu, Fanyi Meng, Zongnan Li, Ryan P. Keenan, Si-Yue Yu, Niankun Yu, Zheng Zheng, Junhao Liu, Yuxiang Liu, Hao Ruan, Fangyuan Deng, Yuanzhen Xiong</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Unprecedentedly deep JCMT-SCUBA2 observations of M31 cataloged hundreds of giant molecular clouds (GMCs) that follow an $M \propto R_c^{2.5}$ relation, demonstrating that inter-arm GMCs are systematically less massive and possess significantly lower star-forming efficiency compared to GMCs located on spiral arms. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We have carried out unprecedentedly deep, nearly confusion-limited JCMT-SCUBA2 mapping observations on the nearest spiral galaxy, M31 (Andromeda). The 850 $μ$m image with a $\sim$50 pc resolution yields a comprehensive catalog of 383 giant molecular clouds (GMCs) that are associated with the spiral arms. In addition, it unveiled a population of 189 compact inter-arm GMCs in M31, which are mostly unresolved or marginally resolved. The masses of all these GMCs are in the range of 2$\times$10$^4$ -- 6$\times$10$^6$ $M_{\odot}$; the sizes are in the range of 30--130 pc. They follow a mass-size correlation, $M$ $\propto$ $R_{c}$$^{2.5}$. The inter-arm GMCs are systematically less massive, more diffuse, colder, and have lower star-forming efficiency (SFE) than on-arm GMCs. Moreover, within individual spatially resolved on-arm and off-arm M31 GMCs, the SFE is considerably lower than the SFE in molecular clouds in main sequence and green valley galaxies. Follow-up investigations on M31 GMCs may provide clues for how star formation may be quenched in galactic environments. Finally, we reconstrained the dust opacity spectral index $β$ in the M31 galaxy by combining our new JCMT observations with archival Herschel and Planck data and found that the radial variation of $β$ may not be as large as was proposed by previous studies.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.14360" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.14360" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.14360" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmological Constraints, Survey Analysis / Galaxy-Dark Matter, Lensing, Clustering</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">44. Improving Iterative Gaussian Processes via Warm Starting Sequential Posteriors
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Alan Yufei Dong, Jihao Andreas Lin, José Miguel Hernández-Lobato</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A novel method enhances the scalability of iterative Gaussian Process inference by accelerating linear solver convergence through the reuse of known solutions from smaller, contained systems, significantly improving speed for incremental data additions and Bayesian optimization tasks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Scalable Gaussian process (GP) inference is essential for sequential decision-making tasks, yet improving GP scalability remains a challenging problem with many open avenues of research. This paper focuses on iterative GPs, where iterative linear solvers, such as conjugate gradients, stochastic gradient descent or alternative projections, are used to approximate the GP posterior. We propose a new method which improves solver convergence of a large linear system by leveraging the known solution to a smaller system contained within. This is significant for tasks with incremental data additions, and we show that our technique achieves speed-ups when solving to tolerance, as well as improved Bayesian optimisation performance under a fixed compute budget.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.16340" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.16340" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.16340" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">45. iLTM: Integrated Large Tabular Model
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>David Bonet, Marçal Comajoan Cara, Alvaro Calafell, Daniel Mas Montserrat, Alexander G. Ioannidis</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The integrated Large Tabular Model (iLTM) architecture, which combines tree-derived embeddings and a meta-trained hypernetwork, establishes a new foundation model framework for tabular data, consistently outperforming gradient-boosted decision trees and existing deep models across classification and regression benchmarks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Tabular data underpins decisions across science, industry, and public services. Despite rapid progress, advances in deep learning have not fully carried over to the tabular domain, where gradient-boosted decision trees (GBDTs) remain a default choice in practice. We present iLTM, an integrated Large Tabular Model that unifies tree-derived embeddings, dimensionality-agnostic representations, a meta-trained hypernetwork, multilayer perceptrons (MLPs), and retrieval within a single architecture. Pretrained on more than 1,800 heterogeneous classification datasets, iLTM achieves consistently superior performance across tabular classification and regression tasks, from small datasets to large and high-dimensional tasks. After light fine-tuning, the meta-trained hypernetwork transfers to regression targets, matching or surpassing strong baselines. Extensive experiments show that iLTM outperforms well-tuned GBDTs and leading deep tabular models while requiring less task-specific tuning. By bridging the gap between tree-based and neural methods, iLTM offers a new framework for tabular foundation models for robust, adaptable, and scalable tabular learning.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.15941" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.15941" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.15941" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">46. Mock Observations for the CSST Mission: Integral Field Spectrograph--Instrument Simulation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Zhao-Jun Yan, Jun Yin, Lei Hao, Shi-Yin Shen, Wei Chen, Shuai Feng, Yi-Fei Xiong, Chun Xu, Xin-Rong Wen, Lin Lin, et al.</span>
                                <span class="author-full" style="display: none;">Zhao-Jun Yan, Jun Yin, Lei Hao, Shi-Yin Shen, Wei Chen, Shuai Feng, Yi-Fei Xiong, Chun Xu, Xin-Rong Wen, Lin Lin, Chao Liu, Lin Long, Zhen-Lei Chen, Mao-Chun Wu, Xiao-Bo Li, Zhang Ban, Xun Yang, Yu-Xi Jiang, Guo-Liang Li, Ke-Xin Li, Jian-Jun Chen, Nan Li, Cheng-Liang Wei, Lei Wang, Bai-Chuan Ren, Jun Wei, Jing Tang, Ran Li</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> An end-to-end simulation workflow has been developed for the CSST Integral Field Spectrograph (IFS) that accurately models complex instrumental effects, such as optical diffraction and detector noise, to characterize performance and enable pre-launch development of the data reduction pipeline. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The Chinese Space Station Survey Telescope (CSST) is a next-generation Stage-IV facility renowned for its wide field of view, high image quality, and multi-band observational capabilities. Among the five instruments onboard the CSST, the Integral Field Spectrograph (IFS) offers the unique ability to simultaneously capture spatial and spectral information across a field of view of no less than $6^{&#39;&#39;}\times6^{&#39;&#39;}$. Key advantages of the IFS include a high spatial resolution of $0.2^{&#39;&#39;}$ and a broad spectral coverage from 350 to 1000 nm, making it an ideal instrument for studying physical processes in the vicinity of supermassive black holes within galaxies. To more accurately assess the technical and scientific performance of the CSST-IFS, it is essential to develop a simulation tool that incorporates realistic effects from all optical components. Such a simulation will form an integral part of the CSST-IFS data and pipeline system, enabling the development of the data reduction pipeline well ahead of actual observations. This paper presents an end-to-end simulation workflow for the CSST-IFS, incorporating a wide range of instrumental effects that may influence its spectral and imaging performance. The simulation accounts for optical diffraction effects introduced by all components, such as image slicers and slit array, as well as sub-pixel effects from gratings. It also includes various detector noises, frame-shifting effects, and charge-transfer inefficiency. Real observational conditions--such as target Doppler shift, cosmic rays, and other in-orbit operational effects--are also considered. We describe the technical implementation of the simulation and present results that quantitatively characterize key instrument parameters.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.12483" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.12483" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.12483" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.05</span>
                        <span class="badge bg-primary">Semantic Score: 0.91</span>
                        
                            <span class="badge bg-secondary">astro-ph.IM</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">47. Challenging the $ω_0ω_a$CDM parametrization through rational expansions in view of DESI data release
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Youri Carloni, Orlando Luongo, Marek Biesiada</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Padé cosmology parameterizations, based on rational series expansions of the dark energy equation of state or deceleration parameter, provide robust alternatives to the $\omega_0\omega_a$CDM model, exhibiting superior statistical fit and enhanced stability when constrained by combined cosmological probes including DESI data. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">In view of the new Dark Energy Spectroscopic Instrument (DESI) 2025 results, we analyze three types of \emph{Padé cosmology}, based on rational series making use of Padé approximants over the equations of state, namely Padé$^ω$ (0,1) and Padé$^ω$ (1,1), plus a Padé$^{q}$ (0,1), i.e., a rational expansion on the dark energy deceleration parameter, in which where the numerator and denominator orders are incorporated into the above brackets. These scenarios appear alternative dark energy parameterizations with respect to the well-known $ω_0ω_a$CDM model, claimed as the most viable model by DESI. Accordingly, we perform Monte Carlo Markov chain (MCMC) analyses with the publicly available \texttt{CLASS} Boltzmann code, including the three Padé cosmology, along with the $ω_0ω_a$CDM and $Λ$CDM standard pictures. To this end, we combine independent probes from high to low redshifts to obtain reliable constraints on the cosmological parameters of these models and compare them using statistical selection criteria. \emph{Our results show that Padé cosmology is neither statistically excluded nor worse than the $ω_0ω_a$CDM parametrization}. On the contrary, the Akaike Information Criterion (AIC) identifies Padé$^{q}$ (0,1) as \emph{the best-fit model}, with weak evidence against the $ω_0ω_a$CDM parameterization, while the Deviance Information Criterion (DIC) provides \emph{strong evidence against the $ω_0ω_a$CDM model, favoring Padé (1,1)}. Based on our bounds, we further investigate the evolution of the squared sound speed, revealing that the Padé$^{q}$ (0,1) and Padé$^ω$ (0,1) parameterizations exhibit enhanced stability compared with the other cases here considered and, therefore, describe robust alternatives for the cosmological background.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.15472" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.15472" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.15472" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">48. Accelerating Reionization Constraints: An ANN-Emulator Framework for the SCRIPT Semi-numerical Model
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Saptarshi Sarkar, Tirthankar Roy Choudhury</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> An efficient emulator-based framework significantly accelerates parameter inference for the Epoch of Reionization (EoR) using the SCRIPT code, leveraging adaptive sampling and neural networks to reduce the required number of high-resolution simulations by two orders of magnitude while preserving posterior accuracy. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Constraining the Epoch of Reionization (EoR) with physically motivated simulations is hampered by the high cost of conventional parameter inference. We present an efficient emulator-based framework that dramatically reduces this bottleneck for the photon-conserving semi-numerical code SCRIPT. Our approach combines (i) a reliable coarse-resolution MCMC to locate the high-likelihood region (exploiting the large-scale convergence of SCRIPT) with (ii) an adaptive, targeted sampling strategy to build a compact high-resolution training set for an artificial neural network based emulator of the model likelihood. With only $\approx 10^3$ high-resolution simulations, the trained emulators achieve excellent predictive accuracy ($R^2 \approx 0.97$--$0.99$) and, when embedded within an MCMC framework, reproduce posterior distributions from full high-resolution runs. Compared to conventional MCMC, our pipeline reduces the number of expensive simulations by a factor of $\sim 100$ and lowers total CPU cost by up to a factor of $\sim 70$, while retaining statistical fidelity. This computational speedup makes inference in much higher-dimensional models tractable (e.g., those needed to incorporate JWST and upcoming 21 cm datasets) and provides a general strategy for building efficient emulators for next generation of EoR constraints.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.16256" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.16256" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.16256" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.IM</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">49. One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zeyuan Wang, Da Li, Yulin Chen, Ye Shi, Liang Bai, Tianyuan Yu, Yanwei Fu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> MeanFlowQL, a residual reformulation of the MeanFlow generative policy, enables efficient one-step noise-to-action generation compatible with Q-learning, successfully modeling complex, multimodal action distributions for strong performance across offline and offline-to-online reinforcement learning benchmarks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We introduce a one-step generative policy for offline reinforcement learning that maps noise directly to actions via a residual reformulation of MeanFlow, making it compatible with Q-learning. While one-step Gaussian policies enable fast inference, they struggle to capture complex, multimodal action distributions. Existing flow-based methods improve expressivity but typically rely on distillation and two-stage training when trained with Q-learning. To overcome these limitations, we propose to reformulate MeanFlow to enable direct noise-to-action generation by integrating the velocity field and noise-to-action transformation into a single policy network-eliminating the need for separate velocity estimation. We explore several reformulation variants and identify an effective residual formulation that supports expressive and stable policy learning. Our method offers three key advantages: 1) efficient one-step noise-to-action generation, 2) expressive modelling of multimodal action distributions, and 3) efficient and stable policy learning via Q-learning in a single-stage training setup. Extensive experiments on 73 tasks across the OGBench and D4RL benchmarks demonstrate that our method achieves strong performance in both offline and offline-to-online reinforcement learning settings. Code is available at https://github.com/HiccupRL/MeanFlowQL.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.13035" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.13035" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.13035" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">50. Self-Adaptive Graph Mixture of Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Mohit Meena, Yash Punjabi, Abhishek A, Vishal Sharma, Mahesh Chandran</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Self-Adaptive Graph Mixture of Models (SAGMM) improves graph neural network performance by employing a topology-aware attention gating mechanism to dynamically select and combine diverse expert architectures, providing a robust and adaptive solution that outperforms leading GNN baselines across multiple graph learning tasks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Graph Neural Networks (GNNs) have emerged as powerful tools for learning over graph-structured data, yet recent studies have shown that their performance gains are beginning to plateau. In many cases, well-established models such as GCN and GAT, when appropriately tuned, can match or even exceed the performance of more complex, state-of-the-art architectures. This trend highlights a key limitation in the current landscape: the difficulty of selecting the most suitable model for a given graph task or dataset. To address this, we propose Self-Adaptive Graph Mixture of Models (SAGMM), a modular and practical framework that learns to automatically select and combine the most appropriate GNN models from a diverse pool of architectures. Unlike prior mixture-of-experts approaches that rely on variations of a single base model, SAGMM leverages architectural diversity and a topology-aware attention gating mechanism to adaptively assign experts to each node based on the structure of the input graph. To improve efficiency, SAGMM includes a pruning mechanism that reduces the number of active experts during training and inference without compromising performance. We also explore a training-efficient variant in which expert models are pretrained and frozen, and only the gating and task-specific layers are trained. We evaluate SAGMM on 16 benchmark datasets covering node classification, graph classification, regression, and link prediction tasks, and demonstrate that it consistently outperforms or matches leading GNN baselines and prior mixture-based methods, offering a robust and adaptive solution for real-world graph learning.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.13062" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.13062" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.13062" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Modeling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
    </div>

    <footer class="footer">
        &copy; 2025 arXiv Daily · Powered by <a href="https://arxiv.org/" target="_blank">arXiv</a>
    </footer>
</body>
</html>