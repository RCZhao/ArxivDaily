<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>arXiv Daily: Backfill (2025-10-19 to 2025-10-26)</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto:400,700&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Roboto', Arial, sans-serif; background: #f8f9fa; transition: background 0.3s; }
        .arxiv-card { margin: 2em auto; max-width: 900px; box-shadow: 0 2px 8px rgba(0,0,0,0.08); border-radius: 16px; transition: box-shadow 0.3s, transform 0.3s; }
        .arxiv-card:hover { box-shadow: 0 4px 12px rgba(0,0,0,0.1); transform: translateY(-1px); }
        .arxiv-title { background: linear-gradient(90deg, #0d6efd 60%, #6610f2 100%); color: #fff; border-radius: 16px 16px 0 0; padding: 1.5em; font-size: 1.5em; font-weight: 700; letter-spacing: 0.5px;}
        .arxiv-meta { font-size: 1em; color: #555; margin-bottom: 1em; }
        .arxiv-abstract { background: #fff; padding: 1.5em; border-radius: 0 0 16px 16px; font-size: 1.1em; line-height: 1.7;}
        .arxiv-links a { margin-right: 1em; }
        .arxiv-scores { margin-top: 1em; font-size: 1em; }
        .navbar { background: #fff; box-shadow: 0 2px 8px rgba(0,0,0,0.04);}
        .footer { text-align: center; color: #888; padding: 2em 0 1em 0; font-size: 0.95em;}
        @media (prefers-color-scheme: dark) {
            body { background: #181a1b; color: #e4e6eb; }
            .arxiv-card { background: #23272b; box-shadow: 0 2px 8px rgba(0,0,0,0.28);}
            .arxiv-card:hover { box-shadow: 0 5px 15px rgba(0,0,0,0.25); transform: translateY(-1px); }
            .arxiv-title { background: linear-gradient(90deg, #375a7f 60%, #6f42c1 100%);}
            .arxiv-abstract { background: #23272b; color: #e4e6eb;}
            .navbar { background: #23272b; color: #e4e6eb;}
            .text-muted { color: #adb5bd !important; }
        }
        /* Improved abstract readability */
        .arxiv-abstract-text {
            font-size: 1.1em;
            line-height: 1.7;
        }
        p.big {
            line-height: 1.6;
            font-size: large;
        }
    </style>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'] ],
          processEscapes: true
        }
      });
    </script>
    <script>
    function toggleAuthors(element) {
        const fullList = element.querySelector('.author-full');
        const shortList = element.querySelector('.author-short');
        const toggleText = element.querySelector('.author-toggle');
        
        fullList.style.display = fullList.style.display === 'none' ? 'inline' : 'none';
        shortList.style.display = shortList.style.display === 'none' ? 'inline' : 'none';
        toggleText.textContent = fullList.style.display === 'none' ? ' (show all)' : ' (show less)';
    }
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>
<body>
    <nav class="navbar navbar-expand-lg mb-4">
        <div class="container">
            <a class="navbar-brand fw-bold text-primary" href="#">arXiv Daily</a>
            <span class="navbar-text">Modern arXiv Reader</span>
        </div>
    </nav>

    
    <div class="container py-4">
        <header class="mb-4"><h2 class="display-6 text-center">Analysis of Your Favorites</h2></header>
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Word Cloud</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/word_cloud.png" class="img-fluid rounded" alt="Word Cloud of favorite paper topics">
                    </div>
                </div>
            </div>
        </div>
        
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Clusters</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/backfill_cluster_map.png" class="img-fluid rounded" alt="2D Cluster plot of favorite papers">
                    </div>
                </div>
            </div>
        </div>
        
    </div>
    

    <div class="container py-4">
        <header class="mb-4"><h1 class="display-5 text-center text-primary">arXiv: Backfill (2025-10-19 to 2025-10-26)</h1></header>

        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Score Distribution of All Papers Found Today</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/score_distribution.png" class="img-fluid rounded" alt="Score distribution of all papers found today">
                    </div>
                </div>
            </div>
        </div>
        

        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">1. Euclid preparation: The flat-sky approximation for the clustering of Euclid&#39;s photometric galaxies
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Euclid Collaboration, W. L. Matthewson, R. Durrer, S. Camera, I. Tutusaus, B. Altieri, A. Amara, S. Andreon, N. Auricchio, C. Baccigalupi, et al.</span>
                                <span class="author-full" style="display: none;">Euclid Collaboration, W. L. Matthewson, R. Durrer, S. Camera, I. Tutusaus, B. Altieri, A. Amara, S. Andreon, N. Auricchio, C. Baccigalupi, M. Baldi, S. Bardelli, P. Battaglia, A. Biviano, E. Branchini, M. Brescia, G. Cañas-Herrera, V. Capobianco, C. Carbone, V. F. Cardone, J. Carretero, S. Casas, M. Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C. Colodro-Conde, G. Congedo, C. J. Conselice, L. Conversi, Y. Copin, F. Courbin, H. M. Courtois, A. Da Silva, H. Degaudenzi, G. De Lucia, H. Dole, F. Dubath, C. A. J. Duncan, X. Dupac, S. Dusini, S. Escoffier, M. Farina, F. Faustini, S. Ferriol, F. Finelli, M. Frailis, E. Franceschi, M. Fumana, S. Galeotta, K. George, B. Gillis, C. Giocoli, J. Gracia-Carpio, A. Grazian, F. Grupp, S. V. H. Haugan, W. Holmes, F. Hormuth, A. Hornstrup, K. Jahnke, M. Jhabvala, B. Joachimi, E. Keihänen, S. Kermiche, A. Kiessling, B. Kubik, M. Kunz, H. Kurki-Suonio, A. M. C. Le Brun, S. Ligori, P. B. Lilje, V. Lindholm, I. Lloro, G. Mainetti, D. Maino, E. Maiorano, O. Mansutti, S. Marcin, O. Marggraf, M. Martinelli, N. Martinet, F. Marulli, R. J. Massey, E. Medinaceli, S. Mei, Y. Mellier, M. Meneghetti, E. Merlin, G. Meylan, A. Mora, M. Moresco, B. Morin, L. Moscardini, C. Neissner, S. -M. Niemi, C. Padilla, S. Paltani, F. Pasian, K. Pedersen, W. J. Percival, V. Pettorino, S. Pires, G. Polenta, M. Poncet, L. A. Popa, F. Raison, R. Rebolo, A. Renzi, J. Rhodes, G. Riccio, E. Romelli, M. Roncarelli, R. Saglia, Z. Sakr, A. G. Sánchez, D. Sapone, B. Sartoris, P. Schneider, T. Schrabback, A. Secroun, E. Sefusatti, G. Seidel, S. Serrano, P. Simon, C. Sirignano, G. Sirri, A. Spurio Mancini, L. Stanco, J. -L. Starck, J. Steinwagner, P. Tallada-Crespí, A. N. Taylor, I. Tereno, N. Tessore, S. Toft, R. Toledo-Moreo, F. Torradeflot, L. Valenziano, J. Valiviita, T. Vassallo, A. Veropalumbo, Y. Wang, J. Weller, G. Zamorani, E. Zucca, M. Ballardini, E. Bozzo, C. Burigana, R. Cabanac, M. Calabrese, A. Cappi, D. Di Ferdinando, J. A. Escartin Vigo, L. Gabarra, W. G. Hartley, J. Martín-Fleitas, S. Matthew, M. Maturi, N. Mauri, R. B. Metcalf, A. Pezzotta, M. Pöntinen, C. Porciani, I. Risso, V. Scottez, M. Sereno, M. Tenti, M. Viel, M. Wiesmann, Y. Akrami, S. Alvi, I. T. Andika, S. Anselmi, M. Archidiacono, F. Atrio-Barandela, D. Bertacca, M. Bethermin, L. Blot, M. Bonici, S. Borgani, M. L. Brown, S. Bruton, A. Calabro, B. Camacho Quevedo, F. Caro, C. S. Carvalho, T. Castro, F. Cogato, S. Conseil, A. R. Cooray, S. Davini, G. Desprez, A. Díaz-Sánchez, J. J. Diaz, S. Di Domizio, J. M. Diego, M. Y. Elkhashab, A. Enia, Y. Fang, A. G. Ferrari, A. Finoguenov, A. Franco, K. Ganga, J. García-Bellido, T. Gasparetto, V. Gautard, E. Gaztanaga, F. Giacomini, F. Gianotti, G. Gozaliasl, C. M. Gutierrez, S. Hemmati, C. Hernández-Monteagudo, H. Hildebrandt, J. Hjorth, J. J. E. Kajava, Y. Kang, V. Kansal, D. Karagiannis, K. Kiiveri, J. Kim, C. C. Kirkpatrick, S. Kruk, F. Lacasa, M. Lattanzi, J. Le Graet, L. Legrand, M. Lembo, F. Lepori, G. Leroy, G. F. Lesci, J. Lesgourgues, T. I. Liaudat, J. Macias-Perez, G. Maggio, M. Magliocchetti, R. Maoli, C. J. A. P. Martins, L. Maurin, M. Miluzio, P. Monaco, C. Moretti, G. Morgante, S. Nadathur, K. Naidoo, A. Navarro-Alsina, S. Nesseris, D. Paoletti, F. Passalacqua, K. Paterson, L. Patrizii, A. Pisani, D. Potter, S. Quai, M. Radovich, G. Rodighiero, S. Sacquegna, M. Sahlén, D. B. Sanders, E. Sarpa, A. Schneider, D. Sciotti, E. Sellentin, A. Silvestri, L. C. Smith, K. Tanidis, C. Tao, G. Testera, R. Teyssier, S. Tosi, A. Troja, M. Tucci, C. Valieri, A. Venhola, D. Vergani, F. Vernizzi, G. Verza, N. A. Walton</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Instead, the flat-sky approximation is accurate to below $5\%$ in recovering the angular power spectra of galaxy number counts in both cases and can be used to simplify the computation of the full power spectrum in harmonic space for the data analysis of DR3. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We compare the performance of the flat-sky approximation and Limber approximation for the clustering analysis of the photometric galaxy catalogue of Euclid. We study a 6 bin configuration representing the first data release (DR1) and a 13 bin configuration representative of the third and final data release (DR3). We find that the Limber approximation is sufficiently accurate for the analysis of the wide bins of DR1. Contrarily, the 13 bins of DR3 cannot be modelled accurately with the Limber approximation. Instead, the flat-sky approximation is accurate to below $5\%$ in recovering the angular power spectra of galaxy number counts in both cases and can be used to simplify the computation of the full power spectrum in harmonic space for the data analysis of DR3.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.17592" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.17592" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.17592" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 14.1</span>
                        <span class="badge bg-info text-dark">Author Score: 0.99</span>
                        <span class="badge bg-primary">Semantic Score: 0.91</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">2. $H_0$ Without the Sound Horizon (or Supernovae): A 2% Measurement in DESI DR1
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">E. A. Zaborowski, P. Taylor, K. Honscheid, A. Cuceu, A. de Mattia, A. Krolewski, M. Rashkovetskyi, A. J. Ross, C. To, J. Aguilar, et al.</span>
                                <span class="author-full" style="display: none;">E. A. Zaborowski, P. Taylor, K. Honscheid, A. Cuceu, A. de Mattia, A. Krolewski, M. Rashkovetskyi, A. J. Ross, C. To, J. Aguilar, S. Ahlen, A. Anand, S. BenZvi, D. Bianchi, D. Brooks, F. J. Castander, T. Claybaugh, A. de la Macorra, J. Della Costa, P. Doel, S. Ferraro, A. Font-Ribera, J. E. Forero-Romero, E. Gaztañaga, G. Gutierrez, H. K. Herrera-Alcantar, C. Howlett, D. Huterer, M. Ishak, R. Joyce, D. Kirkby, T. Kisner, A. Kremin, O. Lahav, C. Lamman, M. Landriau, L. Le Guillou, M. Manera, P. Martini, A. Meisner, R. Miquel, J. Moustakas, S. Nadathur, G. Niz, N. Palanque-Delabrouille, W. J. Percival, F. Prada, I. Pérez-Ràfols, G. Rossi, L. Samushia, E. Sanchez, D. Schlegel, M. Schubnell, H. Seo, J. Silber, D. Sprayberry, G. Tarlé, B. A. Weaver, P. Zarrouk, R. Zhou, H. Zou</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> These remarkably consistent constraints achieve better than 2% precision; they are among the most stringent sound horizon-independent measurements from LSS to date, and provide a powerful avenue for probing the origin of the Hubble tension. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The sound horizon scale $r_s$ is a key source of information for early-time $H_0$ measurements, and is therefore a common target of new physics proposed to solve the Hubble tension. We present a sub-2% measurement of the Hubble constant that is independent of this scale, using data from the first data release of the Dark Energy Spectroscopic Instrument (DESI DR1). Building on previous work, we remove dependency on the sound horizon size using a heuristic rescaling procedure at the power spectrum level. A key innovation is the inclusion of \emph{uncalibrated} (agnostic to $r_s$) post-reconstruction BAO measurements from DESI DR1, as well as using the CMB acoustic scale $\theta_*$ as a high-redshift anchor. Uncalibrated type-Ia supernovae are often included as an independent source of $\Omega_m$ information; here we demonstrate the robustness of our results by additionally considering two supernova-independent alternative datasets. We find somewhat higher values of $H_0$ relative to our previous work: $69.2^{+1.3}_{-1.4}$, $70.3^{+1.4}_{-1.2}$, and $69.6^{+1.3}_{-1.8}\,{\rm km\,s^{-1}\,Mpc^{-1}}$ respectively when including measurements from i) Planck/ACT CMB lensing $\times$ unWISE galaxies, ii) the DES Year 3 6$\times$2pt analysis, and iii) Planck/ACT CMB lensing + the DES Year 5 supernova analysis. These remarkably consistent constraints achieve better than 2% precision; they are among the most stringent sound horizon-independent measurements from LSS to date, and provide a powerful avenue for probing the origin of the Hubble tension.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.19149" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.19149" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.19149" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.7</span>
                        <span class="badge bg-info text-dark">Author Score: 0.30</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">3. Parametric strong lensing model of the galaxy cluster Abell 2390 from Euclid and MUSE observations
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">D. Abriola, M. Lombardi, C. Grillo, P. Bergamini, P. Rosati, M. Meneghetti, A. Bolamperti, A. Acebron, G. Granata, G. Angora, et al.</span>
                                <span class="author-full" style="display: none;">D. Abriola, M. Lombardi, C. Grillo, P. Bergamini, P. Rosati, M. Meneghetti, A. Bolamperti, A. Acebron, G. Granata, G. Angora, H. Atek, J. M. Diego, G. Congedo, R. Gavazzi, Y. Kang, M. Montes, T. T. Thai</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We present a new high precision parametric strong lensing total mass reconstruction of the Euclid Early Release Observations (ERO) galaxy cluster Abell 2390, at redshift z = 0.231. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present a new high precision parametric strong lensing total mass reconstruction of the Euclid Early Release Observations (ERO) galaxy cluster Abell 2390, at redshift z = 0.231. We include in this analysis 35 multiple images from 13 background sources, of which 25 are spectroscopically confirmed thanks to observations from the MUSE, spanning a redshift range from z = 0.535 to z = 4.877. After fully reanalysing the MUSE spectroscopy, we combine it with archival spectroscopic catalogues allowing us to select 65 secure cluster members. This sample is further complemented with 114 photometric member galaxies, identified within the Euclid VIS and NISP imaging down to magnitude H = 23. We also measure the stellar velocity dispersions for 22 cluster members, in order to calibrate the Faber-Jackson relation and hence the scaling relations for the subhalo mass components. We test and compare 11 total mass parametrisations of the galaxy cluster with increasing complexity. To do so, we employ the new parametric strong lensing modelling code Gravity.jl. Our best fit total mass parametrisation is characterised by a single large scale halo, 179 subhalo components, and an external shear term. The reference model yields a mean scatter between the model predicted and observed positions of the multiple images of 0.32 arcseconds. We are able to quantify the systematics arising from our modelling choices by taking advantage of all the different explored total mass parametrisations. When our results are compared with those from other lensing studies, we notice an overall agreement in the reconstructed cluster total mass profile in the outermost strong lensing regime: the discrepancy in the innermost region of the cluster (few kiloparsec from the brightest cluster galaxy, where few or no strong lensing features are observed) could possibly be ascribed to the different data and modelling choices.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.17192" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.17192" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.17192" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.15</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">4. Cosmic shear with one component and its application to future radio surveys
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yu-Hsiu Huang, Elisabeth Krause, Tim Eifler, Gary Bernstein, Jiachuan Xu, Eric Huff, Pranjal R. S</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> However, when applying this method to deeper spectroscopic surveys with stronger emission lines, the one-component KL approach could surpass WL in constraining power, offering a promising and efficient pathway for future shear analyses. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present a new approach to measuring cosmic shear: the one-component Kinematic Lensing (KL) method. This technique provides a simplified implementation of KL that reduces shape noise in weak lensing (WL) by combining kinematic information with imaging data, while requiring less observational effort than the full two-component KL. We perform simulated likelihood analyses to assess the performance of the one-component KL and demonstrate its applicability to future radio surveys. Our forecasts indicate that, for radio surveys, the one-component KL is not yet competitive with traditional WL due to the shallow redshift distribution of HI-selected galaxies. However, when applying this method to deeper spectroscopic surveys with stronger emission lines, the one-component KL approach could surpass WL in constraining power, offering a promising and efficient pathway for future shear analyses.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.18011" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.18011" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.18011" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.16</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">5. Testing the Stellar Feedback-driven Breathing Mode in Low-mass Galaxies with Gas Kinematics
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yifei Luo, Joseph Wick, Alexie Leauthaud, Andrew Wetzel, Tucker Jones, Erin Kado-Fong, Song Huang, Xinjun Chen, Conghao Zhou, Jiaxuan Li</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A key prediction is that feedback-driven gas outflow and inflow cycles can lead to ``breathing modes&#39;&#39; (rapid fluctuations in the global gravitational potential) which drive correlated variations in galaxy size, kinematics, and star-formation rate. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Hydrodynamic simulations have proposed that stellar feedback and bursty star-formation can produce dark matter cores in low-mass galaxies. A key prediction is that feedback-driven gas outflow and inflow cycles can lead to ``breathing modes&#39;&#39; (rapid fluctuations in the global gravitational potential) which drive correlated variations in galaxy size, kinematics, and star-formation rate. In this paper, we test the dynamical effects of feedback-driven breathing modes using a sample of 103 star-forming low-mass galaxies with stellar masses between $7.9&lt;\rm \log M_*/M_\odot&lt;9.6$ and $0.02&lt;z&lt;0.19$. We measure ionized gas velocity dispersions from H$\alpha$ emission lines and compare them to mock observations from the FIRE-2 simulations. We compare gas velocity dispersions ($\rm \sigma_{gas}$), stellar masses, and specific star-formation rates (sSFR). We find a positive correlation between gas velocity dispersion residuals at fixed stellar masses ($\rm \Delta\sigma_{gas}$) and sSFR in both data and simulations. However, the relation is tighter in FIRE-2 compared to the data. FIRE-2 produces more low-sSFR galaxies compared to our observational sample, however, the sSFR distributions agree after limiting both samples to a minimum sSFR. A deeper and more complete photometric sample further indicates that observed low-mass galaxies could span the full range of sSFR predicted in the FIRE-2 simulations. Our results support the existence of short-timescale dynamical effects driven by gas outflow and inflow cycles in low-mass galaxies and motivate additional tests of the breathing mode.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.17996" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.17996" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.17996" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.7</span>
                        <span class="badge bg-info text-dark">Author Score: 0.10</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">6. Using Strong Lensing to Detect Subhalos with Steep Inner Density Profiles
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Kassidy E. Kollmann, James W. Nightingale, Mariangela Lisanti, Andrew Robertson, Oren Slone</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The results of this work highlight how the underlying dark matter physics can significantly impact the expected number of subhalo detections from strong gravitational lensing observations. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The inner region of a subhalo&#39;s density distribution is particularly sensitive to dark matter microphysics, with alternative dark matter models leading to both cored and steeply-rising inner density profiles. This work investigates how the lensing signature and detectability of dark matter subhalos in mock HST-, Euclid-, and JWST-like strong lensing observations depends on the subhalo&#39;s radial density profile, especially with regards to the inner power-law slope, $\beta$. We demonstrate that the minimum-mass subhalo detectable along the Einstein ring of a system is strongly dependent on $\beta$. In particular, we show that subhalos with $\beta \sim 2.2$ can be detected down to masses over an order-of-magnitude lower than their Navarro-Frenk-White (NFW) counterparts with $\beta \sim 1$. Importantly, we find that the detectability of subhalos with steep inner profiles is minimally affected by increasing the complexity of the main lens galaxy&#39;s mass model. This is a unique characteristic of these subhalos, as those with NFW or shallower profiles become essentially undetectable when multipole perturbations are added to the lens model. The results of this work highlight how the underlying dark matter physics can significantly impact the expected number of subhalo detections from strong gravitational lensing observations. This is important for testing Cold Dark Matter against alternatives, such as Self-Interacting Dark Matter, which predict the existence of subhalos with diverse inner density profiles.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.17956" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.17956" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.17956" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.7</span>
                        <span class="badge bg-info text-dark">Author Score: 0.02</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">7. MG-NECOLA: Fast Neural Emulators for Modified Gravity Cosmologies
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>J. Bayron Orjuela-Quintana, Mauricio Reyes, Elena Giusarma, Francisco Villaescusa-Navarro, Neerav Kaushal, César A. Valenzuela-Toledo</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> This combination of precision and robustness establishes MG-NECOLA as a practical emulator for producing large ensembles of high-fidelity simulations, enabling efficient exploration of modified gravity and beyond-$\Lambda$CDM cosmologies in upcoming surveys. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Observations of the large-scale structure (LSS) provide a powerful test of gravity on cosmological scales, but high-resolution N-body simulations of modified gravity (MG) are prohibitively expensive. We present MG-NECOLA, a convolutional neural network that enhances fast MG-PICOLA simulations to near-N-body fidelity at a fraction of the cost. MG-NECOLA reproduces QUIJOTE-MG N-body results in the power spectrum and bispectrum with better than 1% accuracy down to non-linear scales ($k \simeq 1~h~\mathrm{Mpc}^{-1}$), while reducing computational time by several orders of magnitude. Importantly, although trained only on $f(R)$ models with massless neutrinos, the network generalizes robustly to scenarios with massive neutrinos, preserving accuracy to within 5% at non-linear scales. This combination of precision and robustness establishes MG-NECOLA as a practical emulator for producing large ensembles of high-fidelity simulations, enabling efficient exploration of modified gravity and beyond-$\Lambda$CDM cosmologies in upcoming surveys.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.20086" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.20086" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.20086" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.05</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">8. Targeting cluster galaxies for the 4MOST CHANCES Low-z sub-survey with photometric redshifts
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Hugo Méndez-Hernández, Ciria Lima-Dias, Antonela Monachesi, Yara L. Jaffé, Christopher P. Haines, Gabriel S. M. Teixeira, Elismar Lösch, Raúl Baier-Soto, Erik V. R. Lima, Amrutha B. M., et al.</span>
                                <span class="author-full" style="display: none;">Hugo Méndez-Hernández, Ciria Lima-Dias, Antonela Monachesi, Yara L. Jaffé, Christopher P. Haines, Gabriel S. M. Teixeira, Elismar Lösch, Raúl Baier-Soto, Erik V. R. Lima, Amrutha B. M., C. R. Bom, Giuseppe D&#39;Ago, Ricardo Demarco, Alexis Finoguenov, Rodrigo F. Haack, Amanda R. Lopes, C. Mendes de Oliveira, Paola Merluzzi, Franco Piraino-Cerda, Analía V. Smith Castelli, Cristobal Sif&#39;on, Laerte Sodré Jr, Nicolás Tejos, Sergio Torres-Flores, Maria Argudo-Fernández, Jacob P. Crossett, E. Ibar, Ulrike Kuchner, Ivan Lacerna, Vitor H. Lopes-Silva, Sebastián Lopez, Sean McGee, Lorenzo Morelli, Julie Nantais, Patricio Olivares V., Diego Pallero, Bianca M. Poggianti, Emanuela Pompei, V. M. Sampaio, Benedetta Vulcani, Alfredo Zenteno, F. Almeida-Fernandes, Maciej Bilicki, M. S. Carvalho, Cheng Cheng, A. L. Figueiredo, L. A. Gutiérrez-Soto, F. R. Herpich, A. Kanaan, E A. D. Lacerda, L. Nakazono, G. B. Oliveira Schwarz, T. Ribeiro, Boudewijn F. Roukema, Marília J. Sartori, Thaís Santos-Silva, W. Schoenell</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The CHANCES Low-z target catalogues form a statistically robust sample for spectroscopic follow-up, allowing studies of galaxy evolution and environmental effects in nearby cluster and supercluster environments. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The evolution of galaxies is shaped by both internal processes and their external environments. Galaxy clusters and their surroundings provide ideal laboratories to study these effects, particularly mechanisms such as quenching and morphological transformation. The Chilean Cluster galaxy Evolution Survey (CHANCES) Low-z sub-survey is part of the CHileAN Cluster galaxy Evolution Survey, a 4MOST community survey designed to uncover the relationship between the formation and evolution of galaxies and hierarchical structure formation as it happens, through deep and wide multi-object spectroscopy. We present the target selection strategy followed to select galaxy cluster candidate members for the CHANCES low-z sub-survey, in and around 50 clusters and two superclusters at z= 500,000 galaxy cluster candidate members and including confirmed spectroscopic galaxy cluster members, from which we expect to obtain 4MOST low-resolution (R~6500) spectra for ~320,000 galaxies. The CHANCES Low-z target catalogues form a statistically robust sample for spectroscopic follow-up, allowing studies of galaxy evolution and environmental effects in nearby cluster and supercluster environments.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.19958" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.19958" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.19958" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.09</span>
                        <span class="badge bg-primary">Semantic Score: 0.91</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">9. Cosmological constraints from the angular power spectrum and bispectrum of luminous red galaxies and CMB lensing
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Francesco Verdiani, Lea Harscouet, Matteo Zennaro, David Alonso, Boryana Hadzhiyska</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We study the projected clustering of photometric luminous red galaxies from the DESI Legacy Survey, combining their angular power spectrum, bispectrum, and cross-correlation with maps of the CMB lensing convergence from the Planck satellite. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We study the projected clustering of photometric luminous red galaxies from the DESI Legacy Survey, combining their angular power spectrum, bispectrum, and cross-correlation with maps of the CMB lensing convergence from the Planck satellite. We employ a perturbative bias expansion in Eulerian space to describe the clustering of galaxies, modelling the power spectrum and bispectrum at one-loop and tree level, respectively. This allows us to use the power spectrum to self-consistently calibrate the perturbative bias parameters. We validate this model against an $N$-body simulation, and show that it can be used up to scales of at least $k_{\rm max}^P\simeq 0.2\,h{\rm Mpc}^{-1}$ and $k_{\rm max}^B\simeq 0.08\,h{\rm Mpc}^{-1}$, saturating the information recovered from the data. We obtain constraints on the amplitude of matter fluctuations $\sigma_8=0.761\pm 0.020$ and the non-relativistic matter fraction $\Omega_m=0.307\pm 0.015$, as well as the combination $S_8\equiv\sigma_8\sqrt{\Omega_m/0.3}=0.769 \pm 0.020$. Including the galaxy bispectrum leads to a $10$-$20\%$ improvement on the cosmological constraints, which are also in good agreement with previous analyses of the same data, and in mild tension with Planck at the $\sim2.5\sigma$ level. This tension is largely present in the standard two-point function dataset, and the addition of the bispectrum increases it slightly, marginally shifting $\sigma_8$ downwards and $\Omega_m$ upwards. Finally, using the bispectrum allows for a substantially more precise measurement of the bias parameters of this sample, which are in reasonable agreement with existing coevolution relations.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.17796" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.17796" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.17796" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.96</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">10. Symbolic Regression and Differentiable Fits in Beyond the Standard Model Physics
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Shehu AbdusSalam, Steven Abel, Deaglan Bartlett, Miguel Crispim Romão</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Like many incarnations of BSM physics this model has a number (four) of arbitrary parameters, which determine the experimental signals, and cosmological observables such as the dark matter relic density. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We demonstrate the efficacy of symbolic regression (SR) to probe models of particle physics Beyond the Standard Model (BSM), by considering the so-called Constrained Minimal Supersymmetric Standard Model (CMSSM). Like many incarnations of BSM physics this model has a number (four) of arbitrary parameters, which determine the experimental signals, and cosmological observables such as the dark matter relic density. We show that analysis of the phenomenology can be greatly accelerated by using symbolic expressions derived for the observables in terms of the input parameters. Here we focus on the Higgs mass, the cold dark matter relic density, and the contribution to the anomalous magnetic moment of the muon. We find that SR can produce remarkably accurate expressions. Using them we make global fits to derive the posterior probability densities of the CMSSM input parameters which are in good agreement with those performed using conventional methods. Moreover, we demonstrate a major advantage of SR which is the ability to make fits using differentiable methods rather than sampling methods. We also compare the method with neural network (NN) regression. SR produces more globally robust results, while NNs require data that is focussed on the promising regions in order to be equally performant.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.20453" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.20453" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.20453" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.96</span>
                        
                            <span class="badge bg-secondary">hep-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">11. Directional Search for Persistent Gravitational Waves: Results from the First Part of LIGO-Virgo-KAGRA&#39;s Fourth Observing Run
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">The LIGO Scientific Collaboration, the Virgo Collaboration, the KAGRA Collaboration, A. G. Abac, I. Abouelfettouh, F. Acernese, K. Ackley, C. Adamcewicz, S. Adhicary, D. Adhikari, et al.</span>
                                <span class="author-full" style="display: none;">The LIGO Scientific Collaboration, the Virgo Collaboration, the KAGRA Collaboration, A. G. Abac, I. Abouelfettouh, F. Acernese, K. Ackley, C. Adamcewicz, S. Adhicary, D. Adhikari, N. Adhikari, R. X. Adhikari, V. K. Adkins, S. Afroz, A. Agapito, D. Agarwal, M. Agathos, N. Aggarwal, S. Aggarwal, O. D. Aguiar, I. -L. Ahrend, L. Aiello, A. Ain, P. Ajith, T. Akutsu, S. Albanesi, W. Ali, S. Al-Kershi, C. Alléné, A. Allocca, S. Al-Shammari, P. A. Altin, S. Alvarez-Lopez, W. Amar, O. Amarasinghe, A. Amato, F. Amicucci, C. Amra, A. Ananyeva, S. B. Anderson, W. G. Anderson, M. Andia, M. Ando, M. Andrés-Carcasona, T. Andrić, J. Anglin, S. Ansoldi, J. M. Antelis, S. Antier, M. Aoumi, E. Z. Appavuravther, S. Appert, S. K. Apple, K. Arai, A. Araya, M. C. Araya, M. Arca Sedda, J. S. Areeda, N. Aritomi, F. Armato, S. Armstrong, N. Arnaud, M. Arogeti, S. M. Aronson, G. Ashton, Y. Aso, L. Asprea, M. Assiduo, S. Assis de Souza Melo, S. M. Aston, P. Astone, F. Attadio, F. Aubin, K. AultONeal, G. Avallone, E. A. Avila, S. Babak, C. Badger, S. Bae, S. Bagnasco, L. Baiotti, R. Bajpai, T. Baka, A. M. Baker, K. A. Baker, T. Baker, G. Baldi, N. Baldicchi, M. Ball, G. Ballardin, S. W. Ballmer, S. Banagiri, B. Banerjee, D. Bankar, T. M. Baptiste, P. Baral, M. Baratti, J. C. Barayoga, B. C. Barish, D. Barker, N. Barman, P. Barneo, F. Barone, B. Barr, L. Barsotti, M. Barsuglia, D. Barta, A. M. Bartoletti, M. A. Barton, I. Bartos, A. Basalaev, R. Bassiri, A. Basti, M. Bawaj, P. Baxi, J. C. Bayley, A. C. Baylor, P. A. Baynard II, M. Bazzan, V. M. Bedakihale, F. Beirnaert, M. Bejger, D. Belardinelli, A. S. Bell, D. S. Bellie, L. Bellizzi, W. Benoit, I. Bentara, J. D. Bentley, M. Ben Yaala, S. Bera, F. Bergamin, B. K. Berger, S. Bernuzzi, M. Beroiz, D. Bersanetti, T. Bertheas, A. Bertolini, J. Betzwieser, D. Beveridge, G. Bevilacqua, N. Bevins, R. Bhandare, R. Bhatt, D. Bhattacharjee, S. Bhattacharyya, S. Bhaumik, V. Biancalana, A. Bianchi, I. A. Bilenko, G. Billingsley, A. Binetti, S. Bini, C. Binu, S. Biot, O. Birnholtz, S. Biscoveanu, A. Bisht, M. Bitossi, M. -A. Bizouard, S. Blaber, J. K. Blackburn, L. A. Blagg, C. D. Blair, D. G. Blair, N. Bode, N. Boettner, G. Boileau, M. Boldrini, G. N. Bolingbroke, A. Bolliand, L. D. Bonavena, R. Bondarescu, F. Bondu, E. Bonilla, M. S. Bonilla, A. Bonino, R. Bonnand, A. Borchers, S. Borhanian, V. Boschi, S. Bose, V. Bossilkov, Y. Bothra, A. Boudon, L. Bourg, M. Boyle, A. Bozzi, C. Bradaschia, P. R. Brady, A. Branch, M. Branchesi, I. Braun, T. Briant, A. Brillet, M. Brinkmann, P. Brockill, E. Brockmueller, A. F. Brooks, B. C. Brown, D. D. Brown, M. L. Brozzetti, S. Brunett, G. Bruno, R. Bruntz, J. Bryant, Y. Bu, F. Bucci, J. Buchanan, O. Bulashenko, T. Bulik, H. J. Bulten, A. Buonanno, K. Burtnyk, R. Buscicchio, D. Buskulic, C. Buy, R. L. Byer, G. S. Cabourn Davies, R. Cabrita, V. Cáceres-Barbosa, L. Cadonati, G. Cagnoli, C. Cahillane, A. Calafat, T. A. Callister, E. Calloni, S. R. Callos, G. Caneva Santoro, K. C. Cannon, H. Cao, L. A. Capistran, E. Capocasa, E. Capote, G. Capurri, G. Carapella, F. Carbognani, M. Carlassara, J. B. Carlin, T. K. Carlson, M. F. Carney, M. Carpinelli, G. Carrillo, J. J. Carter, G. Carullo, A. Casallas-Lagos, J. Casanueva Diaz, C. Casentini, S. Y. Castro-Lucas, S. Caudill, M. Cavaglià, R. Cavalieri, A. Ceja, G. Cella, P. Cerdá-Durán, E. Cesarini, N. Chabbra, W. Chaibi, A. Chakraborty, P. Chakraborty, S. Chakraborty, S. Chalathadka Subrahmanya, J. C. L. Chan, M. Chan, K. Chang, S. Chao, P. Charlton, E. Chassande-Mottin, C. Chatterjee, Debarati Chatterjee, Deep Chatterjee, M. Chaturvedi, S. Chaty, K. Chatziioannou, A. Chen, A. H. -Y. Chen, D. Chen, H. Chen, H. Y. Chen, S. Chen, Yanbei Chen, Yitian Chen, H. P. Cheng, P. Chessa, H. T. Cheung, S. Y. Cheung, F. Chiadini, G. Chiarini, A. Chiba, A. Chincarini, M. L. Chiofalo, A. Chiummo, C. Chou, S. Choudhary, N. Christensen, S. S. Y. Chua, G. Ciani, P. Ciecielag, M. Cieślar, M. Cifaldi, B. Cirok, F. Clara, J. A. Clark, T. A. Clarke, P. Clearwater, S. Clesse, F. Cleva, E. Coccia, E. Codazzo, P. -F. Cohadon, S. Colace, E. Colangeli, M. Colleoni, C. G. Collette, J. Collins, S. Colloms, A. Colombo, C. M. Compton, G. Connolly, L. Conti, T. R. Corbitt, I. Cordero-Carrión, S. Corezzi, N. J. Cornish, I. Coronado, A. Corsi, R. Cottingham, M. W. Coughlin, A. Couineaux, P. Couvares, D. M. Coward, R. Coyne, A. Cozzumbo, J. D. E. Creighton, T. D. Creighton, P. Cremonese, S. Crook, R. Crouch, J. Csizmazia, J. R. Cudell, T. J. Cullen, A. Cumming, E. Cuoco, M. Cusinato, L. V. Da Conceição, T. Dal Canton, S. Dal Pra, G. Dálya, B. D&#39;Angelo, S. Danilishin, S. D&#39;Antonio, K. Danzmann, K. E. Darroch, L. P. Dartez, R. Das, A. Dasgupta, V. Dattilo, A. Daumas, N. Davari, I. Dave, A. Davenport, M. Davier, T. F. Davies, D. Davis, L. Davis, M. C. Davis, P. Davis, E. J. Daw, M. Dax, J. De Bolle, M. Deenadayalan, J. Degallaix, M. De Laurentis, F. De Lillo, S. Della Torre, W. Del Pozzo, A. Demagny, F. De Marco, G. Demasi, F. De Matteis, N. Demos, T. Dent, A. Depasse, N. DePergola, R. De Pietri, R. De Rosa, C. De Rossi, M. Desai, R. DeSalvo, A. DeSimone, R. De Simone, A. Dhani, R. Diab, M. C. Díaz, M. Di Cesare, G. Dideron, T. Dietrich, L. Di Fiore, C. Di Fronzo, M. Di Giovanni, T. Di Girolamo, D. Diksha, J. Ding, S. Di Pace, I. Di Palma, D. Di Piero, F. Di Renzo, Divyajyoti, A. Dmitriev, J. P. Docherty, Z. Doctor, N. Doerksen, E. Dohmen, A. Doke, A. Domiciano De Souza, L. D&#39;Onofrio, F. Donovan, K. L. Dooley, T. Dooney, S. Doravari, O. Dorosh, W. J. D. Doyle, M. Drago, J. C. Driggers, L. Dunn, U. Dupletsa, P. -A. Duverne, D. D&#39;Urso, P. Dutta Roy, H. Duval, S. E. Dwyer, C. Eassa, M. Ebersold, T. Eckhardt, G. Eddolls, A. Effler, J. Eichholz, H. Einsle, M. Eisenmann, M. Emma, K. Endo, R. Enficiaud, L. Errico, R. Espinosa, M. Esposito, R. C. Essick, H. Estellés, T. Etzel, M. Evans, T. Evstafyeva, B. E. Ewing, J. M. Ezquiaga, F. Fabrizi, V. Fafone, S. Fairhurst, A. M. Farah, B. Farr, W. M. Farr, G. Favaro, M. Favata, M. Fays, M. Fazio, J. Feicht, M. M. Fejer, R. Felicetti, E. Fenyvesi, J. Fernandes, T. Fernandes, D. Fernando, S. Ferraiuolo, T. A. Ferreira, F. Fidecaro, P. Figura, A. Fiori, I. Fiori, M. Fishbach, R. P. Fisher, R. Fittipaldi, V. Fiumara, R. Flaminio, S. M. Fleischer, L. S. Fleming, E. Floden, H. Fong, J. A. Font, F. Fontinele-Nunes, C. Foo, B. Fornal, K. Franceschetti, F. Frappez, S. Frasca, F. Frasconi, J. P. Freed, Z. Frei, A. Freise, O. Freitas, R. Frey, W. Frischhertz, P. Fritschel, V. V. Frolov, G. G. Fronzé, M. Fuentes-Garcia, S. Fujii, T. Fujimori, P. Fulda, M. Fyffe, B. Gadre, J. R. Gair, S. Galaudage, V. Galdi, R. Gamba, A. Gamboa, S. Gamoji, D. Ganapathy, A. Ganguly, B. Garaventa, J. García-Bellido, C. García-Quirós, J. W. Gardner, K. A. Gardner, S. Garg, J. Gargiulo, X. Garrido, A. Garron, F. Garufi, P. A. Garver, C. Gasbarra, B. Gateley, F. Gautier, V. Gayathri, T. Gayer, G. Gemme, A. Gennai, V. Gennari, J. George, R. George, O. Gerberding, L. Gergely, Archisman Ghosh, Sayantan Ghosh, Shaon Ghosh, Shrobana Ghosh, Suprovo Ghosh, Tathagata Ghosh, J. A. Giaime, K. D. Giardina, D. R. Gibson, C. Gier, S. Gkaitatzis, J. Glanzer, F. Glotin, J. Godfrey, R. V. Godley, P. Godwin, A. S. Goettel, E. Goetz, J. Golomb, S. Gomez Lopez, B. Goncharov, G. González, P. Goodarzi, S. Goode, A. W. Goodwin-Jones, M. Gosselin, R. Gouaty, D. W. Gould, K. Govorkova, A. Grado, V. Graham, A. E. Granados, M. Granata, V. Granata, S. Gras, P. Grassia, J. Graves, C. Gray, R. Gray, G. Greco, A. C. Green, L. Green, S. M. Green, S. R. Green, C. Greenberg, A. M. Gretarsson, H. K. Griffin, D. Griffith, H. L. Griggs, G. Grignani, C. Grimaud, H. Grote, S. Grunewald, D. Guerra, D. Guetta, G. M. Guidi, A. R. Guimaraes, H. K. Gulati, F. Gulminelli, H. Guo, W. Guo, Y. Guo, Anuradha Gupta, I. Gupta, N. C. Gupta, S. K. Gupta, V. Gupta, N. Gupte, J. Gurs, N. Gutierrez, N. Guttman, F. Guzman, D. Haba, M. Haberland, S. Haino, E. D. Hall, E. Z. Hamilton, G. Hammond, M. Haney, J. Hanks, C. Hanna, M. D. Hannam, O. A. Hannuksela, A. G. Hanselman, H. Hansen, J. Hanson, S. Hanumasagar, R. Harada, A. R. Hardison, S. Harikumar, K. Haris, I. Harley-Trochimczyk, T. Harmark, J. Harms, G. M. Harry, I. W. Harry, J. Hart, B. Haskell, C. J. Haster, K. Haughian, H. Hayakawa, K. Hayama, M. C. Heintze, J. Heinze, J. Heinzel, H. Heitmann, F. Hellman, A. F. Helmling-Cornell, G. Hemming, O. Henderson-Sapir, M. Hendry, I. S. Heng, M. H. Hennig, C. Henshaw, M. Heurs, A. L. Hewitt, J. Heynen, J. Heyns, S. Higginbotham, S. Hild, S. Hill, Y. Himemoto, N. Hirata, C. Hirose, D. Hofman, B. E. Hogan, N. A. Holland, I. J. Hollows, D. E. Holz, L. Honet, D. J. Horton-Bailey, J. Hough, S. Hourihane, N. T. Howard, E. J. Howell, C. G. Hoy, C. A. Hrishikesh, P. Hsi, H. -F. Hsieh, H. -Y. Hsieh, C. Hsiung, S. -H. Hsu, W. -F. Hsu, Q. Hu, H. Y. Huang, Y. Huang, Y. T. Huang, A. D. Huddart, B. Hughey, V. Hui, S. Husa, R. Huxford, L. Iampieri, G. A. Iandolo, M. Ianni, G. Iannone, J. Iascau, K. Ide, R. Iden, A. Ierardi, S. Ikeda, H. Imafuku, Y. Inoue, G. Iorio, P. Iosif, M. H. Iqbal, J. Irwin, R. Ishikawa, M. Isi, K. S. Isleif, Y. Itoh, M. Iwaya, B. R. Iyer, C. Jacquet, P. -E. Jacquet, T. Jacquot, S. J. Jadhav, S. P. Jadhav, M. Jain, T. Jain, A. L. James, K. Jani, J. Janquart, N. N. Janthalur, S. Jaraba, P. Jaranowski, R. Jaume, W. Javed, A. Jennings, M. Jensen, W. Jia, J. Jiang, H. -B. Jin, G. R. Johns, N. A. Johnson, M. C. Johnston, R. Johnston, N. Johny, D. H. Jones, D. I. Jones, R. Jones, H. E. Jose, P. Joshi, S. K. Joshi, G. Joubert, J. Ju, L. Ju, K. Jung, J. Junker, V. Juste, H. B. Kabagoz, T. Kajita, I. Kaku, V. Kalogera, M. Kalomenopoulos, M. Kamiizumi, N. Kanda, S. Kandhasamy, G. Kang, N. C. Kannachel, J. B. Kanner, S. A. KantiMahanty, S. J. Kapadia, D. P. Kapasi, M. Karthikeyan, M. Kasprzack, H. Kato, T. Kato, E. Katsavounidis, W. Katzman, R. Kaushik, K. Kawabe, R. Kawamoto, D. Keitel, L. J. Kemperman, J. Kennington, F. A. Kerkow, R. Kesharwani, J. S. Key, R. Khadela, S. Khadka, S. S. Khadkikar, F. Y. Khalili, F. Khan, T. Khanam, M. Khursheed, N. M. Khusid, W. Kiendrebeogo, N. Kijbunchoo, C. Kim, J. C. Kim, K. Kim, M. H. Kim, S. Kim, Y. -M. Kim, C. Kimball, K. Kimes, M. Kinnear, J. S. Kissel, S. Klimenko, A. M. Knee, E. J. Knox, N. Knust, K. Kobayashi, S. M. Koehlenbeck, G. Koekoek, K. Kohri, K. Kokeyama, S. Koley, P. Kolitsidou, A. E. Koloniari, K. Komori, A. K. H. Kong, A. Kontos, L. M. Koponen, M. Korobko, X. Kou, A. Koushik, N. Kouvatsos, M. Kovalam, T. Koyama, D. B. Kozak, S. L. Kranzhoff, V. Kringel, N. V. Krishnendu, S. Kroker, A. Królak, K. Kruska, J. Kubisz, G. Kuehn, S. Kulkarni, A. Kulur Ramamohan, Achal Kumar, Anil Kumar, Praveen Kumar, Prayush Kumar, Rahul Kumar, Rakesh Kumar, J. Kume, K. Kuns, N. Kuntimaddi, S. Kuroyanagi, S. Kuwahara, K. Kwak, K. Kwan, S. Kwon, G. Lacaille, D. Laghi, A. H. Laity, E. Lalande, M. Lalleman, P. C. Lalremruati, M. Landry, B. B. Lane, R. N. Lang, J. Lange, R. Langgin, B. Lantz, I. La Rosa, J. Larsen, A. Lartaux-Vollard, P. D. Lasky, J. Lawrence, M. Laxen, C. Lazarte, A. Lazzarini, C. Lazzaro, P. Leaci, L. Leali, Y. K. Lecoeuche, H. M. Lee, H. W. Lee, J. Lee, K. Lee, R. -K. Lee, R. Lee, Sungho Lee, Sunjae Lee, Y. Lee, I. N. Legred, J. Lehmann, L. Lehner, M. Le Jean, A. Lemaître, M. Lenti, M. Leonardi, M. Lequime, N. Leroy, M. Lesovsky, N. Letendre, M. Lethuillier, Y. Levin, K. Leyde, A. K. Y. Li, K. L. Li, T. G. F. Li, X. Li, Y. Li, Z. Li, A. Lihos, E. T. Lin, F. Lin, L. C. -C. Lin, Y. -C. Lin, C. Lindsay, S. D. Linker, A. Liu, G. C. Liu, Jian Liu, F. Llamas Villarreal, J. Llobera-Querol, R. K. L. Lo, J. -P. Locquet, S. C. G. Loggins, M. R. Loizou, L. T. London, A. Longo, D. Lopez, M. Lopez Portilla, A. Lorenzo-Medina, V. Loriette, M. Lormand, G. Losurdo, E. Lotti, T. P. Lott IV, J. D. Lough, H. A. Loughlin, C. O. Lousto, N. Low, N. Lu, L. Lucchesi, H. Lück, D. Lumaca, A. P. Lundgren, A. W. Lussier, R. Macas, M. MacInnis, D. M. Macleod, I. A. O. MacMillan, A. Macquet, K. Maeda, S. Maenaut, S. S. Magare, R. M. Magee, E. Maggio, R. Maggiore, M. Magnozzi, M. Mahesh, M. Maini, S. Majhi, E. Majorana, C. N. Makarem, D. Malakar, J. A. Malaquias-Reis, U. Mali, S. Maliakal, A. Malik, L. Mallick, A. -K. Malz, N. Man, M. Mancarella, V. Mandic, V. Mangano, B. Mannix, G. L. Mansell, M. Manske, M. Mantovani, M. Mapelli, C. Marinelli, F. Marion, A. S. Markosyan, A. Markowitz, E. Maros, S. Marsat, F. Martelli, I. W. Martin, R. M. Martin, B. B. Martinez, D. A. Martinez, M. Martinez, V. Martinez, A. Martini, J. C. Martins, D. V. Martynov, E. J. Marx, L. Massaro, A. Masserot, M. Masso-Reid, S. Mastrogiovanni, T. Matcovich, M. Matiushechkina, L. Maurin, N. Mavalvala, N. Maxwell, G. McCarrol, R. McCarthy, D. E. McClelland, S. McCormick, L. McCuller, S. McEachin, C. McElhenny, G. I. McGhee, J. McGinn, K. B. M. McGowan, J. McIver, A. McLeod, I. McMahon, T. McRae, R. McTeague, D. Meacher, B. N. Meagher, R. Mechum, Q. Meijer, A. Melatos, C. S. Menoni, F. Mera, R. A. Mercer, L. Mereni, K. Merfeld, E. L. Merilh, J. R. Mérou, J. D. Merritt, M. Merzougui, C. Messick, B. Mestichelli, M. Meyer-Conde, F. Meylahn, A. Mhaske, A. Miani, H. Miao, C. Michel, Y. Michimura, H. Middleton, D. P. Mihaylov, S. J. Miller, M. Millhouse, E. Milotti, V. Milotti, Y. Minenkov, E. M. Minihan, Ll. M. Mir, L. Mirasola, M. Miravet-Tenés, C. -A. Miritescu, A. Mishra, C. Mishra, T. Mishra, A. L. Mitchell, J. G. Mitchell, S. Mitra, V. P. Mitrofanov, K. Mitsuhashi, R. Mittleman, O. Miyakawa, S. Miyoki, A. Miyoko, G. Mo, L. Mobilia, S. R. P. Mohapatra, S. R. Mohite, M. Molina-Ruiz, M. Mondin, M. Montani, C. J. Moore, D. Moraru, A. More, S. More, C. Moreno, E. A. Moreno, G. Moreno, A. Moreso Serra, S. Morisaki, Y. Moriwaki, G. Morras, A. Moscatello, M. Mould, B. Mours, C. M. Mow-Lowry, L. Muccillo, F. Muciaccia, D. Mukherjee, Samanwaya Mukherjee, Soma Mukherjee, Subroto Mukherjee, Suvodip Mukherjee, N. Mukund, A. Mullavey, H. Mullock, J. Mundi, C. L. Mungioli, M. Murakoshi, P. G. Murray, D. Nabari, S. L. Nadji, A. Nagar, N. Nagarajan, K. Nakagaki, K. Nakamura, H. Nakano, M. Nakano, D. Nanadoumgar-Lacroze, D. Nandi, V. Napolano, P. Narayan, I. Nardecchia, T. Narikawa, H. Narola, L. Naticchioni, R. K. Nayak, L. Negri, A. Nela, C. Nelle, A. Nelson, T. J. N. Nelson, M. Nery, A. Neunzert, S. Ng, L. Nguyen Quynh, S. A. Nichols, A. B. Nielsen, Y. Nishino, A. Nishizawa, S. Nissanke, W. Niu, F. Nocera, J. Noller, M. Norman, C. North, J. Novak, R. Nowicki, J. F. Nuño Siles, L. K. Nuttall, K. Obayashi, J. Oberling, J. O&#39;Dell, E. Oelker, M. Oertel, G. Oganesyan, T. O&#39;Hanlon, M. Ohashi, F. Ohme, R. Oliveri, R. Omer, B. O&#39;Neal, M. Onishi, K. Oohara, B. O&#39;Reilly, M. Orselli, R. O&#39;Shaughnessy, S. O&#39;Shea, S. Oshino, C. Osthelder, I. Ota, D. J. Ottaway, A. Ouzriat, H. Overmier, B. J. Owen, R. Ozaki, A. E. Pace, R. Pagano, M. A. Page, A. Pai, L. Paiella, A. Pal, S. Pal, M. A. Palaia, M. Pálfi, P. P. Palma, C. Palomba, P. Palud, H. Pan, J. Pan, K. C. Pan, P. K. Panda, Shiksha Pandey, Swadha Pandey, P. T. H. Pang, F. Pannarale, K. A. Pannone, B. C. Pant, F. H. Panther, M. Panzeri, F. Paoletti, A. Paolone, A. Papadopoulos, E. E. Papalexakis, L. Papalini, G. Papigkiotis, A. Paquis, A. Parisi, B. -J. Park, J. Park, W. Parker, G. Pascale, D. Pascucci, A. Pasqualetti, R. Passaquieti, L. Passenger, D. Passuello, O. Patane, A. V. Patel, D. Pathak, A. Patra, B. Patricelli, B. G. Patterson, K. Paul, S. Paul, E. Payne, T. Pearce, M. Pedraza, A. Pele, F. E. Peña Arellano, X. Peng, Y. Peng, S. Penn, M. D. Penuliar, A. Perego, Z. Pereira, C. Périgois, G. Perna, A. Perreca, J. Perret, S. Perriès, J. W. Perry, D. Pesios, S. Peters, S. Petracca, C. Petrillo, H. P. Pfeiffer, H. Pham, K. A. Pham, K. S. Phukon, H. Phurailatpam, M. Piarulli, L. Piccari, O. J. Piccinni, M. Pichot, M. Piendibene, F. Piergiovanni, L. Pierini, G. Pierra, V. Pierro, M. Pietrzak, M. Pillas, F. Pilo, L. Pinard, I. M. Pinto, M. Pinto, B. J. Piotrzkowski, M. Pirello, M. D. Pitkin, A. Placidi, E. Placidi, M. L. Planas, W. Plastino, C. Plunkett, R. Poggiani, E. Polini, J. Pomper, L. Pompili, J. Poon, E. Porcelli, E. K. Porter, C. Posnansky, R. Poulton, J. Powell, G. S. Prabhu, M. Pracchia, B. K. Pradhan, T. Pradier, A. K. Prajapati, K. Prasai, R. Prasanna, P. Prasia, G. Pratten, G. Principe, G. A. Prodi, P. Prosperi, P. Prosposito, A. C. Providence, A. Puecher, J. Pullin, P. Puppo, M. Pürrer, H. Qi, J. Qin, G. Quéméner, V. Quetschke, P. J. Quinonez, N. Qutob, R. Rading, I. Rainho, S. Raja, C. Rajan, B. Rajbhandari, K. E. Ramirez, F. A. Ramis Vidal, M. Ramos Arevalo, A. Ramos-Buades, S. Ranjan, K. Ransom, P. Rapagnani, B. Ratto, A. Ravichandran, A. Ray, V. Raymond, M. Razzano, J. Read, T. Regimbau, S. Reid, C. Reissel, D. H. Reitze, A. I. Renzini, B. Revenu, A. Revilla Peña, R. Reyes, L. Ricca, F. Ricci, M. Ricci, A. Ricciardone, J. Rice, J. W. Richardson, M. L. Richardson, A. Rijal, K. Riles, H. K. Riley, S. Rinaldi, J. Rittmeyer, C. Robertson, F. Robinet, M. Robinson, A. Rocchi, L. Rolland, J. G. Rollins, A. E. Romano, R. Romano, A. Romero, I. M. Romero-Shaw, J. H. Romie, S. Ronchini, T. J. Roocke, L. Rosa, T. J. Rosauer, C. A. Rose, D. Rosińska, M. P. Ross, M. Rossello-Sastre, S. Rowan, S. K. Roy, S. Roy, D. Rozza, P. Ruggi, N. Ruhama, E. Ruiz Morales, K. Ruiz-Rocha, S. Sachdev, T. Sadecki, P. Saffarieh, S. Safi-Harb, M. R. Sah, S. Saha, T. Sainrat, S. Sajith Menon, K. Sakai, Y. Sakai, M. Sakellariadou, S. Sakon, O. S. Salafia, F. Salces-Carcoba, L. Salconi, M. Saleem, F. Salemi, M. Sallé, S. U. Salunkhe, S. Salvador, A. Salvarese, A. Samajdar, A. Sanchez, E. J. Sanchez, L. E. Sanchez, N. Sanchis-Gual, J. R. Sanders, E. M. Sänger, F. Santoliquido, F. Sarandrea, T. R. Saravanan, N. Sarin, P. Sarkar, A. Sasli, P. Sassi, B. Sassolas, B. S. Sathyaprakash, R. Sato, S. Sato, Yukino Sato, Yu Sato, O. Sauter, R. L. Savage, T. Sawada, H. L. Sawant, S. Sayah, V. Scacco, D. Schaetzl, M. Scheel, A. Schiebelbein, M. G. Schiworski, P. Schmidt, S. Schmidt, R. Schnabel, M. Schneewind, R. M. S. Schofield, K. Schouteden, B. W. Schulte, B. F. Schutz, E. Schwartz, M. Scialpi, J. Scott, S. M. Scott, R. M. Sedas, T. C. Seetharamu, M. Seglar-Arroyo, Y. Sekiguchi, D. Sellers, N. Sembo, A. S. Sengupta, E. G. Seo, J. W. Seo, V. Sequino, M. Serra, A. Sevrin, T. Shaffer, U. S. Shah, M. A. Shaikh, L. Shao, A. K. Sharma, Preeti Sharma, Prianka Sharma, Ritwik Sharma, S. Sharma Chaudhary, P. Shawhan, N. S. Shcheblanov, E. Sheridan, Z. -H. Shi, M. Shikauchi, R. Shimomura, H. Shinkai, S. Shirke, D. H. Shoemaker, D. M. Shoemaker, R. W. Short, S. ShyamSundar, A. Sider, H. Siegel, D. Sigg, L. Silenzi, L. Silvestri, M. Simmonds, L. P. Singer, Amitesh Singh, Anika Singh, D. Singh, N. Singh, S. Singh, A. M. Sintes, V. Sipala, V. Skliris, B. J. J. Slagmolen, D. A. Slater, T. J. Slaven-Blair, J. Smetana, J. R. Smith, L. Smith, R. J. E. Smith, W. J. Smith, S. Soares de Albuquerque Filho, M. Soares-Santos, K. Somiya, I. Song, S. Soni, V. Sordini, F. Sorrentino, H. Sotani, F. Spada, V. Spagnuolo, A. P. Spencer, P. Spinicelli, A. K. Srivastava, F. Stachurski, C. J. Stark, D. A. Steer, N. Steinle, J. Steinlechner, S. Steinlechner, N. Stergioulas, P. Stevens, M. StPierre, M. D. Strong, A. Strunk, A. L. Stuver, M. Suchenek, S. Sudhagar, Y. Sudo, N. Sueltmann, L. Suleiman, K. D. Sullivan, J. Sun, L. Sun, S. Sunil, J. Suresh, B. J. Sutton, P. J. Sutton, K. Suzuki, M. Suzuki, B. L. Swinkels, A. Syx, M. J. Szczepańczyk, P. Szewczyk, M. Tacca, H. Tagoshi, K. Takada, H. Takahashi, R. Takahashi, A. Takamori, S. Takano, H. Takeda, K. Takeshita, I. Takimoto Schmiegelow, M. Takou-Ayaoh, C. Talbot, M. Tamaki, N. Tamanini, D. Tanabe, K. Tanaka, S. J. Tanaka, S. Tanioka, D. B. Tanner, W. Tanner, L. Tao, R. D. Tapia, E. N. Tapia San Martín, C. Taranto, A. Taruya, J. D. Tasson, J. G. Tau, D. Tellez, R. Tenorio, H. Themann, A. Theodoropoulos, M. P. Thirugnanasambandam, L. M. Thomas, M. Thomas, P. Thomas, J. E. Thompson, S. R. Thondapu, K. A. Thorne, E. Thrane, J. Tissino, A. Tiwari, Pawan Tiwari, Praveer Tiwari, S. Tiwari, V. Tiwari, M. R. Todd, M. Toffano, A. M. Toivonen, K. Toland, A. E. Tolley, T. Tomaru, V. Tommasini, T. Tomura, H. Tong, C. Tong-Yu, A. Torres-Forné, C. I. Torrie, I. Tosta e Melo, E. Tournefier, M. Trad Nery, K. Tran, A. Trapananti, R. Travaglini, F. Travasso, G. Traylor, M. Trevor, M. C. Tringali, A. Tripathee, G. Troian, A. Trovato, L. Trozzo, R. J. Trudeau, T. Tsang, S. Tsuchida, L. Tsukada, K. Turbang, M. Turconi, C. Turski, H. Ubach, N. Uchikata, T. Uchiyama, R. P. Udall, T. Uehara, K. Ueno, V. Undheim, L. E. Uronen, T. Ushiba, M. Vacatello, H. Vahlbruch, N. Vaidya, G. Vajente, A. Vajpeyi, J. Valencia, M. Valentini, S. A. Vallejo-Peña, S. Vallero, V. Valsan, M. van Dael, E. Van den Bossche, J. F. J. van den Brand, C. Van Den Broeck, M. van der Sluys, A. Van de Walle, J. van Dongen, K. Vandra, M. VanDyke, H. van Haevermaet, J. V. van Heijningen, P. Van Hove, J. Vanier, M. VanKeuren, J. Vanosky, N. van Remortel, M. Vardaro, A. F. Vargas, V. Varma, A. N. Vazquez, A. Vecchio, G. Vedovato, J. Veitch, P. J. Veitch, S. Venikoudis, R. C. Venterea, P. Verdier, M. Vereecken, D. Verkindt, B. Verma, Y. Verma, S. M. Vermeulen, F. Vetrano, A. Veutro, A. Viceré, S. Vidyant, A. D. Viets, A. Vijaykumar, A. Vilkha, N. Villanueva Espinosa, V. Villa-Ortega, E. T. Vincent, J. -Y. Vinet, S. Viret, S. Vitale, H. Vocca, D. Voigt, E. R. G. von Reis, J. S. A. von Wrangel, W. E. Vossius, L. Vujeva, S. P. Vyatchanin, J. Wack, L. E. Wade, M. Wade, K. J. Wagner, L. Wallace, E. J. Wang, H. Wang, J. Z. Wang, W. H. Wang, Y. F. Wang, G. Waratkar, J. Warner, M. Was, T. Washimi, N. Y. Washington, D. Watarai, B. Weaver, S. A. Webster, N. L. Weickhardt, M. Weinert, A. J. Weinstein, R. Weiss, L. Wen, K. Wette, J. T. Whelan, B. F. Whiting, C. Whittle, E. G. Wickens, D. Wilken, A. T. Wilkin, B. M. Williams, D. Williams, M. J. Williams, N. S. Williams, J. L. Willis, B. Willke, M. Wils, L. Wilson, C. W. Winborn, J. Winterflood, C. C. Wipf, G. Woan, J. Woehler, N. E. Wolfe, H. T. Wong, I. C. F. Wong, K. Wong, T. Wouters, J. L. Wright, M. Wright, B. Wu, C. Wu, D. S. Wu, H. Wu, K. Wu, Q. Wu, Y. Wu, Z. Wu, E. Wuchner, D. M. Wysocki, V. A. Xu, Y. Xu, N. Yadav, H. Yamamoto, K. Yamamoto, T. S. Yamamoto, T. Yamamoto, R. Yamazaki, T. Yan, K. Z. Yang, Y. Yang, Z. Yarbrough, J. Yebana, S. -W. Yeh, A. B. Yelikar, X. Yin, J. Yokoyama, T. Yokozawa, S. Yuan, H. Yuzurihara, M. Zanolin, M. Zeeshan, T. Zelenova, J. -P. Zendri, M. Zeoli, M. Zerrad, M. Zevin, L. Zhang, N. Zhang, R. Zhang, T. Zhang, C. Zhao, Yue Zhao, Yuhang Zhao, Z. -C. Zhao, Y. Zheng, H. Zhong, H. Zhou, H. O. Zhu, Z. -H. Zhu, A. B. Zimmerman, L. Zimmermann, M. E. Zucker, J. Zweizig</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> For persistent broadband sources, we constrain the gravitational-wave flux $F_{\alpha, \hat{n}}^{95\%, \mathrm{UL}}(25\, \mathrm{Hz}) &lt; (0.008 - 5.5) \times 10^{-8}\, \mathrm{erg\, cm^{-2}\, s^{-1}\, Hz^{-1}}$, depending on the sky direction $\hat{n}$ and spectral index $\alpha=0,\,2/3,\,3$. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The angular distribution of gravitational-wave power from persistent sources may exhibit anisotropies arising from the large-scale structure of the Universe. This motivates directional searches for astrophysical and cosmological gravitational-wave backgrounds, as well as continuous-wave emitters. We present results of such a search using data from the first observing run through the first portion of the fourth observing run of the LIGO-Virgo-KAGRA Collaborations. We apply gravitational-wave radiometer techniques to generate skymaps and search for both narrowband and broadband persistent gravitational-wave sources. Additionally, we use spherical harmonic decomposition to probe spatially extended sources. No evidence of persistent gravitational-wave signals is found, and we set the most stringent constraints to date on such emissions. For narrowband point sources, our sensitivity estimate to effective strain amplitude lies in the range $(0.03 - 8.4) \times 10^{-24}$ across all sky and frequency range $(20 - 160)$ Hz. For targeted sources -- Scorpius X-1, SN 1987A, the Galactic Center, Terzan 5, and NGC 6397 -- we constrain the strain amplitude with best limits ranging from $\sim 1.1 \times 10^{-25}$ to $6.5 \times 10^{-24}$. For persistent broadband sources, we constrain the gravitational-wave flux $F_{\alpha, \hat{n}}^{95\%, \mathrm{UL}}(25\, \mathrm{Hz}) &lt; (0.008 - 5.5) \times 10^{-8}\, \mathrm{erg\, cm^{-2}\, s^{-1}\, Hz^{-1}}$, depending on the sky direction $\hat{n}$ and spectral index $\alpha=0,\,2/3,\,3$. Finally, for extended sources, we place upper limits on the strain angular power spectrum $C_\ell^{1/2} &lt; (0.63 - 17) \times 10^{-10} \,\mathrm{sr}^{-1}$.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.17487" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.17487" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.17487" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.06</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">12. The Lensing Counter Narrative: An Effective Description of Small-Scale Clustering in Weak Lensing Power Spectra
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Joseph DeRose, Shi-Fan Chen</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Marginalizing over these LCTs isolates the large-scale cosmological signal in weak lensing power spectra while simultaneously constraining the impact of baryonic feedback or new physics (e.g. axion dark matter) at small scales. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present a new formalism to separate large- and small-scale contributions to cosmic shear through $\textit{lensing counterterms}$ (LCT) inspired by effective field theory (EFT). Marginalizing over these LCTs isolates the large-scale cosmological signal in weak lensing power spectra while simultaneously constraining the impact of baryonic feedback or new physics (e.g. axion dark matter) at small scales. Our formalism removes the need for hard scale cuts in standard analyses, even when theoretical predictions are limited to below a physical cutoff $\Lambda$, resulting in significant improvements in constraining power -- up to $5\times$ smaller in the case of a LSST-Y10-like analysis without marginalizing over baryons when the analysis cutoff is set to $\Lambda = 1.0h$ Mpc$^{-1}$. We conduct a proof-of-principle analysis on the publicly available DES Y3 data, finding $S_8= 0.761\pm 0.045$ and $S_8 = 0.794\pm 0.040$ for analyses with cutoffs of $\Lambda = 0.5h$ Mpc$^{-1}$ and $1.0 h$ Mpc$^{-1}$, respectively, with no detection of modifications to small-scale clustering at $k &gt; \Lambda$ beyond the predictions of collisionless dark matter in a $\Lambda$CDM universe. We make our $\texttt{JAX}$-based pipeline, $\texttt{gholax}$, integrated with intrinsic alignment predictions from the EFT of large-scale structure at 1-loop, publicly available.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.18981" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.18981" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.18981" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">13. On the use of the Derivative Approximation for Likelihoods for Gravitational Wave Inference
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Josiel Mendonça Soares de Souza, Miguel Quartin</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We find that using DALI, which extends the traditional Fisher Matrix (FM) method to higher orders, allows for a good approximation of the posterior with a $55$ times smaller computational cost, and that the cost-benefit of the doublet-DALI is better than that of the triplet-DALI. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Posterior inference on the more than a dozen parameters governing a gravitational wave (GW) event is challenging. A typical MCMC analysis can take around $100$ CPU hours, and next generation GW observatories will detect many thousands of events. Here we present a thorough comparison of the accuracy and computational cost of the Fisher Matrix, Derivative Approximation for Likelihoods (DALI) and traditional MCMC methods. We find that using DALI, which extends the traditional Fisher Matrix (FM) method to higher orders, allows for a good approximation of the posterior with a $55$ times smaller computational cost, and that the cost-benefit of the doublet-DALI is better than that of the triplet-DALI. We also show that the singlet-DALI, a hybrid MCMC-Fisher method, is much more accurate than the traditional FM and 10 times faster than the doublet-DALI. A large effort has been invested in forecasting the science case of different detector configurations, and the ability of making fast yet accurate estimations of the posteriors is an important step forward. We also introduce version \texttt{1.0} of the public \texttt{GWDALI} code, which incorporates automatic differentiation, modern waveforms and an optimized parameter decomposition.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.16955" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.16955" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.16955" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.IM</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">14. CHEX-MATE: towards a consistent universal pressure profile and cluster mass reconstruction
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">M. Muñoz-Echeverría, E. Pointecouteau, G. W. Pratt, J. -F. Macías-Pérez, M. Douspis, L. Salvati, I. Bartalucci, H. Bourdin, N. Clerc, F. De Luca, et al.</span>
                                <span class="author-full" style="display: none;">M. Muñoz-Echeverría, E. Pointecouteau, G. W. Pratt, J. -F. Macías-Pérez, M. Douspis, L. Salvati, I. Bartalucci, H. Bourdin, N. Clerc, F. De Luca, M. De Petris, M. Donahue, S. Dupourqué, D. Eckert, S. Ettori, M. Gaspari, F. Gastaldello, M. Gitti, A. Gorce, S. Ilić, S. T. Kay, J. Kim, L. Lovisari, B. J. Maughan, P. Mazzotta, L. McBride, J. -B. Melin, F. Oppizzi, E. Rasia, M. Rossetti, H. Saxena, J. Sayers, M. Sereno, M. Tristram</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> In a self-similar paradigm of structure formation, the thermal pressure of the hot intra-cluster gas follows a universal distribution once the profile of each cluster is normalised based on the proper mass and redshift dependencies. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">In a self-similar paradigm of structure formation, the thermal pressure of the hot intra-cluster gas follows a universal distribution once the profile of each cluster is normalised based on the proper mass and redshift dependencies. The reconstruction of such a universal pressure profile requires an individual estimate of the mass of each cluster. In this context, we present a method to jointly fit, for the first time, the universal pressure profile and individual cluster $M_{500}$ masses over a sample of galaxy clusters, properly accounting for correlations between the profile shape and amplitude, and masses scaling the individual profiles. We demonstrate the power of the method and show that a consistent exploitation of the universal pressure profile and cluster mass estimates when modelling the thermal pressure in clusters is necessary to avoid biases. In particular, the method, informed by a cluster mass scale, outputs individual cluster masses with same accuracy and better precision than input masses. Using data from the {\guillemotleft}Cluster HEritage project with XMM-Newton: Mass Assembly and Thermodynamics at the Endpoint of structure formation{\guillemotright}, we investigate a sample of $\sim 25$ galaxy clusters spanning mass and redshift ranges of $2 \lesssim M_{500}/10^{14} \; \mathrm{M}_{\odot} \lesssim 14$ and $0.07 &lt; z &lt; 0.6$.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.18578" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.18578" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.18578" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.03</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">15. Unbiased Gradient Low-Rank Projection
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Rui Pan, Yang Luo, Yuxing Liu, Yang You, Tong Zhang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> However, a significant drawback of many such methods is their lack of convergence guarantees, as various low-rank projection approaches introduce inherent biases relative to the original optimization algorithms, which contribute to performance gaps compared to full-parameter training. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Memory-efficient optimization is critical for training increasingly large language models (LLMs). A popular strategy involves gradient low-rank projection, storing only the projected optimizer states, with GaLore being a representative example. However, a significant drawback of many such methods is their lack of convergence guarantees, as various low-rank projection approaches introduce inherent biases relative to the original optimization algorithms, which contribute to performance gaps compared to full-parameter training. Aiming to tackle this problem, this paper investigates the layerwise sampling technique for debiasing low-rank projection mechanisms. In particular, an instantiation of the paradigm gives rise to a novel and unbiased low-rank optimization method built upon GaLore&#39;s mechanism and the Muon algorithm, named GaLore Unbiased with Muon (GUM). We theoretically prove our method matches the convergence guarantees of the base Muon algorithm while preserving the memory efficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and pretraining also demonstrate non-trivial improvements over GaLore and even better performance than full-parameter training. Further investigation shows that the improvement of this technique comes from a more uniform distribution of knowledge inside layers, leading to more efficient utilization of the model parameter space and better memorization.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.17802" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.17802" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.17802" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">16. Rapid, out of equilibrium metal enrichment indicated by a flat mass-metallicity relation at z~6 from NIRCam grism spectroscopy
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Gauri Kotiwale, Jorryt Matthee, Daichi Kashino, Aswin P. Vijayan, Alberto Torralba, Claudia Di Cesare, Edoardo Iani, Rongmon Bordoloi, Joel Leja, Michael V. Maseda, et al.</span>
                                <span class="author-full" style="display: none;">Gauri Kotiwale, Jorryt Matthee, Daichi Kashino, Aswin P. Vijayan, Alberto Torralba, Claudia Di Cesare, Edoardo Iani, Rongmon Bordoloi, Joel Leja, Michael V. Maseda, Sandro Tacchella, Irene Shivaei, Kasper E. Heintz, A. Lola Danhaive, Sara Mascia, Ivan Kramarenko, Benjamín Navarrete, Ruari Mackenzie, Rohan P. Naidu, David Sobral</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> With simple models, we show that the effect of an [O III] flux-limited sample on the observed shape of the MZR is strongly dependent on the FMR. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We aim to characterise the mass-metallicity relation (MZR) and the 3D correlation between stellar mass, metallicity and star-formation rate (SFR) known as the fundamental metallicity relation (FMR) for galaxies at $5&lt;z&lt;7$. Using $\sim800$ [O III] selected galaxies from deep NIRCam grism surveys, we present our stacked measurements of direct-$T\rm_e$ metallicities, which we use to test recent strong-line metallicity calibrations. Our measured direct-$T\rm_e$ metallicities ($0.1$-$0.2\,\rm Z_\odot$ for M$_\star$ $\approx5\times10^{7-9}$ M$_{\odot}$, respectively) match recent JWST/NIRSpec-based results. However, there are significant inconsistencies between observations and hydrodynamical simulations. We observe a flatter MZR slope than the SPHINX$^{20}$ and FLARES simulations, which cannot be attributed to selection effects. With simple models, we show that the effect of an [O III] flux-limited sample on the observed shape of the MZR is strongly dependent on the FMR. If the FMR is similar to the one in the local Universe, the intrinsic high-redshift MZR should be even flatter than observed. In turn, a 3D relation where SFR correlates positively with metallicity at fixed mass would imply an intrinsically steeper MZR. Our measurements indicate that metallicity variations at fixed mass show little dependence on the SFR, suggesting a flat intrinsic MZR. This could indicate that the low-mass galaxies at these redshifts are out of equilibrium and that metal enrichment occurs rapidly in low-mass galaxies. However, being limited by our stacking analysis, we are yet to probe the scatter in the MZR and its dependence on SFR. Large carefully selected samples of galaxies with robust metallicity measurements can put tight constraints on the high-redshift FMR and, help to understand the interplay between gas flows, star formation and feedback in early galaxies.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.19959" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.19959" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.19959" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.06</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">17. A Brief History of Inference in Astronomy
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Rafael S. de Souza, Emille E. O. Ishida, Alberto Krone-Martins</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We focus on the shift from classical optimization to Bayesian inference, the rise of gradient-based methods fueled by advances in deep learning, and the emergence of adaptive models that shape the very design of scientific datasets. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">In this short review, we trace the evolution of inference in astronomy, highlighting key milestones rather than providing an exhaustive survey. We focus on the shift from classical optimization to Bayesian inference, the rise of gradient-based methods fueled by advances in deep learning, and the emergence of adaptive models that shape the very design of scientific datasets. Understanding this shift is essential for appreciating the current landscape of astronomical research and the future it is helping to build.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.17433" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.17433" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.17433" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.IM</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">18. C-SWAP: Explainability-Aware Structured Pruning for Efficient Neural Networks Compression
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Baptiste Bauvin, Loïc Baret, Ola Ahmad</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Then, through experiments conducted on convolution neural network and vision transformer baselines, pre-trained on classification tasks, we demonstrate that our method consistently achieves substantial reductions in model size, with minimal impact on performance, and without the need for fine-tuning. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Neural network compression has gained increasing attention in recent years, particularly in computer vision applications, where the need for model reduction is crucial for overcoming deployment constraints. Pruning is a widely used technique that prompts sparsity in model structures, e.g. weights, neurons, and layers, reducing size and inference costs. Structured pruning is especially important as it allows for the removal of entire structures, which further accelerates inference time and reduces memory overhead. However, it can be computationally expensive, requiring iterative retraining and optimization. To overcome this problem, recent methods considered one-shot setting, which applies pruning directly at post-training. Unfortunately, they often lead to a considerable drop in performance. In this paper, we focus on this issue by proposing a novel one-shot pruning framework that relies on explainable deep learning. First, we introduce a causal-aware pruning approach that leverages cause-effect relations between model predictions and structures in a progressive pruning process. It allows us to efficiently reduce the size of the network, ensuring that the removed structures do not deter the performance of the model. Then, through experiments conducted on convolution neural network and vision transformer baselines, pre-trained on classification tasks, we demonstrate that our method consistently achieves substantial reductions in model size, with minimal impact on performance, and without the need for fine-tuning. Overall, our approach outperforms its counterparts, offering the best trade-off. Our code is available on GitHub.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.18636" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.18636" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.18636" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">19. Can the CMB be Odd? Effect of a Parity-Violating Matter 4-Point Function on the Low-$\ell$ CMB Trispectrum
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zachary Slepian, Matthew Reinhard, Michael Bartlett</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Here we explore from a theoretical perspective the sensitivity of the primary CMB anisotropy trispectrum to parity violation (PV) in large-scale structure (LSS). (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Here we explore from a theoretical perspective the sensitivity of the primary CMB anisotropy trispectrum to parity violation (PV) in large-scale structure (LSS). We focus on the Sachs-Wolfe term, which dominates at $\ell &lt; 40$, after which the Doppler term takes over. We consider a model where the PV is only present out to some maximal scale $R$ of order a few hundred Mpc/$h$, consistent with what recent LSS 4PCF measurements seem to indicate. We find that the odd CMB trispectrum must be suppressed by at least one factor of $R/\chi_*$$\simeq$1\%, with $\chi_*$ the distance to last scattering, relative to the input matter 4PCF. Thus, a non-detection of PV in the CMB trispectrum is \textit{not necessarily inconsistent} with a genuine detection of PV in the LSS 4PCF.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.18065" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.18065" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.18065" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.10</span>
                        <span class="badge bg-primary">Semantic Score: 0.90</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">20. Symbolic Emulators for Cosmology: Accelerating Cosmological Analyses Without Sacrificing Precision
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Deaglan J. Bartlett, Shivam Pandey</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> In cosmology, emulators play a crucial role by providing fast and accurate predictions of complex physical models, enabling efficient exploration of high-dimensional parameter spaces that would be computationally prohibitive with direct numerical simulations. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">In cosmology, emulators play a crucial role by providing fast and accurate predictions of complex physical models, enabling efficient exploration of high-dimensional parameter spaces that would be computationally prohibitive with direct numerical simulations. Symbolic emulators have emerged as promising alternatives to numerical approaches, delivering comparable accuracy with significantly faster evaluation times. While previous symbolic emulators were limited to relatively narrow prior ranges, we expand these to cover the parameter space relevant for current cosmological analyses. We introduce approximations to hypergeometric functions used for the $\Lambda$CDM comoving distance and linear growth factor which are accurate to better than 0.001% and 0.05%, respectively, for all redshifts and for $\Omega_{\rm m} \in [0.1, 0.5]$. We show that integrating symbolic emulators into a Dark Energy Survey-like $3\times2$pt analysis produces cosmological constraints consistent with those obtained using standard numerical methods. Our symbolic emulators offer substantial improvements in speed and memory usage, demonstrating their practical potential for scalable, likelihood-based inference.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.18749" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.18749" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.18749" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">21. Neuronal Group Communication for Efficient Neural representation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zhengqi Pei, Qingming Huang, Shuhui Wang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> This low-rank, modular representation yields compact models: groups of neurons exchange low-dimensional signals, enabling intra-group specialization and inter-group information sharing while dramatically reducing redundant parameters. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The ever-increasing scale of modern neural networks has brought unprecedented performance alongside daunting challenges in efficiency and interpretability. This paper addresses the core question of how to build large neural systems that learn efficient, modular, and interpretable representations. We propose Neuronal Group Communication (NGC), a theory-driven framework that reimagines a neural network as a dynamical system of interacting neuronal groups rather than a monolithic collection of neural weights. Instead of treating each weight as an independent trainable parameter, NGC treats weights as transient interactions between embedding-like neuronal states, with neural computation unfolding through iterative communication among groups of neurons. This low-rank, modular representation yields compact models: groups of neurons exchange low-dimensional signals, enabling intra-group specialization and inter-group information sharing while dramatically reducing redundant parameters. By drawing on dynamical systems theory, we introduce a neuronal stability metric (analogous to Lyapunov stability) that quantifies the contraction of neuron activations toward stable patterns during sequence processing. Using this metric, we reveal that emergent reasoning capabilities correspond to an external driving force or ``potential&#39;&#39;, which nudges the neural dynamics away from trivial trajectories while preserving stability. Empirically, we instantiate NGC in large language models (LLMs) and demonstrate improved performance on complex reasoning benchmarks under moderate compression. NGC consistently outperforms standard low-rank approximations and cross-layer basis-sharing methods at comparable compression rates. We conclude by discussing the broader implications of NGC, including how structured neuronal group dynamics might relate to generalization in high-dimensional learning systems.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.16851" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.16851" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.16851" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.CL</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">22. Estimating Orbital Parameters of Direct Imaging Exoplanet Using Neural Network
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Bo Liang, Hanlin Song, Chang Liu, Tianyu Zhao, Yuxiang Xu, Zihao Xiao, Manjia Liang, Minghui Du, Wei-Liang Qian, Li-e Qiang, et al.</span>
                                <span class="author-full" style="display: none;">Bo Liang, Hanlin Song, Chang Liu, Tianyu Zhao, Yuxiang Xu, Zihao Xiao, Manjia Liang, Minghui Du, Wei-Liang Qian, Li-e Qiang, Peng Xu, Ziren Luo</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Beyond astrophysics, our methodology establishes a versatile paradigm for synergizing deep generative models with traditional sampling, which can be adopted to tackle complex inference problems in other fields, such as cosmology, biomedical imaging, and particle physics. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">In this work, we propose a new flow-matching Markov chain Monte Carlo (FM-MCMC) algorithm for estimating the orbital parameters of exoplanetary systems, especially for those only one exoplanet is involved. Compared to traditional methods that rely on random sampling within the Bayesian framework, our approach first leverages flow matching posterior estimation (FMPE) to efficiently constrain the prior range of physical parameters, and then employs MCMC to accurately infer the posterior distribution. For example, in the orbital parameter inference of beta Pictoris b, our model achieved a substantial speed-up while maintaining comparable accuracy-running 77.8 times faster than Parallel Tempered MCMC (PTMCMC) and 365.4 times faster than nested sampling. Moreover, our FM-MCMC method also attained the highest average log-likelihood among all approaches, demonstrating its superior sampling efficiency and accuracy. This highlights the scalability and efficiency of our approach, making it well-suited for processing the massive datasets expected from future exoplanet surveys. Beyond astrophysics, our methodology establishes a versatile paradigm for synergizing deep generative models with traditional sampling, which can be adopted to tackle complex inference problems in other fields, such as cosmology, biomedical imaging, and particle physics.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.17459" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.17459" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.17459" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.EP</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">23. LyαNNA II: Field-level inference with noisy Lyα forest spectra
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Parth Nayak, Michael Walther, Daniel Gruen</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We compared the precision and accuracy of our posterior constraints with a combination of two human defined summaries (the 1D power spectrum and PDF of the Ly{\alpha} transmission) that have been corrected for noise, over a wide range of continuum-to-noise ratios (CNR) in the likelihood case. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Deep learning (DL) has been shown to outperform traditional, human-defined summary statistics of the Ly{\alpha} forest in constraining key astrophysical and cosmological parameters owing to its ability to tap into the realm of non-Gaussian information. An understanding of the impact of nuisance effects such as noise on such field-level frameworks, however, still remains elusive. In this work we conduct a systematic investigation into the efficacy of DL inference from noisy Ly{\alpha} forest spectra. Building upon our previous, proof-of-concept framework (Nayak et al. 2024) for pure spectra, we constructed and trained a ResNet neural network using labeled mock data from hydrodynamical simulations with a range of noise levels to optimally compress noisy spectra into a novel summary statistic that is exclusively sensitive to the power-law temperature-density relation of the intergalactic medium. We fit a Gaussian mixture surrogate with 23 components through our labels and summaries to estimate the joint data-parameter distribution for likelihood free inference, in addition to performing inference with a Gaussian likelihood. The posterior contours in the two cases agree well with each other. We compared the precision and accuracy of our posterior constraints with a combination of two human defined summaries (the 1D power spectrum and PDF of the Ly{\alpha} transmission) that have been corrected for noise, over a wide range of continuum-to-noise ratios (CNR) in the likelihood case. We found a gain in precision in terms of posterior contour area with our pipeline over the said combination of 65% (at a CNR of 20 per 6 km/s) to 112% (at 200 per 6 km/s). While the improvement in posterior precision is not as large as in the noiseless case, these results indicate that DL still remains a powerful tool for inference even with noisy, real-world datasets.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.19899" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.19899" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.19899" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.IM</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">24. ARC-Encoder: learning compressed text representations for large language models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Hippolyte Pilchen, Edouard Grave, Patrick Pérez</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We evaluate ARC-Encoder across a variety of LLM usage scenarios, ranging from in-context learning to context window extension, on both instruct and base decoders. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Recent techniques such as retrieval-augmented generation or chain-of-thought reasoning have led to longer contexts and increased inference costs. Context compression techniques can reduce these costs, but the most effective approaches require fine-tuning the target model or even modifying its architecture. This can degrade its general abilities when not used for this specific purpose. Here we explore an alternative approach: an encoder that compresses the context into continuous representations which replace token embeddings in decoder LLMs. First, we perform a systematic study of training strategies and architecture choices for the encoder. Our findings led to the design of an Adaptable text Representations Compressor, named ARC-Encoder, which outputs $x$-times fewer continuous representations (typically $x\!\in\!\{4,8\}$) than text tokens. We evaluate ARC-Encoder across a variety of LLM usage scenarios, ranging from in-context learning to context window extension, on both instruct and base decoders. Results show that ARC-Encoder achieves state-of-the-art performance on several benchmarks while improving computational efficiency at inference. Finally, we demonstrate that our models can be adapted to multiple decoders simultaneously, allowing a single encoder to generalize across different decoder LLMs. This makes ARC-Encoder a flexible and efficient solution for portable encoders that work seamlessly with multiple LLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder , fine-tuning dataset and pretrained models are available at https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 .</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.20535" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.20535" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.20535" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.CL</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">25. NeuroAda: Activating Each Neuron&#39;s Potential for Parameter-Efficient Fine-Tuning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zhi Zhang, Yixian Shen, Congfeng Cao, Ekaterina Shutova</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Empirical results on 23+ tasks spanning both natural language generation and understanding demonstrate that NeuroAda achieves state-of-the-art performance with as little as $\leq \textbf{0.02}\%$ trainable parameters, while reducing CUDA memory usage by up to 60%. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Existing parameter-efficient fine-tuning (PEFT) methods primarily fall into two categories: addition-based and selective in-situ adaptation. The former, such as LoRA, introduce additional modules to adapt the model to downstream tasks, offering strong memory efficiency. However, their representational capacity is often limited, making them less suitable for fine-grained adaptation. In contrast, the latter directly fine-tunes a carefully chosen subset of the original model parameters, allowing for more precise and effective adaptation, but at the cost of significantly increased memory consumption. To reconcile this trade-off, we propose NeuroAda, a novel PEFT method that enables fine-grained model finetuning while maintaining high memory efficiency. Our approach first identifies important parameters (i.e., connections within the network) as in selective adaptation, and then introduces bypass connections for these selected parameters. During finetuning, only the bypass connections are updated, leaving the original model parameters frozen. Empirical results on 23+ tasks spanning both natural language generation and understanding demonstrate that NeuroAda achieves state-of-the-art performance with as little as $\leq \textbf{0.02}\%$ trainable parameters, while reducing CUDA memory usage by up to 60%. We release our code here: https://github.com/FightingFighting/NeuroAda.git.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.18940" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.18940" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.18940" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">26. Soft-Masked Diffusion Language Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Michael Hersche, Samuel Moor-Smith, Thomas Hofmann, Abbas Rahimi</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We demonstrate that continuing pretraining a 169M parameter model with SM leads to improved perplexity and MAUVE scores. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Diffusion models have demonstrated strong potential in language modeling, offering various advantages over traditional autoregressive approaches. Their ability to generate and revise entire responses in parallel enables faster generation and built-in self-correction mechanisms. Most modern diffusion-based language models employ masked diffusion, where decoding involves iteratively processing masked tokens based on a binary decision: either retaining the mask or replacing it with the predicted token. However, this binary choice discards valuable predictive information when the mask is retained. To address this limitation, we introduce soft-masking (SM), a novel method that dynamically blends the embedding of the mask token with the embeddings of the top-$k$ predicted tokens from the previous decoding step, for each retained mask. This provides the model with a more informative prior, preserving context from earlier computations and allowing partial information about masked tokens to propagate beyond a single step. We propose a training methodology that adapts a pretrained masked diffusion language model to incorporate SM. We demonstrate that continuing pretraining a 169M parameter model with SM leads to improved perplexity and MAUVE scores. Furthermore, we finetune two state-of-the-art diffusion models, Dream-7B and Dream-Coder-7B, with SM. SM consistently improves performance across multiple coding benchmarks, particularly in high-throughput settings.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.17206" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.17206" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.17206" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">27. The slope and scatter of the star forming main sequence at z~5 : reconciling observations with simulations
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Claudia Di Cesare, Jorryt Matthee, Rohan P. Naidu, Alberto Torralba, Gauri Kotiwale, Ivan G. Kramarenko, Jeremy Blazoit, Joakim Rosdahl, Joel Leja, Edoardo Iani, et al.</span>
                                <span class="author-full" style="display: none;">Claudia Di Cesare, Jorryt Matthee, Rohan P. Naidu, Alberto Torralba, Gauri Kotiwale, Ivan G. Kramarenko, Jeremy Blazoit, Joakim Rosdahl, Joel Leja, Edoardo Iani, Angela Adamo, Alba Covelo-Paz, Lukas J. Furtak, Kasper E. Heintz, Sara Mascia, Benjamín Navarrete, Pascal A. Oesch, Michael Romano, Irene Shivaei, Sandro Tacchella</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> With JWST, we can now investigate the SFMS at high redshifts down to masses of $\sim10^6$ M$_{\odot}$, using sensitive star-formation rate tracers such as H$\alpha$ emission -- which allow us to probe the variability in star formation histories. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Galaxies exhibit a tight correlation between their star-formation rate and stellar mass over a wide redshift range known as the star-forming main sequence (SFMS). With JWST, we can now investigate the SFMS at high redshifts down to masses of $\sim10^6$ M$_{\odot}$, using sensitive star-formation rate tracers such as H$\alpha$ emission -- which allow us to probe the variability in star formation histories. We present inferences of the SFMS based on 316 H$\alpha$-selected galaxies at $z\sim4$-$5$ with $\log(\rm M_\star/M_\odot) = 6.4$ -$10.6$. These galaxies were identified behind the Abell 2744 lensing cluster with NIRCam grism spectroscopy from the ``All the Little Things&#39;&#39; (ALT) survey. At face value, our data suggest a shallow slope of the SFMS (SFR $\propto \mathrm{M}_\star^\alpha$, with $\alpha=0.45$). After correcting for the H$\alpha$-flux limited nature of our survey using a Bayesian framework, the slope steepens to $\alpha = 0.59^{+0.10}_{-0.09}$, whereas current data on their own are inconclusive on the mass dependence of the scatter. These slopes differ significantly from the slope of $\approx1$ expected from the observed evolution of the galaxy stellar mass function and from simulations. When fixing the slope to $\alpha=1$, we find evidence for a decreasing intrinsic scatter with stellar mass (from $\approx 0.5$ dex at M$_\star=10^8$ M$_\odot$ to $0.4$ dex at M$_\star=10^{10}$ M$_\odot$). This tension might be explained by a (combination of) luminosity-dependent SFR(H$\alpha$) calibration, a population of (mini)-quenched low-mass galaxies, or underestimated dust attenuation in high-mass galaxies. Future deep observations across facilities can quantify these processes, enabling better insights into the variability of star formation histories.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.19044" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.19044" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.19044" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">28. Constraining the Swift Memory Burden Effect with GW250114-like Events
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Chen Yuan, Richard Brito</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Our results disfavour rapid gap reopening, shedding light on how the swift memory burden effect can be probed with current and next-generation detectors. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Black hole spectroscopy allows to infer the properties of the remnant of a binary black hole coalescence. Motivated by the recent proposal that a black hole&#39;s information load can alter its classical response to small perturbations, an effect known as the swift memory burden, we develop a minimal phenomenological framework to analyze the ringdown of a binary black hole merger and confront it with the data from the GW250114 event. We perform a Bayesian analysis combining the frequencies of the (220) and (440) quasi-normal modes and obtain a lower bound $\log_{10}p \gtrsim 2$, where $p$ controls how the gaps reopen when the black hole&#39;s master mode occupation departs from the critical value. Moreover, using a Fisher information matrix (high signal-to-noise ratio) approximation, we forecast the lower bound $\log_{10}p \gtrsim 5$ for a GW250114-like event observed with Cosmic Explorer. Our results disfavour rapid gap reopening, shedding light on how the swift memory burden effect can be probed with current and next-generation detectors.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.19916" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.19916" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.19916" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">29. Diffusion Autoencoders with Perceivers for Long, Irregular and Multimodal Astronomical Sequences
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yunyi Shen, Alexander Gagliano</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Across diverse spectroscopic and photometric astronomical datasets, daep achieves lower reconstruction errors, produces more discriminative latent spaces, and better preserves fine-scale structure than both VAE and maep baselines. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Self-supervised learning has become a central strategy for representation learning, but the majority of architectures used for encoding data have only been validated on regularly-sampled inputs such as images, audios. and videos. In many scientific domains, data instead arrive as long, irregular, and multimodal sequences. To extract semantic information from these data, we introduce the Diffusion Autoencoder with Perceivers (daep). daep tokenizes heterogeneous measurements, compresses them with a Perceiver encoder, and reconstructs them with a Perceiver-IO diffusion decoder, enabling scalable learning in diverse data settings. To benchmark the daep architecture, we adapt the masked autoencoder to a Perceiver encoder/decoder design, and establish a strong baseline (maep) in the same architectural family as daep. Across diverse spectroscopic and photometric astronomical datasets, daep achieves lower reconstruction errors, produces more discriminative latent spaces, and better preserves fine-scale structure than both VAE and maep baselines. These results establish daep as an effective framework for scientific domains where data arrives as irregular, heterogeneous sequences.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.20595" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.20595" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.20595" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">30. Bayesian Inference of Primordial Magnetic Field Parameters from CMB with Spherical Graph Neural Networks
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Juan Alejandro Pinto Castro, Héctor J. Hortúa, Jorge Enrique García-Farieta, Roger Anderson Hurtado</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Deep learning has emerged as a transformative methodology in modern cosmology, providing powerful tools to extract meaningful physical information from complex astronomical datasets. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Deep learning has emerged as a transformative methodology in modern cosmology, providing powerful tools to extract meaningful physical information from complex astronomical datasets. This paper implements a novel Bayesian graph deep learning framework for estimating key cosmological parameters in a primordial magnetic field (PMF) cosmology directly from simulated Cosmic Microwave Background (CMB) maps. Our methodology utilizes DeepSphere, a spherical convolutional neural network architecture specifically designed to respect the spherical geometry of CMB data through HEALPix pixelization. To advance beyond deterministic point estimates and enable robust uncertainty quantification, we integrate Bayesian Neural Networks (BNNs) into the framework, capturing aleatoric and epistemic uncertainties that reflect the model confidence in its predictions. The proposed approach demonstrates exceptional performance, achieving $R^{2}$ scores exceeding 0.89 for the magnetic parameter estimation. We further obtain well-calibrated uncertainty estimates through post-hoc training techniques including Variance Scaling and GPNormal. This integrated DeepSphere-BNNs framework not only delivers accurate parameter estimation from CMB maps with PMF contributions but also provides reliable uncertainty quantification, providing the necessary tools for robust cosmological inference in the era of precision cosmology.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.20795" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.20795" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.20795" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">31. L-MoE: End-to-End Training of a Lightweight Mixture of Low-Rank Adaptation Experts
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Shihao Ji, Zihui Song</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The Mixture of Experts (MoE) architecture enables the scaling of Large Language Models (LLMs) to trillions of parameters by activating a sparse subset of weights for each input, maintaining constant computational cost during inference. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The Mixture of Experts (MoE) architecture enables the scaling of Large Language Models (LLMs) to trillions of parameters by activating a sparse subset of weights for each input, maintaining constant computational cost during inference. Concurrently, Low-Rank Adaptation (LoRA) has emerged as a dominant technique for parameter-efficiently fine-tuning LLMs on specialized tasks. In this work, we unify these two paradigms into a novel, end-to-end trainable framework named L-MoE: a Lightweight Mixture of LoRA Experts. L-MoE redefines MoE experts not as dense feed-forward networks, but as a collection of task-specialized, low-rank adapters. A lightweight gating network, trained jointly with the experts, learns to dynamically compose these LoRA adapters by computing a weighted average of their parameters for each input token. This composition is fully differentiable, allowing gradients from a standard auto-regressive language modeling objective to flow back through the entire architecture, simultaneously refining both the expert adapters and the routing strategy. This approach creates a highly parameter-efficient MoE model that is modular by design, allows for dynamic skill composition, and is trainable from end-to-end. We present the formal mathematical framework for L-MoE, detailing the differentiable routing mechanism and the joint optimization objective, thereby providing a new path toward building more efficient, scalable, and specialized language models.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.17898" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.17898" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.17898" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">32. ScaleNet: Scaling up Pretrained Neural Networks with Incremental Parameters
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zhiwei Hao, Jianyuan Guo, Li Shen, Kai Han, Yehui Tang, Han Hu, Yunhe Wang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Specifically, ScaleNet achieves model expansion by inserting additional layers into pretrained ViTs, utilizing layer-wise weight sharing to maintain parameters efficiency. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Recent advancements in vision transformers (ViTs) have demonstrated that larger models often achieve superior performance. However, training these models remains computationally intensive and costly. To address this challenge, we introduce ScaleNet, an efficient approach for scaling ViT models. Unlike conventional training from scratch, ScaleNet facilitates rapid model expansion with negligible increases in parameters, building on existing pretrained models. This offers a cost-effective solution for scaling up ViTs. Specifically, ScaleNet achieves model expansion by inserting additional layers into pretrained ViTs, utilizing layer-wise weight sharing to maintain parameters efficiency. Each added layer shares its parameter tensor with a corresponding layer from the pretrained model. To mitigate potential performance degradation due to shared weights, ScaleNet introduces a small set of adjustment parameters for each layer. These adjustment parameters are implemented through parallel adapter modules, ensuring that each instance of the shared parameter tensor remains distinct and optimized for its specific function. Experiments on the ImageNet-1K dataset demonstrate that ScaleNet enables efficient expansion of ViT models. With a 2$\times$ depth-scaled DeiT-Base model, ScaleNet achieves a 7.42% accuracy improvement over training from scratch while requiring only one-third of the training epochs, highlighting its efficiency in scaling ViTs. Beyond image classification, our method shows significant potential for application in downstream vision areas, as evidenced by the validation in object detection task.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.18431" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.18431" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.18431" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">33. Modeling Globular Cluster Counts with Bayesian Latent Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Rafael S. de Souza, Ana L. Chies-Santos</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> GC counts are drawn from a negative-binomial (NB) process linked to host stellar mass, augmented with a newly introduced Gaussian observation layer that enables efficient propagation of measurement errors. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present a Bayesian latent model to describe the scaling relation between globular cluster populations and their host galaxies, updating the framework proposed in de Souza 2015. GC counts are drawn from a negative-binomial (NB) process linked to host stellar mass, augmented with a newly introduced Gaussian observation layer that enables efficient propagation of measurement errors. The revised formulation preserves the underlying NB process while improving computational tractability. The code snippets, implemented in Nimble and PyMC are released under the MIT license at https://github.com/COINtoolbox/Generalized-Linear-Models-Tutorial/blob/master/Count/readme.md</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.19409" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.19409" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.19409" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">34. Adaptive Discretization for Consistency Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jiayu Bai, Zhanbo Feng, Zhijie Deng, Tianqi Hou, Robert C. Qiu, Zenan Ling</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> However, most existing CMs rely on manually designed discretization schemes, which can cause repeated adjustments for different noise schedules and datasets. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Consistency Models (CMs) have shown promise for efficient one-step generation. However, most existing CMs rely on manually designed discretization schemes, which can cause repeated adjustments for different noise schedules and datasets. To address this, we propose a unified framework for the automatic and adaptive discretization of CMs, formulating it as an optimization problem with respect to the discretization step. Concretely, during the consistency training process, we propose using local consistency as the optimization objective to ensure trainability by avoiding excessive discretization, and taking global consistency as a constraint to ensure stability by controlling the denoising error in the training target. We establish the trade-off between local and global consistency with a Lagrange multiplier. Building on this framework, we achieve adaptive discretization for CMs using the Gauss-Newton method. We refer to our approach as ADCMs. Experiments demonstrate that ADCMs significantly improve the training efficiency of CMs, achieving superior generative performance with minimal training overhead on both CIFAR-10 and ImageNet. Moreover, ADCMs exhibit strong adaptability to more advanced DM variants. Code is available at https://github.com/rainstonee/ADCM.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.17266" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.17266" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.17266" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">35. Generalizable Reasoning through Compositional Energy Minimization
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Alexandru Oarga, Yilun Du</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Generalization is a key challenge in machine learning, specifically in reasoning tasks, where models are expected to solve problems more complex than those encountered during training. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Generalization is a key challenge in machine learning, specifically in reasoning tasks, where models are expected to solve problems more complex than those encountered during training. Existing approaches typically train reasoning models in an end-to-end fashion, directly mapping input instances to solutions. While this allows models to learn useful heuristics from data, it often results in limited generalization beyond the training distribution. In this work, we propose a novel approach to reasoning generalization by learning energy landscapes over the solution spaces of smaller, more tractable subproblems. At test time, we construct a global energy landscape for a given problem by combining the energy functions of multiple subproblems. This compositional approach enables the incorporation of additional constraints during inference, allowing the construction of energy landscapes for problems of increasing difficulty. To improve the sample quality from this newly constructed energy landscape, we introduce Parallel Energy Minimization (PEM). We evaluate our approach on a wide set of reasoning problems. Our method outperforms existing state-of-the-art methods, demonstrating its ability to generalize to larger and more complex problems. Project website can be found at: https://alexoarga.github.io/compositional_reasoning/</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.20607" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.20607" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.20607" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">36. Hubble tension in an anisotropic Universe
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Maksym Deliyergiyev, Morgan Le Delliou, Antonino Del Popolo</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Future work may extend this methodology to more complex anisotropic scenarios and incorporate additional cosmological probes such as CMB polarization and gravitational wave standard sirens. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We explore the Hubble tension within an anisotropic cosmological framework by revisiting the Bianchi type-I model introduced in Le Delliou et al. 2020. Motivated by ongoing debates surrounding back-reaction effects and observed anomalies in the cosmic microwave background (CMB), we investigate whether a departure from isotropy in the late Universe could reconcile the observed discrepancies in Hubble constant measurements. Using a Bayesian inference framework, we constrain the model parameters employing multiple nested sampling algorithms: bilby, PyMultiNest, and nessai. We perform the analysis under both uniform and Gaussian priors, allowing us to systematically assess the sensitivity of the inferred cosmological parameters to different prior assumptions. This dual-prior strategy balances agnostic parameter exploration with constraints informed by theory and observation. Our findings demonstrate the reliability of our inference pipeline across different samplers and emphasize the crucial role of prior selection in non-standard cosmological model testing. The results suggest that anisotropic models remain viable contenders in addressing current cosmological tensions: even though the present model does not show alleviation of the Hubble tension, the data points towards anisotropies. Future work may extend this methodology to more complex anisotropic scenarios and incorporate additional cosmological probes such as CMB polarization and gravitational wave standard sirens.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.19069" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.19069" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.19069" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">37. A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Jiacheng Liu, Xinyu Wang, Yuqi Lin, Zhikai Wang, Peiru Wang, Peiliang Cai, Qinming Zhou, Zhengan Yan, Zexuan Yan, Zhengyi Shi, et al.</span>
                                <span class="author-full" style="display: none;">Jiacheng Liu, Xinyu Wang, Yuqi Lin, Zhikai Wang, Peiru Wang, Peiliang Cai, Qinming Zhou, Zhengan Yan, Zexuan Yan, Zhengyi Shi, Chang Zou, Yue Ma, Linfeng Zhang</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> This trend enhances caching flexibility across diverse tasks and enables integration with other acceleration techniques such as sampling optimization and model distillation, paving the way for a unified, efficient inference framework for future multimodal and interactive applications. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Diffusion Models have become a cornerstone of modern generative AI for their exceptional generation quality and controllability. However, their inherent \textit{multi-step iterations} and \textit{complex backbone networks} lead to prohibitive computational overhead and generation latency, forming a major bottleneck for real-time applications. Although existing acceleration techniques have made progress, they still face challenges such as limited applicability, high training costs, or quality degradation. Against this backdrop, \textbf{Diffusion Caching} offers a promising training-free, architecture-agnostic, and efficient inference paradigm. Its core mechanism identifies and reuses intrinsic computational redundancies in the diffusion process. By enabling feature-level cross-step reuse and inter-layer scheduling, it reduces computation without modifying model parameters. This paper systematically reviews the theoretical foundations and evolution of Diffusion Caching and proposes a unified framework for its classification and analysis. Through comparative analysis of representative methods, we show that Diffusion Caching evolves from \textit{static reuse} to \textit{dynamic prediction}. This trend enhances caching flexibility across diverse tasks and enables integration with other acceleration techniques such as sampling optimization and model distillation, paving the way for a unified, efficient inference framework for future multimodal and interactive applications. We argue that this paradigm will become a key enabler of real-time and efficient generative AI, injecting new vitality into both theory and practice of \textit{Efficient Generative Intelligence}.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.19755" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.19755" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.19755" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">38. The selection function of the Gaia DR3 open cluster census
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Emily L. Hunt, Tristan Cantat-Gaudin, Friedrich Anders, Sagar Malhotra, Lorenzo Spina, Alfred Castro-Ginard, Lorenzo Cavallo</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> For the first time ever, we establish the selection function of an entire open cluster census, publishing our results as an open-source Python package for use by the community. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Open clusters are among the most useful and widespread tracers of Galactic structure. The completeness of the Galactic open cluster census, however, remains poorly understood. For the first time ever, we establish the selection function of an entire open cluster census, publishing our results as an open-source Python package for use by the community. Our work is valid for the Hunt &amp; Reffert catalogue of clusters in Gaia DR3. We developed and open-sourced our cluster simulator from our first work. Then, we performed 80,590 injection and retrievals of simulated open clusters to test the Hunt &amp; Reffert catalogue&#39;s sensitivity. We fit a logistic model of cluster detectability that depends only on a cluster&#39;s number of stars, median parallax error, Gaia data density, and a user-specified significance threshold. We find that our simple model accurately predicts cluster detectability, with a 94.53\% accuracy on our training data that is comparable to a machine-learning based model with orders of magnitude more parameters. Our model itself offers numerous insights on why certain clusters are detected. We briefly use our model to show that cluster detectability depends on non-intuitive parameters, such as a cluster&#39;s proper motion, and we show that even a modest 25 km/s boost to a cluster&#39;s orbital speed can result in an almost 3$\times$ higher detection probability, depending on its position. In addition, we publish our raw cluster injection and retrievals and cluster memberships, which could be used for a number of other science cases -- such as estimating cluster membership incompleteness. Using our results, selection effect-corrected studies are now possible with the open cluster census. Our work will enable a number of brand new types of study, such as detailed comparisons between the Milky Way&#39;s cluster census and recent extragalactic cluster samples.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.18343" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.18343" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.18343" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">39. Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Ling Team, Bin Han, Caizhi Tang, Chen Liang, Donghao Zhang, Fan Yuan, Feng Zhu, Jie Gao, Jingyu Hu, Longfei Li, et al.</span>
                                <span class="author-full" style="display: none;">Ling Team, Bin Han, Caizhi Tang, Chen Liang, Donghao Zhang, Fan Yuan, Feng Zhu, Jie Gao, Jingyu Hu, Longfei Li, Meng Li, Mingyang Zhang, Peijie Jiang, Peng Jiao, Qian Zhao, Qingyuan Yang, Wenbo Shen, Xinxing Yang, Yalin Zhang, Yankun Ren, Yao Zhao, Yibo Cao, Yixuan Sun, Yue Zhang, Yuchen Fang, Zibin Lin, Zixuan Cheng, Jun Zhou</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Furthermore, through systematic exploration of the ratio between different attention mechanisms in the hybrid architecture, we have identified the currently optimal model structure. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">In this technical report, we present the Ring-linear model series, specifically including Ring-mini-linear-2.0 and Ring-flash-linear-2.0. Ring-mini-linear-2.0 comprises 16B parameters and 957M activations, while Ring-flash-linear-2.0 contains 104B parameters and 6.1B activations. Both models adopt a hybrid architecture that effectively integrates linear attention and softmax attention, significantly reducing I/O and computational overhead in long-context inference scenarios. Compared to a 32 billion parameter dense model, this series reduces inference cost to 1/10, and compared to the original Ring series, the cost is also reduced by over 50%. Furthermore, through systematic exploration of the ratio between different attention mechanisms in the hybrid architecture, we have identified the currently optimal model structure. Additionally, by leveraging our self-developed high-performance FP8 operator library-linghe, overall training efficiency has been improved by 50%. Benefiting from the high alignment between the training and inference engine operators, the models can undergo long-term, stable, and highly efficient optimization during the reinforcement learning phase, consistently maintaining SOTA performance across multiple challenging complex reasoning benchmarks.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.19338" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.19338" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.19338" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">40. Imbalanced Gradients in RL Post-Training of Multi-Task LLMs
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Runzhe Wu, Ankur Samanta, Ayush Jain, Scott Fujimoto, Jeongyeol Kwon, Ben Kretzu, Youliang Yu, Kaveh Hassani, Boris Vidolov, Yonathan Efroni</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Multi-task post-training of large language models (LLMs) is typically performed by mixing datasets from different tasks and optimizing them jointly. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Multi-task post-training of large language models (LLMs) is typically performed by mixing datasets from different tasks and optimizing them jointly. This approach implicitly assumes that all tasks contribute gradients of similar magnitudes; when this assumption fails, optimization becomes biased toward large-gradient tasks. In this paper, however, we show that this assumption fails in RL post-training: certain tasks produce significantly larger gradients, thus biasing updates toward those tasks. Such gradient imbalance would be justified only if larger gradients implied larger learning gains on the tasks (i.e., larger performance improvements) -- but we find this is not true. Large-gradient tasks can achieve similar or even much lower learning gains than small-gradient ones. Further analyses reveal that these gradient imbalances cannot be explained by typical training statistics such as training rewards or advantages, suggesting that they arise from the inherent differences between tasks. This cautions against naive dataset mixing and calls for future work on principled gradient-level corrections for LLMs.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.19178" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.19178" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.19178" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">41. Strong Lensing Model and Dust Extinction Maps of the Host Galaxy of Type Ia Supernova H0pe
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">A. Galan, S. Schuldt, G. B. Caminha, S. H. Suyu, R. Cañameras, S. Ertl, C. Grillo, A. Acebron, B. Frye, A. M. Koekemoer, et al.</span>
                                <span class="author-full" style="display: none;">A. Galan, S. Schuldt, G. B. Caminha, S. H. Suyu, R. Cañameras, S. Ertl, C. Grillo, A. Acebron, B. Frye, A. M. Koekemoer, R. Windhorst, J. M. Diego, N. Foo</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We find good statistical agreement of the extinction estimate at $\lesssim1\sigma$ with three fully independent methods based on spectral energy distribution fitting. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Strong gravitational lensing by massive galaxy clusters offers particularly rare opportunities to observe multiple images of distant ($z\gtrsim2$) Type Ia supernovae (SNe) and resolve the properties of their host galaxies. A recent outstanding example is the Type Ia SN &#34;H0pe&#34; ($z=1.78$), discovered in James Webb Space Telescope (JWST) NIRCam images when it was still triply imaged by the galaxy cluster PLCK G165.7+67.0 (G165, $z=0.35$). In this work we build a new strong lensing model of G165, first by using only the position of multiple images of background galaxies. We then increase significantly the number of constraints around the position of SN H0pe by modeling the extended surface brightness of the SN host galaxy. The average uncertainty on mass model parameters is reduced by more than an order of magnitude. We also study the spatial distribution of dust in the arc to estimate the dust extinction at the position of SN H0pe. We find good statistical agreement of the extinction estimate at $\lesssim1\sigma$ with three fully independent methods based on spectral energy distribution fitting. Moreover, our extended-image lens model of G165 allows us to map the dust distribution of the host galaxy from the image plane to the source plane. Supernova H0pe exploded in a region with a relatively high extinction of $A_V \approx 0.9\ {\rm mag}$ at around $\sim 1\ {\rm kpc}$ from its host center. This work shows that extended image modeling in lensing clusters simultaneously reduces the uncertainty on lens model parameters and enables spatially resolved analyses of lensed transients host galaxies. Such modeling advances are expected to play an important role in future cosmological analyses using strongly lensed SNe.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.20561" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.20561" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.20561" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">42. EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Mingzheng Zhang, Jinfeng Gao, Dan Xu, Jiangrui Yu, Yuhan Qiao, Lan Chen, Jin Tang, Xiao Wang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Additionally, while Transformer-based models dominate vision-language tasks, non-Transformer architectures, such as the Mamba network, remain underexplored for medical report generation, presenting a promising avenue for future research. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">X-ray image-based medical report generation (MRG) is a pivotal area in artificial intelligence that can significantly reduce diagnostic burdens for clinicians and patient wait times. Existing MRG models predominantly rely on Large Language Models (LLMs) to improve report generation, with limited exploration of pre-trained vision foundation models or advanced fine-tuning techniques. Mainstream frameworks either avoid fine-tuning or utilize simplistic methods like LoRA, often neglecting the potential of enhancing cross-attention mechanisms. Additionally, while Transformer-based models dominate vision-language tasks, non-Transformer architectures, such as the Mamba network, remain underexplored for medical report generation, presenting a promising avenue for future research. In this paper, we propose EMRRG, a novel X-ray report generation framework that fine-tunes pre-trained Mamba networks using parameter-efficient methods. Specifically, X-ray images are divided into patches, tokenized, and processed by an SSM-based vision backbone for feature extraction, with Partial LoRA yielding optimal performance. An LLM with a hybrid decoder generates the medical report, enabling end-to-end training and achieving strong results on benchmark datasets. Extensive experiments on three widely used benchmark datasets fully validated the effectiveness of our proposed strategies for the X-ray MRG. The source code of this paper will be released on https://github.com/Event-AHU/Medical_Image_Analysis.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.16776" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.16776" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.16776" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">43. Transfer Learning Beyond the Standard Model
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Veena Krishnaraj, Adrian E. Bayer, Christian Kragh Jespersen, Peter Melchior</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Transfer learning offers a way to reduce the simulation cost by reusing knowledge across models. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Machine learning enables powerful cosmological inference but typically requires many high-fidelity simulations covering many cosmological models. Transfer learning offers a way to reduce the simulation cost by reusing knowledge across models. We show that pre-training on the standard model of cosmology, $\Lambda$CDM, and fine-tuning on various beyond-$\Lambda$CDM scenarios -- including massive neutrinos, modified gravity, and primordial non-Gaussianities -- can enable inference with significantly fewer beyond-$\Lambda$CDM simulations. However, we also show that negative transfer can occur when strong physical degeneracies exist between $\Lambda$CDM and beyond-$\Lambda$CDM parameters. We consider various transfer architectures, finding that including bottleneck structures provides the best performance. Our findings illustrate the opportunities and pitfalls of foundation-model approaches in physics: pre-training can accelerate inference, but may also hinder learning new physics.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.19168" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.19168" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.19168" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">44. BaryonBridge: Stochastic Interpolant Model for Fast Hydrodynamical Simulations
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Benjamin Horowitz, Carolina Cuesta-Lazaro, Omar Yehia</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> In this work, we present a new approach utilizing stochastic interpolants to map between cheap fast particle mesh simulations and baryonic quantities in three dimensions, requiring a total of 7 GPU minutes per 256^3 grid size simulation. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Constructing a general-purpose framework for mapping between dark matter simulations and observable hydrodynamical simulation outputs is a long-standing problem in modern astrophysics. In this work, we present a new approach utilizing stochastic interpolants to map between cheap fast particle mesh simulations and baryonic quantities in three dimensions, requiring a total of 7 GPU minutes per 256^3 grid size simulation. Using the CAMELS multifield dataset, we are able to condition our mapping on both cosmological and astrophysical properties. We focus this work on hydrodynamical quantities suitable for Lya observables finding excellent agreement up to small spatial scales, k ~ 10.0 (h^(-1) Mpc) at z=2.0, for Lya flux statistics. Our approach is fully convolutional, allowing training on comparatively small volumes and application to larger volumes, which was tested on TNG50.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.19224" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.19224" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.19224" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">45. The Photometric Analysis of the Environment Around Two Dusty Star-Forming Galaxies at $z \sim 2$
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Joe Bhangal, Allison W. S. Man, Tom J. L. C. Bakx, Darko Donevski, Pierre Cox, Helmut Dannerbauer, Stephen Serjeant, Masato Hagimoto, Pluto Jiang, Wenxiao Liu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Using Gemini South Flamingos-2 (F2) $K_s$-band imaging together with ancillary Subaru Hyper Suprime-Cam and Hubble Space Telescope multi-band photometry, we obtain photometric redshifts, $z_\text{phot}$, as well as star formation rates and stellar mass estimates for companion galaxies of the DSFGs. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Studying the environments of dusty star-forming galaxies (DSFGs) provides insight into whether these luminous systems are reliable signposts of large-scale overdensities. Evidence suggests that individual DSFGs can trace overdense environments, although this association may not be universal. To test this, we investigate the environments surrounding two luminous, gravitationally-lensed DSFGs (SDP.17b at $z_\text{spec} = 2.3049$ and HELMS-55 at $z_\text{spec} = 2.2834$). Using Gemini South Flamingos-2 (F2) $K_s$-band imaging together with ancillary Subaru Hyper Suprime-Cam and Hubble Space Telescope multi-band photometry, we obtain photometric redshifts, $z_\text{phot}$, as well as star formation rates and stellar mass estimates for companion galaxies of the DSFGs. At least $5\pm2$ and $15\pm3$ companion galaxies exist with consistent $z_\text{phot}$ ($dz \leq 0.2$) within a projected separation of 5.5 cMpc of SDP.17b and HELMS-55, respectively. These correspond to galaxy overdensities of $\delta = 0.1 \pm 0.2$ and ${\delta} =1.0 \pm 0.3$, with significances of $(0.2 \pm 0.4)\sigma$ and $(2.2 \pm 0.6) \sigma$, respectively. On the $M_{\rm H_2}$-overdensity-significance plane, HELMS-55 may follow the positive correlation between the gas mass and the overdensity significance, while SDP.17b lies well above the relation despite its large gas reservoir, making it a potential outlier. Based on this study of two DSFGs, our photometric analysis suggests that DSFGs can trace the outskirts of protoclusters or associated large-scale structures. However, our small sample prevents firm conclusions about their ability to pinpoint dense cluster cores. Future multi-object spectroscopic observations are required to confirm the membership and star formation properties of the companion galaxies.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.18139" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.18139" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.18139" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">46. Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and Reduced Training Time in Pre-trained Model-based Continual Representation Learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Heming Zou, Yunliang Zang, Wutong Xu, Xiangyang Ji</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> However, directly leveraging pretrained features for downstream tasks often suffers from multicollinearity in the similarity-matching stage, and more advanced methods can be computationally prohibitive for real-time, low-latency applications. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Using a nearly-frozen pretrained model, the continual representation learning paradigm reframes parameter updates as a similarity-matching problem to mitigate catastrophic forgetting. However, directly leveraging pretrained features for downstream tasks often suffers from multicollinearity in the similarity-matching stage, and more advanced methods can be computationally prohibitive for real-time, low-latency applications. Inspired by the fly olfactory circuit, we propose Fly-CL, a bio-inspired framework compatible with a wide range of pretrained backbones. Fly-CL substantially reduces training time while achieving performance comparable to or exceeding that of current state-of-the-art methods. We theoretically show how Fly-CL progressively resolves multicollinearity, enabling more effective similarity matching with low time complexity. Extensive simulation experiments across diverse network architectures and data regimes validate Fly-CL&#39;s effectiveness in addressing this challenge through a biologically inspired design. Code is available at https://github.com/gfyddha/Fly-CL.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.16877" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.16877" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.16877" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">47. The $\mathtt{WebSky}$ $\mathrm{[CII]}$ Forecasts and the search for primordial intermittent non-Gaussianity
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Nathan J. Carlson, J. Richard Bond, Dongwoo T. Chung, Patrick Horlaville, Thomas Morrison</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Using the $\mathtt{Peak~Patch}$ algorithm to generate light-cone dark matter (DM) halo catalogues and the $\mathtt{WebSky}$ framework to forward-model the cosmological $\mathrm{[CII]}$ signal, we construct tomographic mock surveys matched to the CCAT Observatory. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present the $\mathtt{WebSky}$ $\mathrm{[CII]}$ line-intensity mock maps and forecast the capabilities of upcoming wide-field submillimeter-wave surveys of cosmological $\mathrm{[CII]}$ emission from the epoch of reionization (EoR). Using the $\mathtt{Peak~Patch}$ algorithm to generate light-cone dark matter (DM) halo catalogues and the $\mathtt{WebSky}$ framework to forward-model the cosmological $\mathrm{[CII]}$ signal, we construct tomographic mock surveys matched to the CCAT Observatory. We investigate both astrophysical models of $\mathrm{[CII]}$ emission from interstellar gas and the potential for the study of primordial intermittent non-Gaussianity (PING) as a science case for Stage 2 line intensity mapping (LIM) surveys. The $\mathrm{[CII]}$ voxel intensity distribution (VID) is used as a summary statistic in forecasts. Additional constraints on PING are derived from a relative entropy study of $\mathtt{Peak~Patch}$ halo mass functions. We show that upcoming LIM surveys will provide insights into the way we model cosmological line emission, and next-generation surveys can place competitive bounds on novel inflationary scenarios such as PING. The $\mathtt{WebSky}$ $\mathrm{[CII]}$ mocks and corresponding $\mathtt{Peak~Patch}$ halo catalogues are publicly available at https://uoft.me/webskycii .</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.18312" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.18312" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.18312" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">48. Simple Context Compression: Mean-Pooling and Multi-Ratio Training
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yair Feldman, Yoav Artzi</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We develop a lightweight and simple mean-pooling approach that consistently outperforms the widely used compression-tokens architecture, and study training the same compressor to output multiple compression ratios. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">A common strategy to reduce the computational costs of using long contexts in retrieval-augmented generation (RAG) with large language models (LLMs) is soft context compression, where the input sequence is transformed into a shorter continuous representation. We develop a lightweight and simple mean-pooling approach that consistently outperforms the widely used compression-tokens architecture, and study training the same compressor to output multiple compression ratios. We conduct extensive experiments across in-domain and out-of-domain QA datasets, as well as across model families, scales, and compression ratios. Overall, our simple mean-pooling approach achieves the strongest performance, with a relatively small drop when training for multiple compression ratios. More broadly though, across architectures and training regimes the trade-offs are more nuanced, illustrating the complex landscape of compression methods.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.20797" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.20797" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.20797" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CL</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">49. Scalable, Explainable and Provably Robust Anomaly Detection with One-Step Flow Matching
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zhong Li, Qi Huang, Yuxuan Zhu, Lincen Yang, Mohammad Mohammadi Amiri, Niki van Stein, Matthijs van Leeuwen</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Extensive experiments on the ADBench benchmark show that TCCM strikes a favorable balance between detection accuracy and inference cost, outperforming state-of-the-art methods -- especially on high-dimensional and large-scale datasets. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We introduce Time-Conditioned Contraction Matching (TCCM), a novel method for semi-supervised anomaly detection in tabular data. TCCM is inspired by flow matching, a recent generative modeling framework that learns velocity fields between probability distributions and has shown strong performance compared to diffusion models and generative adversarial networks. Instead of directly applying flow matching as originally formulated, TCCM builds on its core idea -- learning velocity fields between distributions -- but simplifies the framework by predicting a time-conditioned contraction vector toward a fixed target (the origin) at each sampled time step. This design offers three key advantages: (1) a lightweight and scalable training objective that removes the need for solving ordinary differential equations during training and inference; (2) an efficient scoring strategy called one time-step deviation, which quantifies deviation from expected contraction behavior in a single forward pass, addressing the inference bottleneck of existing continuous-time models such as DTE (a diffusion-based model with leading anomaly detection accuracy but heavy inference cost); and (3) explainability and provable robustness, as the learned velocity field operates directly in input space, making the anomaly score inherently feature-wise attributable; moreover, the score function is Lipschitz-continuous with respect to the input, providing theoretical guarantees under small perturbations. Extensive experiments on the ADBench benchmark show that TCCM strikes a favorable balance between detection accuracy and inference cost, outperforming state-of-the-art methods -- especially on high-dimensional and large-scale datasets. The source code is available at our GitHub repository.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.18328" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.18328" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.18328" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">50. Dark Matter profiles of &#34;in silico&#34; galaxies: deep learning inference
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Martín de los Rios, Serafina Di Gioia, Fabio Iocco, Roberto Trotta</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The equivariant CNN recovers the dark matter profile of galaxies within the stellar mass range $[10^{10} - 10^{12} ]$ $M_{\odot}$ with excellent precision and accuracy: the mean squared error is reduced by a factor of ~ 3 from its value under the training distribution, demonstrating that the network has learnt from the data features. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Machine learning has the potential to improve the reconstruction of the dark matter profile of galaxies with respect to traditional methods, like rotation curves. We demonstrate on the simulation suite Illustris-TNG that a steerable equivariant convolutional neural network (CNN) is able to infer the dark matter profiles within and around individual galaxies from photometric and interferometric data, improving on a standard CNN. Within the in silico environment of the simulations, our architecture is able to capture the dark matter distribution within galaxies without a parametrization of the profile. We perform an interpretability analysis to understand the internal mechanisms of the trained model and the most important data features used to estimate the dark matter profiles. The equivariant CNN recovers the dark matter profile of galaxies within the stellar mass range $[10^{10} - 10^{12} ]$ $M_{\odot}$ with excellent precision and accuracy: the mean squared error is reduced by a factor of ~ 3 from its value under the training distribution, demonstrating that the network has learnt from the data features. While this holds within the controlled &#39;in silico&#39; environment of the simulation, we argue that few additional steps are needed before this method can be reliably applied to galaxies in the real field observations.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.18964" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.18964" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.18964" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
    </div>

    <footer class="footer">
        &copy; 2025 arXiv Daily · Powered by <a href="https://arxiv.org/" target="_blank">arXiv</a>
    </footer>
</body>
</html>