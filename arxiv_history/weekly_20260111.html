<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>arXiv Daily: Backfill (2026-01-04 to 2026-01-11)</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto:400,700&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Roboto', Arial, sans-serif; background: #f8f9fa; transition: background 0.3s; }
        .arxiv-card { margin: 2em auto; max-width: 900px; box-shadow: 0 2px 8px rgba(0,0,0,0.08); border-radius: 16px; transition: box-shadow 0.3s, transform 0.3s; }
        .arxiv-card:hover { box-shadow: 0 4px 12px rgba(0,0,0,0.1); transform: translateY(-1px); }
        .arxiv-title { background: linear-gradient(90deg, #0d6efd 60%, #6610f2 100%); color: #fff; border-radius: 16px 16px 0 0; padding: 1.5em; font-size: 1.5em; font-weight: 700; letter-spacing: 0.5px;}
        .arxiv-meta { font-size: 1em; color: #555; margin-bottom: 1em; }
        .arxiv-abstract { background: #fff; padding: 1.5em; border-radius: 0 0 16px 16px; font-size: 1.1em; line-height: 1.7;}
        .arxiv-links a { margin-right: 1em; }
        .arxiv-scores { margin-top: 1em; font-size: 1em; }
        .navbar { background: #fff; box-shadow: 0 2px 8px rgba(0,0,0,0.04);}
        .footer { text-align: center; color: #888; padding: 2em 0 1em 0; font-size: 0.95em;}
        @media (prefers-color-scheme: dark) {
            body { background: #181a1b; color: #e4e6eb; }
            .arxiv-card { background: #23272b; box-shadow: 0 2px 8px rgba(0,0,0,0.28);}
            .arxiv-card:hover { box-shadow: 0 5px 15px rgba(0,0,0,0.25); transform: translateY(-1px); }
            .arxiv-title { background: linear-gradient(90deg, #375a7f 60%, #6f42c1 100%);}
            .arxiv-abstract { background: #23272b; color: #e4e6eb;}
            .navbar { background: #23272b; color: #e4e6eb;}
            .text-muted { color: #adb5bd !important; }
        }
        /* Improved abstract readability */
        .arxiv-abstract-text {
            font-size: 1.1em;
            line-height: 1.7;
        }
        p.big {
            line-height: 1.6;
            font-size: large;
        }
    </style>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'] ],
          processEscapes: true
        }
      });
    </script>
    <script>
    function toggleAuthors(element) {
        const fullList = element.querySelector('.author-full');
        const shortList = element.querySelector('.author-short');
        const toggleText = element.querySelector('.author-toggle');
        
        fullList.style.display = fullList.style.display === 'none' ? 'inline' : 'none';
        shortList.style.display = shortList.style.display === 'none' ? 'inline' : 'none';
        toggleText.textContent = fullList.style.display === 'none' ? ' (show all)' : ' (show less)';
    }
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>
<body>
    <nav class="navbar navbar-expand-lg mb-4">
        <div class="container">
            <a class="navbar-brand fw-bold text-primary" href="#">arXiv Daily</a>
            <span class="navbar-text">Modern arXiv Reader</span>
        </div>
    </nav>

    
    <div class="container py-4">
        <header class="mb-4"><h2 class="display-6 text-center">Analysis of Your Favorites</h2></header>
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Word Cloud</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/word_cloud.png" class="img-fluid rounded" alt="Word Cloud of favorite paper topics">
                    </div>
                </div>
            </div>
        </div>
        
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Clusters</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/backfill_cluster_map.png" class="img-fluid rounded" alt="2D Cluster plot of favorite papers">
                    </div>
                </div>
            </div>
        </div>
        
    </div>
    

    <div class="container py-4">
        <header class="mb-4"><h1 class="display-5 text-center text-primary">arXiv: Backfill (2026-01-04 to 2026-01-11)</h1></header>

        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Score Distribution of All Papers Found Today</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/score_distribution.png" class="img-fluid rounded" alt="Score distribution of all papers found today">
                    </div>
                </div>
            </div>
        </div>
        

        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">1. Euclid preparation. Galaxy 2-point correlation function modelling in redshift space
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Euclid Collaboration, M. Kärcher, M. -A. Breton, S. de la Torre, A. Veropalumbo, A. Eggemeier, M. Crocce, E. Sefusatti, E. Sarpa, R. E. Angulo, et al.</span>
                                <span class="author-full" style="display: none;">Euclid Collaboration, M. Kärcher, M. -A. Breton, S. de la Torre, A. Veropalumbo, A. Eggemeier, M. Crocce, E. Sefusatti, E. Sarpa, R. E. Angulo, B. Camacho Quevedo, L. Castiblanco, E. Castorina, A. Chudaykin, V. Desjacques, A. Farina, G. Gambardella, M. Guidi, D. Linde, F. Marulli, A. Moradinezhad Dizgah, M. Moresco, C. Moretti, K. Pardede, A. Pezzotta, M. Pellejero Ibañez, C. Porciani, A. Pugno, M. Zennaro, N. Aghanim, B. Altieri, L. Amendola, S. Andreon, N. Auricchio, C. Baccigalupi, M. Baldi, S. Bardelli, A. Biviano, E. Branchini, M. Brescia, S. Camera, G. Cañas-Herrera, V. Capobianco, C. Carbone, V. F. Cardone, J. Carretero, M. Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C. Colodro-Conde, G. Congedo, L. Conversi, Y. Copin, F. Courbin, H. M. Courtois, H. Degaudenzi, G. De Lucia, H. Dole, F. Dubath, X. Dupac, S. Dusini, A. Ealet, S. Escoffier, M. Farina, R. Farinelli, F. Faustini, S. Ferriol, F. Finelli, P. Fosalba, N. Fourmanoit, M. Frailis, E. Franceschi, M. Fumana, S. Galeotta, K. George, W. Gillard, B. Gillis, C. Giocoli, J. Gracia-Carpio, A. Grazian, F. Grupp, L. Guzzo, S. V. H. Haugan, W. Holmes, F. Hormuth, A. Hornstrup, K. Jahnke, M. Jhabvala, B. Joachimi, E. Keihänen, S. Kermiche, A. Kiessling, B. Kubik, M. Kümmel, M. Kunz, H. Kurki-Suonio, A. M. C. Le Brun, S. Ligori, P. B. Lilje, V. Lindholm, I. Lloro, G. Mainetti, D. Maino, E. Maiorano, O. Mansutti, S. Marcin, O. Marggraf, M. Martinelli, N. Martinet, R. J. Massey, E. Medinaceli, S. Mei, M. Melchior, Y. Mellier, M. Meneghetti, E. Merlin, G. Meylan, A. Mora, L. Moscardini, C. Neissner, S. -M. Niemi, C. Padilla, F. Pasian, J. A. Peacock, K. Pedersen, W. J. Percival, V. Pettorino, S. Pires, G. Polenta, M. Poncet, L. A. Popa, F. Raison, G. Riccio, E. Romelli, M. Roncarelli, C. Rosset, R. Saglia, Z. Sakr, A. G. Sánchez, D. Sapone, B. Sartoris, P. Schneider, T. Schrabback, A. Secroun, G. Seidel, S. Serrano, P. Simon, C. Sirignano, G. Sirri, L. Stanco, J. Steinwagner, P. Tallada-Crespí, A. N. Taylor, I. Tereno, N. Tessore, S. Toft, R. Toledo-Moreo, F. Torradeflot, I. Tutusaus, J. Valiviita, T. Vassallo, Y. Wang, J. Weller, G. Zamorani, F. M. Zerbi, E. Zucca, V. Allevato, M. Ballardini, M. Bolzonella, A. Boucaud, E. Bozzo, C. Burigana, R. Cabanac, M. Calabrese, A. Cappi, T. Castro, J. A. Escartin Vigo, L. Gabarra, J. García-Bellido, V. Gautard, J. Macias-Perez, R. Maoli, J. Martín-Fleitas, M. Maturi, N. Mauri, R. B. Metcalf, P. Monaco, M. Pöntinen, I. Risso, V. Scottez, M. Sereno, M. Tenti, M. Tucci, M. Viel, M. Wiesmann, Y. Akrami, I. T. Andika, G. Angora, M. Archidiacono, F. Atrio-Barandela, E. Aubourg, L. Bazzanini, J. Bel, D. Bertacca, M. Bethermin, F. Beutler, L. Blot, M. Bonici, S. Borgani, M. L. Brown, S. Bruton, A. Calabro, F. Caro, C. S. Carvalho, F. Cogato, S. Conseil, A. R. Cooray, S. Davini, G. Desprez, A. Díaz-Sánchez, S. Di Domizio, J. M. Diego, V. Duret, M. Y. Elkhashab, A. Enia, Y. Fang, A. G. Ferrari, P. G. Ferreira, A. Finoguenov, A. Fontana, F. Fontanot, A. Franco, K. Ganga, T. Gasparetto, E. Gaztanaga, F. Giacomini, F. Gianotti, G. Gozaliasl, A. Gruppuso, C. M. Gutierrez, A. Hall, H. Hildebrandt, J. Hjorth, S. Joudaki, J. J. E. Kajava, Y. Kang, V. Kansal, D. Karagiannis, K. Kiiveri, J. Kim, C. C. Kirkpatrick, S. Kruk, M. Lattanzi, J. Le Graet, L. Legrand, M. Lembo, F. Lepori, G. Leroy, G. F. Lesci, J. Lesgourgues, T. I. Liaudat, M. Magliocchetti, A. Manjón-García, F. Mannucci, C. J. A. P. Martins, L. Maurin, M. Migliaccio, M. Miluzio, A. Montoro, G. Morgante, S. Nadathur, K. Naidoo, P. Natoli, A. Navarro-Alsina, S. Nesseris, L. Pagano, D. Paoletti, F. Passalacqua, K. Paterson, L. Patrizii, R. Paviot, A. Pisani, D. Potter, G. W. Pratt, S. Quai, M. Radovich, K. Rojas, W. Roster, S. Sacquegna, M. Sahlén, D. B. Sanders, A. Schneider, D. Sciotti, E. Sellentin, L. C. Smith, J. G. Sorce, K. Tanidis, C. Tao, F. Tarsitano, G. Testera, R. Teyssier, S. Tosi, A. Troja, A. Venhola, D. Vergani, F. Vernizzi, G. Verza, S. Vinciguerra, N. A. Walton, A. H. Wright</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Non-perturbative models, specifically VDG$_{\infty}$ and CLEFT, are identified as the most accurate baseline approaches for analyzing Euclid galaxy clustering data in configuration space, achieving unbiased cosmological parameter recovery down to small scales ($s_{\rm min}=20\,h^{-1}\,{\rm Mpc}$) at low redshift. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The Euclid satellite will measure spectroscopic redshifts for tens of millions of emission-line galaxies. In the context of Stage-IV surveys, the 3-dimensional clustering of galaxies plays a key role in providing cosmological constraints. In this paper, we conduct a model comparison for the multipole moments of the galaxy 2-point correlation function (2PCF) in redshift space. We test state-of-the-art models, in particular the effective field theory of large-scale structure (EFT), one based on the velocity difference generating function (VDG$_{\infty}$), and different variants of Lagrangian perturbation theory (LPT) models, such as convolutional LPT (CLPT) and its effective-field-theory extension (CLEFT). We analyse the first three even multipoles of the 2PCF in the Flagship 1 simulation, which consists of four snapshots at $z\in\{0.9,1.2,1.5,1.8\}$. We study both template-fitting and full-shape approaches and find that with the template-fitting approach, only the VDG$_{\infty}$ model is able to reach a minimum fitting scale of $s_{\rm min}=20\,h^{-1}\,{\rm Mpc}$ at $z=0.9$ without biasing the recovered parameters. Indeed, the EFT model becomes inaccurate already at $s_{\rm min}=30\,h^{-1}\,{\rm Mpc}$. Conversely, in the full-shape analysis, the CLEFT and VDG$_{\infty}$ models perform similarly well, but only the CLEFT model can reach $s_{\rm min}=20\,h^{-1}\,{\rm Mpc}$ while the VDG$_{\infty}$ model is unbiased down to $s_{\rm min}=25\,h^{-1}\,{\rm Mpc}$ at the lowest redshift. Overall, in order to achieve the accuracy required by Euclid, non-perturbative modelling such as in the VDG$_{\infty}$ or CLEFT models should be considered. At $z=1.8$, the CLPT model is sufficient to describe the data with high figure of merit. This comparison selects baseline models that perform best in ideal conditions and sets the stage for an optimal analysis of Euclid data in configuration space.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.04780" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.04780" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.04780" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 14.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.99</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">2. Euclid: Improving redshift distribution reconstruction using a deep-to-wide transfer function
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Y. Kang, S. Paltani, W. G. Hartley, M. Bolzonella, A. H. Wright, F. Dubath, F. J. Castander, D. C. Masters, W. d&#39;Assignies, H. Hildebrandt, et al.</span>
                                <span class="author-full" style="display: none;">Y. Kang, S. Paltani, W. G. Hartley, M. Bolzonella, A. H. Wright, F. Dubath, F. J. Castander, D. C. Masters, W. d&#39;Assignies, H. Hildebrandt, O. Ilbert, M. Manera, W. Roster, S. A. Stanford, N. Aghanim, B. Altieri, S. Andreon, N. Auricchio, H. Aussel, C. Baccigalupi, M. Baldi, S. Bardelli, P. Battaglia, A. Biviano, E. Branchini, M. Brescia, J. Brinchmann, S. Camera, G. Cañas-Herrera, V. Capobianco, C. Carbone, V. F. Cardone, J. Carretero, S. Casas, M. Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C. Colodro-Conde, G. Congedo, L. Conversi, Y. Copin, A. Costille, F. Courbin, H. M. Courtois, M. Cropper, H. Degaudenzi, G. De Lucia, H. Dole, C. A. J. Duncan, X. Dupac, S. Dusini, A. Ealet, S. Escoffier, M. Farina, R. Farinelli, S. Farrens, F. Faustini, S. Ferriol, F. Finelli, N. Fourmanoit, M. Frailis, E. Franceschi, M. Fumana, S. Galeotta, K. George, B. Gillis, C. Giocoli, J. Gracia-Carpio, A. Grazian, F. Grupp, S. V. H. Haugan, H. Hoekstra, W. Holmes, F. Hormuth, A. Hornstrup, P. Hudelot, K. Jahnke, M. Jhabvala, B. Joachimi, E. Keihänen, S. Kermiche, A. Kiessling, B. Kubik, M. Kümmel, M. Kunz, H. Kurki-Suonio, R. Laureijs, A. M. C. Le Brun, S. Ligori, P. B. Lilje, V. Lindholm, I. Lloro, G. Mainetti, D. Maino, E. Maiorano, O. Mansutti, S. Marcin, O. Marggraf, M. Martinelli, N. Martinet, F. Marulli, R. J. Massey, E. Medinaceli, S. Mei, Y. Mellier, M. Meneghetti, E. Merlin, G. Meylan, A. Mora, M. Moresco, L. Moscardini, R. Nakajima, C. Neissner, S. -M. Niemi, C. Padilla, F. Pasian, K. Pedersen, V. Pettorino, S. Pires, G. Polenta, M. Poncet, L. A. Popa, L. Pozzetti, F. Raison, A. Renzi, J. Rhodes, G. Riccio, E. Romelli, M. Roncarelli, R. Saglia, Z. Sakr, A. G. Sánchez, D. Sapone, B. Sartoris, P. Schneider, T. Schrabback, A. Secroun, G. Seidel, S. Serrano, P. Simon, C. Sirignano, G. Sirri, L. Stanco, J. Steinwagner, P. Tallada-Crespí, A. N. Taylor, I. Tereno, N. Tessore, S. Toft, R. Toledo-Moreo, F. Torradeflot, I. Tutusaus, J. Valiviita, T. Vassallo, A. Veropalumbo, Y. Wang, J. Weller, G. Zamorani, F. M. Zerbi, I. A. Zinchenko, E. Zucca, J. García-Bellido, J. Martín-Fleitas, V. Scottez, M. Viel, R. Teyssier</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A novel photometric transfer method, which degrades deep reference photometry to match shallower survey properties while preserving flux correlations, significantly reduces mean redshift biases in Euclid&#39;s self-organizing map (SOM) based redshift distribution reconstruction, thereby meeting required accuracy standards. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The Euclid mission seeks to understand the Universe expansion history and the nature of dark energy, which requires a very accurate estimate of redshift distribution. Achieving this accuracy relies on reference samples with spectroscopic redshifts, together with a procedure to match them to survey sources for which only photometric redshifts are available. One important source of systematic uncertainty is the mismatch in photometric properties between galaxies in the Euclid survey and the reference objects. We develop a method to degrade the photometry of objects with deep photometry to match the properties of any shallower survey in the multi-band photometric space, preserving all the correlations between the fluxes and their uncertainties. We compare our transfer method with more demanding image-based methods, such as Balrog from the Dark Energy Survey Collaboration. According to metrics, our method outperforms Balrog. We implement it in the redshift distribution reconstruction, based on the self-organising map approach of arXiv:1509.03318, and test it using a realistic sample from the Euclid Flagship Simulation. We find that the key ingredient is to ensure that the reference objects are distributed in the colour space the same way as the wide-survey objects, which can be efficiently achieved with our transfer method. In our best implementation, the mean redshift biases are consistently reduced across the tomographic bins, bringing a significant fraction of them within the Euclid accuracy requirements in all tomographic bins. Equally importantly, the tests allow us to pinpoint which step in the calibration pipeline has the strongest impact on achieving the required accuracy. Our approach also reproduces the overall redshift distributions, which are crucial for applications such as angular clustering.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.02005" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.02005" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.02005" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 14.1</span>
                        <span class="badge bg-info text-dark">Author Score: 0.97</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">3. Filtering interlopers with photometry and diagnostic features: A machine learning framework validated with CSST slitless spectroscopy
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Hui Peng, Yu Yu, Yiyang Guo, Yizhou Gu, Run Wen, Yunkun Han, Jipeng Sui, Hu Zou, Xiaohu Yang, Pengjie Zhang, et al.</span>
                                <span class="author-full" style="display: none;">Hui Peng, Yu Yu, Yiyang Guo, Yizhou Gu, Run Wen, Yunkun Han, Jipeng Sui, Hu Zou, Xiaohu Yang, Pengjie Zhang, Xian Zhong Zheng, Hong Guo, Yipeng Jing, Cheng Li, Hu Zhan, Gongbo Zhao</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Utilizing an XGBoost classifier that combines photometric data and spectroscopic diagnostics successfully maximizes completeness while ensuring high redshift purity (96.6% accuracy, 0.13% outlier fraction) for slitless spectroscopic surveys such as CSST, enabling robust cosmological analysis. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The slitless spectroscopic method employed by missions such as Euclid and the Chinese Space Station Survey Telescope (CSST) faces a fundamental challenge: spectroscopic redshifts derived from their data are susceptible to emission line misidentification due to the limited spectral resolution and signal-to-noise ratio. This effect systematically introduces interloper galaxies into the sample. Conventional strict selection not only struggles to secure high redshift purity but also drastically reduces completeness by discarding valuable data. To overcome this limitation, we develop an XGBoost classifier that leverages photometric properties and spectroscopic diagnostics to construct a high-purity redshift catalog while maximizing completeness. We validate this method on a simulated sample with spectra generated by the CSST emulator for slitless spectroscopy. Of the $\sim$62 million galaxies that obtain valid redshifts (parent sample), approximately 43% achieve accurate measurements, defined as $|Δz| \leq 0.002(1+z)$. From this parent sample, the XGBoost classifier selects galaxies with a selection efficiency of 42.3% on the test set and 42.2% when deployed on the entire parent sample. Crucially, among the retained galaxies, 96.6% (parent sample: 96.5%) achieve accurate measurements, while the outlier fraction ($|Δz|&gt;0.01(1+z)$) is constrained to 0.13% (0.11%). We verified that simplified configurations which exclude either spectroscopic diagnostics (except the measured redshift) or photometric data yield significantly higher outlier fractions, increasing by factors of approximately 3.5 and 6.3 respectively, with the latter case also introducing notable catastrophic interloper contamination. This framework effectively resolves the purity-completeness trade-off, enabling robust large-scale cosmological studies with CSST and similar surveys.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.03883" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.03883" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.03883" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.7</span>
                        <span class="badge bg-info text-dark">Author Score: 0.31</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">4. Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jiawen Zhang, Lipeng He, Kejia Chen, Jian Lou, Jian Liu, Xiaohu Yang, Ruoxi Jia</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Safety alignment in fine-tuned large language models can be completely restored using just a single safety example, maintaining utility and minimizing cost, a surprising efficiency attributed to the underlying low-rank structure of the safety gradient. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.01887" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.01887" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.01887" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.21</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">5. Electron temperature relations and the direct N, O, Ne, S and Ar abundances of 49959 star-forming galaxies in DESI Data Release 2
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">D. Scholte, F. Cullen, J. M. Moustakas, H. Zou, A. Saintonge, K. Z. Arellano-Cordova, T. M. Stanton, B. Andrews, J. Sui, J. Aguilar, et al.</span>
                                <span class="author-full" style="display: none;">D. Scholte, F. Cullen, J. M. Moustakas, H. Zou, A. Saintonge, K. Z. Arellano-Cordova, T. M. Stanton, B. Andrews, J. Sui, J. Aguilar, S. Ahlen, D. Bianchi, D. Brooks, F. J. Castander, T. Cheng, T. Claybaugh, A. de la Macorra, B. Dey, P. Doel, K. Douglass, S. Ferraro, J. E. Forero-Romero, E. Gaztañaga, S. Gontcho A Gontcho, G. Gutierrez, R. Joyce, A. Kremin, O. Lahav, M. Landriau, L. Le Guillou, P. Martini, A. Meisner, R. Miquel, W. J. Percival, C. Poppett, F. Prada, I. Pérez-Ràfols, G. Rossi, E. Sanchez, D. Schlegel, Z. Shao, J. Silber, D. Sprayberry, G. Tarlé, B. A. Weaver</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Analysis of the largest direct-method abundance catalog of nearly 50,000 star-forming galaxies at $z&lt;0.1$ establishes a tight mass-metallicity relation (0.08 dex scatter) and confirms that correlated S/O and Ar/O ratios are consistent with Type Ia enrichment contributing to sulfur and argon abundances. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present the largest direct-method abundance catalogue of galaxies to date, containing measurements of 49$\,$959 star-forming galaxies at $z 8.105\pm0.004$ dex. We show that the S/O and Ar/O abundance ratios are strongly correlated, consistent with the expected additional Type Ia enrichment channel for S and Ar. In this work we present an initial survey of the key properties of the sample, with this dataset serving as a foundation for extensive future work on galaxy abundances at low redshift.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.02463" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.02463" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.02463" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.20</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">6. Measuring the homogeneity scale using the peculiar velocity field
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Leonardo Giani, Cullan Howlett, Chris Blake, Ryan J. Turner, Tamara M. Davis</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A novel, bias-independent definition of the cosmic homogeneity scale based on peculiar velocity correlation functions—the distance where galaxy pair motions switch from correlated to anti-correlated—was measured using SDSS data to be $R_H\approx 133\,\rm{Mpc/h}$, demonstrating its potential as a standard cosmological ruler. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We propose an innovative definition of the scale at which the Universe becomes homogeneous based on measurements of velocities rather than densities. When using the matter density field, one has to choose an arbitrary scale (e.g. within 1\% of the average density) to define the transition to homogeneity. Furthermore, the resulting homogeneity scale is strongly degenerate with the galaxy bias. By contrast, peculiar velocities (PV) allow us to define an unambiguous scale of homogeneity, namely the distance at which the velocities between pairs of galaxies change from being on-average correlated to anti-correlated. Physically, this relates to when the motion of pairs of galaxies is influenced by the matter density between them, rather than beyond. The disadvantage is that peculiar velocities are more difficult to measure than positions, resulting in smaller samples with larger uncertainties. Nevertheless, we illustrate the potential of this approach using the peculiar velocity correlation functions obtained from the Sloan Digital Sky Survey PV catalog, finding an homogeneity scale of $R_H\approx 133\substack{+28 \\ -52}\, \rm{Mpc/h}$. Finally, we show that more precise measurements are within reach of upcoming peculiar velocity surveys, and highlight this homogeneity scale&#39;s potential use as a standard ruler within the standard cosmological model.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.02886" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.02886" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.02886" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.7</span>
                        <span class="badge bg-info text-dark">Author Score: 0.09</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">7. First measurement of the Hubble constant from a combined weak lensing and gravitational-wave standard siren analysis
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Felipe Andrade-Oliveira, David Sanchez-Cid, Danny Laghi, Marcelle Soares-Santos</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Combining standard siren data from LVK (including GW170817 jet constraints) with DES Year 3 3\times2pt weak lensing and galaxy clustering observables resulted in a $6.4\%$ precision measurement of the Hubble constant, $H_0 = 67.9^{+4.4}_{-4.3}$~km~s$^{-1}$~Mpc$^{-1}$, and substantially tightened constraints on the total matter abundance $\Omega_m$. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present a new measurement of the Hubble constant ($H_0$) resulting from the first joint analysis of standard sirens with weak gravitational lensing and galaxy clustering observables comprising three two-point correlation functions (3$\times$2pt). For the 3$\times$2pt component of the analysis, we use data from the Dark Energy Survey (DES) Year 3 release. For the standard sirens component, we use data from the Gravitational-Wave Transient Catalog 4.0 released by the LIGO-Virgo-KAGRA (LVK) Collaboration. For GW170817, the only standard siren for which extensive electromagnetic follow-up observations exist, we also use measurements of the host galaxy redshift and inclination angle estimates derived from observations of a superluminal jet from its remnant. Our joint analysis yields $H_0 = 67.9^{+4.4}_{-4.3}$~km~s$^{-1}$~Mpc$^{-1}$, a $6.4\%$ measurement, while improving the DES constraint on the total abundance of matter $Ω_m$ by $22\%$. Removing the jet information degrades the $H_0$ precision to $9.9\%$. The measurement of $H_0$ remains a central problem in cosmology with a multitude of approaches being vigorously pursued in the community aiming to reconcile significantly discrepant measurements at the percent-level. In light of the impending new data releases from DES and LVK, and anticipating much more constraining power from 3$\times$2pt observables using newly commissioned survey instruments, we demonstrate that incorporating standard sirens into the cosmology framework of large cosmic surveys is a viable route towards that goal.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.04774" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.04774" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.04774" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.96</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmic Shear, Galaxy Clustering, Survey Analysis</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">8. Symbolically regressing dark matter halo profiles using weak lensing
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Alicia Martín, Tariq Yasin, Deaglan J. Bartlett, Harry Desmond, Pedro G. Ferreira</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Exhaustive Symbolic Regression applied to weak lensing data from 149 HSC-XXL galaxy clusters identifies optimal halo density profiles that statistically surpass the NFW model, often showing shallow inner regions and a density maximum, which implies potential systematic biases in cluster mass estimates based on standard assumptions. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The structure of dark matter haloes is often described by radial density profiles motivated by cosmological simulations. These are typically assumed to have a fixed functional form (e.g. NFW), with some free parameters that can be constrained with observations. However, relying on simulations has the disadvantage that the resulting profiles depend on the dark matter model and the baryonic physics implementation, which are highly uncertain. Instead, we present a method to constrain halo density profiles directly from observations. This is done using a symbolic regression algorithm called Exhaustive Symbolic Regression (ESR). ESR searches for the optimal analytic expression to fit data, combining both accuracy and simplicity. We apply ESR to a sample of 149 galaxy clusters from the HSC-XXL survey to identify which functional forms perform best across the entire sample of clusters. We identify density profiles that statistically outperform NFW under a minimum-description-length criterion. Within the radial range probed by the weak-lensing data ($R \sim 0.3 - 3$ h$^{-1}$ Mpc), the highest-ranked ESR profiles exhibit shallow inner behaviour and a maximum in the density profile. As a practical application, we show how the best-fitting ESR models can be used to obtain enclosed mass estimates. We find masses that are, on average, higher than those derived using NFW, highlighting a source of potential bias when assuming the wrong density profile. These results have important knock-on effects for analyses that utilise clusters, for example cosmological constraints on $σ_8$ and $Ω_m$ from cluster abundance and clustering. Beyond the HSC dataset, the method is readily applicable to any data constraining the dark matter distribution in galaxies and galaxy clusters, such as other weak lensing surveys, galactic rotation curves, or complementary probes.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.05203" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.05203" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.05203" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">9. A possible challenge for Cold and Warm Dark Matter
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>S. Vegetti, S. D. M. White, J. P. McKean, D. M. Powell, C. Spingola, D. Massari, G. Despali, C. D. Fassnacht</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Modeling the structure of a million-solar-mass object detected via gravitational lensing reveals a two-component mass profile inconsistent with cold dark matter, but potentially compatible with a self-interacting dark matter halo that has collapsed to form a central black hole. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Measuring the density profile and mass concentration of dark-matter haloes is a key test of the standard cold dark matter paradigm. Such objects are dark and thus challenging to characterise, but they can be studied via gravitational lensing. Recently, a million-solar-mass object was discovered superposed on an extended and extremely thin gravitational arc. Here we report on extensive tests of various assumptions for the mass density profile and redshift of this object. We find models that best describe the data have two components: an unresolved point-mass of radius $\leq10$ pc centred on an extended mass distribution with an almost constant surface density out to a truncation radius of 139 pc. These properties do not resemble any known astronomical object. However, if the object is dark matter-dominated, its structure is incompatible with cold dark matter models, but may be compatible with a self-interacting dark matter halo where the central region has collapsed to form a black hole. This detection could thus carry substantial implications for our current understanding of dark matter.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.02466" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.02466" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.02466" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmic Shear, Galaxy Clustering, Survey Analysis / Galaxy-Halo Connection, Lensing, Clustering</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">10. Accelerated evolution of galaxy host halo masses during Cosmic Dawn from deep JWST clustering
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Nicolò Dalmasso, Giovanni Ferrami, Nicha Leethochawalit, Emanuele M. Ventura, Michele Trenti</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The deepest clustering analysis of JWST JADES Lyman Break Galaxies demonstrates that early galaxies at $z=10.6$ inhabit dark matter halos over an order of magnitude less massive ($M_{\rm{h}} \sim 10^{10.12} M_{\odot}$) than their $z=5.5$ counterparts, offering new constraints on galaxy-halo coevolution during cosmic dawn. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present the deepest clustering analysis of early galaxies to date, analyzing $N_{\rm{g}} \simeq 6500$ photometrically-selected Lyman Break Galaxies from JWST&#39;s Advanced Deep Extragalactic Survey (JADES) to reveal how galaxies and dark matter evolved during cosmic dawn ($5 \leq z &lt; 11$). Using halo occupation distribution (HOD) modeling of the two-point angular correlation function, we trace the galaxy-halo relationships across the first billion years of cosmic history. Our analysis reveals that galaxies at $z = 10.6$ reside in dark matter halos over an order of magnitude less massive ($M_{\rm{h}} \sim 10^{10.12} M_{\odot}$) than their counterparts at $z = 5.5$ ($M_{\rm{h}} \sim 10^{11.45} M_{\odot}$), while exhibiting correspondingly higher effective bias values ($b_{\rm{g}}^{\rm{eff}} = 8.13^{+0.04}_{-0.02}$ compared to $5.64^{+0.10}_{-0.13}$). Correspondingly, the satellite galaxy fraction hints at a declining trend with decreasing redshift, reaching $&lt;1\%$ by $z \sim 5-6$. However, the significant systematic and random uncertainties in the data-model comparison prevent us from drawing robust conclusions on the evolution - if any - of the satellite fraction during the epoch of reionization. These results provide the first view of the coevolution between galaxies and dark matter evolved at redshift $\gtrsim 10$, offering additional and independent constraints on early galaxy formation models tuned to reproducing luminosity function evolution.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.01697" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.01697" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.01697" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmic Shear, Galaxy Clustering, Survey Analysis / Galaxy-Halo Connection, Lensing, Clustering</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">11. The Squeezed Bispectrum from CHIME HI Emission and Planck CMB Lensing: Current Sensitivity and Forecasts
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">CHIME Collaboration, Arnab Chakraborty, Matt Dobbs, Simon Foreman, Liam Gray, Mark Halpern, Gary Hinshaw, Albin Joseph, Joshua MacEachern, Kiyoshi W. Masui, et al.</span>
                                <span class="author-full" style="display: none;">CHIME Collaboration, Arnab Chakraborty, Matt Dobbs, Simon Foreman, Liam Gray, Mark Halpern, Gary Hinshaw, Albin Joseph, Joshua MacEachern, Kiyoshi W. Masui, Juan Mena-Parra, Laura Newburgh, Tristan Pinsonneault-Marotte, Alex Reda, Shabbir Shaikh, Seth Siegel, Haochen Wang, Dallas Wulf, Zeeshan Ahmed, Nickolas Kokron, Emmanuel Schaan</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A method measuring the non-linear gravitational coupling between CHIME 21cm power and Planck CMB lensing, implemented via a squeezed integrated bispectrum, currently shows low signal-to-noise but is forecasted to reach SNR=3 using the full collected CHIME dataset. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Line intensity mapping using atomic hydrogen (HI) has the potential to efficiently map large volumes of the universe if the signal can be successfully separated from overwhelmingly bright radio foreground emission. This motivates cross-correlations, to ascertain the cosmological nature of measured HI fluctuations, and to study their connections with galaxies and the underlying matter density field. However, these same foregrounds render the cross-correlation with projected fields such as the lensing of the cosmic microwave background (CMB) difficult. Indeed, the correlated Fourier modes vary slowly along the line of sight, and are thus most contaminated by the smooth-spectrum radio continuum foregrounds. In this paper, we implement a method that avoids this issue by attempting to measure the non-linear gravitational coupling of the small-scale 21cm power from the Canadian Hydrogen Intensity Mapping Experiment (CHIME) with large-scale Planck CMB lensing. This measurement is a position-dependent power spectrum, i.e. a squeezed integrated bispectrum. Using 94 nights of CHIME data between $1.0 &lt; z &lt; 1.3$ and aggressive foreground filtering, we find that the expected signal is five times smaller than the current noise. We forecast that incorporating the additional nights of CHIME data already collected would enable a signal-to-noise ratio of 3, without any further improvements in filtering for foreground cleaning.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.03240" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.03240" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.03240" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmic Shear, Galaxy Clustering, Survey Analysis</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">12. Cracks in the Standard Cosmological Model: Anomalies, Tensions, and Hints of New Physics
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Eleonora Di Valentino</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Increasing precision in cosmology has exposed a network of tensions within the standard $\Lambda$CDM model, necessitating critical assessment of model assumptions, parameter degeneracies, and dataset consistency before claiming new physics. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Cosmology has entered an era of unprecedented precision, yet increasing accuracy has revealed cracks in the standard $Λ$CDM paradigm. Although the model remains highly successful when confronted with individual datasets, joint analyses expose a network of tensions involving the Hubble constant, CMB lensing, curvature, neutrino masses, and the nature of dark energy. In this contribution to the 3rd General Meeting of the COST Action COSMIC WISPers (CA21106), within the context of Working Group~2, we critically assess these discrepancies, emphasizing the role of model assumptions, parameter degeneracies, and dataset consistency. We review proposed early- and late-time solutions, discuss how recent DESI BAO results alter the viability of late-time extensions, and explore interacting dark-sector scenarios. Our analysis highlights the need for caution in interpreting cosmological measurements and underscores the importance of internal consistency among cosmological probes before claiming percent-level accuracy or invoking new physics.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.01525" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.01525" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.01525" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.02</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">13. Cosmological constraints on viable $f(R)$ models using weak lensing
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Leandro Pardo, Leonardo Castañeda</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Weak gravitational lensing and CMB lensing data, analyzed within a Bayesian framework using MGCobaya, successfully constrain characteristic parameters of several viable $f(R)$ modified gravity models while maintaining consistency with standard $\Lambda$CDM background evolution. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The accelerated expansion of the Universe remains one of the central open problems in modern cosmology. While the $Λ$CDM model successfully describes a wide range of observations, the physical nature of dark energy is still unknown, motivating the study of alternative theories of gravity. Among these, $f(R)$ models provide a well-established extension of General Relativity, capable of reproducing a $Λ$CDM-like background evolution without introducing an explicit dark energy component. However, they can induce deviations in the growth of cosmic structures, making them testable through observables sensitive to cosmological perturbations. In this work, we use weak gravitational lensing to constrain several viable $f(R)$ gravity models. We analyze their impact on the matter power spectrum, as well as on the convergence and cosmic shear power spectra. Our analysis is carried out within a Bayesian framework using the \textit{Cobaya} code and its modified gravity extension, \textit{MGCobaya}, which enables consistent theoretical predictions and their comparison with current weak lensing and CMB lensing data. We find that standard cosmological parameters remain consistent with the $Λ$CDM scenario for all models considered, as expected from their background degeneracy. Nevertheless, we obtain non-trivial and model-dependent constraints on the characteristic parameters of several $f(R)$ theories.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.04048" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.04048" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.04048" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Cosmological Inference, Structure Modeling, Bayesian Methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">14. Nonlinear Scales in Luminal Horndeski -- I. Halo mass function and power spectrum boost in models with Vainshtein screening
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Dani de Boe, Mattia Pantiri, Gen Ye, Alessandra Silvestri</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Nonlinear structure formation in luminal Horndeski gravity, incorporating Vainshtein screening, is accurately modeled by a new flexible framework that computes the halo mass function and the nonlinear matter power spectrum for use in future large-scale structure surveys. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We investigate nonlinear structure formation in Horndeski gravity with a luminal gravitational wave speed ($c_T = 1$) using the spherical collapse model incorporating Vainshtein screening. We compute the critical and virial overdensities and use these to evaluate the halo mass function within several commonly employed formalisms. Building on the reaction method, we develop a flexible and accurate framework for computing the nonlinear matter power spectrum across a broad class of viable modified gravity models within luminal Horndeski theories. The framework interfaces seamlessly with EFTCAMB and is applicable to both covariant Horndeski models and effective field theory descriptions of dark energy, allowing for a range of background cosmologies. This approach enables systematic exploration of a wide space of theories and cosmological parameters, with the goal of informing future analyses of upcoming large-scale structure surveys.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.02074" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.02074" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.02074" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">15. Learning Latent Action World Models In The Wild
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Quentin Garrido, Tushar Nagarajan, Basile Terver, Nicolas Ballas, Yann LeCun, Michael Rabbat</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Latent action world models trained on diverse, in-the-wild videos successfully utilize continuous, constrained latent actions to capture complex real-world dynamics and solve planning tasks, demonstrating scalability beyond simple simulation environments. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Agents capable of reasoning and planning in the real world require the ability of predicting the consequences of their actions. While world models possess this capability, they most often require action labels, that can be complex to obtain at scale. This motivates the learning of latent action models, that can learn an action space from videos alone. Our work addresses the problem of learning latent actions world models on in-the-wild videos, expanding the scope of existing works that focus on simple robotics simulations, video games, or manipulation data. While this allows us to capture richer actions, it also introduces challenges stemming from the video diversity, such as environmental noise, or the lack of a common embodiment across videos. To address some of the challenges, we discuss properties that actions should follow as well as relevant architectural choices and evaluations. We find that continuous, but constrained, latent actions are able to capture the complexity of actions from in-the-wild videos, something that the common vector quantization does not. We for example find that changes in the environment coming from agents, such as humans entering the room, can be transferred across videos. This highlights the capability of learning actions that are specific to in-the-wild videos. In the absence of a common embodiment across videos, we are mainly able to learn latent actions that become localized in space, relative to the camera. Nonetheless, we are able to train a controller that maps known actions to latent ones, allowing us to use latent actions as a universal interface and solve planning tasks with our world model with similar performance as action-conditioned baselines. Our analyses and experiments provide a step towards scaling latent action models to the real world.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.05230" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.05230" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.05230" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.07</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">16. Combining simulation-based inference and universal relations for precise and accurate neutron star science
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Christian J. Krüger, Sebastian H. Völkel</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Simulation-Based Inference (SBI) offers a robust framework for constructing and validating precise neutron star universal relations by treating nuclear equation of state uncertainty as intrinsic noise, yielding predictive power superior to explicit relations and mitigating systematic errors. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">In this work, we propose a novel approach for identifying, constructing, and validating precise and accurate universal relations for neutron star bulk quantities. A central element is simulation-based inference (SBI), which we adopt to treat uncertainties due to the unknown nuclear equation of state (EOS) as intrinsic non-trivial noise. By assembling a large set of bulk properties of non-rotating neutron stars across multiple state-of-the-art EOS models, we are able to systematically explore universal relations in high-dimensional parameter spaces. Our framework further identifies the most promising parameter combinations, enabling a more focused and traditional construction of explicit universal relations. At the same time, SBI does not rely on explicit relations; instead, it directly provides predictive distributions together with a quantitative measure of systematic uncertainties, which are not captured by conventional approaches. As an example, we report a new universal relation that allows us to obtain the radius as a function of mass, fundamental mode, and one pressure mode. Our analysis shows that SBI can surpass the predictive power of this universal relation while also mitigating systematic errors. Finally, we demonstrate how universal relations can be further calibrated to mitigate systematic errors accurately.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.03945" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.03945" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.03945" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">17. sidmkit: A Reproducible Toolkit for SIDM Phenomenology and Galaxy Rotation-Curve Modeling
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Nalin Dhiman</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The new Python package `sidmkit` facilitates reproducible SIDM micro-to-macro calculations, including velocity-dependent cross-sections and a robust pipeline for fitting SPARC rotation curves, demonstrating that the Burkert halo profile is preferred over NFW for the majority of observed galaxies. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Self-interacting dark matter (SIDM) is a well-motivated extension of cold dark matter that can modify halo structure on galactic and group scales while remaining consistent with large-scale structure. However, practical SIDM work often requires bridging several layers, including microphysical scattering models, velocity-dependent effective cross sections, phenomenological astrophysical constraints, and (separately) data-driven halo fits, such as rotation curves. In this paper, we describe \texttt{sidmkit}, a transparent and reproducible Python package designed to support SIDM ``micro$\rightarrow$macro&#39;&#39; calculations and to provide a robust batch pipeline for fitting rotation curves in the SPARC data. On the SIDM side, \texttt{sidmkit} implements velocity-dependent momentum-transfer cross sections for a Yukawa interaction using standard analytic approximations (Born, classical, and Hulthén-based) with a numerical partial-wave option for spot checks. It also provides consistent velocity-moment averaging for Maxwellian relative speeds, scattering-rate utilities, and curated literature \emph{summary} constraints for regression tests and exploratory scans. On the rotation-curve side, we implement bounded non-linear least squares fits of NFW and Burkert halo models to SPARC baryonic decompositions, with optional mass-to-light priors and information-criterion summaries (AIC/BIC). For the demonstration dataset, we process 191 \texttt{rotmod} galaxies (LTG+ETG bundles) and fit both NFW and Burkert models (382 total fits). We find that Burkert is preferred by $Δ\mathrm{BIC} &gt; 0$ for $65.4\%$ of galaxies, with ``strong&#39;&#39; preference ($Δ\mathrm{BIC}&gt;6$) in $32.5\%$ of galaxies;</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.04735" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.04735" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.04735" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.IM</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">18. Data Complexity-aware Deep Model Performance Forecasting
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yen-Chia Chen, Hsing-Kuo Pao, Hanjuan Huang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A lightweight, two-stage framework accurately estimates deep learning model performance prior to training by combining a baseline prediction derived from dataset properties with adjustments based on architectural details, thereby guiding model selection and data quality assessment. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Deep learning models are widely used across computer vision and other domains. When working on the model induction, selecting the right architecture for a given dataset often relies on repetitive trial-and-error procedures. This procedure is time-consuming, resource-intensive, and difficult to automate. While previous work has explored performance prediction using partial training or complex simulations, these methods often require significant computational overhead or lack generalizability. In this work, we propose an alternative approach: a lightweight, two-stage framework that can estimate model performance before training given the understanding of the dataset and the focused deep model structures. The first stage predicts a baseline based on the analysis of some measurable properties of the dataset, while the second stage adjusts the estimation with additional information on the model&#39;s architectural and hyperparameter details. The setup allows the framework to generalize across datasets and model types. Moreover, we find that some of the underlying features used for prediction - such as dataset variance - can offer practical guidance for model selection, and can serve as early indicators of data quality. As a result, the framework can be used not only to forecast model performance, but also to guide architecture choices, inform necessary preprocessing procedures, and detect potentially problematic datasets before training begins.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.01383" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.01383" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.01383" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">19. Credit Assignment via Neural Manifold Noise Correlation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Byungwoo Kang, Maceo Richards, Bernardo Sabatini</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Neural Manifold Noise Correlation (NMNC) achieves superior performance and sample efficiency in credit assignment by restricting perturbations to the low-dimensional neural manifold, leveraging the observation that the Jacobian row space aligns with this manifold in trained networks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Credit assignment--how changes in individual neurons and synapses affect a network&#39;s output--is central to learning in brains and machines. Noise correlation, which estimates gradients by correlating perturbations of activity with changes in output, provides a biologically plausible solution to credit assignment but scales poorly as accurately estimating the Jacobian requires that the number of perturbations scale with network size. Moreover, isotropic noise conflicts with neurobiological observations that neural activity lies on a low-dimensional manifold. To address these drawbacks, we propose neural manifold noise correlation (NMNC), which performs credit assignment using perturbations restricted to the neural manifold. We show theoretically and empirically that the Jacobian row space aligns with the neural manifold in trained networks, and that manifold dimensionality scales slowly with network size. NMNC substantially improves performance and sample efficiency over vanilla noise correlation in convolutional networks trained on CIFAR-10, ImageNet-scale models, and recurrent networks. NMNC also yields representations more similar to the primate visual system than vanilla noise correlation. These findings offer a mechanistic hypothesis for how biological circuits could support credit assignment, and suggest that biologically inspired constraints may enable, rather than limit, effective learning at scale.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.02636" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.02636" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.02636" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">20. The Sequential Monte Carlo goes NUTS: Boosting Gravitational-Wave Inference
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Gabriele Demasi, Giulia Capurri, Massimo Lenti, Angelo Ricciardone, Barbara Patricelli, Adriano Frattale Mascioli, Lorenzo Piccari, Saulo Albuquerque, Gianluca M. Guidi, Francesco Pannarale, et al.</span>
                                <span class="author-full" style="display: none;">Gabriele Demasi, Giulia Capurri, Massimo Lenti, Angelo Ricciardone, Barbara Patricelli, Adriano Frattale Mascioli, Lorenzo Piccari, Saulo Albuquerque, Gianluca M. Guidi, Francesco Pannarale, Giulia Stratta, Walter Del Pozzo</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The SHARPy Bayesian inference framework combines the parallelism of Sequential Monte Carlo with the efficiency of the gradient-based No-U-Turn Sampler, enabling gravitational-wave inference on binary black-hole events in approximately ten minutes. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Sequential Monte Carlo (SMC) methods have recently been applied to gravitational-wave inference as a powerful alternative to standard sampling techniques, such as Nested Sampling. At the same time, gradient-based Markov Chain Monte Carlo algorithms, most notably the No-U-Turn Sampler (NUTS), provide an efficient way to explore high-dimensional parameter spaces. In this work we present SHARPy, a Bayesian inference framework that combines the parallelism and evidence-estimation capabilities of SMC with the state-of-the-art sampling performance of NUTS. Moreover, SHARPy exploits the local geometric structure of the posterior to further improve efficiency. Built on JAX and accelerated on GPUs, SHARPy performs gravitational-wave inference on binary black-hole events in around ten minutes, yielding posterior samples and Bayesian evidence estimates that are consistent with those obtained through Nested Sampling. This work sets a new milestone in GW inference with likelihood-based methods and paves the way for model comparison tasks to be accomplished in minutes.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.02336" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.02336" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.02336" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">21. Learning to Reason: Temporal Saliency Distillation for Interpretable Knowledge Transfer
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Nilushika Udayangani Hewa Dehigahawattage, Kishor Nandakishor, Marimuthu Palaniswami</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Temporal Saliency Distillation (TSD) enhances time series knowledge transfer by explicitly conveying the teacher model&#39;s temporal reasoning—the importance of each input timestep—leading to improved student performance and interpretability without architectural changes. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Knowledge distillation has proven effective for model compression by transferring knowledge from a larger network called the teacher to a smaller network called the student. Current knowledge distillation in time series is predominantly based on logit and feature aligning techniques originally developed for computer vision tasks. These methods do not explicitly account for temporal data and fall short in two key aspects. First, the mechanisms by which the transferred knowledge helps the student model learning process remain unclear due to uninterpretability of logits and features. Second, these methods transfer only limited knowledge, primarily replicating the teacher predictive accuracy. As a result, student models often produce predictive distributions that differ significantly from those of their teachers, hindering their safe substitution for teacher models. In this work, we propose transferring interpretable knowledge by extending conventional logit transfer to convey not just the right prediction but also the right reasoning of the teacher. Specifically, we induce other useful knowledge from the teacher logits termed temporal saliency which captures the importance of each input timestep to the teacher prediction. By training the student with Temporal Saliency Distillation we encourage it to make predictions based on the same input features as the teacher. Temporal Saliency Distillation requires no additional parameters or architecture specific assumptions. We demonstrate that Temporal Saliency Distillation effectively improves the performance of baseline methods while also achieving desirable properties beyond predictive accuracy. We hope our work establishes a new paradigm for interpretable knowledge distillation in time series analysis.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.04263" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.04263" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.04263" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">22. Towards an optimal extraction of cosmological parameters from galaxy cluster surveys using convolutional neural networks
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Iñigo Sáez-Casares, Matteo Calabrese, Davide Bianchi, Marina S. Cagliari, Marco Chiarenza, Jean-Marc Christille, Luigi Guzzo</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Field-level analysis using a 3D Convolutional Neural Network trained on large mock X-ray cluster catalogs generated by Pinocchio significantly improves cosmological parameter constraints (Ωm and σ8) compared to traditional summary statistics, with precision gains exceeding 50% when cluster luminosity is incorporated. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The possibility to constrain cosmological parameters from galaxy surveys using field-level machine learning methods that bypass traditional summary statistics analyses, depends crucially on our ability to generate simulated training sets. The latter need to be both realistic, as to reproduce the key features of the real data, and produced in large numbers, as to allow us to refine the precision of the training process. The analysis presented in this paper is an attempt to respond to these needs by (a) using clusters of galaxies as tracers of large-scale structure, together with (b) adopting a 3LPT code (Pinocchio) to generate a large training set of $32\,768$ mock X-ray cluster catalogues. X-ray luminosities are stochastically assigned to dark matter haloes using an empirical $M-L_X$ scaling relation. Using this training set, we test the ability and performances of a 3D convolutional neural network (CNN) to predict the cosmological parameters, based on an input overdensity field derived from the cluster distribution. We perform a comparison with a neural network trained on traditional summary statistics, that is, the abundance of clusters and their power spectrum. Our results show that the field-level analysis combined with the cluster abundance yields a mean absolute relative error on the predicted values of $Ω_{\rm m}$ and $σ_8$ that is a factor of $\sim 10 \%$ and $\sim 20\%$ better than that obtained from the summary statistics. Furthermore, when information about the individual luminosity of each cluster is passed to the CNN, the gain in precision exceeds $50\%$.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.03894" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.03894" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.03894" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmic Shear, Galaxy Clustering, Survey Analysis / Galaxy-Halo Connection, Lensing, Clustering</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">23. Mind the peak: improving cosmological constraints from GWTC-4.0 spectral sirens using semiparametric mass models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Matteo Tagliazucchi, Michele Moresco, Nicola Borghi, Chiara Ciapetti</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Employing a flexible Bspline-based semiparametric model to resolve three distinct peaks in the binary black hole mass distribution yields a statistically superior fit to GWTC-4.0 data and improves the precision of the Hubble constant constraint derived from gravitational wave spectral sirens by up to 21%. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Gravitational wave spectral sirens can provide cosmological constraints by using the shape of the binary black hole (BBH) mass distribution (MD). However, the precision and accuracy of these constraints depends critically on the capturing all the MD features. In this work, we analyze 137 BBH events from the latest GWTC-4.0 with a novel data-driven semiparametric approach based on \textsc{Bspline} that adaptively places knots around the most informative structures in the MD, while keeping the dimensionality of the parameter space moderate. Our flexible models resolve three distinct peaks at $\sim10$, $18$, and $33\,\mathrm{M}_\odot$ and are statistically preferred over standard parametric models, with Bayes factors up to 226. Because these features are correlated with $H_0$, the semiparametric model yields, under different prior assumptions, 12%-21% improvement in the precision of $H_0$ relative to parametric models, providing $H_0 = 57.8^{+21.9}_{-20.6}\,\mathrm{km/s/Mpc}$ in the best case. Our results demonstrate that capturing the full complexity of the BBH mass distribution is essential for realizing the cosmological potential of spectral sirens as gravitational wave catalogs continue to grow.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.03347" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.03347" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.03347" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">24. Attention mechanisms in neural networks
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Hasi Hays</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A comprehensive monograph provides a rigorous mathematical and empirical analysis of attention mechanisms, detailing their theoretical underpinnings, computational characteristics, diverse applications in deep learning, scaling behavior, interpretability, and existing limitations. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Attention mechanisms represent a fundamental paradigm shift in neural network architectures, enabling models to selectively focus on relevant portions of input sequences through learned weighting functions. This monograph provides a comprehensive and rigorous mathematical treatment of attention mechanisms, encompassing their theoretical foundations, computational properties, and practical implementations in contemporary deep learning systems. Applications in natural language processing, computer vision, and multimodal learning demonstrate the versatility of attention mechanisms. We examine language modeling with autoregressive transformers, bidirectional encoders for representation learning, sequence-to-sequence translation, Vision Transformers for image classification, and cross-modal attention for vision-language tasks. Empirical analysis reveals training characteristics, scaling laws that relate performance to model size and computation, attention pattern visualizations, and performance benchmarks across standard datasets. We discuss the interpretability of learned attention patterns and their relationship to linguistic and visual structures. The monograph concludes with a critical examination of current limitations, including computational scalability, data efficiency, systematic generalization, and interpretability challenges.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.03329" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.03329" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.03329" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">25. DeepWeightFlow: Re-Basined Flow Matching for Generating Neural Network Weights
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Saumya Gupta, Scott Biggs, Moritz Laber, Zohair Shafi, Robin Walters, Ayan Paul</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> DeepWeightFlow, a Flow Matching model utilizing canonicalization techniques, efficiently generates complete, high-accuracy, and diverse neural network weights directly in weight space for various architectures, eliminating the need for post-generation fine-tuning and significantly outperforming diffusion methods in speed. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Building efficient and effective generative models for neural network weights has been a research focus of significant interest that faces challenges posed by the high-dimensional weight spaces of modern neural networks and their symmetries. Several prior generative models are limited to generating partial neural network weights, particularly for larger models, such as ResNet and ViT. Those that do generate complete weights struggle with generation speed or require finetuning of the generated models. In this work, we present DeepWeightFlow, a Flow Matching model that operates directly in weight space to generate diverse and high-accuracy neural network weights for a variety of architectures, neural network sizes, and data modalities. The neural networks generated by DeepWeightFlow do not require fine-tuning to perform well and can scale to large networks. We apply Git Re-Basin and TransFusion for neural network canonicalization in the context of generative weight models to account for the impact of neural network permutation symmetries and to improve generation efficiency for larger model sizes. The generated networks excel at transfer learning, and ensembles of hundreds of neural networks can be generated in minutes, far exceeding the efficiency of diffusion-based methods. DeepWeightFlow models pave the way for more efficient and scalable generation of diverse sets of neural networks.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.05052" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.05052" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.05052" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">26. Bayesian Subspace Gradient Estimation for Zeroth-Order Optimization of Large Language Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jian Feng, Zhihong Huang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Bayesian Subspace Zeroth-Order optimization (BSZO) enhances LLM fine-tuning efficiency by applying Kalman filtering to synthesize gradient information from multiple perturbation directions, resulting in improved convergence rates and superior performance compared to standard zeroth-order optimizers while maintaining minimal memory overhead. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Fine-tuning large language models (LLMs) with zeroth-order (ZO) optimization reduces memory by approximating gradients through function evaluations, but existing methods rely on one-step gradient estimates from random perturbations. We introduce Bayesian Subspace Zeroth-Order optimization (BSZO), a ZO optimizer that applies Kalman filtering to combine finite-difference information across multiple perturbation directions. By treating each finite-difference measurement as a noisy observation, BSZO builds a posterior distribution over the projected gradient and updates it through Bayesian inference, with a residual-based adaptive mechanism to adjust perturbation scales. Theoretical analysis shows that BSZO improves the convergence rate by a factor of $k/γ$ compared to standard ZO methods. Experiments on RoBERTa, Mistral, and OPT models show that BSZO outperforms MeZO, MeZO-Adam, and HiZOO across various tasks, achieving up to 6.67\% absolute average improvement on OPT-13B while keeping memory usage close to inference-only baselines (1.00$\times$--1.08$\times$ of MeZO).</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.01452" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.01452" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.01452" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">27. Logic Tensor Network-Enhanced Generative Adversarial Network
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Nijesh Upreti, Vaishak Belle</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The Logic Tensor Network-Enhanced Generative Adversarial Network (LTN-GAN) framework successfully integrates Logic Tensor Networks to enforce domain-specific first-order logical constraints during data synthesis, significantly improving the logical consistency and diversity of generated samples over traditional GANs. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">In this paper, we introduce Logic Tensor Network-Enhanced Generative Adversarial Network (LTN-GAN), a novel framework that enhances Generative Adversarial Networks (GANs) by incorporating Logic Tensor Networks (LTNs) to enforce domain-specific logical constraints during the sample generation process. Although GANs have shown remarkable success in generating realistic data, they often lack mechanisms to incorporate prior knowledge or enforce logical consistency, limiting their applicability in domains requiring rule adherence. LTNs provide a principled way to integrate first-order logic with neural networks, enabling models to reason over and satisfy logical constraints. By combining the strengths of GANs for realistic data synthesis with LTNs for logical reasoning, we gain valuable insights into how logical constraints influence the generative process while improving both the diversity and logical consistency of the generated samples. We evaluate LTN-GAN across multiple datasets, including synthetic datasets (gaussian, grid, rings) and the MNIST dataset, demonstrating that our model significantly outperforms traditional GANs in terms of adherence to predefined logical constraints while maintaining the quality and diversity of generated samples. This work highlights the potential of neuro-symbolic approaches to enhance generative modeling in knowledge-intensive domains.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.03839" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.03839" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.03839" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">28. Output Embedding Centering for Stable LLM Pretraining
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Felix Stollenwerk, Anna Lokrantz, Niclas Hertzberg</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Output Embedding Centering (OEC), implemented as either μ-centering or μ-loss, effectively suppresses output logit divergence instability in large language model training by addressing the geometry of output embeddings, proving superior to the conventional z-loss in stability and tolerance to large learning rates. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Pretraining of large language models is not only expensive but also prone to certain training instabilities. A specific instability that often occurs for large learning rates at the end of training is output logit divergence. The most widely used mitigation strategy, z-loss, merely addresses the symptoms rather than the underlying cause of the problem. In this paper, we analyze the instability from the perspective of the output embeddings&#39; geometry and identify its cause. Based on this, we propose output embedding centering (OEC) as a new mitigation strategy, and prove that it suppresses output logit divergence. OEC can be implemented in two different ways, as a deterministic operation called μ-centering, or a regularization method called μ-loss. Our experiments show that both variants outperform z-loss in terms of training stability and learning rate sensitivity. In particular, they ensure that training converges even for large learning rates when z-loss fails. Furthermore, we find that μ-loss is significantly less sensitive to regularization hyperparameter tuning than z-loss.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.02031" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.02031" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.02031" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">29. BEACON: JWST NIRCam Pure-parallel Imaging Survey. II. Physical Properties of $z=7-14$ Galaxies
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Yechi Zhang, Takahiro Morishita, Kimi C. Kreilgaard, Charlotte A. Mason, Abdurro&#39;uf, Hakim Atek, Marusa Bradac, Larry D. Bradley, Andrew J. Bunker, Viola Gelli, et al.</span>
                                <span class="author-full" style="display: none;">Yechi Zhang, Takahiro Morishita, Kimi C. Kreilgaard, Charlotte A. Mason, Abdurro&#39;uf, Hakim Atek, Marusa Bradac, Larry D. Bradley, Andrew J. Bunker, Viola Gelli, Novan Saputra Haryana, Matthew J. Hayes, George Helou, Nicha Leethochawalit, Zhaoran Liu, Marc Rafelski, Guido Roberts-Borsani, Michael Rutkowski, Claudia Scarlata, Massimo Stiavelli, Ryo A. Sutanto, Harry Teplitz, Tommaso Treu, Michele Trenti, Benedetta Vulcani, Xin Wang</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Photometric analysis of 161 high-redshift galaxy candidates (z=7-14) from the JWST BEACON DR2 reveals a remarkably bright galaxy at z=13.71 and indicates that environmental differences in galaxy properties are not yet significant, possibly due to limited sample size or the lack of widespread accelerated evolution at these epochs. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present photometric properties of 161 galaxy candidates at $z=7-14$ selected from the second data release (DR2) of BEACON, a JWST Cycle 2 pure-parallel NIRCam imaging program. Carefully selected from 36 independent pointings (corresponding to $\sim350$\,arcmin$^2$ sky coverage), and hence with reduced cosmic variance, our galaxy candidates provide an unbiased sample for investigating galaxy properties over a wide range of environments. We measure the physical properties, including UV continuum slope ($β_{\rm UV}$), stellar mass ($M_*$), star formation rate (SFR), and sizes. Our highest redshift galaxy candidate at $z=13.71\pm0.15$ has a remarkably bright UV luminosity of $M_{\rm UV}=-21.19\pm0.08$, making it the brightest galaxy at $z&gt;12$ if spectroscopically confirmed. With an extremely blue UV slope, compact morphology, and high star formation rate surface density ($Σ_{\rm SFR}$), this candidate may have extremely low metallicity, high ionizing photon escape fraction, or contributions from an AGN. Among our multiple independent sightlines, we identify three fields of galaxy number overdensity with $&gt;3σ$ significance. The properties of galaxies in various environments do not exhibit significant differences, implying either that accelerated galaxy evolution in overdense regions is not yet widespread at $z&gt;7$, or that the current constraints are limited by sample size. Our simulations indicate that increasing the sample by an order of magnitude would allow such environmental trends to be robustly confirmed or ruled out, underscoring the importance of future pure-parallel observations.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.02861" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.02861" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.02861" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmic Shear, Galaxy Clustering, Survey Analysis / Galaxy-Halo Connection, Lensing, Clustering</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">30. Personalizing black-box models for nonparametric regression with minimax optimality
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Sai Li, Linjun Zhang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A theoretical framework for few-shot personalization in nonparametric regression establishes the minimax optimal rate and proposes algorithms that effectively integrate black-box pre-trained models, providing significant statistical benefits and robustness guarantees when adapting to target domains with limited samples. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Recent advances in large-scale models, including deep neural networks and large language models, have substantially improved performance across a wide range of learning tasks. The widespread availability of such pre-trained models creates new opportunities for data-efficient statistical learning, provided they can be effectively integrated into downstream tasks. Motivated by this setting, we study few-shot personalization, where a pre-trained black-box model is adapted to a target domain using a limited number of samples. We develop a theoretical framework for few-shot personalization in nonparametric regression and propose algorithms that can incorporate a black-box pre-trained model into the regression procedure. We establish the minimax optimal rate for the personalization problem and show that the proposed method attains this rate. Our results clarify the statistical benefits of leveraging pre-trained models under sample scarcity and provide robustness guarantees when the pre-trained model is not informative. We illustrate the finite-sample performance of the methods through simulations and an application to the California housing dataset with several pre-trained models.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.01432" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.01432" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.01432" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">stat.ME</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">31. Binary neutron star mergers with SPHINCS_BSSN: temperature-dependent equations of state and damping of constraint violations
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Bhaskar Biswas, Stephan Rosswog, Peter Diener, Lukas Schnabel</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Enhancements to the SPHINCS_BSSN numerical relativity code, including constraint damping and Fermi liquid thermal physics, substantially reduce constraint violations and show that Dirac effective mass parametrization shifts the dominant post-merger gravitational wave frequency by approximately 150 Hz. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Neutron star mergers hold the key to several grand challenges of contemporary (astro-)physics. In view of the upcoming next generation of ground-based detectors, it is crucial to keep improving theoretical predictions to harvest the full scientific returns from these investments. We introduce here a substantial update of our Lagrangian numerical relativity code SPHINCS_BSSN. Apart from changing our unit system, we add constraint damping terms to the BSSN spacetime evolution equations. We demonstrate that this measure reduces, without noteworthy computational cost, the Hamiltonian constraint violations by more than an order of magnitude. We further implement contributions to thermal energy and pressure that are based on Fermi liquid theory and contain a parametrization of the Dirac effective mass. These terms can be combined with any cold equation of state, and they enhance the physical realism of our simulations and introduce a physics-based concept of a temperature. In a set of merger simulations, we demonstrate good agreement with other temperature-dependent numerical relativity simulations. We find that different parametrizations of the Dirac effective mass can translate into shifts of $\sim 150$ Hz in the dominant post-merger gravitational wave peak frequency.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.01402" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.01402" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.01402" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.HE</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">32. Decentralized Autoregressive Generation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Stepan Maschan, Haoxuan Qu, Jun Liu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The Decentralized Discrete Flow Matching objective, which models probability generating velocity via expert flows, theoretically analyzes decentralization in autoregressive generation and confirms its empirical equivalence to centralized training for multimodal language models. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present a theoretical analysis of decentralization of autoregressive generation. We define the Decentralized Discrete Flow Matching objective, by expressing probability generating velocity as a linear combination of expert flows. We also conduct experiments demonstrating the equivalence between decentralized and centralized training settings for multimodal language models across diverse set of benchmarks. Specifically, we compare two distinct paradigms: LLaVA and InternVL 2.5-1B, which uses a fixed CLIP vision encoder and performs full-parameter fine-tuning (ViT+MLP+LLM) during the instruction tuning stage.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.03184" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.03184" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.03184" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">33. Aligned explanations in neural networks
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Corentin Lobet, Francesca Chiaromonte</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The PiNets modeling framework achieves explanatory alignment by implementing pseudo-linear networks that produce linearly readable, instance-wise linear predictions, thereby generating faithful explanations for deep neural network tasks like image classification and segmentation. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Feature attribution is the dominant paradigm for explaining deep neural networks. However, most existing methods only loosely reflect the model&#39;s prediction-making process, thereby merely white-painting the black box. We argue that explanatory alignment is a key aspect of trustworthiness in prediction tasks: explanations must be directly linked to predictions, rather than serving as post-hoc rationalizations. We present model readability as a design principle enabling alignment, and PiNets as a modeling framework to pursue it in a deep learning context. PiNets are pseudo-linear networks that produce instance-wise linear predictions in an arbitrary feature space, making them linearly readable. We illustrate their use on image classification and segmentation tasks, demonstrating how PiNets produce explanations that are faithful across multiple criteria in addition to alignment.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.04378" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.04378" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.04378" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">34. Probing Trans-Planckian Signatures in the Early Universe: A Bayesian Analysis of the Generalized Sasaki-Mukhanov Equation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Mahdieh Eskandari Merajin</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A generalized inflationary perturbation theory, solving the modified Sasaki-Mukhanov equation analytically, uses CMB data to place stringent constraints ($|f| \le 10^{-4}$) on the modification parameter while showing intriguing potential to alleviate the persistent low-quadrupole anomaly compared to $\Lambda$CDM. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present a rigorous and comprehensive investigation of a generalized inflationary perturbation theory designed to address persistent large-scale anomalies in the Cosmic Microwave Background (CMB). Motivated by the Trans-Planckian problem and potential non-canonical dynamics in the early Universe, we introduce a generalized Sasaki-Mukhanov equation characterized by a time-dependent correction term, parameterized by a coupling constant f. Unlike the standard slow-roll approximation, we derive the exact analytical solutions for the mode functions in terms of Whittaker functions, ensuring a precise treatment of the mode evolution across the horizon. We compute the resulting primordial scalar power spectrum, which exhibits scale-dependent oscillatory modulations and a distinct suppression of power at low multipoles. We numerically implement this modified framework within the Cobaya Bayesian inference engine. Utilizing the latest Planck 2018 temperature and polarization likelihoods combined with high-resolution data from the Atacama Cosmology Telescope (ACT) DR6, we perform a robust Monte Carlo Markov Chain (MCMC) analysis. Our results place stringent constraints on the modification parameter, |f| &lt;= 10^-4, at a 95% confidence level. However, we find intriguing hints that the generalized model provides a better fit to the low-l CMB spectrum compared to the standard LambdaCDM model, effectively alleviating the low-quadrupole anomaly without compromising the fit at smaller scales. We discuss the implications of these findings for the energy scale of inflation and the validity of the effective field theory description during the inflationary epoch.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.04760" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.04760" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.04760" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">35. Routing by Analogy: kNN-Augmented Expert Assignment for Mixture-of-Experts
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Boxuan Lyu, Soichiro Murakami, Hidetaka Kamigaito, Peinan Zhang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> kNN-MoE introduces a retrieval-augmented routing framework for Mixture-of-Experts models that mitigates distribution shift brittleness by reusing optimal expert assignments from memory, using aggregate neighbor similarity as a confidence coefficient to outperform zero-shot baselines. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Mixture-of-Experts (MoE) architectures scale large language models efficiently by employing a parametric &#34;router&#34; to dispatch tokens to a sparse subset of experts. Typically, this router is trained once and then frozen, rendering routing decisions brittle under distribution shifts. We address this limitation by introducing kNN-MoE, a retrieval-augmented routing framework that reuses optimal expert assignments from a memory of similar past cases. This memory is constructed offline by directly optimizing token-wise routing logits to maximize the likelihood on a reference set. Crucially, we use the aggregate similarity of retrieved neighbors as a confidence-driven mixing coefficient, thus allowing the method to fall back to the frozen router when no relevant cases are found. Experiments show kNN-MoE outperforms zero-shot baselines and rivals computationally expensive supervised fine-tuning.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.02144" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.02144" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.02144" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CL</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">36. Normalized Conditional Mutual Information Surrogate Loss for Deep Neural Classifiers
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Linfeng Ye, Zhixiang Chi, Konstantinos N. Plataniotis, En-hui Yang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Normalized conditional mutual information (NCMI) is introduced as an efficient, alternating-algorithm surrogate loss for deep neural network classifiers, consistently surpassing cross-entropy performance by substantial margins across image recognition and whole-slide imaging benchmarks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">In this paper, we propose a novel information theoretic surrogate loss; normalized conditional mutual information (NCMI); as a drop in alternative to the de facto cross-entropy (CE) for training deep neural network (DNN) based classifiers. We first observe that the model&#39;s NCMI is inversely proportional to its accuracy. Building on this insight, we introduce an alternating algorithm to efficiently minimize the NCMI. Across image recognition and whole-slide imaging (WSI) subtyping benchmarks, NCMI-trained models surpass state of the art losses by substantial margins at a computational cost comparable to that of CE. Notably, on ImageNet, NCMI yields a 2.77% top-1 accuracy improvement with ResNet-50 comparing to the CE; on CAMELYON-17, replacing CE with NCMI improves the macro-F1 by 8.6% over the strongest baseline. Gains are consistent across various architectures and batch sizes, suggesting that NCMI is a practical and competitive alternative to CE.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.02543" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.02543" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.02543" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">37. Causally-Aware Information Bottleneck for Domain Adaptation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Mohammad Ali Javidian</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Causal domain adaptation, focused on imputing a missing target variable in the target domain, is solved by learning a mechanism-stable representation via the Information Bottleneck, yielding a closed-form Gaussian solution for linear models and a scalable Variational Information Bottleneck for nonlinear data. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We tackle a common domain adaptation setting in causal systems. In this setting, the target variable is observed in the source domain but is entirely missing in the target domain. We aim to impute the target variable in the target domain from the remaining observed variables under various shifts. We frame this as learning a compact, mechanism-stable representation. This representation preserves information relevant for predicting the target while discarding spurious variation. For linear Gaussian causal models, we derive a closed-form Gaussian Information Bottleneck (GIB) solution. This solution reduces to a canonical correlation analysis (CCA)-style projection and offers Directed Acyclic Graph (DAG)-aware options when desired. For nonlinear or non-Gaussian data, we introduce a Variational Information Bottleneck (VIB) encoder-predictor. This approach scales to high dimensions and can be trained on source data and deployed zero-shot to the target domain. Across synthetic and real datasets, our approach consistently attains accurate imputations, supporting practical use in high-dimensional causal models and furnishing a unified, lightweight toolkit for causal domain adaptation.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.04361" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.04361" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.04361" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">38. How to Set the Learning Rate for Large-Scale Pre-training?
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yunhua Zhou, Shuhao Xing, Junhao Huang, Xipeng Qiu, Qipeng Guo</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A systematic investigation into optimal learning rate configuration for large-scale pre-training introduces a search factor Scaling Law and extends $\mu$Transfer to Mixture of Experts architectures, ultimately concluding that $\mu$Transfer&#39;s scalability is challenged in high-scale MoE environments. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Optimal configuration of the learning rate (LR) is a fundamental yet formidable challenge in large-scale pre-training. Given the stringent trade-off between training costs and model performance, the pivotal question is whether the optimal LR can be accurately extrapolated from low-cost experiments. In this paper, we formalize this investigation into two distinct research paradigms: Fitting and Transfer. Within the Fitting Paradigm, we innovatively introduce a Scaling Law for search factor, effectively reducing the search complexity from O(n^3) to O(n*C_D*C_η) via predictive modeling. Within the Transfer Paradigm, we extend the principles of $μ$Transfer to the Mixture of Experts (MoE) architecture, broadening its applicability to encompass model depth, weight decay, and token horizons. By pushing the boundaries of existing hyperparameter research in terms of scale, we conduct a comprehensive comparison between these two paradigms. Our empirical results challenge the scalability of the widely adopted $μ$ Transfer in large-scale pre-training scenarios. Furthermore, we provide a rigorous analysis through the dual lenses of training stability and feature learning to elucidate the underlying reasons why module-wise parameter tuning underperforms in large-scale settings. This work offers systematic practical guidelines and a fresh theoretical perspective for optimizing industrial-level pre-training.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.05049" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.05049" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.05049" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">39. Sommerfeld Effect and Bound State Formation for Dark Matter Models with Colored Mediators with SE+BSF4DM
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Mathias Becker, Emanuele Copello, Julia Harz, Martin Napetschnig</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Relic abundance calculations for simplified $t$-channel dark matter models show that incorporating Sommerfeld effects and bound state formation significantly enhances mediator annihilation near mass degeneracy, leading to order one corrections in the dark matter mass constraints within the coannihilation regime. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">In the universal framework of simplified $t$-channel dark matter models, the calculation of the relic abundance can be dominated by mediator annihilation when the dark matter and mediator masses are almost degenerate. We analyze four representative models with scalar and fermionic mediators, confront them with direct detection limits and highlight the differences and common features between them. The mediator annihilations are considerably enhanced by the Sommerfeld effect and bound state formation. Albeit their effect is subdominant in the coannihilation regime, excited bound state levels are included as well. We find that Sommerfeld and bound-state effects can lead to order one corrections to the constraints on the DM mass in the coannihilating regime, with the precise magnitude depending on the specific model realization. In addition we provide SE+BSF4DM, an intuitive and easy to use add-on to micrOMEGAs, allowing for an automated inclusion of these effects for a generic $t$-channel Dark Matter Model, which is publicly available on Github.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.03026" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.03026" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.03026" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">hep-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">40. Addicted to Flavour: 1976-2026
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Andrzej J. Buras</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A retrospective on five decades of Flavour Physics emphasizes that discovering new particle physics phenomena requires precise theoretical calculations, including NLO/NNLO QCD, and strategic analysis focusing on correlations among observables rather than relying exclusively on global fits. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">I describe my activities in Flavour Physics from 1976 to 2026. However, this 50th anniversary is not the only motivation for this writing. The second reason is the 350th anniversary of the discovery of the first animalcula by van Leeuvanhoek in 1676. Flavour physics makes it possible to search for new animalcula at distance scales far shorter than those resolved by van Leeuwenhoek in 1676 and even shorter than those directly accessible at the Large Hadron Collider. Achieving this goal requires not only precise measurements of a wide variety of processes, but also equally precise theoretical calculations, both within the Standard Model (SM) and beyond it. In this respect, next-to-leading-order (NLO) and next-to-next-to-leading-order (NNLO) QCD calculations of various Wilson coefficients in the SM and beyond it, in which I was involved for two decades, as well as reliable treatments of non-perturbative QCD effects, are indispensable. Equally important is the proper choice of observables that are best suited to revealing these new animalcula of particle physics. Moreover, in my view it is crucial to develop strategies for the search for New Physics (NP) that go beyond the global fits that are very popular today. While effective field theories such as WET and SMEFT are formulated in terms of Wilson coefficients of the relevant operators, with correlations characteristic of the SM and of specific NP scenarios, the most direct tests of the SM and its extensions are, in my opinion, correlations among different observables that are characteristic of particular new animalcula at work. Numerous colourful plots in this article illustrate this point. I hope that these ideas are clearly conveyed in my Flavour Autobiography, which also includes my memories of many conferences, workshops, and schools, as well as related anecdotes that are not always directly connected to physics.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.03722" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.03722" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.03722" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">hep-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">41. From Mice to Trains: Amortized Bayesian Inference on Graph Data
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Svenja Jedhoff, Elizaveta Semenova, Aura Raulo, Anne Meyer, Paul-Christian Bürkner</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Amortized Bayesian Inference, adapted with permutation-invariant graph encoders and flexible neural posterior estimators, provides a scalable, likelihood-free framework for performing fast inference on parameters within complex graph-structured data. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Graphs arise across diverse domains, from biology and chemistry to social and information networks, as well as in transportation and logistics. Inference on graph-structured data requires methods that are permutation-invariant, scalable across varying sizes and sparsities, and capable of capturing complex long-range dependencies, making posterior estimation on graph parameters particularly challenging. Amortized Bayesian Inference (ABI) is a simulation-based framework that employs generative neural networks to enable fast, likelihood-free posterior inference. We adapt ABI to graph data to address these challenges to perform inference on node-, edge-, and graph-level parameters. Our approach couples permutation-invariant graph encoders with flexible neural posterior estimators in a two-module pipeline: a summary network maps attributed graphs to fixed-length representations, and an inference network approximates the posterior over parameters. In this setting, several neural architectures can serve as the summary network. In this work we evaluate multiple architectures and assess their performance on controlled synthetic settings and two real-world domains - biology and logistics - in terms of recovery and calibration.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.02241" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.02241" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.02241" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">42. Optimization of Deep Learning Models for Radio Galaxy Classification
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Philipp Denzel, Manuel Weiss, Elena Gavagnin, Frank-Peter Schilling</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Pretrained, standard deep learning architectures, including CNNs and Transformers, can achieve high-accuracy radio galaxy classification comparable to customized models by adapting input data through channel transformations, with ensemble analysis further improving robustness for future large-scale surveys like SKAO. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Modern radio telescope surveys, capable of detecting billions of galaxies in wide-field surveys, have made manual morphological classification impracticable. This applies in particular when the Square Kilometre Array Observatory (SKAO) becomes operable in 2027, which is expected to close an important gap in our understanding of the Epoch of Reionization (EoR) and other areas of astrophysics. To this end, foreground objects, contaminants of the 21-cm signal, need to be identified and subtracted. Source finding and identification is thus an important albeit challenging task. We investigate the ability of AI and deep learning (DL) methods that have been previously trained on other data domains to localize and classify radio galaxies with minimal changes to their architectures. Various well-known pretrained neural network architectures for image classification and object detection are trained and fine-tuned and their performance is evaluated on a public radio galaxy dataset derived from the Radio Galaxy Zoo. A comparison between convolutional neural network (CNN)- and transformer-based algorithms is performed. The best performing architecture is systematically optimized and an uncertainty estimation is performed by means of an ensemble analysis. Radio source classification performance nearly comparable to the current leading customized models can be obtained using existing standard pretrained DL architectures, without modification and increase in complexity of the model architectures but rather adaptation of the data, by combining various transformations on replicated image channels. Using an ensemble of models can also further improve performance to over 90% accuracy, on par with top-performing models in the literature. The results can be transferred to other survey data, e.g. from the Murchison Wide-field Array (MWA), and in the future be used to study the EoR with the SKAO.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.04773" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.04773" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.04773" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">43. Lil: Less is Less When Applying Post-Training Sparse-Attention Algorithms in Long-Decode Stage
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Junhao Hu, Fangze Li, Mingtao Xu, Feifan Meng, Shiju Zhao, Tiancheng Hu, Ting Peng, Anmin Liu, Wenrui Huang, Chenxu Liu, et al.</span>
                                <span class="author-full" style="display: none;">Junhao Hu, Fangze Li, Mingtao Xu, Feifan Meng, Shiju Zhao, Tiancheng Hu, Ting Peng, Anmin Liu, Wenrui Huang, Chenxu Liu, Ziyue Hua, Tao Xie</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Sparse attention in large language models often leads to the &#34;Less is Less&#34; phenomenon, where information loss increases sequence length and overall complexity, a problem addressed by an early-stopping algorithm that reduces token consumption by up to 90% with minimal accuracy degradation. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Large language models (LLMs) demonstrate strong capabilities across a wide range of complex tasks and are increasingly deployed at scale, placing significant demands on inference efficiency. Prior work typically decomposes inference into prefill and decode stages, with the decode stage dominating total latency. To reduce time and memory complexity in the decode stage, a line of work introduces sparse-attention algorithms. In this paper, we show, both empirically and theoretically, that sparse attention can paradoxically increase end-to-end complexity: information loss often induces significantly longer sequences, a phenomenon we term ``Less is Less&#39;&#39; (Lil). To mitigate the Lil problem, we propose an early-stopping algorithm that detects the threshold where information loss exceeds information gain during sparse decoding. Our early-stopping algorithm reduces token consumption by up to 90% with a marginal accuracy degradation of less than 2% across reasoning-intensive benchmarks.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.03043" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.03043" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.03043" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CL</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">44. Investigating Knowledge Distillation Through Neural Networks for Protein Binding Affinity Prediction
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Wajid Arshad Abbasi, Syed Ali Abbas, Maryum Bibi, Saiqa Andleeb, Muhammad Naveed Akhtar</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Knowledge distillation effectively transfers structural information from a structure-based teacher model to a sequence-only student model, substantially improving the prediction accuracy of protein-protein binding affinity and narrowing the performance gap between sequence- and structure-dependent methods. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The trade-off between predictive accuracy and data availability makes it difficult to predict protein--protein binding affinity accurately. The lack of experimentally resolved protein structures limits the performance of structure-based machine learning models, which generally outperform sequence-based methods. In order to overcome this constraint, we suggest a regression framework based on knowledge distillation that uses protein structural data during training and only needs sequence data during inference. The suggested method uses binding affinity labels and intermediate feature representations to jointly supervise the training of a sequence-based student network under the guidance of a structure-informed teacher network. Leave-One-Complex-Out (LOCO) cross-validation was used to assess the framework on a non-redundant protein--protein binding affinity benchmark dataset. A maximum Pearson correlation coefficient (P_r) of 0.375 and an RMSE of 2.712 kcal/mol were obtained by sequence-only baseline models, whereas a P_r of 0.512 and an RMSE of 2.445 kcal/mol were obtained by structure-based models. With a P_r of 0.481 and an RMSE of 2.488 kcal/mol, the distillation-based student model greatly enhanced sequence-only performance. Improved agreement and decreased bias were further confirmed by thorough error analyses. With the potential to close the performance gap between sequence-based and structure-based models as larger datasets become available, these findings show that knowledge distillation is an efficient method for transferring structural knowledge to sequence-based predictors. The source code for running inference with the proposed distillation-based binding affinity predictor can be accessed at https://github.com/wajidarshad/ProteinAffinityKD.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.03704" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.03704" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.03704" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">45. On the Hidden Objective Biases of Group-based Reinforcement Learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Aleksandar Fontana, Marco Simoni, Giulio Rossolini, Andrea Saracino, Paolo Mori</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Theoretical analysis of Group Relative Policy Optimization (GRPO) methods used for post-training large language models identifies critical structural mismatches, including systematic gradient biases from non-uniform group weighting and training dynamics largely insensitive to reward scaling due to the AdamW optimizer. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Group-based reinforcement learning methods, like Group Relative Policy Optimization (GRPO), are widely used nowadays to post-train large language models. Despite their empirical success, they exhibit structural mismatches between reward optimization and the underlying training objective. In this paper, we present a theoretical analysis of GRPO style methods by studying them within a unified surrogate formulation. This perspective reveals recurring properties that affect all the methods under analysis: (i) non-uniform group weighting induces systematic gradient biases on shared prefix tokens; (ii) interactions with the AdamW optimizer make training dynamics largely insensitive to reward scaling; and (iii) optimizer momentum can push policy updates beyond the intended clipping region under repeated optimization steps. We believe that these findings highlight fundamental limitations of current approaches and provide principled guidance for the design of future formulations.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.05002" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.05002" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.05002" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">46. LinMU: Multimodal Understanding Made Linear
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Hongjie Wang, Niraj K. Jha</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The LinMU architecture achieves linear complexity in Vision-Language Models by replacing quadratic self-attention with the M-MATE block—a combination of a state-space model and localized window attention—thereby matching teacher model performance while drastically improving inference speed for high-resolution and long-context data. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Modern Vision-Language Models (VLMs) achieve impressive performance but are limited by the quadratic complexity of self-attention, which prevents their deployment on edge devices and makes their understanding of high-resolution images and long-context videos prohibitively expensive. To address this challenge, we introduce LinMU (Linear-complexity Multimodal Understanding), a VLM design that achieves linear complexity without using any quadratic-complexity modules while maintaining the performance of global-attention-based VLMs. LinMU replaces every self-attention layer in the VLM with the M-MATE block: a dual-branch module that combines a bidirectional state-space model for global context (Flex-MA branch) with localized Swin-style window attention (Local-Swin branch) for adjacent correlations. To transform a pre-trained VLM into the LinMU architecture, we propose a three-stage distillation framework that (i) initializes both branches with self-attention weights and trains the Flex-MA branch alone, (ii) unfreezes the Local-Swin branch and fine-tunes it jointly with the Flex-MA branch, and (iii) unfreezes the remaining blocks and fine-tunes them using LoRA adapters, while regressing on hidden states and token-level logits of the frozen VLM teacher. On MMMU, TextVQA, LongVideoBench, Video-MME, and other benchmarks, LinMU matches the performance of teacher models, yet reduces Time-To-First-Token (TTFT) by up to 2.7$\times$ and improves token throughput by up to 9.0$\times$ on minute-length videos. Ablations confirm the importance of each distillation stage and the necessity of the two branches of the M-MATE block. The proposed framework demonstrates that state-of-the-art multimodal reasoning can be achieved without quadratic attention, thus opening up avenues for long-context VLMs that can deal with high-resolution images and long videos.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.01322" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.01322" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.01322" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">47. Mitigating Simulator Dependence in AI Parameter Inference for the Epoch of Reionization: The Importance of Simulation Diversity
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jasper Solt, Jonathan C. Pober, Stephen H. Bach</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Training AI models for Epoch of Reionization parameter inference using diverse data aggregated from multiple cosmological simulators enhances model robustness and generalization, effectively mitigating simulator-specific biases when predicting on unseen simulation data. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The 21cm signal of neutral hydrogen contains a wealth of information about the poorly constrained era of cosmological history, the Epoch of Reionization (EoR). Recently, AI models trained on EoR simulations have gained significant attention as a powerful and flexible option for inferring parameters from 21cm observations. However, previous works show that AI models trained on data from one simulator fail to generalize to data from another, raising doubts about AI models&#39; ability to accurately infer parameters from observation. We develop a new strategy for training AI models on cosmological simulations based on the principle that increasing the diversity of the training dataset improves model robustness by averaging out spurious and contradictory information. We train AI models on data from different combinations of four simulators, then compare the models&#39; performance when predicting on data from held-out simulators acting as proxies for the real universe. We find that models trained on data from multiple simulators perform better on data from a held-out simulator than models trained on data from a single simulator, indicating that increasing the diversity of the training dataset improves a model&#39;s ability to generalize. This result suggests that future EoR parameter inference methods can mitigate simulator-specific bias by incorporating multiple simulation approaches into their analyses.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.05229" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.05229" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.05229" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">48. A refined model of secondary photon emission from heavy WIMP annihilations in the Galactic Centre
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Rajat Shinde, Julia Djuvsland, Davide Dapaoli, Jim Hinton</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Incorporating a realistic, spatially-dependent model of secondary inverse Compton emission from heavy WIMP annihilation products in the Galactic Centre significantly enhances the predicted gamma-ray signal, confirming its necessity for future indirect dark matter detection efforts. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Heavy Weakly Interacting Massive Particles (WIMPs) remain a prominent yet less constrained dark matter (DM) candidate, with the Galactic Centre (GC) serving as a prime target for indirect detection via gamma-ray signals. Extending our previous work that highlighted the significance of secondary inverse Compton (IC) emission from annihilation-produced electrons, we expand the analysis to a broader range of WIMP masses and introduce a more realistic spatially-dependent modelling framework for the GC environment. This approach incorporates complexities such as the three-dimensional DM distribution, spatially varying radiation and magnetic fields, and electron transport mechanisms like Galactic winds and diffusion. We assess the impact of these environmental factors on both the spatial and spectral characteristics of the resulting secondary emissions. Our results demonstrate the robustness and necessity of incorporating this emission, and highlight its role in enhancing the prospects for detecting heavy WIMPs through observations of the inner Galaxy. We provide the resulting data products to the community to support future analyses and observational studies.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.01397" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.01397" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.01397" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.HE</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">49. Foreground-Aware Dataset Distillation via Dynamic Patch Selection
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Longzhen Li, Guang Li, Ren Togo, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A novel foreground-aware dataset distillation method utilizes foreground occupancy detection to dynamically select informative image patches, resulting in more representative synthetic datasets that consistently improve distillation performance and architectural generalization across various benchmarks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">In this paper, we propose a foreground-aware dataset distillation method that enhances patch selection in a content-adaptive manner. With the rising computational cost of training large-scale deep models, dataset distillation has emerged as a promising approach for constructing compact synthetic datasets that retain the knowledge of their large original counterparts. However, traditional optimization-based methods often suffer from high computational overhead, memory constraints, and the generation of unrealistic, noise-like images with limited architectural generalization. Recent non-optimization methods alleviate some of these issues by constructing distilled data from real image patches, but the used rigid patch selection strategies can still discard critical information about the main objects. To solve this problem, we first leverage Grounded SAM2 to identify foreground objects and compute per-image foreground occupancy, from which we derive a category-wise patch decision threshold. Guided by these thresholds, we design a dynamic patch selection strategy that, for each image, either selects the most informative patch from multiple candidates or directly resizes the full image when the foreground dominates. This dual-path mechanism preserves more key information about the main objects while reducing redundant background content. Extensive experiments on multiple benchmarks show that the proposed method consistently improves distillation performance over existing approaches, producing more informative and representative distilled datasets and enhancing robustness across different architectures and image compositions.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.02727" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.02727" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.02727" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">50. Exponential capacity scaling of classical GANs compared to hybrid latent style-based quantum GANs
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Milan Liepelt, Julien Baglio</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Comprehensive experimental analysis of the hybrid latent style-based Quantum Generative Adversarial Network architecture reveals an exponential advantage in capacity scaling for the quantum generator, suggesting a significant quantum advantage in generative modeling efficiency compared to classical counterparts. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Quantum generative modeling is a very active area of research in looking for practical advantage in data analysis. Quantum generative adversarial networks (QGANs) are leading candidates for quantum generative modeling and have been applied to diverse areas, from high-energy physics to image generation. The latent style-based QGAN, relying on a classical variational autoencoder to encode the input data into a latent space and then using a style-based QGAN for data generation has been proven to be efficient for image generation or drug design, hinting at the use of far less trainable parameters than their classical counterpart to achieve comparable performance, however this advantage has never been systematically studied. We present in this work the first comprehensive experimental analysis of this advantage of QGANS applied to SAT4 image generation, obtaining an exponential advantage in capacity scaling for a quantum generator in the hybrid latent style-based QGAN architecture. Careful tuning of the autoencoder is crucial to obtain stable, reliable results. Once this tuning is performed and defining training optimality as when the training is stable and the FID score is low and stable as well, the optimal capacity (or number of trainable parameters) of the classical discriminator scales exponentially with respect to the capacity of the quantum generator, and the same is true for the capacity of the classical generator. This hints toward a type of quantum advantage for quantum generative modeling.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.05036" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.05036" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.05036" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">quant-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Sampling Methods / Advanced Neural Models, Representation Learning</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
    </div>

    <footer class="footer">
        &copy; 2026 arXiv Daily · Powered by <a href="https://arxiv.org/" target="_blank">arXiv</a>
    </footer>
</body>
</html>