<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>arXiv Daily: Backfill (2025-10-26 to 2025-11-02)</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto:400,700&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Roboto', Arial, sans-serif; background: #f8f9fa; transition: background 0.3s; }
        .arxiv-card { margin: 2em auto; max-width: 900px; box-shadow: 0 2px 8px rgba(0,0,0,0.08); border-radius: 16px; transition: box-shadow 0.3s, transform 0.3s; }
        .arxiv-card:hover { box-shadow: 0 4px 12px rgba(0,0,0,0.1); transform: translateY(-1px); }
        .arxiv-title { background: linear-gradient(90deg, #0d6efd 60%, #6610f2 100%); color: #fff; border-radius: 16px 16px 0 0; padding: 1.5em; font-size: 1.5em; font-weight: 700; letter-spacing: 0.5px;}
        .arxiv-meta { font-size: 1em; color: #555; margin-bottom: 1em; }
        .arxiv-abstract { background: #fff; padding: 1.5em; border-radius: 0 0 16px 16px; font-size: 1.1em; line-height: 1.7;}
        .arxiv-links a { margin-right: 1em; }
        .arxiv-scores { margin-top: 1em; font-size: 1em; }
        .navbar { background: #fff; box-shadow: 0 2px 8px rgba(0,0,0,0.04);}
        .footer { text-align: center; color: #888; padding: 2em 0 1em 0; font-size: 0.95em;}
        @media (prefers-color-scheme: dark) {
            body { background: #181a1b; color: #e4e6eb; }
            .arxiv-card { background: #23272b; box-shadow: 0 2px 8px rgba(0,0,0,0.28);}
            .arxiv-card:hover { box-shadow: 0 5px 15px rgba(0,0,0,0.25); transform: translateY(-1px); }
            .arxiv-title { background: linear-gradient(90deg, #375a7f 60%, #6f42c1 100%);}
            .arxiv-abstract { background: #23272b; color: #e4e6eb;}
            .navbar { background: #23272b; color: #e4e6eb;}
            .text-muted { color: #adb5bd !important; }
        }
        /* Improved abstract readability */
        .arxiv-abstract-text {
            font-size: 1.1em;
            line-height: 1.7;
        }
        p.big {
            line-height: 1.6;
            font-size: large;
        }
    </style>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'] ],
          processEscapes: true
        }
      });
    </script>
    <script>
    function toggleAuthors(element) {
        const fullList = element.querySelector('.author-full');
        const shortList = element.querySelector('.author-short');
        const toggleText = element.querySelector('.author-toggle');
        
        fullList.style.display = fullList.style.display === 'none' ? 'inline' : 'none';
        shortList.style.display = shortList.style.display === 'none' ? 'inline' : 'none';
        toggleText.textContent = fullList.style.display === 'none' ? ' (show all)' : ' (show less)';
    }
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>
<body>
    <nav class="navbar navbar-expand-lg mb-4">
        <div class="container">
            <a class="navbar-brand fw-bold text-primary" href="#">arXiv Daily</a>
            <span class="navbar-text">Modern arXiv Reader</span>
        </div>
    </nav>

    
    <div class="container py-4">
        <header class="mb-4"><h2 class="display-6 text-center">Analysis of Your Favorites</h2></header>
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Word Cloud</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/word_cloud.png" class="img-fluid rounded" alt="Word Cloud of favorite paper topics">
                    </div>
                </div>
            </div>
        </div>
        
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Clusters</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/backfill_cluster_map.png" class="img-fluid rounded" alt="2D Cluster plot of favorite papers">
                    </div>
                </div>
            </div>
        </div>
        
    </div>
    

    <div class="container py-4">
        <header class="mb-4"><h1 class="display-5 text-center text-primary">arXiv: Backfill (2025-10-26 to 2025-11-02)</h1></header>

        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Score Distribution of All Papers Found Today</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/score_distribution.png" class="img-fluid rounded" alt="Score distribution of all papers found today">
                    </div>
                </div>
            </div>
        </div>
        

        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">1. A Unified Photometric Redshift Calibration for Weak Lensing Surveys using the Dark Energy Spectroscopic Instrument
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Johannes U. Lange, Diana Blanco, Alexie Leauthaud, Angus Wright, Abigail Fisher, Joshua Ratajczak, Jessica Nicole Aguilar, Steven Ahlen, Stephen Bailey, Davide Bianchi, et al.</span>
                                <span class="author-full" style="display: none;">Johannes U. Lange, Diana Blanco, Alexie Leauthaud, Angus Wright, Abigail Fisher, Joshua Ratajczak, Jessica Nicole Aguilar, Steven Ahlen, Stephen Bailey, Davide Bianchi, Chris Blake, David Brooks, Todd Claybaugh, Andrei Cuceu, Kyle Dawson, Axel de la Macorra, Joseph DeRose, Arjun Dey, Peter Doel, Ni Putu Audita Placida Emas, Simone Ferraro, Andreu Font-Ribera, Jaime E. Forero-Romero, Cristhian Garcia-Quintero, Enrique Gaztañaga, Satya Gontcho A Gontcho, Gaston Gutierrez, Sven Heydenreich, Hendrik Hildebrandt, Mustapha Ishak, Jorge Jimenez, Shahab Joudaki, Robert Kehoe, David Kirkby, Theodore Kisner, Anthony Kremin, Ofer Lahav, Claire Lamman, Martin Landriau, Laurent Le Guillou, Michael Levi, Leonel Medina Varela, Aaron Meisner, Ramon Miquel, John Moustakas, Seshadri Nadathur, Jeffrey A. Newman, Nathalie Palanque-Delabrouille, Anna Porredon, Francisco Prada, Ignasi Pérez-Ràfols, Graziano Rossi, Rossana Ruggeri, Eusebio Sanchez, Christoph Saulder, David Schlegel, Michael Schubnell, David Sprayberry, Zechang Sun, Gregory Tarlé, Benjamin Alan Weaver, Sihan Yuan, Pauline Zarrouk, Hu Zou</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Here, we introduce a new method for determining $n(z)$ for weak lensing surveys based on high-quality redshifts and neural network-based importance weights. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The effective redshift distribution $n(z)$ of galaxies is a critical component in the study of weak gravitational lensing. Here, we introduce a new method for determining $n(z)$ for weak lensing surveys based on high-quality redshifts and neural network-based importance weights. Additionally, we present the first unified photometric redshift calibration of the three leading stage-III weak lensing surveys, the Dark Energy Survey (DES), the Hyper Suprime-Cam (HSC) survey and the Kilo-Degree Survey (KiDS), with state-of-the-art spectroscopic data from the Dark Energy Spectroscopic Instrument (DESI). We verify our method using a new, data-driven approach and obtain $n(z)$ constraints with statistical uncertainties of order $\sigma_{\bar z} \sim 0.01$ and smaller. Our analysis is largely independent of previous photometric redshift calibrations and, thus, provides an important cross-check in light of recent cosmological tensions. Overall, we find excellent agreement with previously published results on the DES Y3 and HSC Y1 data sets while there are some differences on the mean redshift with respect to the previously published KiDS-1000 results. We attribute the latter to mismatches in photometric noise properties in the COSMOS field compared to the wider KiDS SOM-gold catalog. At the same time, the new $n(z)$ estimates for KiDS do not significantly change estimates of cosmic structure growth from cosmic shear. Finally, we discuss how our method can be applied to future weak lensing calibrations with DESI data.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.25419" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.25419" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.25419" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 13.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.72</span>
                        <span class="badge bg-primary">Semantic Score: 0.97</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">2. Dark Energy Survey Year 6 Results: Clustering-redshifts and importance sampling of Self-Organised-Maps $n(z)$ realizations for $3\times2$pt samples
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">W. d&#39;Assignies, G. M. Bernstein, B. Yin, G. Giannini, A. Alarcon, M. Manera, C. To, M. Yamamoto, N. Weaverdyck, R. Cawthon, et al.</span>
                                <span class="author-full" style="display: none;">W. d&#39;Assignies, G. M. Bernstein, B. Yin, G. Giannini, A. Alarcon, M. Manera, C. To, M. Yamamoto, N. Weaverdyck, R. Cawthon, M. Gatti, A. Amon, D. Anbajagane, S. Avila, M. R. Becker, K. Bechtol, C. Chang, M. Crocce, J. De Vicente, S. Dodelson, J. Fang, A. Ferté, D. Gruen, E. Legnani, A. Porredon, J. Prat, M. Rodriguez-Monroy, C. Sánchez, T. Schutt, I. Sevilla-Noarbe, D. Sanchez Cid, M. A. Troxel, T. M. C. Abbott, M. Aguena, O. Alves, D. Bacon, S. Bocquet, D. Brooks, R. Camilleri, A. Carnero Rosell, M. Carrasco Kind, J. Carretero, F. J. Castander, L. N. da Costa, M. E. da Silva Pereira, T. M. Davis, S. Desai, P. Doel, C. Doux, A. Drlica-Wagner, T. Eifler, J. Elvin-Poole, S. Everett, B. Flaugher, P. Fosalba, J. Frieman, J. Garcia-Bellido, E. Gaztanaga, P. Giles, G. Gutierrez, S. R. Hinton, D. L. Hollowood, K. Honscheid, D. Huterer, B. Jain, D. J. James, K. Kuehn, O. Lahav, S. Lee, J. L. Marshall, J. Mena-Fernandez, F. Menanteau, R. Miquel, J. Muir, J. Myles, R. L. C. Ogando, A. Palmese, M. Paterno, P. Petravick, A. A. Plazas Malagon, M. Raveri, A. Roodman, S. Samuroff, E. Sanchez, E. Sheldon, T. Shin, M. Smith, E. Suchyta, M. E. C. Swanson, G. Tarle, D. Thomas, V. Vikram, A. R. Walker</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The analysis uses angular scales of 1.5-5 Mpc to optimize signal-to-noise while mitigating modeling uncertainties, and marginalizes over redshift-dependent galaxy bias and other systematics informed by the N-body simulation Cardinal. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">This work is part of a series establishing the redshift framework for the $3\times2$pt analysis of the Dark Energy Survey Year 6 (DES Y6). For DES Y6, photometric redshift distributions are estimated using self-organizing maps (SOMs), calibrated with spectroscopic and many-band photometric data. To overcome limitations from color-redshift degeneracies and incomplete spectroscopic coverage, we enhance this approach by incorporating clustering-based redshift constraints (clustering-z, or WZ) from angular cross-correlations with BOSS and eBOSS galaxies, and eBOSS quasar samples. We define a WZ likelihood and apply importance sampling to a large ensemble of SOM-derived $n(z)$ realizations, selecting those consistent with the clustering measurements to produce a posterior sample for each lens and source bin. The analysis uses angular scales of 1.5-5 Mpc to optimize signal-to-noise while mitigating modeling uncertainties, and marginalizes over redshift-dependent galaxy bias and other systematics informed by the N-body simulation Cardinal. While a sparser spectroscopic reference sample limits WZ constraining power at $z&gt;1.1$, particularly for source bins, we demonstrate that combining SOMPZ with WZ improves redshift accuracy and enhances the overall cosmological constraining power of DES Y6. We estimate an improvement in $S_8$ of approximately 10\% for cosmic shear and $3\times2$pt analysis, primarily due to the WZ calibration of the source samples.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.23565" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.23565" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.23565" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 12.9</span>
                        <span class="badge bg-info text-dark">Author Score: 0.69</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">3. Dark Energy Survey Year 6 Results: Redshift Calibration of the Weak Lensing Source Galaxies
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">B. Yin, A. Amon, A. Campos, M. A. Troxel, W. d&#39;Assignies, G. M. Bernstein, G. Camacho-Ciurana, S. Mau, M. R. Becker, G. Giannini, et al.</span>
                                <span class="author-full" style="display: none;">B. Yin, A. Amon, A. Campos, M. A. Troxel, W. d&#39;Assignies, G. M. Bernstein, G. Camacho-Ciurana, S. Mau, M. R. Becker, G. Giannini, A. Alarcón, D. Gruen, J. McCullough, M. Yamamoto, D. Anbajagane, S. Dodelson, C. Sánchez, J. Myles, J. Prat, C. Chang, M. Crocce, K. Bechtol, A. Ferté, M. Gatti, N. MacCrann, R. Marco, A. Porredón, D. Sánchez Cid, T. Schutt, M. Tabbut, C. To, T. Abbott, M. Aguena, O. Alves, D. Bacon, S. Bocquet, D. Brooks, R. Camilleri, A. Carnero Rosell, M. Carrasco Kind, J. Carretero, F. Castander, R. Cawthon, C. Conselice, L. da Costa, M. da Silva Pereira, T. Davis, J. De Vicente, S. Desai, H. Diehl, C. Doux, A. Drlica-Wagner, T. Eifler, J. Elvin-Poole, S. Everett, B. Flaugher, P. Fosalba, D. Francis de Souza, J. Frieman, J. Garcia-Bellido, E. Gaztañaga, P. Giles, G. Gutierrez, S. Hinton, D. Hollowood, K. Honscheid, D. Huterer, B. Jain, D. James, K. Kuehn, S. Lee, H. Lin, J. Marshall, J. Mena-Fernández, F. Menanteau, R. Miquel, J. Muir, R. Ogando, A. Palmese, D. Petravick, A. Plazas Malagón, A. Roodman, R. Rosenfeld, S. Samuroff, E. Sánchez, I. Sevilla, E. Sheldon, T. Shin, M. Smith, E. Suchyta, M. Swanson, G. Tarlé, D. Thomas, V. Vikram, A. Walker, P. Wiseman</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We ensure the robustness of the redshift distributions by leveraging new image simulations and a cross-check with galaxy shape information via the shear ratio (SR) method. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Determining the distribution of redshifts for galaxies in wide-field photometric surveys is essential for robust cosmological studies of weak gravitational lensing. We present the methodology, calibrated redshift distributions, and uncertainties of the final Dark Energy Survey Year 6 (Y6) weak lensing galaxy data, divided into four redshift bins centered at $\langle z \rangle = [0.414, 0.538, 0.846, 1.157]$. We combine independent information from two methods on the full shape of redshift distributions: optical and near-infrared photometry within an improved Self-Organizing Map $p(z)$ (SOMPZ) framework, and cross-correlations with spectroscopic galaxy clustering measurements (WZ), which we demonstrate to be consistent both in terms of the redshift calibration itself and in terms of resulting cosmological constraints within 0.1$\sigma$. We describe the process used to produce an ensemble of redshift distributions that account for several known sources of uncertainty. Among these, imperfection in the calibration sample due to the lack of faint, representative spectra is the dominant factor. The final uncertainty on mean redshift in each bin is $\sigma_{\langle z\rangle} = [0.012, 0.008,0.009, 0.024]$. We ensure the robustness of the redshift distributions by leveraging new image simulations and a cross-check with galaxy shape information via the shear ratio (SR) method.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.23566" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.23566" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.23566" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 12.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.61</span>
                        <span class="badge bg-primary">Semantic Score: 0.96</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">4. Constraining baryonic feedback and cosmology from DES Y3 and Planck PR4 6$\times$2pt data. I. $Λ$CDM models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jiachuan Xu, Tim Eifler, Elisabeth Krause, Vivian Miranda, Jaime Salcido, Ian McCarthy</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We include all data points in the DES Y3 cosmic shear two-point correlation function (2PCF) down to 2.$^\prime$5 and model baryonic feedback processes via principal components (PCs) that are constructed from the ANTILLES simulations. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We combine weak lensing, galaxy clustering, cosmic microwave background (CMB) lensing, and their cross-correlations (so-called 6$\times$2pt) to constrain cosmology and baryonic feedback scenarios using data from the Dark Energy Survey (DES) Y3 Maglim catalog and the Planck satellite PR4 data release. We include all data points in the DES Y3 cosmic shear two-point correlation function (2PCF) down to 2.$^\prime$5 and model baryonic feedback processes via principal components (PCs) that are constructed from the ANTILLES simulations. We find a tight correlation between the amplitude of the first PC $Q_1$ and mean normalized baryon mass fraction $\bar{Y_\mathrm{b}}=\bar{f}_\mathrm{b}/(\Omega_\mathrm{b}/\Omega_\mathrm{m})$ from the ANTILLES simulations and employ an independent $\bar{Y_\mathrm{b}}$ measurement from Akino et al. (2022) as a prior of $Q_1$. We train a neural network $6\times2$pt emulator to boost the analysis speed by $\mathcal{O}(10^3)$, which enables us to run an impressive number of simulated analyses to validate our analysis against various systematics. For our 6$\times$2pt analysis, we find $S_8=0.8073\pm0.0094$ when including a $Q_1$ prior from $\bar{Y_\mathrm{b}}$ observations. This level of cosmological constraining power allows us to put tight constraints on the strength of baryonic feedback. We find $Q_1=0.025^{+0.024}_{-0.029}$ for our 6$\times$2pt analysis and $Q_1=0.043\pm{0.016}$ when combining with external information from Planck, ACT, DESI. All these results indicate weak feedback, e.g., the tensions to Illustris ($Q_1=0.095$) and OWLS AGN T8.7 ($Q_1=0.137$) are 2.9$\sigma$-3.3$\sigma$ and 4.7$\sigma$-5.9$\sigma$, respectively.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.25596" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.25596" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.25596" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.15</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">5. Flinch: A Differentiable Framework for Field-Level Inference of Cosmological parameters from curved sky data
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Andrea Crespi, Marco Bonici, Arthur Loureiro, Jaime Ruiz-Zapatero, Ivan Sladoljev, Zack Li, Adrian Bayer, Marius Millea, Uroš Seljak</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We present Flinch, a fully differentiable and high-performance framework for field-level inference on angular maps, developed to improve the flexibility and scalability of current methodologies. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present Flinch, a fully differentiable and high-performance framework for field-level inference on angular maps, developed to improve the flexibility and scalability of current methodologies. Flinch is integrated with differentiable cosmology tools, allowing gradients to propagate from individual map pixels directly to the underlying cosmological parameters. This architecture allows cosmological inference to be carried out directly from the map itself, bypassing the need to specify a likelihood for intermediate summary statistics. Using simulated, masked CMB temperature maps, we validate our pipeline by reconstructing both maps and angular power spectra, and we perform cosmological parameter inference with competitive precision. In comparison with the standard pseudo-$C_\ell$ approach, Flinch delivers substantially tighter constraints, with error bars reduced by up to 40%. Among the gradient-based samplers routinely employed in field-level analyses, we further show that MicroCanonical Langevin Monte Carlo provides orders-of-magnitude improvements in sampling efficiency over currently employed Hamiltonian Monte Carlo samplers, greatly reducing computational expense.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.26691" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.26691" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.26691" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.15</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">6. The Local Distance Network: a community consensus report on the measurement of the Hubble constant at 1% precision
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">H0DN Collaboration, Stefano Casertano, Gagandeep Anand, Richard I. Anderson, Rachael Beaton, Anupam Bhardwaj, John P. Blakeslee, Paula Boubel, Louise Breuval, Dillon Brout, et al.</span>
                                <span class="author-full" style="display: none;">H0DN Collaboration, Stefano Casertano, Gagandeep Anand, Richard I. Anderson, Rachael Beaton, Anupam Bhardwaj, John P. Blakeslee, Paula Boubel, Louise Breuval, Dillon Brout, Michele Cantiello, Mauricio Cruz Reyes, Geza Csörnyei, Thomas de Jaeger, Suhail Dhawan, Eleonora Di Valentino, Lluís Galbany, Héctor Gil-Marín, Dariusz Graczyk, Caroline Huang, Joseph B. Jensen, Pierre Kervella, Bruno Leibundgut, Bastian Lengen, Siyang Li, Lucas Macri, Emre Özülker, Dominic W. Pesce, Adam Riess, Martino Romaniello, Khaled Said, Nils Schöneberg, Dan Scolnic, Teresa Sicignano, Dorota M. Skowron, Syed A. Uddin, Licia Verde, Antonella Nota</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Using covariance weighting and leveraging the broad and comprehensive community of experts, we constructed a rigorous and transparent Distance Network (DN) to find a consensus value and uncertainty for the local H0. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The direct, empirical determination of the local value of the Hubble constant (H0) has markedly advanced thanks to improved instrumentation, measurement techniques, and distance estimators. However, combining determinations from different estimators is non-trivial, due to correlated calibrations and different analysis methodologies. Using covariance weighting and leveraging the broad and comprehensive community of experts, we constructed a rigorous and transparent Distance Network (DN) to find a consensus value and uncertainty for the local H0. All critically reviewed the available data sets, spanning parallaxes, detached eclipsing binaries, masers, Cepheids, the TRGB, Miras, JAGB stars, SN Ia, Surface Brightness Fluctuations, SN II, the Fundamental Plane, and Tully-Fisher relations and voted for indicators to define a `baseline&#39; DN and others to assess robustness and sensitivity of the results. We provide open-source software and data products to support full transparency and future extensions of this effort. Our conclusions: 1) Local H0 is robustly determined, with first-rank indicators internally consistent within their uncertainties; 2) A covariance-weighted combination yields an uncertainty of 1.1% (baseline) or 0.9% (all estimators); 3) The contribution from SNe Ia is consistent across four current compilations of optical magnitudes or using NIR-only magnitudes; 4) Removing either Cepheids or TRGB has minimal effect; 5) Replacing SNe Ia with galaxy-based indicators changes H0 by less than 0.1 km/s/Mpc, while doubling its uncertainty; 6) The baseline result is H0=73.50+/-0.81 km/s/Mpc. Compared to early Universe results, our result differs by 7.1sigma from flat {\Lambda}CDM with Planck+SPT+ACT and 5.0 sigma with BBN+BAO (DESI2). A networked approach is invaluable for enabling further progress in accuracy and precision without overreliance on any single method, sample or group.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.23823" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.23823" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.23823" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.8</span>
                        <span class="badge bg-info text-dark">Author Score: 0.08</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">7. Hybrid Physical-Neural Simulator for Fast Cosmological Hydrodynamics
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Arne Thomsen, Tilman Tröster, François Lanusse</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We propose a hybrid approach where gravitational forces are computed using a differentiable particle-mesh solver, while the hydrodynamics are parametrized by a neural network that maps local quantities to an effective pressure field. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Cosmological field-level inference requires differentiable forward models that solve the challenging dynamics of gas and dark matter under hydrodynamics and gravity. We propose a hybrid approach where gravitational forces are computed using a differentiable particle-mesh solver, while the hydrodynamics are parametrized by a neural network that maps local quantities to an effective pressure field. We demonstrate that our method improves upon alternative approaches, such as an Enthalpy Gradient Descent baseline, both at the field and summary-statistic level. The approach is furthermore highly data efficient, with a single reference simulation of cosmological structure formation being sufficient to constrain the neural pressure model. This opens the door for future applications where the model is fit directly to observational data, rather than a training set of simulations.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.26593" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.26593" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.26593" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.8</span>
                        <span class="badge bg-info text-dark">Author Score: 0.09</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">8. The cosmological analysis of DES 3$\times$2pt data from the Effective Field Theory of Large-Scale Structure
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Guido D&#39;Amico, Alexandre Refregier, Leonardo Senatore, Pierre Zhang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Specifically, we fit three two-point observables (3$\times$2pt), galaxy clustering, galaxy-galaxy lensing, and cosmic shear, using the one-loop expressions for the projected angular correlation functions. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We analyze the Dark Energy Survey (DES) Year 3 data using predictions from the Effective Field Theory of Large-Scale Structure (EFTofLSS). Specifically, we fit three two-point observables (3$\times$2pt), galaxy clustering, galaxy-galaxy lensing, and cosmic shear, using the one-loop expressions for the projected angular correlation functions. We validate our pipeline against numerical simulations and we check for several internal consistencies before applying it to the observational data. Fixing the spectral tilt and the baryons abundance, we measure $S_8=0.833\pm 0.032$, $\Omega_m = 0.272\pm 0.022$, and $h = 0.773\pm 0.049$, to about $3.8\%$, $8.1\%$, and $6.3\%$, at $68\%$CL, respectively. While our results are consistent within $\lesssim 2\sigma$ with those from Planck and the BOSS full-shape analyses, they exhibit stronger tension with those from DES collaboration 3$\times$2pt analysis combined with a Big-Bang Nucleosynthesis prior, highlighting the impact of modeling, scale cuts, and choice of prior. The theory code and likelihood used for our analyses, \texttt{PyFowl}, is made publicly available.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.24878" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.24878" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.24878" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.7</span>
                        <span class="badge bg-info text-dark">Author Score: 0.03</span>
                        <span class="badge bg-primary">Semantic Score: 0.96</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">9. Cosmological Constraints from Dark Energy Survey Year 1 Cluster Lensing and Abundances with Simulation-based Forward-Modeling
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Andrés N. Salcedo, Eduardo Rozo, Hao-Yi Wu, David H. Weinberg, Pranav Chiploonkar, Chun-Hao To, Shulei Cao, Eli S. Rykoff, Nicole Marcelina Gountanis, Conghao Zhou</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Applied to DES-Y1, and assuming a flat $\Lambda$CDM cosmology, we obtain $\Omega_m=0.254^{+0.026}_{-0.020}$ and $\sigma_8=0.826^{+0.030}_{-0.034}$, consistent with a broad suite of low-redshift structure measurements, including recent full-shape analyses, the DES/KiDS/HSC 3$\times$2 results, and most cluster-abundance studies. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present a simulation-based forward-modeling framework for cosmological inference from optical galaxy-cluster samples, and apply it to the abundance and weak-lensing signals of DES-Y1 redMaPPer clusters. The model embeds cosmology-dependent optical selection using a counts-in-cylinders approach, while also accounting for cluster miscentering and baryonic feedback in lensing. Applied to DES-Y1, and assuming a flat $\Lambda$CDM cosmology, we obtain $\Omega_m=0.254^{+0.026}_{-0.020}$ and $\sigma_8=0.826^{+0.030}_{-0.034}$, consistent with a broad suite of low-redshift structure measurements, including recent full-shape analyses, the DES/KiDS/HSC 3$\times$2 results, and most cluster-abundance studies. Our results are also consistent with \textit{Planck}, with the difference being significant at $2.58\sigma$. These results establish simulation-based forward-modeling of cluster abundances as a promising new tool for precision cosmology with Stage~IV survey data.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.25706" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.25706" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.25706" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.02</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 2</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">10. Cosmic magnification on multi-catalogue Herschel submillimetre galaxies
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>R. Fernandez-Fernandez, M. M. Cueli, J. González-Nuevo, L. Bonavera, D. Crespo, E. Goitia, J. M. Casas, J. A. Cano, M. Migliaccio</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The combined analysis, dominated by the more powerful H-ATLAS sample, gives results consistent with $\Lambda$CDM: $\Omega_m = 0.30^{+0.05}_{-0.07}$, $\sigma_8 = 0.80 (+/- 0.07)$, and $h &lt; 0.80$, in better agreement with \textit{Planck} 2018 than previous non-tomographic studies.} (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">{Submillimetre galaxies (SMGs) are excellent background sources for magnification-bias studies, but the limited sky coverage in the submillimetre (sub-mm) band constrains their statistical power. Beyond H-ATLAS, Herschel produced additional sub-mm catalogues, though not optimised for spatial statistical lensing analyses.} {Our goal is to refine cosmological constraints from SMG magnification bias by exploiting the full sub-mm sky surveyed by Herschel.} {We expanded the SMG sample by incorporating other Herschel catalogues overlapping SDSS spectroscopic lenses. Random catalogues were generated via kernel density estimation to compute cross-correlations, and Markov Chain Monte Carlo methods were applied to infer astrophysical and cosmological parameters for each catalogue and for the combined dataset.} {We report the first detection of magnification bias in SMGs beyond H-ATLAS, reinforcing the robustness of this observable. Individual Herschel catalogues yield reasonable central values for $\Omega_m$ and $\sigma_8$, although with large uncertainties. The combined analysis, dominated by the more powerful H-ATLAS sample, gives results consistent with $\Lambda$CDM: $\Omega_m = 0.30^{+0.05}_{-0.07}$, $\sigma_8 = 0.80 (+/- 0.07)$, and $h &lt; 0.80$, in better agreement with \textit{Planck} 2018 than previous non-tomographic studies.} {SMGs are promising tracers for magnification bias, but the narrow sub-mm coverage remains a major limitation. Wider surveys optimised for lensing would enable cross-correlations on larger scales, yielding tighter cosmological constraints.}</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.23582" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.23582" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.23582" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.96</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">11. No Evidence for a Dynamically Evolving Dark Energy Scenario: Tomographic Alcock-Paczyński Test with Redshift-Space Correlation Function II
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Fuyu Dong, Changbom Park</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> When combined with other results from the low-redshift universe, such as the PantheonPlus supernova compilation and DESI BAO data, the constraint on $w_a$ becomes $w_a=-0.124_{-0.368}^{+0.334}$, which is still consistent with zero. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We apply an extended Alcock-Paczy\&#39;nski (AP) test to the Sloan Digital Sky Survey data to constrain the dark energy models with the Chevallier-Polarski-Linder (CPL) parametrization of the dark energy equation of state. The extended AP test method uses the full shape of redshift-space two-point correlation funcion(CF) as the standard shape in order to measure the expansion history of the universe. We calibrate the standard shape by using the cosmology-dependent nonlinear evolution of the CF shape in the Multiverse simulations. Further validation of the method and calibration of possible systematics are performed based on mock samples from the Horizon Run 4 simulation. Using the AP test alone, we constrain the flat CDM plus CPL-type dark energy model (flat $w^{\rm CPL}$CDM) to have $\Omega_m=0.289_{-0.029}^{+0.031}$, $w_0=-0.798_{-0.102}^{+0.192}$ and $w_a=-0.165_{-0.945}^{+0.610}$. The result does not show evidence for a dynamically evolving dark energy model. When combined with other results from the low-redshift universe, such as the PantheonPlus supernova compilation and DESI BAO data, the constraint on $w_a$ becomes $w_a=-0.124_{-0.368}^{+0.334}$, which is still consistent with zero.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.24089" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.24089" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.24089" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.03</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">12. MPRU: Modular Projection-Redistribution Unlearning as Output Filter for Classification Pipelines
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Minyi Peng, Darian Gunamardi, Ivan Tjuawinata, Kwok-Yan Lam</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> As a new and promising approach, existing machine unlearning (MU) works typically emphasize theoretical formulations or optimization objectives to achieve knowledge removal. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">As a new and promising approach, existing machine unlearning (MU) works typically emphasize theoretical formulations or optimization objectives to achieve knowledge removal. However, when deployed in real-world scenarios, such solutions typically face scalability issues and have to address practical requirements such as full access to original datasets and model. In contrast to the existing approaches, we regard classification training as a sequential process where classes are learned sequentially, which we call \emph{inductive approach}. Unlearning can then be done by reversing the last training sequence. This is implemented by appending a projection-redistribution layer in the end of the model. Such an approach does not require full access to the original dataset or the model, addressing the challenges of existing methods. This enables modular and model-agnostic deployment as an output filter into existing classification pipelines with minimal alterations. We conducted multiple experiments across multiple datasets including image (CIFAR-10/100 using CNN-based model) and tabular datasets (Covertype using tree-based model). Experiment results show consistently similar output to a fully retrained model with a high computational cost reduction. This demonstrates the applicability, scalability, and system compatibility of our solution while maintaining the performance of the output in a more practical setting.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.26230" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.26230" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.26230" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">13. The Ray Tracing Sampler: Bayesian Sampling of Neural Networks for Everyone
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Peter Behroozi</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We derive a Markov Chain Monte Carlo sampler based on following ray paths in a medium where the refractive index $n(x)$ is a function of the desired likelihood $\mathcal{L}(x)$. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We derive a Markov Chain Monte Carlo sampler based on following ray paths in a medium where the refractive index $n(x)$ is a function of the desired likelihood $\mathcal{L}(x)$. The sampling method propagates rays at constant speed through parameter space, leading to orders of magnitude higher resilience to heating for stochastic gradients as compared to Hamiltonian Monte Carlo (HMC), as well as the ability to cross any likelihood barrier, including holes in parameter space. Using the ray tracing method, we sample the posterior distributions of neural network outputs for a variety of different architectures, up to the 1.5 billion-parameter GPT-2 (Generative Pre-trained Transformer 2) architecture, all on a single consumer-level GPU. We also show that prior samplers including traditional HMC, microcanonical HMC, Metropolis, Gibbs, and even Monte Carlo integration are special cases within a generalized ray tracing framework, which can sample according to an arbitrary weighting function. Public code and documentation for C, JAX, and PyTorch are available at https://bitbucket.org/pbehroozi/ray-tracing-sampler/src</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.25824" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.25824" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.25824" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.04</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.IM</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">14. Does Machine Learning Work? A Comparative Analysis of Strong Gravitational Lens Searches in the Dark Energy Survey
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">J. Gonzalez, T. Collett, K. Rojas, K. Bechtol, J. A. Acevedo Barroso, A. Melo, A. More, D. Sluse, C. Tortora, P. Holloway, et al.</span>
                                <span class="author-full" style="display: none;">J. Gonzalez, T. Collett, K. Rojas, K. Bechtol, J. A. Acevedo Barroso, A. Melo, A. More, D. Sluse, C. Tortora, P. Holloway, N. E. P. Lines, A. Verma</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Additionally, we explore ensemble strategies: average, median, linear regression, decision trees, random forests, and an Independent Bayesian method. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present a systematic comparison of three independent machine learning (ML)-based searches for strong gravitational lenses applied to the Dark Energy Survey (Jacobs et al. 2019a,b; Rojas et al. 2022; Gonzalez et al. 2025). Each search employs a distinct ML architecture and training strategy, allowing us to evaluate their relative performance, completeness, and complementarity. Using a visually inspected sample of 1651 systems previously reported as lens candidates, we assess how each model scores these systems and quantify their agreement with expert classifications. The three models show progressive improvement in performance, with F1-scores of 0.31, 0.35, and 0.54 for Jacobs, Rojas, and Gonzalez, respectively. Their completeness for moderate- to high-confidence lens candidates follows a similar trend (31%, 52%, and 70%). When combined, the models recover 82% of all such systems, highlighting their strong complementarity. Additionally, we explore ensemble strategies: average, median, linear regression, decision trees, random forests, and an Independent Bayesian method. We find that all but averaging achieve higher maximum F1 scores than the best individual model, with some ensemble methods improving precision by up to a factor of six. These results demonstrate that combining multiple, diverse ML classifiers can substantially improve the completeness of lens samples while drastically reducing false positives, offering practical guidance for optimizing future ML-based strong lens searches in wide-field surveys.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.23782" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.23782" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.23782" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">15. Definition and Treatment of Systematic Uncertainties in High Energy Physics and Astrophysics
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Pekka K. Sinervo, C. M</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> However, consistent definition and practice is elusive, as there are few formal definitions and there exists significant ambiguity in what is defined as a systematic and statistical uncertainty in a given analysis. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Systematic uncertainties in high energy physics and astrophysics are often significant contributions to the overall uncertainty in a measurement, in many cases being comparable to the statistical uncertainties. However, consistent definition and practice is elusive, as there are few formal definitions and there exists significant ambiguity in what is defined as a systematic and statistical uncertainty in a given analysis. I will describe current practice, and recommend a definition and classification of systematic uncertainties that allows one to treat these sources of uncertainty in a consistent and robust fashion. Classical and Bayesian approaches will be contrasted.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.24313" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.24313" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.24313" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">hep-ex</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">16. Encoder-Decoder Diffusion Language Models for Efficient Training and Inference
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Marianne Arriola, Yair Schiff, Hao Phung, Aaron Gokaslan, Volodymyr Kuleshov</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We introduce a framework for Efficient Encoder-Decoder Diffusion (E2D2), consisting of an architecture with specialized training and sampling algorithms, and we show that E2D2 achieves superior trade-offs between generation quality and inference throughput on summarization, translation, and mathematical reasoning tasks. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Discrete diffusion models enable parallel token sampling for faster inference than autoregressive approaches. However, prior diffusion models use a decoder-only architecture, which requires sampling algorithms that invoke the full network at every denoising step and incur high computational cost. Our key insight is that discrete diffusion models perform two types of computation: 1) representing clean tokens and 2) denoising corrupted tokens, which enables us to use separate modules for each task. We propose an encoder-decoder architecture to accelerate discrete diffusion inference, which relies on an encoder to represent clean tokens and a lightweight decoder to iteratively refine a noised sequence. We also show that this architecture enables faster training of block diffusion models, which partition sequences into blocks for better quality and are commonly used in diffusion language model inference. We introduce a framework for Efficient Encoder-Decoder Diffusion (E2D2), consisting of an architecture with specialized training and sampling algorithms, and we show that E2D2 achieves superior trade-offs between generation quality and inference throughput on summarization, translation, and mathematical reasoning tasks. We provide the code, model weights, and blog post on the project page: https://m-arriola.com/e2d2</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.22852" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.22852" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.22852" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">17. COMAP Pathfinder -- Season 2 results IV. A stack on eBOSS/DESI quasars
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">D. A. Dunne, K. A. Cleary, J. G. S. Lunde, D. T. Chung, P. C. Breysse, N. O. Stutzer, J. R. Bond, H. K. Eriksen, J. O. Gundersen, G. A. Hoerning, et al.</span>
                                <span class="author-full" style="display: none;">D. A. Dunne, K. A. Cleary, J. G. S. Lunde, D. T. Chung, P. C. Breysse, N. O. Stutzer, J. R. Bond, H. K. Eriksen, J. O. Gundersen, G. A. Hoerning, J. Kim, E. M. Mansfield, S. R. Mason, N. Murray, T. J. Rennie, D. Tolgay, S. Valentine, I. K. Wehus, COMAP Collaboration</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We compare this upper limit to models of the CO emission stacked on quasars and find a tentative ($\sim 3 \sigma$) tension between the limit and the brightest stack models after accounting for a suite of additional sources of experimental attenuation and uncertainty, including quasar velocity uncertainty, pipeline signal loss, cosmic variance, and interloper emission in the LIM data. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present a stack of data from the second season of the CO Mapping Array Project (COMAP) Pathfinder on the positions of quasars from eBOSS and DESI. COMAP is a Line Intensity Mapping (LIM) experiment targeting dense molecular gas via CO(1--0) emission at $z\sim3$. COMAP&#39;s Season 2 represents a $3\times$ increase in map-level sensitivity over the previous Early Science data release. We do not detect any CO emission in the stack, instead finding an upper limit of $10.0\times 10^{10}\ \mathrm{K\ km\ s^{-1}\ pc^2}$ at 95\% confidence within an $\sim 18\ \mathrm{cMpc}$ box. We compare this upper limit to models of the CO emission stacked on quasars and find a tentative ($\sim 3 \sigma$) tension between the limit and the brightest stack models after accounting for a suite of additional sources of experimental attenuation and uncertainty, including quasar velocity uncertainty, pipeline signal loss, cosmic variance, and interloper emission in the LIM data. The COMAP-eBOSS/DESI stack is primarily a measurement of the CO luminosity in the quasars&#39; wider environment and is therefore potentially subject to environmental effects such as feedback. With our current simple models of the galaxy-halo connection, we are thus unable to confidently rule out any models of cosmic CO with the stack alone. Conversely, the stack&#39;s sensitivity to these large-scale environmental effects has the potential to make it a powerful tool for galaxy formation science, once we are able to constrain the average CO luminosity via the auto power spectrum (a key goal of COMAP).</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.23568" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.23568" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.23568" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">18. XRISM Observations of The Prototypical Cold Front in Abell 3667
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Yuki Omiya, Yuto Ichinohe, Kazuhiro Nakazawa, Hisamitsu Awaki, Dominique Eckert, Yutaka Fujita, Isamu Hatsukade, Maxim Markevitch, François Mernier, Ikuyuki Mitsuishi, et al.</span>
                                <span class="author-full" style="display: none;">Yuki Omiya, Yuto Ichinohe, Kazuhiro Nakazawa, Hisamitsu Awaki, Dominique Eckert, Yutaka Fujita, Isamu Hatsukade, Maxim Markevitch, François Mernier, Ikuyuki Mitsuishi, Naomi Ota, Aurora Simionescu, Yuusuke Uchida, Shutaro Ueda, Irina Zhuravleva, John Zuhone</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> As one moves further off-center across the front, the line-of-sight (LoS) velocity changes significantly, by $\Delta v_z=535^{+167}_{-154}$ km s$^{-1}$, back to the value similar to that in the core. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present high-resolution X-ray spectroscopy of the merging galaxy cluster Abell 3667 with \textit{XRISM}/Resolve. Two observations, targeting the cluster X-ray core and the prototypical cold front, were performed with exposures of 105 ks and 276 ks, respectively. We find that the gas in the core is blueshifted by $v_z\sim-200$ km s$^{-1}$ relative to the brightest cluster galaxy, while the low-entropy gas inside the cold front is redshifted by $v_z\sim 200$ km s$^{-1}$. As one moves further off-center across the front, the line-of-sight (LoS) velocity changes significantly, by $\Delta v_z=535^{+167}_{-154}$ km s$^{-1}$, back to the value similar to that in the core. There are no significant LoS velocity gradients perpendicular to the cluster symmetry axis. These features suggest that the gas forming the cold front is flowing in the plane oriented along the LoS, supporting an offset merger scenario in which the main cluster has passed in front of the subcluster and induced rotation of the core gas in the plane perpendicular to the sky. The region just inside the front exhibits the largest LoS velocity dispersion seen across two pointings, $\sigma_z\sim420$ km s$^{-1}$, which can be interpreted as a developing turbulence or a projection of the LoS velocity shear within the front. The large LoS velocity jump across the cold front, combined with the lack of Kelvin-Helmholtz instability on the surface of the front, suggests some mechanism to suppress it. For example, a magnetic field with $B&gt;5\,\mu$G is required if the cold front is stabilized by magnetic draping.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.26405" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.26405" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.26405" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.08</span>
                        <span class="badge bg-primary">Semantic Score: 0.90</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">19. Faithful and Fast Influence Function via Advanced Sampling
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jungyeon Koh, Hyeonsu Lyu, Jonggyu Jang, Hyun Jong Yang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> These samplers select a small yet representative subset of the entire dataset by considering the stochastic distribution of features or logits, thereby enhancing the accuracy of IF estimations. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">How can we explain the influence of training data on black-box models? Influence functions (IFs) offer a post-hoc solution by utilizing gradients and Hessians. However, computing the Hessian for an entire dataset is resource-intensive, necessitating a feasible alternative. A common approach involves randomly sampling a small subset of the training data, but this method often results in highly inconsistent IF estimates due to the high variance in sample configurations. To address this, we propose two advanced sampling techniques based on features and logits. These samplers select a small yet representative subset of the entire dataset by considering the stochastic distribution of features or logits, thereby enhancing the accuracy of IF estimations. We validate our approach through class removal experiments, a typical application of IFs, using the F1-score to measure how effectively the model forgets the removed class while maintaining inference consistency on the remaining classes. Our method reduces computation time by 30.1% and memory usage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.26776" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.26776" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.26776" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">20. VIKING: Deep variational inference with stochastic projections
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Samuel G. Fadel, Hrittik Roy, Nicholas Krämer, Yevgen Zainchkovskyy, Stas Syrota, Alejandro Valverde Mahou, Carl Henrik Ek, Søren Hauberg</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Building upon recent work on reparametrizations of neural networks, we propose a simple variational family that considers two independent linear subspaces of the parameter space. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Variational mean field approximations tend to struggle with contemporary overparametrized deep neural networks. Where a Bayesian treatment is usually associated with high-quality predictions and uncertainties, the practical reality has been the opposite, with unstable training, poor predictive power, and subpar calibration. Building upon recent work on reparametrizations of neural networks, we propose a simple variational family that considers two independent linear subspaces of the parameter space. These represent functional changes inside and outside the support of training data. This allows us to build a fully-correlated approximate posterior reflecting the overparametrization that tunes easy-to-interpret hyperparameters. We develop scalable numerical routines that maximize the associated evidence lower bound (ELBO) and sample from the approximate posterior. Empirically, we observe state-of-the-art performance across tasks, models, and datasets compared to a wide array of baseline methods. Our results show that approximate Bayesian inference applied to deep neural networks is far from a lost cause when constructing inference mechanisms that reflect the geometry of reparametrizations.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.23684" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.23684" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.23684" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">21. How Data Mixing Shapes In-Context Learning: Asymptotic Equivalence for Transformers with MLPs
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Samet Demir, Zafer Dogan</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> However, theoretical studies often rely on simplified architectures (e.g., omitting MLPs), data models (e.g., linear regression with isotropic inputs), and single-source training, limiting their relevance to realistic settings. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Pretrained Transformers demonstrate remarkable in-context learning (ICL) capabilities, enabling them to adapt to new tasks from demonstrations without parameter updates. However, theoretical studies often rely on simplified architectures (e.g., omitting MLPs), data models (e.g., linear regression with isotropic inputs), and single-source training, limiting their relevance to realistic settings. In this work, we study ICL in pretrained Transformers with nonlinear MLP heads on nonlinear tasks drawn from multiple data sources with heterogeneous input, task, and noise distributions. We analyze a model where the MLP comprises two layers, with the first layer trained via a single gradient step and the second layer fully optimized. Under high-dimensional asymptotics, we prove that such models are equivalent in ICL error to structured polynomial predictors, leveraging results from the theory of Gaussian universality and orthogonal polynomials. This equivalence reveals that nonlinear MLPs meaningfully enhance ICL performance, particularly on nonlinear tasks, compared to linear baselines. It also enables a precise analysis of data mixing effects: we identify key properties of high-quality data sources (low noise, structured covariances) and show that feature learning emerges only when the task covariance exhibits sufficient structure. These results are validated empirically across various activation functions, model sizes, and data distributions. Finally, we experiment with a real-world scenario involving multilingual sentiment analysis where each language is treated as a different source. Our experimental results for this case exemplify how our findings extend to real-world cases. Overall, our work advances the theoretical foundations of ICL in Transformers and provides actionable insight into the role of architecture and data in ICL.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.25753" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.25753" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.25753" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">22. Morphological Zoo of Inflationary Gravitational Wave Spectra imprinted by a Sequence of Post-Inflationary Epochs
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Swagat S. Mishra, Athul K. Soman</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Remaining model agnostic, we demonstrate that a wide variety of spectral shapes, ranging from convex and concave monotonic profiles to multi-peaked non-monotonic spectra, can naturally emerge depending on the sequence and duration of these epochs. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The expansion history of the Universe prior to Big Bang Nucleosynthesis (BBN) remains largely unconstrained. The high-energy post-inflationary era may involve multiple distinct epochs, each characterized by a different equation of state (EoS). A key prediction of inflation is the generation of tensor perturbations that later manifest as a stochastic background of primordial gravitational waves (GWs). The large-scale amplitude and small-scale spectral tilt ($n_{\rm GW}$) of these GWs encode the inflationary energy scale and the subsequent expansion history, respectively. A soft post-inflationary EoS ($w1/3$) results in a blue-tilt ($n_{\rm GW}&gt;0$). In our previous work [arXiv:2407.07956], we developed an analytical framework for computing the GW spectral energy density, $\Omega_{\rm GW}(f)$, for multiple post-inflationary transitions ($w_1 \to w_2 \to \cdots \to w_n \to 1/3$), focusing on the parameter space relevant for future GW observations. In this paper, we extend that framework to systematically investigate the $morphological~diversity$ of inflationary GW spectra generated by multi-epoch post-inflationary histories. Remaining model agnostic, we demonstrate that a wide variety of spectral shapes, ranging from convex and concave monotonic profiles to multi-peaked non-monotonic spectra, can naturally emerge depending on the sequence and duration of these epochs. We also introduce GWInSpect, a publicly available Python package that computes $\Omega_{\rm GW}(f)$ for arbitrary sequences of EoS transitions, providing a practical tool to study the pre-BBN expansion history of the Universe.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.25672" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.25672" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.25672" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">23. BOLT-GAN: Bayes-Optimal Loss for Stable GAN Training
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Mohammadreza Tavasoli Naeini, Ali Bereyhi, Morteza Noshad, Ben Liang, Alfred O. Hero III</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We introduce BOLT-GAN, a simple yet effective modification of the WGAN framework inspired by the Bayes Optimal Learning Threshold (BOLT). (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We introduce BOLT-GAN, a simple yet effective modification of the WGAN framework inspired by the Bayes Optimal Learning Threshold (BOLT). We show that with a Lipschitz continuous discriminator, BOLT-GAN implicitly minimizes a different metric distance than the Earth Mover (Wasserstein) distance and achieves better training stability. Empirical evaluations on four standard image generation benchmarks (CIFAR-10, CelebA-64, LSUN Bedroom-64, and LSUN Church-64) show that BOLT-GAN consistently outperforms WGAN, achieving 10-60% lower Frechet Inception Distance (FID). Our results suggest that BOLT is a broadly applicable principle for enhancing GAN training.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.25609" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.25609" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.25609" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">24. Clustering properties of the CatWISE2020 quasar catalogue and their impact on the cosmic dipole anomaly
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Sebastian von Hausegger, Nathan Secrest, Harry Desmond, Mohamed Rameez, Roya Mohayaee, Subir Sarkar</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We therefore analyse clustering properties of this catalogue by performing an inference analysis of large-scale multipoles in real space, and by computing its angular power spectrum on small scales to test for convergence with $\Lambda$CDM. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The cosmic dipole anomaly -- the observation of a significant mismatch between the dipole observed in the matter distribution and that expected given the kinematic interpretation of the cosmic microwave background dipole -- poses a serious challenge to the Cosmological Principle upon which the standard model of cosmology rests. Measurements of the dipole ($\ell=1$) in a given sample crucially depend on having control over other large-scale power ($\ell &gt; 1$) so as to avoid biases, in particular those potentially caused by correlations among multipoles during fitting, and those by local source clustering. Currently, the most powerful catalogue that exhibits the cosmic dipole anomaly is the sample of 1.6~million mid-infrared quasars derived from CatWISE2020. We therefore analyse clustering properties of this catalogue by performing an inference analysis of large-scale multipoles in real space, and by computing its angular power spectrum on small scales to test for convergence with $\Lambda$CDM. After accounting for the known trend of the quasar number counts with ecliptic latitude, we find that any other large-scale power is consistent with noise, find no evidence for the presence of an octupole ($\ell=3$) in the data, and quantify the clustering dipole&#39;s proportion to be marginal. Our results therefore reaffirm the anomalously high dipole in the distribution of quasars.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.23769" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.23769" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.23769" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.02</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">25. Nirvana: A Specialized Generalist Model With Task-Aware Memory Mechanism
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yuhua Jiang, Shuang Cheng, Yihao Liu, Ermo Hua, Che Jiang, Weigao Sun, Yu Cheng, Feifei Gao, Biqing Qi, Bowen Zhou</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> However, traditional LLM structures including Transformer, Linear Attention, and hybrid models do not employ specialized memory mechanism guided by task information. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Specialized Generalist Models (SGMs) aim to preserve broad capabilities while achieving expert-level performance in target domains. However, traditional LLM structures including Transformer, Linear Attention, and hybrid models do not employ specialized memory mechanism guided by task information. In this paper, we present Nirvana, an SGM with specialized memory mechanism, linear time complexity, and test-time task information extraction. Besides, we propose the Task-Aware Memory Trigger ($\textit{Trigger}$) that flexibly adjusts memory mechanism based on the current task&#39;s requirements. In Trigger, each incoming sample is treated as a self-supervised fine-tuning task, enabling Nirvana to adapt its task-related parameters on the fly to domain shifts. We also design the Specialized Memory Updater ($\textit{Updater}$) that dynamically memorizes the context guided by Trigger. We conduct experiments on both general language tasks and specialized medical tasks. On a variety of natural language modeling benchmarks, Nirvana achieves competitive or superior results compared to the existing LLM structures. To prove the effectiveness of Trigger on specialized tasks, we test Nirvana&#39;s performance on a challenging medical task, i.e., Magnetic Resonance Imaging (MRI). We post-train frozen Nirvana backbone with lightweight codecs on paired electromagnetic signals and MRI images. Despite the frozen Nirvana backbone, Trigger guides the model to adapt to the MRI domain with the change of task-related parameters. Nirvana achieves higher-quality MRI reconstruction compared to conventional MRI models as well as the models with traditional LLMs&#39; backbone, and can also generate accurate preliminary clinical reports accordingly.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.26083" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.26083" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.26083" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">26. Distributional Multi-objective Black-box Optimization for Diffusion-model Inference-time Multi-Target Generation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Kim Yong Tan, Yueming Lyu, Ivor Tsang, Yew-Soon Ong</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Specifically, our IMG performs weighted resampling during the diffusion generation process according to the expected aggregated multi-objective values. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Diffusion models have been successful in learning complex data distributions. This capability has driven their application to high-dimensional multi-objective black-box optimization problem. Existing approaches often employ an external optimization loop, such as an evolutionary algorithm, to the diffusion model. However, these approaches treat the diffusion model as a black-box refiner, which overlooks the internal distribution transition of the diffusion generation process, limiting their efficiency. To address these challenges, we propose the Inference-time Multi-target Generation (IMG) algorithm, which optimizes the diffusion process at inference-time to generate samples that simultaneously satisfy multiple objectives. Specifically, our IMG performs weighted resampling during the diffusion generation process according to the expected aggregated multi-objective values. This weighted resampling strategy ensures the diffusion-generated samples are distributed according to our desired multi-target Boltzmann distribution. We further derive that the multi-target Boltzmann distribution has an interesting log-likelihood interpretation, where it is the optimal solution to the distributional multi-objective optimization problem. We implemented IMG for a multi-objective molecule generation task. Experiments show that IMG, requiring only a single generation pass, achieves a significantly higher hypervolume than baseline optimization algorithms that often require hundreds of diffusion generations. Notably, our algorithm can be viewed as an optimized diffusion process and can be integrated into existing methods to further improve their performance.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.26278" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.26278" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.26278" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">27. Recasting and Forecasting Dark Matter Limits Without Raw Data: A Generalized Algorithm for Gamma-Ray Telescopes
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Giacomo D&#39;Amico, Michele Doro, Michela De Caria</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The algorithm is generally applicable to any scenario where the expected signal model is parametric, offering a powerful tool for reinterpreting existing gamma-ray limits and efficiently exploring the DM parameter space in current and future indirect detection experiments. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present a novel method for both forecasting and recasting upper limits (ULs) on dark matter (DM) annihilation cross sections, $\left^{UL}$, or decay lifetime $\tau^{LL}$ . The forecasting method relies solely on the instrument response functions (IRFs) to predict ULs for a given observational setup, without the need for full analysis pipelines. The recasting procedure uses published ULs to reinterpret constraints for alternative DM models or channels. We demonstrate its utility across a range of canonical annihilation channels, including $b\bar{b}$, $W^+W^-$, $\tau^+\tau^-$, and $\mu^+\mu^-$, and apply it to several major gamma-ray experiments, including MAGIC, \textit{Fermi}-LAT, and CTAO. Notably, we develop a recasting approach that remains effective even when the IRF is unavailable by extracting generalized IRF-dependent coefficients from benchmark channels. We apply this method to reinterpret ULs derived from standard spectra (e.g., PPPC4DMID) in terms of more recent DM scenarios, including a Higgsino-like model with mixed final states and spectra generated with the CosmiXs model. Extensive Monte Carlo simulations and direct comparison with published results confirm the robustness and accuracy of our method, with discrepancies remaining within statistical uncertainties. The algorithm is generally applicable to any scenario where the expected signal model is parametric, offering a powerful tool for reinterpreting existing gamma-ray limits and efficiently exploring the DM parameter space in current and future indirect detection experiments.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.23168" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.23168" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.23168" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.HE</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">28. A Theory of the Mechanics of Information: Generalization Through Measurement of Uncertainty (Learning is Measuring)
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Christopher J. Hazard, Michael Resnick, Jacob Beel, Jack Xia, Cade Mack, Dominic Glennie, Matthew Fulp, David Maze, Andrew Bassett, Martin Koistinen</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Empirical results indicate that this may be a viable alternative path to neural networks with regard to scalable machine learning and artificial intelligence that can maintain human understandability of the underlying mechanics. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Traditional machine learning relies on explicit models and domain assumptions, limiting flexibility and interpretability. We introduce a model-free framework using surprisal (information theoretic uncertainty) to directly analyze and perform inferences from raw data, eliminating distribution modeling, reducing bias, and enabling efficient updates including direct edits and deletion of training data. By quantifying relevance through uncertainty, the approach enables generalizable inference across tasks including generative inference, causal discovery, anomaly detection, and time series forecasting. It emphasizes traceability, interpretability, and data-driven decision making, offering a unified, human-understandable framework for machine learning, and achieves at or near state-of-the-art performance across most common machine learning tasks. The mathematical foundations create a ``physics&#39;&#39; of information, which enable these techniques to apply effectively to a wide variety of complex data types, including missing data. Empirical results indicate that this may be a viable alternative path to neural networks with regard to scalable machine learning and artificial intelligence that can maintain human understandability of the underlying mechanics.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.22809" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.22809" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.22809" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">29. IBNorm: Information-Bottleneck Inspired Normalization for Representation Learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Xiandong Zou, Pan Zhou</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Normalization is fundamental to deep learning, but existing approaches such as BatchNorm, LayerNorm, and RMSNorm are variance-centric by enforcing zero mean and unit variance, stabilizing training without controlling how representations capture task-relevant information. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Normalization is fundamental to deep learning, but existing approaches such as BatchNorm, LayerNorm, and RMSNorm are variance-centric by enforcing zero mean and unit variance, stabilizing training without controlling how representations capture task-relevant information. We propose IB-Inspired Normalization (IBNorm), a simple yet powerful family of methods grounded in the Information Bottleneck principle. IBNorm introduces bounded compression operations that encourage embeddings to preserve predictive information while suppressing nuisance variability, yielding more informative representations while retaining the stability and compatibility of standard normalization. Theoretically, we prove that IBNorm achieves a higher IB value and tighter generalization bounds than variance-centric methods. Empirically, IBNorm consistently outperforms BatchNorm, LayerNorm, and RMSNorm across large-scale language models (LLaMA, GPT-2) and vision models (ResNet, ViT), with mutual information analysis confirming superior information bottleneck behavior. Code will be released publicly.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.25262" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.25262" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.25262" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">30. Common Task Framework For a Critical Evaluation of Scientific Machine Learning Algorithms
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Philippe Martin Wyder, Judah Goldfeder, Alexey Yermakov, Yue Zhao, Stefano Riva, Jan P. Williams, David Zoro, Amy Sara Rude, Matteo Tomasetto, Joe Germany, et al.</span>
                                <span class="author-full" style="display: none;">Philippe Martin Wyder, Judah Goldfeder, Alexey Yermakov, Yue Zhao, Stefano Riva, Jan P. Williams, David Zoro, Amy Sara Rude, Matteo Tomasetto, Joe Germany, Joseph Bakarji, Georg Maierhofer, Miles Cranmer, J. Nathan Kutz</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The CTF features a curated set of datasets and task-specific metrics spanning forecasting, state reconstruction, and generalization under realistic constraints, including noise and limited data. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Machine learning (ML) is transforming modeling and control in the physical, engineering, and biological sciences. However, rapid development has outpaced the creation of standardized, objective benchmarks - leading to weak baselines, reporting bias, and inconsistent evaluations across methods. This undermines reproducibility, misguides resource allocation, and obscures scientific progress. To address this, we propose a Common Task Framework (CTF) for scientific machine learning. The CTF features a curated set of datasets and task-specific metrics spanning forecasting, state reconstruction, and generalization under realistic constraints, including noise and limited data. Inspired by the success of CTFs in fields like natural language processing and computer vision, our framework provides a structured, rigorous foundation for head-to-head evaluation of diverse algorithms. As a first step, we benchmark methods on two canonical nonlinear systems: Kuramoto-Sivashinsky and Lorenz. These results illustrate the utility of the CTF in revealing method strengths, limitations, and suitability for specific classes of problems and diverse objectives. Next, we are launching a competition around a global real world sea surface temperature dataset with a true holdout dataset to foster community engagement. Our long-term vision is to replace ad hoc comparisons with standardized evaluations on hidden test sets that raise the bar for rigor and reproducibility in scientific ML.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.23166" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.23166" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.23166" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.02</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.CE</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">31. On the limitation of evaluating machine unlearning using only a single training seed
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jamie Lanyon, Axel Finke, Petros Andreou, Georgina Cosma</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Machine unlearning (MU) aims to remove the influence of certain data points from a trained model without costly retraining. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Machine unlearning (MU) aims to remove the influence of certain data points from a trained model without costly retraining. Most practical MU algorithms are only approximate and their performance can only be assessed empirically. Care must therefore be taken to make empirical comparisons as representative as possible. A common practice is to run the MU algorithm multiple times independently starting from the same trained model. In this work, we demonstrate that this practice can give highly non-representative results because -- even for the same architecture and same dataset -- some MU methods can be highly sensitive to the choice of random number seed used for model training. We therefore recommend that empirical comphttps://info.arxiv.org/help/prep#commentsarisons of MU algorithms should also reflect the variability across different model training seeds.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.26714" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.26714" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.26714" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">32. Cosmic Vine: High abundance of massive galaxies and dark matter halos in a forming cluster at z=3.44
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Nikolaj B. Sillassen, Shuowen Jin, Georgios E. Magdis, Francesco Valentino, Emanuele Daddi, Raphael Gobat, Malte Brinch, Kei Ito, Tao Wang, Hanwen Sun, et al.</span>
                                <span class="author-full" style="display: none;">Nikolaj B. Sillassen, Shuowen Jin, Georgios E. Magdis, Francesco Valentino, Emanuele Daddi, Raphael Gobat, Malte Brinch, Kei Ito, Tao Wang, Hanwen Sun, Gabriel Brammer, Sune Toft, Thomas Greve</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Using the data from the DAWN JWST Archive, we conduct a comprehensive study on the large-scale structure, stellar mass function (SMF), quiescent members, and dark matter halos in the Cosmic Vine. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The Cosmic Vine is a massive protocluster at z=3.44 in the JWST CEERS field, offering an ideal laboratory for studying the early phases of cluster formation. Using the data from the DAWN JWST Archive, we conduct a comprehensive study on the large-scale structure, stellar mass function (SMF), quiescent members, and dark matter halos in the Cosmic Vine. First, we spectroscopically confirm 136 galaxies in the Vine at z=3.44, and an additional 47 galaxies belonging to a diffuse foreground structure at z=3.34 which we dub the Leaf. We identify four subgroups comprising the Cosmic Vine and two subgroups within the Leaf. Second, we identified 11 quiescent members with log(M*/Msun)=9.5-11.0, the largest sample of quiescent galaxies in overdense environments at z&gt;3, which gives an enhanced quiescent galaxy number density 2x10^(-4)cMpc^(-3) that is three times above the field level at log(M*/Msun) &gt; 10. Notably, these quiescent members form a tight red sequence on the color-magnitude diagram, making it one of the earliest red sequences known to date. Third, by constructing the SMFs for both star-forming and quiescent members, we find that both SMFs are top-heavy, with a significantly enhanced quiescent fraction at log(M*/Msun)&gt;10.5 compared to field counterparts. The stellar mass-size analysis reveals that star-forming members are more compact at higher masses than their field counterparts. Finally, we estimate a halo mass of log(Mh/Msun)=13.2+-0.3 for the protocluster core, and log(Mh/Msun)=11.9-12.4 for satellite subgroups. The phase-space analysis indicates that three subgroups are likely infalling to the core. This work reveals a high abundance of massive galaxies and dark matter halos in a forming cluster, demonstrating the accelerated assembly of massive galaxies in massive halos when the Universe was less than 2 billion years old.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.23549" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.23549" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.23549" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">33. FaCT: Faithful Concept Traces for Explaining Neural Network Decisions
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Amin Parchami-Araghi, Sukrut Rao, Jonas Fischer, Bernt Schiele</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Deep networks have shown remarkable performance across a wide range of tasks, yet getting a global concept-level understanding of how they function remains a key challenge. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Deep networks have shown remarkable performance across a wide range of tasks, yet getting a global concept-level understanding of how they function remains a key challenge. Many post-hoc concept-based approaches have been introduced to understand their workings, yet they are not always faithful to the model. Further, they make restrictive assumptions on the concepts a model learns, such as class-specificity, small spatial extent, or alignment to human expectations. In this work, we put emphasis on the faithfulness of such concept-based explanations and propose a new model with model-inherent mechanistic concept-explanations. Our concepts are shared across classes and, from any layer, their contribution to the logit and their input-visualization can be faithfully traced. We also leverage foundation models to propose a new concept-consistency metric, C$^2$-Score, that can be used to evaluate concept-based methods. We show that, compared to prior work, our concepts are quantitatively more consistent and users find our concepts to be more interpretable, all while retaining competitive ImageNet performance.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.25512" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.25512" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.25512" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">34. Dynamical friction shear and rotation in Chaplygin cosmology
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>A. Del Popolo, Saeed Fakhry, Maryam Shiravand, Morgan Le Delliou</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> In this study, we build upon the findings of Del Popolo et al. (2013) by further analyzing the influence of dynamical friction on the evolution of cosmological perturbations within the framework of the spherical collapse model (SCM) in a Universe dominated by generalized Chaplygin gas (GCG). (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">In this study, we build upon the findings of Del Popolo et al. (2013) by further analyzing the influence of dynamical friction on the evolution of cosmological perturbations within the framework of the spherical collapse model (SCM) in a Universe dominated by generalized Chaplygin gas (GCG). Specifically, we investigate how dynamical friction alters the growth rate of density perturbations, the effective sound speed, the equation-of-state parameter www, and the evolution of the cosmic expansion rate. Our results demonstrate that dynamical friction significantly delays the collapse process compared to the standard SCM. Accurate computation of these parameters is crucial for obtaining consistent results and reliable physical interpretations when employing the GCG model. Furthermore, our analysis confirms that the suppression of perturbation growth due to dynamical friction is considerably more pronounced than that caused by shear and rotation, as previously indicated by Del Popolo et al. (2013). This enhanced suppression effectively addresses the instability issues, such as oscillations or exponential divergences in the dark-matter power spectrum, highlighted in linear perturbation studies, such as those by Sandvik et al. (2004).</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.23481" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.23481" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.23481" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">35. The dwarf stellar mass function in different environments and the lack of a generic missing dwarfs problem in ΛCDM
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Ilin Lazar, Sugata Kaviraj, Garreth Martin, Aaron Watkins, Darshan Kakkad, Brian Bichang&#39;a, Katarina Kraljic, Sukyoung K. Yi, Yohan Dubois, Julien E. G. Devriendt, et al.</span>
                                <span class="author-full" style="display: none;">Ilin Lazar, Sugata Kaviraj, Garreth Martin, Aaron Watkins, Darshan Kakkad, Brian Bichang&#39;a, Katarina Kraljic, Sukyoung K. Yi, Yohan Dubois, Julien E. G. Devriendt, Sebastien Peirani, Christophe Pichon</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We combine deep photometric data in the COSMOS and XMM-LSS fields with high-resolution cosmological hydrodynamical simulations to explore two key questions: (1) how does the galaxy stellar mass function, particularly in the dwarf (Mstar &lt; 10^9.5 MSun ) regime, vary with environment, defined as distance from the large-scale structure (LSS) traced by nodes and filaments in the cosmic web? (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We combine deep photometric data in the COSMOS and XMM-LSS fields with high-resolution cosmological hydrodynamical simulations to explore two key questions: (1) how does the galaxy stellar mass function, particularly in the dwarf (Mstar &lt; 10^9.5 MSun ) regime, vary with environment, defined as distance from the large-scale structure (LSS) traced by nodes and filaments in the cosmic web? (2) is there a generic &#39;missing dwarfs&#39; problem in LambdaCDM predictions when all environments - and not just satellites around Milky Way like galaxies - are considered? The depth of the observational data used here enables us to construct complete, unbiased samples of galaxies, down to Mstar ~ 10^7 MSun and out to z ~ 0.4. Strong environmental differences are found for the galaxy stellar mass function when considering distance from LSS. As we move closer to LSS, the dwarf mass function becomes progressively flatter and the knee of the mass function shifts to larger stellar masses, both of which result in a higher ratio of massive to dwarf galaxies. While the stellar mass functions from the three simulations (NewHorizon, TNG50 and FIREbox) considered here do not completely agree across the dwarf regime, there is no evidence of a generic missing dwarfs problem in the context of LambdaCDM, akin to the results of recent work that demonstrates that there is no missing satellites problem around Galactic analogues.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.24838" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.24838" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.24838" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">36. Mapping Anisotropies in the Stochastic Gravitational-Wave Background with space detector networks
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zhi-Yuan Li, Zheng-Cheng Liang, Cong-mao Zhang, Jian-dong Zhang, Yi-Ming Hu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Such a multi-detector network will provide complementary viewing angles for the anisotropic stochastic gravitational-wave background (SGWB), thereby significantly enhancing the capability to reconstruct and localize its spatial distribution. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Future space-based gravitational-wave detectors such as TianQin, LISA, and Taiji are expected to conduct joint observations. Such a multi-detector network will provide complementary viewing angles for the anisotropic stochastic gravitational-wave background (SGWB), thereby significantly enhancing the capability to reconstruct and localize its spatial distribution. In this paper, we have established the first dedicated data analysis pipeline for the anisotropic stochastic gravitational-wave background using a joint network of TianQin, LISA, and Taiji. Our analysis incorporates both Gaussian, stationary, and unpolarized point sources from diverse sky locations as well as a random sky map. We have performed full-sky map reconstruction in pixel space using maximum likelihood estimation to extract the angular distribution of the SGWB. The results demonstrate that, when considering the detector noise, the TianQin+LISA+Taiji detector network can reconstruct the angular power spectrum of the stochastic background up to a maximum multipole moment of $l = 14 $, which can provide valuable information for studies on the spatial distribution of galactic compact binaries and physical imprints from the early Universe.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.26359" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.26359" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.26359" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">37. Rethinking Visual Intelligence: Insights from Video Pretraining
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Pablo Acuaviva, Aram Davtyan, Mariam Hassan, Sebastian Stapf, Ahmad Rahimi, Alexandre Alahi, Paolo Favaro</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Across benchmarks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata, VDMs demonstrate higher data efficiency than their language counterparts. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Large language models (LLMs) have demonstrated that large-scale pretraining enables systems to adapt rapidly to new problems with little supervision in the language domain. This success, however, has not translated as effectively to the visual domain, where models, including LLMs, continue to struggle with compositional understanding, sample efficiency, and general-purpose problem-solving. We investigate Video Diffusion Models (VDMs) as a promising direction for bridging this gap. Pretraining on spatiotemporal data endows these models with strong inductive biases for structure and dynamics, which we hypothesize can support broad task adaptability. To test this, we design a controlled evaluation in which both a pretrained LLM and a pretrained VDM are equipped with lightweight adapters and presented with tasks in their natural modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata, VDMs demonstrate higher data efficiency than their language counterparts. Taken together, our results indicate that video pretraining offers inductive biases that support progress toward visual foundation models.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.24448" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.24448" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.24448" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.02</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">38. Towards constraining cosmological parameters with SPT-3G observations of 25% of the sky
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">A. Vitrier, K. Fichman, L. Balkenhol, E. Camphuis, F. Guidi, A. R. Khalife, A. J. Anderson, B. Ansarinejad, M. Archipley, K. Benabed, et al.</span>
                                <span class="author-full" style="display: none;">A. Vitrier, K. Fichman, L. Balkenhol, E. Camphuis, F. Guidi, A. R. Khalife, A. J. Anderson, B. Ansarinejad, M. Archipley, K. Benabed, A. N. Bender, B. A. Benson, F. Bianchini, L. E. Bleem, F. R. Bouchet, L. Bryant, M. G. Campitiello, J. E. Carlstrom, C. L. Chang, P. Chaubal, P. M. Chichura, A. Chokshi, T. -L. Chou, A. Coerver, T. M. Crawford, C. Daley, T. de Haan, K. R. Dibert, M. A. Dobbs, M. Doohan, A. Doussot, D. Dutcher, W. Everett, C. Feng, K. R. Ferguson, N. C. Ferree, A. Foster, S. Galli, A. E. Gambrel, R. W. Gardner, F. Ge, N. Goeckner-Wald, R. Gualtieri, S. Guns, N. W. Halverson, E. Hivon, G. P. Holder, W. L. Holzapfel, J. C. Hood, A. Hryciuk, N. Huang, F. Kéruzoré, L. Knox, M. Korman, K. Kornoelje, C. -L. Kuo, K. Levy, Y. Li, A. E. Lowitz, C. Lu, G. P. Lynch, A. Maniyar, E. S. Martsen, F. Menanteau, M. Millea, J. Montgomery, Y. Nakato, T. Natoli, G. I. Noble, Y. Omori, A. Ouellette, Z. Pan, P. Paschos, K. A. Phadke, A. W. Pollak, K. Prabhu, W. Quan, M. Rahimi, A. Rahlin, C. L. Reichardt, M. Rouble, J. E. Ruhl, E. Schiappucci, A. C. Silva Oliveira, A. Simpson, J. A. Sobrin, A. A. Stark, J. Stephen, C. Tandoi, B. Thorne, C. Trendafilova, C. Umilta, J. D. Vieira, Y. Wan, N. Whitehorn, W. L. K. Wu, M. R. Young, J. A. Zebrowski</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> In this work, we explore its potential to address the Hubble tension by forecasting constraints from temperature, polarization, and CMB lensing on Early Dark Energy (EDE) and the variation in electron mass in spatially flat and curved universes. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The South Pole Telescope (SPT), using its third-generation camera, SPT-3G, is conducting observations of the cosmic microwave background (CMB) in temperature and polarization across approximately 10 000 deg$^2$ of the sky at 95, 150, and 220 GHz. This comprehensive dataset should yield stringent constraints on cosmological parameters. In this work, we explore its potential to address the Hubble tension by forecasting constraints from temperature, polarization, and CMB lensing on Early Dark Energy (EDE) and the variation in electron mass in spatially flat and curved universes. For this purpose, we investigate first whether analyzing the distinct SPT-3G observation fields independently, as opposed to as a single, unified region, results in a loss of information relevant to cosmological parameter estimation. We develop a realistic temperature and polarization likelihood pipeline capable of analyzing these fields in these two ways, and subsequently forecast constraints on cosmological parameters. Our findings indicate that any loss of constraining power from analyzing the fields separately is primarily concentrated at low multipoles ($\ell$ &lt; 50) and the overall impact on the relative uncertainty on standard $\Lambda$CDM parameters is minimal (&lt; 3%). Our forecasts suggest that SPT-3G data should improve by more than a factor of 200 and 3000 the Figure of Merit (FoM) of the EDE and the varying electron mass models, respectively, when combined with Planck data. The likelihood pipeline developed and used in this work is made publicly available online.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.24669" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.24669" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.24669" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">39. On the Anisotropy of Score-Based Generative Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Andreas Floros, Seyed-Mohsen Moosavi-Dezfooli, Pier Luigi Dragotti</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Our analysis shows that SADs form adaptive bases aligned with the architecture&#39;s output geometry, providing a principled way to predict generalization ability in score models prior to training. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We investigate the role of network architecture in shaping the inductive biases of modern score-based generative models. To this end, we introduce the Score Anisotropy Directions (SADs), architecture-dependent directions that reveal how different networks preferentially capture data structure. Our analysis shows that SADs form adaptive bases aligned with the architecture&#39;s output geometry, providing a principled way to predict generalization ability in score models prior to training. Through both synthetic data and standard image benchmarks, we demonstrate that SADs reliably capture fine-grained model behavior and correlate with downstream performance, as measured by Wasserstein metrics. Our work offers a new lens for explaining and predicting directional biases of generative models.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.22899" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.22899" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.22899" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">40. EMTSF:Extraordinary Mixture of SOTA Models for Time Series Forecasting
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Musleh Alharthi, Kaleel Mahmood, Sarosh Patel, Ausif Mahmood</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The immense success of the Transformer architecture in Natural Language Processing has led to its adoption in Time Se ries Forecasting (TSF), where superior performance has been shown. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The immense success of the Transformer architecture in Natural Language Processing has led to its adoption in Time Se ries Forecasting (TSF), where superior performance has been shown. However, a recent important paper questioned their effectiveness by demonstrating that a simple single layer linear model outperforms Transformer-based models. This was soon shown to be not as valid, by a better transformer-based model termed PatchTST. More re cently, TimeLLM demonstrated even better results by repurposing a Large Language Model (LLM) for the TSF domain. Again, a follow up paper challenged this by demonstrating that removing the LLM component or replacing it with a basic attention layer in fact yields better performance. One of the challenges in forecasting is the fact that TSF data favors the more recent past, and is sometimes subject to unpredictable events. Based upon these recent insights in TSF, we propose a strong Mixture of Experts (MoE) framework. Our method combines the state-of-the-art (SOTA) models including xLSTM, en hanced Linear, PatchTST, and minGRU, among others. This set of complimentary and diverse models for TSF are integrated in a Trans former based MoE gating network. Our proposed model outperforms all existing TSF models on standard benchmarks, surpassing even the latest approaches based on MoE frameworks.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.23396" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.23396" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.23396" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CL</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">41. Latent Chain-of-Thought for Visual Reasoning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Guohao Sun, Hang Hua, Jian Wang, Jiebo Luo, Sohail Dianat, Majid Rabbani, Raghuveer Rao, Zhiqiang Tao</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Additionally, we implement a Bayesian inference-scaling strategy that replaces costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank optimal rationales and answers. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Chain-of-thought (CoT) reasoning is critical for improving the interpretability and reliability of Large Vision-Language Models (LVLMs). However, existing training algorithms such as SFT, PPO, and GRPO may not generalize well across unseen reasoning tasks and heavily rely on a biased reward model. To address this challenge, we reformulate reasoning in LVLMs as posterior inference and propose a scalable training algorithm based on amortized variational inference. By leveraging diversity-seeking reinforcement learning algorithms, we introduce a novel sparse reward function for token-level learning signals that encourage diverse, high-likelihood latent CoT, overcoming deterministic sampling limitations and avoiding reward hacking. Additionally, we implement a Bayesian inference-scaling strategy that replaces costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank optimal rationales and answers. We empirically demonstrate that the proposed method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in terms of effectiveness, generalization, and interpretability.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.23925" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.23925" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.23925" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">42. Backward-Friendly Optimization: Training Large Language Models with Approximate Gradients under Memory Constraints
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jing Yang, Kaitong Cai, Yijia Fan, Yufeng Yang, Keze Wang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Empirically, GradLite reduces optimizer-state and activation memory consumption by up to 50\% without architectural changes, and achieves on-par or superior downstream performance on reasoning (MMLU, GSM8K), multilingual, and dialogue benchmarks compared to checkpointing and optimizer-centric baselines (LoMo, GaLore). (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Full fine-tuning of Large Language Models (LLMs) is notoriously memory-intensive, primarily because conventional optimizers such as SGD or Adam assume access to exact gradients derived from cached activations. Existing solutions either alter the model architecture (e.g., reversible networks) or trade memory for computation (e.g., activation checkpointing), but the optimizer itself remains untouched. In this work, we introduce GradLite, a backward-friendly optimizer that relaxes the requirement of exact gradients, enabling efficient training even when intermediate activations are aggressively discarded or approximated. GradLite leverages two key techniques: (i) low-rank Jacobian approximation, which reduces the dimensionality of backpropagated error signals, and (ii) error-feedback correction, which accumulates and compensates approximation errors across iterations to preserve convergence guarantees. We provide a theoretical analysis showing that GradLite maintains unbiased gradient estimates with bounded variance, ensuring convergence rates comparable to Adam. Empirically, GradLite reduces optimizer-state and activation memory consumption by up to 50\% without architectural changes, and achieves on-par or superior downstream performance on reasoning (MMLU, GSM8K), multilingual, and dialogue benchmarks compared to checkpointing and optimizer-centric baselines (LoMo, GaLore).</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.22467" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.22467" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.22467" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">43. FastVLM: Self-Speculative Decoding for Fast Vision-Language Model Inference
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Divya Jyoti Bajpai, Manjesh Kumar Hanawal</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Vision-language Models (VLMs) have made significant strides in visual understanding and query response generation, but often face challenges of high computational cost and inference latency due to autoregressive decoding. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Vision-language Models (VLMs) have made significant strides in visual understanding and query response generation, but often face challenges of high computational cost and inference latency due to autoregressive decoding. In this work, we introduce an imitation-learning-based Self-Speculative Decoding (SSD) framework, named FastVLM, to address these limitations. Our approach employs a lightweight draft model for token generation in an autoregressive manner, while a full model verifies these tokens non-autoregressively. Accepted tokens proceed seamlessly, while rejected tokens are corrected by the full model and used to guide the draft model&#39;s refinement. Through an imitation network, FastVLM enhances the draft model by integrating deeper level insights from the full model&#39;s architecture. Also, it maintains the performance integrity of the full model while training the draft model, achieving a balance between efficiency and accuracy. Our method speeds up the inference process by 1.55-1.85x as compared to the final layer with minimal loss in performance.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.22641" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.22641" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.22641" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">44. Key and Value Weights Are Probably All You Need: On the Necessity of the Query, Key, Value weight Triplet in Decoder-Only Transformers
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Marko Karbevski, Antonij Mijoski</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The Query, Key, Value weight triplet is a building block of current attention mechanisms in state-of-the-art LLMs. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The Query, Key, Value weight triplet is a building block of current attention mechanisms in state-of-the-art LLMs. We theoretically investigate whether this triplet can be reduced, proving under simplifying assumptions that the Query weights are redundant, thereby reducing the number of non-embedding/lm-head parameters by over 8%. We validate the theory on full-complexity GPT-3 small architectures (with layer normalization, skip connections, and weight decay) trained from scratch, demonstrating that the reduced model achieves comparable validation loss to standard baselines. These findings motivate the investigation of the Query weight redundancy at scale.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.23912" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.23912" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.23912" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">45. LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via Asymptotic Analysis
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Qingyue Zhang, Chang Chu, Tianren Peng, Qi Li, Xiangyang Luo, Zhihao Jiang, Shao-Lun Huang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Starting from a general optimization objective that minimizes the expectation of the parameter discrepancy between the fine-tuned and target models, we derive an optimization problem with two components: a bias term, which is related to the parameter distance between the fine-tuned and target models, and is approximated using a Fisher-gradient formulation to preserve anisotropy; and a variance term, which accounts for the uncertainty introduced by sampling stochasticity through the Fisher information. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">With the widespread adoption of LLMs, LoRA has become a dominant method for PEFT, and its initialization methods have attracted increasing attention. However, existing methods have notable limitations: many methods do not incorporate target-domain data, while gradient-based methods exploit data only at a shallow level by relying on one-step gradient decomposition, which remains unsatisfactory due to the weak empirical performance of the one-step fine-tuning model that serves as their basis, as well as the fact that these methods either lack a rigorous theoretical foundation or depend heavily on restrictive isotropic assumptions. In this paper, we establish a theoretical framework for data-aware LoRA initialization based on asymptotic analysis. Starting from a general optimization objective that minimizes the expectation of the parameter discrepancy between the fine-tuned and target models, we derive an optimization problem with two components: a bias term, which is related to the parameter distance between the fine-tuned and target models, and is approximated using a Fisher-gradient formulation to preserve anisotropy; and a variance term, which accounts for the uncertainty introduced by sampling stochasticity through the Fisher information. By solving this problem, we obtain an optimal initialization strategy for LoRA. Building on this theoretical framework, we develop an efficient algorithm, LoRA-DA, which estimates the terms in the optimization problem from a small set of target domain samples and obtains the optimal LoRA initialization. Empirical results across multiple benchmarks demonstrate that LoRA-DA consistently improves final accuracy over existing initialization methods. Additional studies show faster, more stable convergence, robustness across ranks, and only a small initialization overhead for LoRA-DA. The source code will be released upon publication.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.24561" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.24561" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.24561" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">46. BOB the (Waveform) Builder: Optimizing Analytical Black-Hole Binary Merger Waveforms
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Anuj Kankani, Sean T. McWilliams</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Our findings establish BOB as a powerful tool for gravitational wave analysis, for providing independent tests of NR-calibrated models, and for better understanding the underlying physics of the merger. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The Backwards-One-Body (BOB) model provides a fully analytical and physically motivated description of the merger-ringdown gravitational radiation emanating from a black hole binary merger. We perform a comprehensive validation of BOB for the dominant $(2,2)$ mode of quasi-circular and non-precessing systems, assessing its accuracy against numerical relativity (NR) simulations, state-of-the-art waveform models, and a sum of quasinormal modes. We demonstrate that BOB most accurately describes the gravitational wave news, achieving accuracy comparable to highly-calibrated Effective-One-Body and NR surrogate models. Because BOB is minimally tuned to NR catalogs, it retains a high level of accuracy in regions of the parameter space sparsely covered by current NR catalogs. BOB yields an analytic link between the amplitude of the fundamental quasinormal mode and the peak amplitude of the News, which we verify to within the errors of a surrogate ringdown model. We identify a flavor of BOB that requires only the remnant mass and spin, yet matches the accuracy of models that fit a sum of many overtones. Lastly, we show that BOB accurately models both the mass and current quadrupole waves for superkick configurations, contrary to a claim in the literature, and explain why that study was not actually implementing BOB as it has been defined. Our findings establish BOB as a powerful tool for gravitational wave analysis, for providing independent tests of NR-calibrated models, and for better understanding the underlying physics of the merger. We provide a companion python package, gwBOB, allowing for the easy construction of various flavors of BOB and comparison to NR waveforms.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.25012" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.25012" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.25012" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">47. Adapting Interleaved Encoders with PPO for Language-Guided Reinforcement Learning in BabyAI
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Aryan Mathur, Asaduddin Ahmed</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We evaluate the PDiT encoders on the BabyAI GoToLocal environment and find that the approach achieves more stable rewards and stronger alignment compared to a standard PPO baseline. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Deep reinforcement learning agents often struggle when tasks require understanding both vision and language. Conventional architectures typically isolate perception (for example, CNN-based visual encoders) from decision-making (policy networks). This separation can be inefficient, since the policy&#39;s failures do not directly help the perception module learn what is important. To address this, we implement the Perception-Decision Interleaving Transformer (PDiT) architecture introduced by Mao et al. (2023), a model that alternates between perception and decision layers within a single transformer. This interleaving allows feedback from decision-making to refine perceptual features dynamically. In addition, we integrate a contrastive loss inspired by CLIP to align textual mission embeddings with visual scene features. We evaluate the PDiT encoders on the BabyAI GoToLocal environment and find that the approach achieves more stable rewards and stronger alignment compared to a standard PPO baseline. The results suggest that interleaved transformer encoders are a promising direction for developing more integrated autonomous agents.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.23148" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.23148" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.23148" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">48. Learning Without Augmenting: Unsupervised Time Series Representation Learning via Frame Projections
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Berken Utku Demirel, Christian Holz</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> By jointly leveraging the complementary geometry of these distinct manifolds, our approach achieves superior performance without artificially increasing data diversity through strong augmentations. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Self-supervised learning (SSL) has emerged as a powerful paradigm for learning representations without labeled data. Most SSL approaches rely on strong, well-established, handcrafted data augmentations to generate diverse views for representation learning. However, designing such augmentations requires domain-specific knowledge and implicitly imposes representational invariances on the model, which can limit generalization. In this work, we propose an unsupervised representation learning method that replaces augmentations by generating views using orthonormal bases and overcomplete frames. We show that embeddings learned from orthonormal and overcomplete spaces reside on distinct manifolds, shaped by the geometric biases introduced by representing samples in different spaces. By jointly leveraging the complementary geometry of these distinct manifolds, our approach achieves superior performance without artificially increasing data diversity through strong augmentations. We demonstrate the effectiveness of our method on nine datasets across five temporal sequence tasks, where signal-specific characteristics make data augmentations particularly challenging. Without relying on augmentation-induced diversity, our method achieves performance gains of up to 15--20\% over existing self-supervised approaches. Source code: https://github.com/eth-siplab/Learning-with-FrameProjections</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.22655" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.22655" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.22655" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">49. Variational Polya Tree
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Lu Xu, Tsai Hor Chan, Kwok Fai Lam, Lequan Yu, Guosheng Yin</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Traditional techniques like Markov chain Monte Carlo (MCMC) face high computational complexity and scalability limitations, hindering the use of Bayesian nonparametric methods in deep learning. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Density estimation is essential for generative modeling, particularly with the rise of modern neural networks. While existing methods capture complex data distributions, they often lack interpretability and uncertainty quantification. Bayesian nonparametric methods, especially the \polya tree, offer a robust framework that addresses these issues by accurately capturing function behavior over small intervals. Traditional techniques like Markov chain Monte Carlo (MCMC) face high computational complexity and scalability limitations, hindering the use of Bayesian nonparametric methods in deep learning. To tackle this, we introduce the variational \polya tree (VPT) model, which employs stochastic variational inference to compute posterior distributions. This model provides a flexible, nonparametric Bayesian prior that captures latent densities and works well with stochastic gradient optimization. We also leverage the joint distribution likelihood for a more precise variational posterior approximation than traditional mean-field methods. We evaluate the model performance on both real data and images, and demonstrate its competitiveness with other state-of-the-art deep density estimation methods. We also explore its ability in enhancing interpretability and uncertainty quantification. Code is available at https://github.com/howardchanth/var-polya-tree.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.22651" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.22651" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.22651" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">50. Uncertainty-Aware Diagnostics for Physics-Informed Machine Learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Mara Daniels, Liam Hodgkinson, Michael Mahoney</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> This is related to a poor understanding of epistemic uncertainty, and it can lead to surprising failure modes, even when existing statistical metrics suggest strong fits. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Physics-informed machine learning (PIML) integrates prior physical information, often in the form of differential equation constraints, into the process of fitting machine learning models to physical data. Popular PIML approaches, including neural operators, physics-informed neural networks, neural ordinary differential equations, and neural discrete equilibria, are typically fit to objectives that simultaneously include both data and physical constraints. However, the multi-objective nature of this approach creates ambiguity in the measurement of model quality. This is related to a poor understanding of epistemic uncertainty, and it can lead to surprising failure modes, even when existing statistical metrics suggest strong fits. Working within a Gaussian process regression framework, we introduce the Physics-Informed Log Evidence (PILE) score. Bypassing the ambiguities of test losses, the PILE score is a single, uncertainty-aware metric that provides a selection principle for hyperparameters of a PIML model. We show that PILE minimization yields excellent choices for a wide variety of model parameters, including kernel bandwidth, least squares regularization weights, and even kernel function selection. We also show that, even prior to data acquisition, a special &#39;data-free&#39; case of the PILE score identifies a priori kernel choices that are &#39;well-adapted&#39; to a given PDE. Beyond the kernel setting, we anticipate that the PILE score can be extended to PIML at large, and we outline approaches to do so.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.26121" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.26121" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.26121" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
    </div>

    <footer class="footer">
        &copy; 2025 arXiv Daily · Powered by <a href="https://arxiv.org/" target="_blank">arXiv</a>
    </footer>
</body>
</html>