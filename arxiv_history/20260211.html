<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>arXiv Daily: 2026-02-11</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto:400,700&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Roboto', Arial, sans-serif; background: #f8f9fa; transition: background 0.3s; }
        .arxiv-card { margin: 2em auto; max-width: 900px; box-shadow: 0 2px 8px rgba(0,0,0,0.08); border-radius: 16px; transition: box-shadow 0.3s, transform 0.3s; }
        .arxiv-card:hover { box-shadow: 0 4px 12px rgba(0,0,0,0.1); transform: translateY(-1px); }
        .arxiv-title { background: linear-gradient(90deg, #0d6efd 60%, #6610f2 100%); color: #fff; border-radius: 16px 16px 0 0; padding: 1.5em; font-size: 1.5em; font-weight: 700; letter-spacing: 0.5px;}
        .arxiv-meta { font-size: 1em; color: #555; margin-bottom: 1em; }
        .arxiv-abstract { background: #fff; padding: 1.5em; border-radius: 0 0 16px 16px; font-size: 1.1em; line-height: 1.7;}
        .arxiv-links a { margin-right: 1em; }
        .arxiv-scores { margin-top: 1em; font-size: 1em; }
        .navbar { background: #fff; box-shadow: 0 2px 8px rgba(0,0,0,0.04);}
        .footer { text-align: center; color: #888; padding: 2em 0 1em 0; font-size: 0.95em;}
        @media (prefers-color-scheme: dark) {
            body { background: #181a1b; color: #e4e6eb; }
            .arxiv-card { background: #23272b; box-shadow: 0 2px 8px rgba(0,0,0,0.28);}
            .arxiv-card:hover { box-shadow: 0 5px 15px rgba(0,0,0,0.25); transform: translateY(-1px); }
            .arxiv-title { background: linear-gradient(90deg, #375a7f 60%, #6f42c1 100%);}
            .arxiv-abstract { background: #23272b; color: #e4e6eb;}
            .navbar { background: #23272b; color: #e4e6eb;}
            .text-muted { color: #adb5bd !important; }
        }
        /* Improved abstract readability */
        .arxiv-abstract-text {
            font-size: 1.1em;
            line-height: 1.7;
        }
        p.big {
            line-height: 1.6;
            font-size: large;
        }
    </style>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'] ],
          processEscapes: true
        }
      });
    </script>
    <script>
    function toggleAuthors(element) {
        const fullList = element.querySelector('.author-full');
        const shortList = element.querySelector('.author-short');
        const toggleText = element.querySelector('.author-toggle');
        
        fullList.style.display = fullList.style.display === 'none' ? 'inline' : 'none';
        shortList.style.display = shortList.style.display === 'none' ? 'inline' : 'none';
        toggleText.textContent = fullList.style.display === 'none' ? ' (show all)' : ' (show less)';
    }
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>
<body>
    <nav class="navbar navbar-expand-lg mb-4">
        <div class="container">
            <a class="navbar-brand fw-bold text-primary" href="#">arXiv Daily</a>
            <span class="navbar-text">Modern arXiv Reader</span>
        </div>
    </nav>

    
    <div class="container py-4">
        <header class="mb-4"><h2 class="display-6 text-center">Analysis of Your Favorites</h2></header>
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Word Cloud</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/word_cloud.png" class="img-fluid rounded" alt="Word Cloud of favorite paper topics">
                    </div>
                </div>
            </div>
        </div>
        
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Clusters</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/daily_cluster_map.png" class="img-fluid rounded" alt="2D Cluster plot of favorite papers">
                    </div>
                </div>
            </div>
        </div>
        
    </div>
    

    <div class="container py-4">
        <header class="mb-4"><h1 class="display-5 text-center text-primary">arXiv: 2026-02-11</h1></header>

        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Score Distribution of All Papers Found Today</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/score_distribution.png" class="img-fluid rounded" alt="Score distribution of all papers found today">
                    </div>
                </div>
            </div>
        </div>
        

        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">1. CFHT MegaCam Two Deep Fields Imaging Survey (2DFIS) II: Decoding the Lensing Profile of a &#34;Rotating&#34; Cluster with Deep CFHT Imaging
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Yicheng Li, Liping Fu, Wentao Luo, Binyang Liu, Wei Du, Martin Kilbinger, Calum Murray, Christopher J. Miller, Ray Wang, David Turner, et al.</span>
                                <span class="author-full" style="display: none;">Yicheng Li, Liping Fu, Wentao Luo, Binyang Liu, Wei Du, Martin Kilbinger, Calum Murray, Christopher J. Miller, Ray Wang, David Turner, Lance Miller, Dezi Liu, Mario Radovich, Jean-Paul Kneib, Huanyuan Shan, Kaiwen Mai, Zicheng Wang, Haoran Zhao</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Multi-wavelength observations reveal that the apparent optical bimodality and rotation signal in the galaxy cluster RXCJ0110.0+1358 are spurious effects caused by filamentary projection rather than true dynamical substructure. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present a multi-wavelength analysis of the galaxy cluster RXCJ0110.0+1358 ($z=0.058$), a rotating cluster candidate, combining deep CFHT imaging, SDSS photometry, spectroscopic redshifts, and XMM-Newton X-ray observations. We find a notable discrepancy between the optical and X-ray views: while optical data reveal a pronounced bimodal galaxy distribution with significant kinematic substructure signatures, the X-ray emission exhibits a single, smoothly extended component centered on the BCG. Our weak lensing analysis resolves this discrepancy by revealing that the mass is predominantly concentrated in the southeast ($\log M_{200}/M_\odot = 14.04_{-0.40}^{+0.24}$), while the northwestern substructure has a negligible mass ($\sim 10^{13} M_\odot$). This immense mass disparity rules out the dynamical possibility of a rotating system. We demonstrate that the apparent optical bimodality arises from the projection of a filament, which led optical group-finding algorithms to misclassify these galaxies as cluster members. This contamination creates a spurious substructure that mimics a rotation signal and leads to an overestimation of the luminosity-based halo mass, resolving the observed inconsistencies.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.08360" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.08360" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.08360" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.21</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: weak lensing, galaxy clustering, cosmology / weak lensing, galaxy clustering, halo connection</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">2. AMICO galaxy clusters in KiDS-1000: Splashback radius from weak lensing and cluster-galaxy correlation function
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">G. F. Lesci, C. Giocoli, F. Marulli, M. Romanello, L. Moscardini, M. Sereno, M. Maturi, M. Radovich, G. Castignani, H. Hildebrandt, et al.</span>
                                <span class="author-full" style="display: none;">G. F. Lesci, C. Giocoli, F. Marulli, M. Romanello, L. Moscardini, M. Sereno, M. Maturi, M. Radovich, G. Castignani, H. Hildebrandt, L. Ingoglia, E. Puddu</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Analysis of the splashback radius in thousands of galaxy clusters from the Kilo Degree Survey demonstrates that both weak lensing and cluster-galaxy correlations provide precise constraints consistent with standard cosmological model predictions. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present the splashback radius analysis of the Adaptive Matched Identifier of Clustered Objects (AMICO) galaxy cluster sample in the fourth data release of the Kilo Degree Survey (KiDS). The sample contains 9049 rich galaxy clusters within $z\in[0.1,0.8]$, with shear measurements available for 8730 of them. We measure and model the stacked reduced shear, $g_{\rm t}$, and the cluster-galaxy correlation function, $w_{\rm cg}$, in bins of observed intrinsic richness, $λ^*$, and redshift, $z$. Building on the methods employed in recent cosmological analyses, we model the average splashback radius, $r_{\rm sp}$, of the underlying dark matter halo distribution, accounting for the known systematic uncertainties affecting measurements and theoretical models. By modelling $g_{\rm t}$ and $w_{\rm cg}$ separately, in the cluster-centric radial range $R\in[0.4,5]$ $h^{-1}$Mpc, we constrain $r_{\rm sp}$, the mass accretion rate, $Γ$, and the relation between $\mathcal{R}_{\rm sp}\equiv r_{\rm sp}/r_{200\rm m}$ and the peak height, $ν_{200\rm m}$, over the mass range $M_{200\rm m}\in[0.4,20]$ $10^{14}h^{-1}$M$_\odot$. The two probes provide consistent results that also agree with $Λ$-cold dark matter model predictions. Our $\mathcal{R}_{\rm sp}$ constraints are consistent with those from previous observations. For $g_{\rm t}$ and $w_{\rm cg}$, we achieve a precision of 14% and 10% per cluster stack, respectively. The higher precision of $w_{\rm cg}$, enabled by its combination with weak-lensing constraints on the mass-richness relation, highlights the complementarity of lensing and clustering in measuring $r_{\rm sp}$ and constraining the properties of the infalling material region.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07114" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07114" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07114" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.9</span>
                        <span class="badge bg-info text-dark">Author Score: 0.08</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: weak lensing, galaxy clustering, cosmology / weak lensing, galaxy clustering, halo connection</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">3. The stellar velocity anisotropy of strong lensing massive elliptical galaxies and its role in the inference of the Hubble parameter $H_0$ using spatially resolved kinematics
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Vishal Verma, Quinn Minor</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Joint modeling of strong lensing and spatially resolved kinematics using a generalized Osipkov–Merritt anisotropy profile significantly reduces systematic biases in Hubble constant measurements compared to simpler kinematic assumptions. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">One of the biggest challenges in cosmology, the Hubble Tension, requires independent measurements of $H_0$, and strong lensing with time-delay cosmography is a promising avenue. The inclusion of spatially resolved kinematic data helps break the mass--sheet degeneracy, a key limitation in strong lensing. Kinematics, however, suffers from its own degeneracy due to unknown stellar velocity anisotropy, which can bias galaxy mass profile inferences. We investigate the bias in $H_0$ using a sample of ten massive elliptical galaxies at $z=0.2$ from the Illustris $TNG100$ simulations. We generate mock line-of-sight velocity-dispersion maps resembling JWST NIRSpec observations and test four anisotropy models: Osipkov--Merritt (OM), Mamon--Lokas (ML), constant $β$, and a generalized--OM (gOM) profile, under both kinematics-only and joint kinematics plus strong lensing analyses. We find a sub-percent average bias in $H_{0}$ across ten galaxies with joint modeling for three models: $+0.2 \pm 1.6\%$ (ML), $-0.9 \pm 1.9\%$ (constant) and $-0.9 \pm 1.6\%$ (gOM), with $\sim 5\%$ scatter. Joint modeling reduces bias, improves precision, and mitigates outlier results. Overall, the gOM model best recovers galaxy parameters and delivers the most accurate $H_{0}$ relative to posterior uncertainties considering both analyses. However, the single-parameter OM model produces large systematic biases: with kinematics only data, $H_{0}$ errors can exceed $20\%$, and even with joint modeling, produces an overall bias of $+11.5 \pm 1.3\%$ (OM). The higher bias in OM is unlikely to average out across an ensemble of galaxies. Our findings highlight the impact of anisotropy assumptions on $H_{0}$ inference and, more broadly, in galaxy dynamics.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07159" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07159" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07159" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.96</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: weak lensing, galaxy clustering, cosmology / weak lensing, cosmic shear, cosmological constraints</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">4. Cosmology with one galaxy: An analytic formula relating $Ω_{\rm m}$ with galaxy properties
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Kito Liao, Francisco Villaescusa-Navarro, Romain Teysser, Natalí S. M. de Santi</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Symbolic regression applied to hydrodynamical simulations identifies a physically interpretable relationship between individual galaxy baryonic properties and the cosmic matter density parameter, enabling cosmological inference from single-object observables. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Standard cosmological analyses typically treat galaxy formation and cosmological parameter inference as decoupled problems, relying on population-level statistics such as clustering, lensing, or halo abundances. However, classical studies of baryon fractions in massive galaxy clusters have long suggested that gravitationally bound systems may retain cosmological information through their baryonic content. Building on this insight, we present the first analytic and physically interpretable cosmological tracer that links the matter density parameter, $Ω_m$, directly to intrinsic galaxy-scale observables, demonstrating that cosmological information can be extracted from individual galaxies. Using symbolic regression applied to state-of-the-art hydrodynamical simulations from the CAMELS project, we identify a compact functional form that robustly recovers $Ω_m$ across multiple simulation suites (IllustrisTNG, ASTRID, SIMBA, and Swift-EAGLE), requiring only modest recalibration of a small number of coefficients. The resulting expression admits a transparent physical interpretation in terms of baryonic retention and enrichment efficiency regulated by gravitational potential depth, providing a clear explanation for why $Ω_m$ is locally encoded in galaxy properties. Our work establishes a direct, interpretable bridge between small-scale galaxy physics and large-scale cosmology, opening a complementary pathway to cosmological inference that bypasses traditional clustering-based statistics and enables new synergies between galaxy formation theory and precision cosmology.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07651" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07651" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07651" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.04</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">5. Kosmulator: A Python framework for cosmological inference with MCMC
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Renier T. Hough, Robert Rugg, Shambel Sahlu, Amare Abebe</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Kosmulator provides a high-performance, vectorized Python environment for rapid Bayesian inference and hypothesis testing of alternative cosmological expansion histories with significantly reduced computational overhead. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present Kosmulator, a modular and vectorised Python framework designed to accelerate the statistical testing of cosmological models. As the theoretical landscape expands beyond standard $Λ$CDM, implementing new expansion histories into traditional Einstein--Boltzmann solvers becomes a significant computational bottleneck. Kosmulator addresses this by leveraging array-native execution and efficient ensemble slice sampling (via Zeus) to perform rapid Bayesian inference. We validate the framework against the industry-standard Cobaya code using a combination of Type Ia Supernovae, Cosmic Chronometers, and Baryon Acoustic Oscillation (BAO) data. Our results demonstrate that Kosmulator reproduces Cobaya&#39;s posterior constraints to within $\leq0.3σ$ statistical agreement on $H_{0}$ and $Ω_{m}$ and $&lt;0.6\%$ precision on $χ^{2}$, while achieving a $\sim 4.5\times$ reduction in wall-clock time on a single CPU core compared to a standard MPI-parallelised baseline. Furthermore, we showcase the framework&#39;s utility by constraining the implicit power-law $f(Q)$ &#34;$f_1$CDM&#34; model and demonstrating its automated model selection capabilities (AIC/BIC). Kosmulator is introduced as a &#34;scientific sieve&#34; for rapid hypothesis testing, allowing researchers to efficiently filter theoretical candidates before deploying high-precision resources.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.08424" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.08424" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.08424" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">6. MePo: Meta Post-Refinement for Rehearsal-Free General Continual Learnin
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Guanglong Sun, Hongwei Yan, Liyuan Wang, Zhiqi Kang, Shuang Cui, Hang Su, Jun Zhu, Yi Zhong</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Meta Post-Refinement enhances general continual learning by utilizing a bi-level meta-learning paradigm and second-order statistics to adapt pretrained representations to evolving downstream tasks without requiring data rehearsal. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">To cope with uncertain changes of the external world, intelligent systems must continually learn from complex, evolving environments and respond in real time. This ability, collectively known as general continual learning (GCL), encapsulates practical challenges such as online datastreams and blurry task boundaries. Although leveraging pretrained models (PTMs) has greatly advanced conventional continual learning (CL), these methods remain limited in reconciling the diverse and temporally mixed information along a single pass, resulting in sub-optimal GCL performance. Inspired by meta-plasticity and reconstructive memory in neuroscience, we introduce here an innovative approach named Meta Post-Refinement (MePo) for PTMs-based GCL. This approach constructs pseudo task sequences from pretraining data and develops a bi-level meta-learning paradigm to refine the pretrained backbone, which serves as a prolonged pretraining phase but greatly facilitates rapid adaptation of representation learning to downstream GCL tasks. MePo further initializes a meta covariance matrix as the reference geometry of pretrained representation space, enabling GCL to exploit second-order statistics for robust output alignment. MePo serves as a plug-in strategy that achieves significant performance gains across a variety of GCL benchmarks and pretrained checkpoints in a rehearsal-free manner (e.g., 15.10\%, 13.36\%, and 12.56\% on CIFAR-100, ImageNet-R, and CUB-200 under Sup-21/1K). Our source code is available at \href{https://github.com/SunGL001/MePo}{MePo}</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07940" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07940" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07940" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">7. Amortising Inference and Meta-Learning Priors in Neural Networks
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Tommy Rochussen, Vincent Fortuin</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Amortized variational inference enables the learning of weight priors from diverse datasets, effectively bridging Bayesian deep learning and neural processes to facilitate robust inference under data-starved conditions. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">One of the core facets of Bayesianism is in the updating of prior beliefs in light of new evidence$\text{ -- }$so how can we maintain a Bayesian approach if we have no prior beliefs in the first place? This is one of the central challenges in the field of Bayesian deep learning, where it is not clear how to represent beliefs about a prediction task by prior distributions over model parameters. Bridging the fields of Bayesian deep learning and probabilistic meta-learning, we introduce a way to $\textit{learn}$ a weights prior from a collection of datasets by introducing a way to perform per-dataset amortised variational inference. The model we develop can be viewed as a neural process whose latent variable is the set of weights of a BNN and whose decoder is the neural network parameterised by a sample of the latent variable itself. This unique model allows us to study the behaviour of Bayesian neural networks under well-specified priors, use Bayesian neural networks as flexible generative models, and perform desirable but previously elusive feats in neural processes such as within-task minibatching or meta-learning under extreme data-starvation.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.08782" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.08782" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.08782" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">8. From the confluent Heun equation to a new factorized and resummed gravitational waveform for circularized, nonspinning, compact binaries
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Andrea Cipriani, Alessandro Nagar, Francesco Fucito, José Francisco Morales</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Mapping the Teukolsky equation to a confluent Heun form allows for the construction of resummed gravitational waveforms that eliminate transcendental terms and improve accuracy for compact binary coalescences. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We introduce a new factorized and resummed waveform for circularized, nonspinning, compact binaries that leverages on the solution of the Teukolsky equation once mapped into a confluent Heun equation. The structure of the solution allows one to identify new resummed factors that completely absorb all test-mass logarithms and transcendental numbers via exponentials and $Γ$-functions at any post-Newtonian (PN) order. The corresponding residual relativistic and phase corrections are thus polynomial with rational coefficients, that are in fact PN-truncated hypergeometric functions. Our approach complements the recent proposal of Ivanov et al. [Phys. Rev. Lett. 135 (2025) 14, 141401], notably recovering the corresponding renormalization group scaling of multipole moments from first principles and fixing the scaling constant. In the test mass limit, our approach (pushed up to 10PN) yields waveforms and fluxes that are globally more accurate than those obtained using the standard factorized approach of Damour et al. [Phys. Rev. D 79 (2009), 064004]. The method generalizes straightforwardly to comparable mass binaries implementing the new concept of universal anomalous dimension of multipole moments and might be eventually useful to improve current state of the art effective-one-body waveform models for coalescing binaries.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.08833" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.08833" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.08833" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">9. Accelerating Black Hole Image Generation via Latent Space Diffusion Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Ao Liu, Xudong Zhang, Cuihong Wen, Wentao Liu, Jieci Wang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Physics-conditioned latent diffusion models serve as high-fidelity, computationally efficient alternatives to traditional ray-tracing simulations for generating and analyzing horizon-scale black hole images. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Interpreting horizon-scale black hole images currently relies on computationally intensive General Relativistic Ray Tracing (GRRT) simulations, which pose a significant bottleneck for rapid parameter exploration and high-precision tests of strong-field gravity. We demonstrate that physically accurate black hole images, synthesized from magnetized accretion flows, inherently reside on a low-dimensional manifold-encoding the essential features of spacetime geometry, plasma distribution, and relativistic emission. Leveraging this structure, we introduce a physics-conditioned diffusion model that operates in a compact latent space to generate high-fidelity black hole imagery directly from physical parameters. The model accurately reproduces critical observational signatures from full GRRT simulations-such as shadow diameter, photon-ring structure, and relativistic brightness asymmetry-while achieving over a fourfold reduction in computational expense. Compared with the previous generation of denoising diffusion models, the proposed approach achieves significant improvements in image quality, reconstruction fidelity, and parameter estimation accuracy, while reducing the average inference time per black hole image from 5.25 seconds to 1.15 seconds. Our work establishes diffusion-based latent models as efficient and scalable substitutes for traditional radiative transfer solvers, offering a practical framework toward real-time modeling and inference for next-generation black hole imaging.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07786" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07786" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07786" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">10. Efficient and Stable Reinforcement Learning for Diffusion Language Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jiawei Liu, Xiting Wang, Yuanyuan Zhong, Defu Lian, Yu Yang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Spatio-Temporal Pruning stabilizes reinforcement learning for diffusion-based large language models by reducing variance and computational redundancy through constrained exploration and truncated refinement steps. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.08905" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.08905" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.08905" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">11. Hints of sign-changing scalar field energy density and a transient acceleration phase at $z\sim 2$ from model-agnostic reconstructions
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Özgür Akarsu, Maria Caruana, Konstantinos F. Dialektopoulos, Luis A. Escamilla, Emre O. Kahya, Jackson Levi Said</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Data-driven reconstructions of the late-time expansion history using Gaussian processes reveal a sign-switching dark energy density that necessitates phantom or quintom scalar-field frameworks rather than minimal canonical models. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present a data-driven reconstruction of the late-time expansion history and its implications for dark-energy dynamics. Modeling the reduced Hubble rate with a node-based Gaussian-process-kernel interpolant, we constrain the reconstruction using CC, Pantheon+ SNIa, BAO data from SDSS and DESI, transversal BAO data, and external $H_0$ priors (SH0ES and H0DN). Assuming GR at the background level, we map the reconstructed kinematics onto a dark-energy fluid and a scalar-field description, yielding the total potential and kinetic contributions that reproduce the inferred $H(z)$. To interpret the reconstruction, we consider both a minimal single-field model (canonical or phantom) and a two-field (quintom) system consisting of one canonical and one phantom scalar field (or families). Within the GR-based effective-fluid mapping, the inferred dark-energy density changes sign for all dataset combinations explored, transitioning from $ρ_{\rm DE}0$ toward the present, and defining a transition redshift $z_\dagger$ by $ρ_{\rm DE}(z_\dagger)=0$. A single canonical scalar cannot realize such a smooth evolution during expansion, whereas a phantom field or a two-field quintom framework can accommodate the required behavior; in particular, the two-field system permits smooth phantom-divide crossings at finite $ρ_{\rm DE}&gt;0$ and distinguishes them from the separate notion of a density zero crossing. The reconstructed kinematics admit intermediate-redshift structure in some combinations, including hints of an additional accelerated-expansion interval around $z\sim 1.7$--$2.3$. The present-day equation of state remains close to a cosmological constant: combinations including supernovae give $w_0\simeq -1$, while combinations without supernovae but with an external $H_0$ prior show only a mild preference for $w_0&lt;-1$ at the $\sim1.5$--$1.7σ$ level.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.08928" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.08928" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.08928" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">12. Fast Model Selection and Stable Optimization for Softmax-Gated Multinomial-Logistic Mixture of Experts Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>TrungKhang Tran, TrungTin Nguyen, Md Abul Bashar, Nhat Ho, Richi Nayak, Christopher Drovandi</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A novel batch minorization-maximization algorithm for softmax-gated multinomial-logistic Mixture-of-Experts provides guaranteed monotone convergence and a principled, sweep-free method for selecting the optimal number of experts through mixing measure dendrograms. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Mixture-of-Experts (MoE) architectures combine specialized predictors through a learned gate and are effective across regression and classification, but for classification with softmax multinomial-logistic gating, rigorous guarantees for stable maximum-likelihood training and principled model selection remain limited. We address both issues in the full-data (batch) regime. First, we derive a batch minorization-maximization (MM) algorithm for softmax-gated multinomial-logistic MoE using an explicit quadratic minorizer, yielding coordinate-wise closed-form updates that guarantee monotone ascent of the objective and global convergence to a stationary point (in the standard MM sense), avoiding approximate M-steps common in EM-type implementations. Second, we prove finite-sample rates for conditional density estimation and parameter recovery, and we adapt dendrograms of mixing measures to the classification setting to obtain a sweep-free selector of the number of experts that achieves near-parametric optimal rates after merging redundant fitted atoms. Experiments on biological protein--protein interaction prediction validate the full pipeline, delivering improved accuracy and better-calibrated probabilities than strong statistical and machine-learning baselines.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07997" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07997" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07997" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">13. From Out-of-Distribution Detection to Hallucination Detection: A Geometric View
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Litian Liu, Reza Pourreza, Yubing Jian, Yao Qin, Roland Memisevic</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Reframing hallucination detection as an out-of-distribution detection problem enables training-free, single-sample identification of errors in large language models, particularly improving performance on complex reasoning tasks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Detecting hallucinations in large language models is a critical open problem with significant implications for safety and reliability. While existing hallucination detection methods achieve strong performance in question-answering tasks, they remain less effective on tasks requiring reasoning. In this work, we revisit hallucination detection through the lens of out-of-distribution (OOD) detection, a well-studied problem in areas like computer vision. Treating next-token prediction in language models as a classification task allows us to apply OOD techniques, provided appropriate modifications are made to account for the structural differences in large language models. We show that OOD-based approaches yield training-free, single-sample-based detectors, achieving strong accuracy in hallucination detection for reasoning tasks. Overall, our work suggests that reframing hallucination detection as OOD detection provides a promising and scalable pathway toward language model safety.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07253" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07253" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07253" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">14. stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Lucas Maes, Quentin Le Lidec, Dan Haramati, Nassim Massaudi, Damien Scieur, Yann LeCun, Randall Balestriero</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The stable-worldmodel (SWM) research ecosystem provides a modular and standardized framework for developing, testing, and evaluating world models, featuring controllable environments to facilitate studies on robustness and continual learning. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardization. To mitigate these issues, we introduce stable-worldmodel (SWM), a modular, tested, and documented world-model research ecosystem that provides efficient data-collection tools, standardized environments, planning algorithms, and baseline implementations. In addition, each environment in SWM enables controllable factors of variation, including visual and physical properties, to support robustness and continual learning research. Finally, we demonstrate the utility of SWM by using it to study zero-shot robustness in DINO-WM.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.08968" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.08968" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.08968" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.06</span>
                        <span class="badge bg-primary">Semantic Score: 0.90</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">15. Joint Reward Modeling: Internalizing Chain-of-Thought for Efficient Visual Reward Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Yankai Yang, Yancheng Long, Hongyang Wei, Wei Chen, Tianke Zhang, Kaiyu Jiang, Haonan Fan, Changyi Liu, Jiankang Chen, Kaiyu Tang, et al.</span>
                                <span class="author-full" style="display: none;">Yankai Yang, Yancheng Long, Hongyang Wei, Wei Chen, Tianke Zhang, Kaiyu Jiang, Haonan Fan, Changyi Liu, Jiankang Chen, Kaiyu Tang, Bin Wen, Fan Yang, Tingting Gao, Han Li, Shuo Yang</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Joint Reward Modeling (JRM) integrates preference learning and language modeling on a shared vision-language backbone to imbue efficient discriminative reward models with the complex semantic reasoning capabilities typically reserved for generative architectures. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Reward models are critical for reinforcement learning from human feedback, as they determine the alignment quality and reliability of generative models. For complex tasks such as image editing, reward models are required to capture global semantic consistency and implicit logical constraints beyond local similarity. Existing reward modeling approaches have clear limitations. Discriminative reward models align well with human preferences but struggle with complex semantics due to limited reasoning supervision. Generative reward models offer stronger semantic understanding and reasoning, but they are costly at inference time and difficult to align directly with human preferences. To this end, we propose Joint Reward Modeling (JRM), which jointly optimizes preference learning and language modeling on a shared vision-language backbone. This approach internalizes the semantic and reasoning capabilities of generative models into efficient discriminative representations, enabling fast and accurate evaluation. JRM achieves state-of-the-art results on MMRB2 and EditReward-Bench, and significantly improves stability and performance in downstream online reinforcement learning. These results show that joint training effectively bridges efficiency and semantic understanding in reward modeling.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07533" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07533" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07533" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">16. iGRPO: Self-Feedback-Driven LLM Reasoning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Ali Hatamizadeh, Shrimai Prabhumoye, Igor Gitman, Ximing Lu, Seungju Han, Wei Ping, Yejin Choi, Jan Kautz</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Iterative Group Relative Policy Optimization (iGRPO) enhances mathematical reasoning in large language models by employing a two-stage self-conditioning process that optimizes policy updates based on the highest-reward drafts from previous iterations. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\% and 79.64\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.09000" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.09000" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.09000" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">17. Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Emmett Bicker</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Aster, an autonomous scientific discovery agent, achieves state-of-the-art results across diverse domains like mathematics and biology by iteratively refining programs at speeds exceeding existing frameworks by twentyfold. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We introduce Aster, an AI agent for autonomous scientific discovery capable of operating over 20 times faster than existing frameworks. Given a task, an initial program, and a script to evaluate the performance of the program, Aster iteratively improves the program, often leading to new state-of-the-art performances. Aster&#39;s significant reduction in the number of iterations required for novel discovery expands the domain of tractable problems to include tasks with long evaluation durations, such as multi-hour machine learning training runs. We applied Aster to problems in mathematics, GPU kernel engineering, biology, neuroscience, and language model training. More specifically: the Erdos minimum overlap problem, optimizing the TriMul kernel, a single-cell analysis denoising problem, training a neural activity prediction model to perform well on ZAPBench, and the NanoGPT Speedrun Competition. Aster attains SOTA results in every task, except for ZAPBench, where it matches the performance of the best human solution with less than 1/190th of the compute. Aster is accessible via a web interface and API at asterlab.ai.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07040" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07040" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07040" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">18. Discrete Adjoint Matching
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Oswin So, Brian Karrer, Chuchu Fan, Ricky T. Q. Chen, Guan-Horng Liu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Discrete Adjoint Matching (DAM) extends adjoint-based fine-tuning to discrete generative models by introducing a statistically derived discrete adjoint estimator suitable for Continuous-Time Markov Chains. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Computation methods for solving entropy-regularized reward optimization -- a class of problems widely used for fine-tuning generative models -- have advanced rapidly. Among those, Adjoint Matching (AM, Domingo-Enrich et al., 2025) has proven highly effective in continuous state spaces with differentiable rewards. Transferring these practical successes to discrete generative modeling, however, remains particularly challenging and largely unexplored, mainly due to the drastic shift in generative model classes to discrete state spaces, which are nowhere differentiable. In this work, we propose Discrete Adjoint Matching (DAM) -- a discrete variant of AM for fine-tuning discrete generative models characterized by Continuous-Time Markov Chains, such as diffusion-based large language models. The core of DAM is the introduction of discrete adjoint-an estimator of the optimal solution to the original problem but formulated on discrete domains-from which standard matching frameworks can be applied. This is derived via a purely statistical standpoint, in contrast to the control-theoretic viewpoint in AM, thereby opening up new algorithmic opportunities for general adjoint-based estimators. We showcase DAM&#39;s effectiveness on synthetic and mathematical reasoning tasks.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07132" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07132" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07132" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">19. Prospective bounds on f(Q) gravity with pulsar timing arrays
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Mohammadreza Davari, Alireza Allahyari</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Analysis of pulsar timing array data within the framework of symmetric teleparallel f(Q) gravity constrains modified tensor mode damping, with future observatories like the Square Kilometre Array poised to distinguish these non-metricity effects from General Relativity. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Pulsar timing arrays (PTAs) have recently provided compelling evidence for a stochastic gravitational wave background (SGWB) in the nanohertz frequency band, offering a unique window into fundamental physics. Here, we explore implications for symmetric teleparallel $f(Q)$ gravity, a theory in which deviations from General Relativity (GR) arise through the non-metricity scalar $f(Q)$. Crucially, tensor modes propagate at the speed of light in this framework. However, their amplitude undergoes a modified damping during their evolution. We adopt a model-independent parameterization and derive an analytic approximation to the tensor mode transfer function to obtain the spectral energy density of primordial inflationary gravitational waves. Comparison with the NANOGrav 15-year and IPTA second data releases show that the inferred damping parameter $n$ remains consistent with GR, yet allows small deviations that could be observable. We then conduct a Fisher information matrix forecasts which demonstrate that the Square Kilometre Array (SKA) observatory will improve these constraints by several orders of magnitude, offering the potential to distinguish $f(Q)$ gravity from GR with high precision. These results highlight PTAs as powerful probes of non-metricity-based modifications to gravity.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07557" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07557" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07557" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">20. Gravitational Wave Informed Inference of 21-cm Global Signal Parameters
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Avinash Tiwari, Sajad A. Bhat, Tirthankar Roy Choudhury, Susmita Adhikari, Mukesh Kumar Singh, Shasvath J. Kapadia</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Integrating binary black hole merger observations from next-generation gravitational wave detectors into a multi-messenger framework provides a method to break parameter degeneracies in the 21-cm cosmic hydrogen signal and better constrain the early Universe&#39;s star formation history. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Understanding how and when the first stars and galaxies formed remains one of the central challenges in modern cosmology. These structures emerged during the transition from the Dark Ages to the Cosmic Dawn, a period that remains observationally unconstrained despite strong theoretical progress. During this epoch, neutral hydrogen absorbed a fraction of cosmic microwave background photons through its 21-cm hyperfine transition, producing a 21-cm absorption signal whose evolution encodes the early Universe&#39;s thermal and ionization history. However, extracting the underlying astrophysical parameters from this signal is limited by severe parameter degeneracies, which cannot be resolved without independent observational probes. The next-generation gravitational wave (GW) detectors, such as Cosmic Explorer (CE), will observe binary black hole (BBH) mergers up to very large redshifts and hence will detect a fraction of them formed within the redshift range $\sim 13-25$. The merger rate of these BBHs will depend on the star formation rate density (SFRD) at these redshifts, together with the BBH formation efficiency and a time delay distribution. Therefore, the merger rate of these BBHs can work as a tracer of the SFRD in the redshift range $\sim 13-25$. In this Letter, we establish a novel multi-messenger framework and present a proof-of-principle concept of how the observations of BBH mergers form next-generation GW detectors can improve the inference of parameters generating the 21-cm cosmic hydrogen signal, and help break degeneracies between them.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07631" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07631" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07631" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">21. diffpy.morph: Python tools for model independent comparisons between sets of 1D functions
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Andrew Yang, Christopher L. Farrow, Pavol Juhás, Luis Kitsu Iglesias, Chia-Hao Liu, Samuel D. Marks, Vivian R. K. Wall, Joshua Safin, Sean M. Drewry, Caden Myers, et al.</span>
                                <span class="author-full" style="display: none;">Andrew Yang, Christopher L. Farrow, Pavol Juhás, Luis Kitsu Iglesias, Chia-Hao Liu, Samuel D. Marks, Vivian R. K. Wall, Joshua Safin, Sean M. Drewry, Caden Myers, Dillon F. Hanlon, Nicholas Leonard, Cedomir Petrovic, Ahhyun Jeong, Dmitri V. Talapin, Linda F. Nazar, Haidong Zhou, Samuel W. Teitelbaum, Tim B. van Driel, Soham Banerjee, Emil S. Bozin, Michael F. Toney, Katharine Page, Naomi S. Ginsberg, Simon J. L. Billinge</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Model-independent analysis of one-dimensional scientific spectra is enhanced by a Python package that applies transformations to isolate meaningful structural changes from experimental artifacts. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">diffpy.morph addresses a need to gain scientific insights from 1D scientific spectra in model independent ways. A powerful approach for this is to take differences between pairs of spectra and look for meaningful changes that might indicate underlying chemical, structural, or other modifications. The challenge is that the difference curve may contain uninteresting differences such as experimental inconsistencies and benign physical changes such as the effects of thermal expansion. diffpy$.$morph allows researchers to apply simple transformations, or &#34;morphs&#34;, to one of the datasets to remove the unwanted differences revealing, when they are present, non-trivial differences. diffpy$.$morph is an open-source Python package available on the Python Package Index and conda-forge. Here, we describe its functionality and apply it to solve a range of experimental challenges on diffraction and PDF data from x-rays and neutrons, though we note that it may be applied to any 1D function in principle.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.06987" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.06987" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.06987" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">physics.comp-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">22. Emergent Misalignment is Easy, Narrow Misalignment is Hard
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Anna Soligo, Edward Turner, Senthooran Rajamanoharan, Neel Nanda</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Emergent misalignment in large language models stems from general solutions that are more stable and robust than narrow ones, offering a linear representation for monitoring and mitigating undesirable behaviors. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Finetuning large language models on narrowly harmful datasets can cause them to become emergently misaligned, giving stereotypically `evil&#39; responses across diverse unrelated settings. Concerningly, a pre-registered survey of experts failed to predict this result, highlighting our poor understanding of the inductive biases governing learning and generalisation in LLMs. We use emergent misalignment (EM) as a case study to investigate these inductive biases and find that models can just learn the narrow dataset task, but that the general solution appears to be more stable and more efficient. To establish this, we build on the result that different EM finetunes converge to the same linear representation of general misalignment, which can be used to mediate misaligned behaviour. We find a linear representation of the narrow solution also exists, and can be learned by introducing a KL divergence loss. Comparing these representations reveals that general misalignment achieves lower loss, is more robust to perturbations, and is more influential in the pre-training distribution. This work isolates a concrete representation of general misalignment for monitoring and mitigation. More broadly, it offers a detailed case study and preliminary metrics for investigating how inductive biases shape generalisation in LLMs. We open-source all code, datasets and model finetunes.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07852" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07852" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07852" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">23. K-DRIFT Science Theme: Galaxies in the Faint Universe
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Woowon Byun, Yongmin Yoon, Jongwan Ko, Yun Hee Lee, Gain Lee, Ho Seong Hwang, Cristiano G. Sabiu, Kwang-il Seon, Kyungwon Chun, Jihye Shin, et al.</span>
                                <span class="author-full" style="display: none;">Woowon Byun, Yongmin Yoon, Jongwan Ko, Yun Hee Lee, Gain Lee, Ho Seong Hwang, Cristiano G. Sabiu, Kwang-il Seon, Kyungwon Chun, Jihye Shin, Jinsu Rhee, Jae-Woo Kim, Jaewon Yoo, Jaehyun Lee, Sang-Hyun Chun, Hong Soo Park, Soung-Chul Yang, Sungryong Hong, Jeehye Shin, Hyowon Kim</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Overcoming traditional photometric depth limitations, the K-DRIFT telescope enables the detection of faint low-surface-brightness structures to clarify the evolutionary history of nearby galaxies. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Low-surface-brightness (LSB) structures serve as evidence of the intricate mass assembly of galaxies, and dedicatedly studying them promises to give us profound insights into the evolutionary history of galaxies. Furthermore, delving into the properties of star formation (SF) in the LSB regime can broaden our understanding of SF activity in regions characterized by low surface gas density, thereby shedding light on fundamental cosmic processes. However, systematic uncertainties may hamper the exploration of the LSB universe by limiting detectable SB levels. Indeed, despite dedicated advancements in telescope and observing techniques over decades, achieving ultra-deep photometric depths in optical wavelengths remains a formidable challenge. To overcome this challenge and explore the LSB universe that we have yet to see, we have been developing a novel telescope called K-DRIFT. This paper outlines the telescope&#39;s specification and describes various LSB features we aim for, explicitly focusing on nearby individual galaxies. To further advance the capabilities of the K-DRIFT survey, focused on LSB detection, we present several feasible research topics that utilize other survey data together and discuss the role of LSB observation in understanding the evolution of galaxies.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.08283" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.08283" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.08283" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">24. Weak-Driven Learning: How Weak Agents make Strong Agents Stronger
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Zehao Chen, Gongxun Li, Tianxiang Ai, Yifei Li, Zixuan Huang, Wang Zhou, Fuzhen Zhuang, Xianglong Liu, Jianxin Li, Deqing Wang, et al.</span>
                                <span class="author-full" style="display: none;">Zehao Chen, Gongxun Li, Tianxiang Ai, Yifei Li, Zixuan Huang, Wang Zhou, Fuzhen Zhuang, Xianglong Liu, Jianxin Li, Deqing Wang, Yikun Ban</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Utilizing historical weak checkpoints to identify learning gaps through entropy dynamics allows large language models to surpass post-training performance saturation without increasing inference costs. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models&#39; own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.08222" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.08222" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.08222" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">25. GCN-MPPR: Enhancing the Propagation of Message Passing Neural Networks via Motif-Based Personalized PageRank
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Mingcan Wang, Junchang Xin, Zhongming Yao, Kaifu Long, Zhiqiong Wang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Incorporating motif-based personalized PageRank into graph convolutional networks captures higher-order structural relationships and mitigates over-smoothing to improve accuracy and stability. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The algorithms based on message passing neural networks (MPNNs) on graphs have recently achieved great success for various graph applications. However, studies find that these methods always propagate the information to very limited neighborhoods with shallow depth, particularly due to over-smoothing. That means most of the existing MPNNs fail to be so `deep&#39;. Although some previous work tended to handle this challenge via optimization- or structure-level remedies, the overall performance of GCNs still suffers from limited accuracy, poor stability, and unaffordable computational cost. Moreover, neglect of higher-order relationships during the propagation of MPNNs has further limited the performance of them. To overcome these challenges, a novel variant of PageRank named motif-based personalized PageRank (MPPR) is proposed to measure the influence of one node to another on the basis of considering higher-order motif relationships. Secondly, the MPPR is utilized to the message passing process of GCNs, thereby guiding the message passing process at a relatively `high&#39; level. The experimental results show that the proposed method outperforms almost all of the baselines on accuracy, stability, and time consumption. Additionally, the proposed method can be considered as a component that can underpin almost all GCN tasks, with DGCRL being demonstrated in the experiment. The anonymous code repository is available at: https://anonymous.4open.science/r/GCN-MPPR-AFD6/.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07903" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07903" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07903" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">26. Dynamic Black-hole Emission Tomography with Physics-informed Neural Fields
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Berthy T. Feng, Andrew A. Chael, David Bromley, Aviad Levis, William T. Freeman, Katherine L. Bouman</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Physics-informed differentiable neural rendering reconstructs dynamic three-dimensional gas environments near black holes by jointly estimating velocity and emissivity fields from sparse interferometric data. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">With the success of static black-hole imaging, the next frontier is the dynamic and 3D imaging of black holes. Recovering the dynamic 3D gas near a black hole would reveal previously-unseen parts of the universe and inform new physics models. However, only sparse radio measurements from a single viewpoint are possible, making the dynamic 3D reconstruction problem significantly ill-posed. Previously, BH-NeRF addressed the ill-posed problem by assuming Keplerian dynamics of the gas, but this assumption breaks down near the black hole, where the strong gravitational pull of the black hole and increased electromagnetic activity complicate fluid dynamics. To overcome the restrictive assumptions of BH-NeRF, we propose PI-DEF, a physics-informed approach that uses differentiable neural rendering to fit a 4D (time + 3D) emissivity field given EHT measurements. Our approach jointly reconstructs the 3D velocity field with the 4D emissivity field and enforces the velocity as a soft constraint on the dynamics of the emissivity. In experiments on simulated data, we find significantly improved reconstruction accuracy over both BH-NeRF and a physics-agnostic approach. We demonstrate how our method may be used to estimate other physics parameters of the black hole, such as its spin.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.08029" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.08029" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.08029" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">27. Selective Fine-Tuning for Targeted and Robust Concept Unlearning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Mansi, Avinash Kori, Francesca Toni, Soteris Demetriou</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Dynamic neuron estimation combined with Hessian-based regularization enables efficient and robust unlearning of harmful concepts in text-guided diffusion models. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Text guided diffusion models are used by millions of users, but can be easily exploited to produce harmful content. Concept unlearning methods aim at reducing the models&#39; likelihood of generating harmful content. Traditionally, this has been tackled at an individual concept level, with only a handful of recent works considering more realistic concept combinations. However, state of the art methods depend on full finetuning, which is computationally expensive. Concept localisation methods can facilitate selective finetuning, but existing techniques are static, resulting in suboptimal utility. In order to tackle these challenges, we propose TRUST (Targeted Robust Selective fine Tuning), a novel approach for dynamically estimating target concept neurons and unlearning them through selective finetuning, empowered by a Hessian based regularization. We show experimentally, against a number of SOTA baselines, that TRUST is robust against adversarial prompts, preserves generation quality to a significant degree, and is also significantly faster than the SOTA. Our method achieves unlearning of not only individual concepts but also combinations of concepts and conditional concepts, without any specific regularization.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07919" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07919" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07919" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">28. Complementary Roles of Distance and Growth Probes in Testing Time-Varying Dark Energy
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Seokcheon Lee</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Fisher information analysis reveals that while distance measurements suffer from an inherent sensitivity bottleneck, high-precision growth data can uniquely characterize the time-dependent features of dark energy. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Distance measurements have long provided the primary observational constraints on the expansion history of the Universe and the properties of dark energy. However, because such observables depend on cumulative line-of-sight integrals over the Hubble rate, their sensitivity to time-dependent features of the dark energy equation of state is intrinsically limited. In this work, we examine this limitation from an information-based perspective using the eigenvalue structure of the Fisher information matrix constructed from distance, expansion rate, and growth observables. We show that distance and expansion-rate data generically produce a strongly hierarchical Fisher spectrum dominated by a single information mode, reflecting an irreducible loss of sensitivity to temporal variations in dark energy. This behavior can be traced directly to the integrated kernel structure of geometric observables. Growth measurements, by contrast, respond through differential dynamics and can introduce additional independent information directions. Using both controlled mock data and survey-like configurations representative of next-generation experiments, we find that the impact of growth information depends not only on its nominal precision but also on the structure of the data covariance. In simplified mock setups, growth measurements can partially activate a second information direction even at moderate precision. In Euclid-like configurations, however, the information remains effectively one-dimensional until growth precision reaches the percent level, below which a second mode emerges rapidly. These results clarify the complementary roles of distance and growth probes and provide a model-independent criterion for assessing the physical content of cosmological constraints on dynamical dark energy.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.08207" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.08207" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.08207" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">29. Structure-Aware Robust Counterfactual Explanations via Conditional Gaussian Network Classifiers
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zhan-Yi Liao, Jaewon Yoo, Hao-Tsung Yang, Po-An Chen</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Integrating conditional Gaussian networks with mixed-integer linear programming produces robust, structure-aware counterfactual explanations that maintain causal consistency through global optimization. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Counterfactual explanation (CE) is a core technique in explainable artificial intelligence (XAI), widely used to interpret model decisions and suggest actionable alternatives. This work presents a structure-aware and robustness-oriented counterfactual search method based on the conditional Gaussian network classifier (CGNC). The CGNC has a generative structure that encodes conditional dependencies and potential causal relations among features through a directed acyclic graph (DAG). This structure naturally embeds feature relationships into the search process, eliminating the need for additional constraints to ensure consistency with the model&#39;s structural assumptions. We adopt a convergence-guaranteed cutting-set procedure as an adversarial optimization framework, which iteratively approximates solutions that satisfy global robustness conditions. To address the nonconvex quadratic structure induced by feature dependencies, we apply piecewise McCormick relaxation to reformulate the problem as a mixed-integer linear program (MILP), ensuring global optimality. Experimental results show that our method achieves strong robustness, with direct global optimization of the original formulation providing especially stable and efficient results. The proposed framework is extensible to more complex constraint settings, laying the groundwork for future advances in counterfactual reasoning under nonconvex quadratic formulations.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.08021" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.08021" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.08021" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">30. Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Yanming Li, Xuelin Zhang, WenJie Lu, Ziye Tang, Maodong Wu, Haotian Luo, Tongtong Wu, Zijie Peng, Hongze Mi, Yibo Feng, et al.</span>
                                <span class="author-full" style="display: none;">Yanming Li, Xuelin Zhang, WenJie Lu, Ziye Tang, Maodong Wu, Haotian Luo, Tongtong Wu, Zijie Peng, Hongze Mi, Yibo Feng, Naiqiang Tan, Chao Huang, Hong Chen, Li Shen</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Decomposing rewards via Shapley-based credit attribution stabilizes multi-agent reinforcement learning and improves the coordination of large language models in complex tool-use scenarios. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Integrating Large Language Models (LLMs) with external tools via multi-agent systems offers a promising new paradigm for decomposing and solving complex problems. However, training these systems remains notoriously difficult due to the credit assignment challenge, as it is often unclear which specific functional agent is responsible for the success or failure of decision trajectories. Existing methods typically rely on sparse or globally broadcast rewards, failing to capture individual contributions and leading to inefficient reinforcement learning. To address these limitations, we introduce the Shapley-based Hierarchical Attribution for Reinforcement Policy (SHARP), a novel framework for optimizing multi-agent reinforcement learning via precise credit attribution. SHARP effectively stabilizes training by normalizing agent-specific advantages across trajectory groups, primarily through a decomposed reward mechanism comprising a global broadcast-accuracy reward, a Shapley-based marginal-credit reward for each agent, and a tool-process reward to improve execution efficiency. Extensive experiments across various real-world benchmarks demonstrate that SHARP significantly outperforms recent state-of-the-art baselines, achieving average match improvements of 23.66% and 14.05% over single-agent and multi-agent approaches, respectively.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.08335" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.08335" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.08335" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">31. Provably robust learning of regression neural networks using $β$-divergences
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Abhik Ghosh, Suryasis Jana</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Robust regression neural networks are enhanced through a β-divergence-based framework that provides theoretical convergence guarantees and a 50% asymptotic breakdown point for improved performance against outliers. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Regression neural networks (NNs) are most commonly trained by minimizing the mean squared prediction error, which is highly sensitive to outliers and data contamination. Existing robust training methods for regression NNs are often limited in scope and rely primarily on empirical validation, with only a few offering partial theoretical guarantees. In this paper, we propose a new robust learning framework for regression NNs based on the $β$-divergence (also known as the density power divergence) which we call `rRNet&#39;. It applies to a broad class of regression NNs, including models with non-smooth activation functions and error densities, and recovers the classical maximum likelihood learning as a special case. The rRNet is implemented via an alternating optimization scheme, for which we establish convergence guarantees to stationary points under mild, verifiable conditions. The (local) robustness of rRNet is theoretically characterized through the influence functions of both the parameter estimates and the resulting rRNet predictor, which are shown to be bounded for suitable choices of the tuning parameter $β$, depending on the error density. We further prove that rRNet attains the optimal 50\% asymptotic breakdown point at the assumed model for all $β\in(0, 1]$, providing a strong global robustness guarantee that is largely absent for existing NN learning methods. Our theoretical results are complemented by simulation experiments and real-data analyses, illustrating practical advantages of rRNet over existing approaches in both function approximation problems and prediction tasks with noisy observations.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.08933" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.08933" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.08933" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">32. DLLM-Searcher: Adapting Diffusion Large Language Model for Search Agents
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jiahao Zhao, Shaoxuan Xu, Zhongxiang Sun, Fengqi Zhu, Jingyang Ou, Yuling Shi, Chongxuan Li, Xiao Zhang, Jun Xu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Diffusion-based search agents achieve improved reasoning and reduced inference latency through a post-training pipeline and a parallel execution paradigm that prioritizes tool-call instructions. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Recently, Diffusion Large Language Models (dLLMs) have demonstrated unique efficiency advantages, enabled by their inherently parallel decoding mechanism and flexible generation paradigm. Meanwhile, despite the rapid advancement of Search Agents, their practical deployment is constrained by a fundamental limitation, termed as 1) Latency Challenge: the serial execution of multi-round reasoning, tool calling, and tool response waiting under the ReAct agent paradigm induces severe end-to-end latency. Intuitively, dLLMs can leverage their distinctive strengths to optimize the operational efficiency of agents under the ReAct agent paradigm. Practically, existing dLLM backbones face the 2) Agent Ability Challenge. That is, existing dLLMs exhibit remarkably weak reasoning and tool-calling capabilities, preventing these advantages from being effectively realized in practice. In this paper, we propose DLLM-Searcher, an optimization framework for dLLM-based Search Agents. To solve the Agent Ability Challenge, we design a two-stage post-training pipeline encompassing Agentic Supervised Fine-Tuning (Agentic SFT) and Agentic Variance-Reduced Preference Optimization Agentic VRPO, which enhances the backbone dLLM&#39;s information seeking and reasoning capabilities. To mitigate the Latency Challenge, we leverage the flexible generation mechanism of dLLMs and propose a novel agent paradigm termed Parallel-Reasoning and Acting P-ReAct. P-ReAct guides the model to prioritize decoding tool_call instructions, thereby allowing the model to keep thinking while waiting for the tool&#39;s return. Experimental results demonstrate that DLLM-Searcher achieves performance comparable to mainstream LLM-based search agents and P-ReAct delivers approximately 15% inference acceleration. Our code is available at https://anonymous.4open.science/r/DLLM-Searcher-553C</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07035" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07035" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07035" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">33. Assessing the Impact of Fitting Methodology at aN$^3$LO with FPPDF: an Open Source Tool for Extracting Parton Distribution Functions in the Hessian Approach
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>J. M. Cruz-Martinez, T. Giani, L. A. Harland-Lang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A new open-source tool enables global parton distribution function fits using fixed polynomial parameterizations and Hessian error propagation, facilitating direct comparisons between different perturbative orders and fitting methodologies. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present a new public code, FPPDF, to perform global fits of parton distribution functions (PDFs). The fitting methodology follows that implemented by the MSHT collaboration, namely applying a fixed polynomial parameterisation of the PDFs and Hessian approach to error propagation, while for data and theory settings the libraries used by the NNPDF collaboration are taken. This therefore complements the already publicly available NNPDF fitting code to enable fits with both neural network and fixed polynomial PDF parameterisations to be performed by the community, with otherwise identical theoretical and experimental inputs. As a first application, we use the new code to compare the PDFs found from fits at both NNLO and aN$^3$LO perturbative orders, but applying these two fitting approaches. We assess the impact of the two different methodologies on the PDFs and their uncertainties, providing results that complement previous comparisons between published PDF sets at NNLO and aN$^3$LO. We in particular find that the relative impact of going to the higher perturbative order and/or including missing higher order uncertainties is rather insensitive to which of these PDF parameterisation methodologies are used.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07118" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07118" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07118" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">hep-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">34. Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Siqu Ou, Tianrui Wan, Zhiyuan Zhao, Junyu Gao, Xuelong Li</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Reinforcement learning with region-level attention rewards improves the visual grounding and reasoning stability of multimodal large language models by correcting early-stage alignment errors. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.08241" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.08241" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.08241" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">35. OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Teng Wang, Rong Shan, Jianghao Lin, Junjie Wu, Tianyi Xu, Jianping Zhang, Wenteng Chen, Changwang Zhang, Zhaoxiang Wang, Weinan Zhang, et al.</span>
                                <span class="author-full" style="display: none;">Teng Wang, Rong Shan, Jianghao Lin, Junjie Wu, Tianyi Xu, Jianping Zhang, Wenteng Chen, Changwang Zhang, Zhaoxiang Wang, Weinan Zhang, Jun Wang</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Composed image retrieval is optimized by transforming heuristic agentic search into a formal trajectory optimization problem that utilizes mixed-integer programming and offline-derived demonstrations to guide visual-language model planners. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Composed image retrieval (CIR) requires complex reasoning over heterogeneous visual and textual constraints. Existing approaches largely fall into two paradigms: unified embedding retrieval, which suffers from single-model myopia, and heuristic agentic retrieval, which is limited by suboptimal, trial-and-error orchestration. To this end, we propose OSCAR, an optimization-steered agentic planning framework for composed image retrieval. We are the first to reformulate agentic CIR from a heuristic search process into a principled trajectory optimization problem. Instead of relying on heuristic trial-and-error exploration, OSCAR employs a novel offline-online paradigm. In the offline phase, we model CIR via atomic retrieval selection and composition as a two-stage mixed-integer programming problem, mathematically deriving optimal trajectories that maximize ground-truth coverage for training samples via rigorous boolean set operations. These trajectories are then stored in a golden library to serve as in-context demonstrations for online steering of VLM planner at online inference time. Extensive experiments on three public benchmarks and a private industrial benchmark show that OSCAR consistently outperforms SOTA baselines. Notably, it achieves superior performance using only 10% of training data, demonstrating strong generalization of planning logic rather than dataset-specific memorization.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.08603" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.08603" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.08603" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">36. MemFly: On-the-Fly Memory Optimization via Information Bottleneck
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zhenyuan Zhang, Xianzhang Jia, Zhiqin Yang, Zhenbo Song, Wei Xue, Sirui Han, Yike Guo</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Long-term memory in large language model agents is optimized through an information bottleneck-based framework that balances efficient data compression with a multi-pathway retrieval mechanism for complex queries. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Long-term memory enables large language model agents to tackle complex tasks through historical interactions. However, existing frameworks encounter a fundamental dilemma between compressing redundant information efficiently and maintaining precise retrieval for downstream tasks. To bridge this gap, we propose MemFly, a framework grounded in information bottleneck principles that facilitates on-the-fly memory evolution for LLMs. Our approach minimizes compression entropy while maximizing relevance entropy via a gradient-free optimizer, constructing a stratified memory structure for efficient storage. To fully leverage MemFly, we develop a hybrid retrieval mechanism that seamlessly integrates semantic, symbolic, and topological pathways, incorporating iterative refinement to handle complex multi-hop queries. Comprehensive experiments demonstrate that MemFly substantially outperforms state-of-the-art baselines in memory coherence, response fidelity, and accuracy.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07885" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07885" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07885" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">37. CFHT MegaCam Two Deep Fields Imaging Survey (2DFIS) I: Overview
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Binyang Liu, Wentao Luo, Martin Kilbinger, Shenming Fu, Ian Dell&#39;Antonio, Liping Fu, Xian Zhong Zheng, Yi-fu Cai, Cheng Jia, Ning Jiang, et al.</span>
                                <span class="author-full" style="display: none;">Binyang Liu, Wentao Luo, Martin Kilbinger, Shenming Fu, Ian Dell&#39;Antonio, Liping Fu, Xian Zhong Zheng, Yi-fu Cai, Cheng Jia, Ning Jiang, Qinxun Li, Yicheng Li, Shurui Lin, Christopher J. Miller, Surhud S. More, Huiyuan Wang, Yibo Wang</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A wide-field imaging survey using the Canada-France-Hawaii Telescope provides deep optical data and science-ready catalogs for investigating fast radio burst environments and galaxy cluster mass distributions. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present the Two Deep Fields Imaging Survey (2DFIS), a wide-field imaging program conducted with the Canada-France-Hawaii Telescope (CFHT) targeting two astrophysically distinct regions: one containing a repeating fast radio burst (FRB) source and another hosting a candidate of a rotating galaxy cluster. Achieving a depth of r~26mag, the survey enables a search for faint optical counterparts and environmental signatures associated with the FRB, while high-quality photometric and galaxy shape measurements in the cluster field support a weak-lensing analysis of its mass distribution. This paper describes the observing strategy and data processing methodology adopted for 2DFIS, including the use of the LSST Science Pipelines with survey-specific adaptations for CFHT/MegaCam data. We outline a complete workflow for transforming raw CFHT exposures into science-ready data products, including calibrated single-epoch images, multi-band coadded mosaics, and extensive source catalogs. These data products provide the foundation for ongoing and future studies of FRB host environments, cluster mass reconstruction, and related cosmological applications.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.08312" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.08312" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.08312" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.03</span>
                        <span class="badge bg-primary">Semantic Score: 0.91</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">38. MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Heewoong Noh, Gyoung S. Na, Namkyeong Lee, Chanyoung Park</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Material synthesis planning is unified into a single large language model framework that utilizes intermediate material classifications and hierarchical precursor biases to coordinate precursor and operation prediction. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Material synthesis planning (MSP) remains a fundamental and underexplored bottleneck in AI-driven materials discovery, as it requires not only identifying suitable precursor materials but also designing coherent sequences of synthesis operations to realize a target material. Although several AI-based approaches have been proposed to address isolated subtasks of MSP, a unified methodology for solving the entire MSP task has yet to be established. We propose MSP-LLM, a unified LLM-based framework that formulates MSP as a structured process composed of two constituent subproblems: precursor prediction (PP) and synthesis operation prediction (SOP). Our approach introduces a discrete material class as an intermediate decision variable that organizes both tasks into a chemically consistent decision chain. For OP, we further incorporate hierarchical precursor types as synthesis-relevant inductive biases and employ an explicit conditioning strategy that preserves precursor-related information in the autoregressive decoding state. Extensive experiments show that MSP-LLM consistently outperforms existing methods on both PP and SOP, as well as on the complete MSP task, demonstrating an effective and scalable framework for MSP that can accelerate real-world materials discovery.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07543" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07543" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07543" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">39. Far-from-Equilibrium Attractors and Universality in Ultra-Relativistic Heavy-Ion Collisions within Relativistic Kinetic Theory
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Vincenzo Nugara</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Relativistic kinetic theory and transport models are employed to investigate the emergence of collective behavior and universal attractors in hot quark-gluon plasma during ultra-relativistic heavy-ion collisions. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">This PhD Thesis is devoted to the study of the emergence of attractors, universality and collectivity in ultra-relativistic collisions by means of relativistic kinetic theory. After an introduction about Quantum Chromodynamics (QCD), Quark-Gluon Plasma (QGP) and the importance of heavy-ion collisions to investigate both, we give an overview about the two main models able to describe the hot QCD matter collective behaviour, namely kinetic theory and hydrodynamics. Afterwards, the Relativistic Boltzmann Transport (RBT) model, which has been employed to obtain most part of the results of this thesis, is carefully described, from the numerical and physical perspectives. The study of attractors and universality proceeds then by starting from a simple one-dimensional massless model, moving to increasingly more complex scenarios, involving the full 3+1D setup, non-conformal systems and realistic event-by-event fluctuations. Particular attention is paid to the physical scales which govern the system collectivity and their interplay. We show that a very good description of collective behaviour can be carried out by means of a few variables which characterise the systems under study.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07675" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07675" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07675" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">hep-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">40. Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Pengrui Han, Xueqiang Xu, Keyang Xuan, Peiyang Song, Siru Ouyang, Runchu Tian, Yuqing Jiang, Cheng Qian, Pengcheng Jiang, Jiashuo Sun, et al.</span>
                                <span class="author-full" style="display: none;">Pengrui Han, Xueqiang Xu, Keyang Xuan, Peiyang Song, Siru Ouyang, Runchu Tian, Yuqing Jiang, Cheng Qian, Pengcheng Jiang, Jiashuo Sun, Junxia Cui, Ming Zhong, Ge Liu, Jiawei Han, Jiaxuan You</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Large language models are efficiently adapted to new tasks by composing basis vectors from a low-dimensional semantic subspace, providing a data-efficient alternative to learning static steering directions. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07276" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07276" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07276" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">41. Probing Quantum Gravity effects with Extreme Mass Ratio Inspirals around Rotating Hayward Black Holes
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Dan Zhang, Chao Zhang, Qiyuan Pan, Guoyang Fu, Jian-Pin Wu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> High-precision observations of extreme mass-ratio inspirals around rotating Hayward black holes with LISA can reveal quantum-gravity signatures through detectable waveform dephasing. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We investigate extreme mass-ratio inspirals (EMRIs) around a rotating Hayward black hole to assess the detectability of signatures arising from quantum gravity.The quantum parameter $α_0$, which encodes deviations from general relativity (GR), introduces extra correction terms in both the orbital frequency and the fluxes. Our results show that after one year of accumulated observation, these corrections induce a detectable dephasing in the EMRI waveform. Using the modified orbital evolution driven by $α_0$, we generate waveforms via the augmented analytic kludge (AAK) model implemented in the \texttt{FastEMRIWaveforms} package. Furthermore, we utilize the time-delay interferometry (TDI) to suppress the laser noise and phase fluctuations induced by spacecraft motion, and then employ the Fisher information matrix (FIM) to test the sensitivity of LISA in detecting deviations from GR. Our results demonstrate the potential of LISA to probe quantum-gravity effects through high-precision observations of EMRIs.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07436" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07436" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07436" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">42. Negative-Aware Diffusion Process for Temporal Knowledge Graph Extrapolation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yanglei Gan, Peng He, Yuxiang Cai, Run Lin, Guanyu Zhou, Qiao Liu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Integrating negative context and a cosine-alignment regularizer into a diffusion-based framework enhances the calibration and predictive accuracy of temporal knowledge graph extrapolation. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Temporal Knowledge Graph (TKG) reasoning seeks to predict future missing facts from historical evidence. While diffusion models (DM) have recently gained attention for their ability to capture complex predictive distributions, two gaps remain: (i) the generative path is conditioned only on positive evidence, overlooking informative negative context, and (ii) training objectives are dominated by cross-entropy ranking, which improves candidate ordering but provides little supervision over the calibration of the denoised embedding. To bridge this gap, we introduce Negative-Aware Diffusion model for TKG Extrapolation (NADEx). Specifically, NADEx encodes subject-centric histories of entities, relations and temporal intervals into sequential embeddings. NADEx perturbs the query object in the forward process and reconstructs it in reverse with a Transformer denoiser conditioned on the temporal-relational context. We further derive a cosine-alignment regularizer derived from batch-wise negative prototypes, which tightens the decision boundary against implausible candidates. Comprehensive experiments on four public TKG benchmarks demonstrate that NADEx delivers state-of-the-art performance.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.08815" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.08815" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.08815" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">43. DISCOVER: A Physics-Informed, GPU-Accelerated Symbolic Regression Framework
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Udaykumar Gajera, Mohsen Sotoudeh, Kanchan Sarkar, Axel Groß</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> DISCOVER provides a modular, GPU-accelerated symbolic regression framework designed to extract physically consistent and interpretable mathematical descriptors from complex scientific datasets. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Symbolic Regression (SR) enables the discovery of interpretable mathematical relationships from experimental and simulation data. These relationships are often coined descriptors which are defined as a fundamental materials property that is directly correlated to a desired or undesired functional property of the material. Although established approaches such as Sure Independence Screening and Sparsifying Operator (SISSO) have successfully identified low-dimensional descriptors within large feature spaces many existing SR tools integrate poorly with modern Python workflows, offer limited control over the symbolic search space, or struggle with the computational demands of large-scale studies. This paper introduces DISCOVER (Data-Informed Symbolic Combination of Operators for Variable Equation Regression), an open-source symbolic regression package developed to address these challenges through a modular, physics-motivated design. DISCOVER allows users to guide the symbolic search using domain knowledge, constrain the feature space explicitly, and take advantage of optional GPU acceleration to improve computational efficiency in data-intensive workflows, enabling reproducible and scalable SR workflows. The software is intended for applications in computational physics, computational chemistry, and materials science, where interpretability, physical consistency, and execution time are especially important, and it complements general-purpose SR frameworks by emphasizing the discovery of physically meaningful models.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.06986" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.06986" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.06986" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">physics.comp-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">44. MemAdapter: Fast Alignment across Agent Memory Paradigms via Generative Subgraph Retrieval
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Xin Zhang, Kailai Yang, Chenyue Li, Hao Li, Qiyu Wei, Jun&#39;ichi Tsujii, Sophia Ananiadou</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> MemAdapter unifies heterogeneous memory paradigms for large language model agents using a generative subgraph retriever and a lightweight contrastive alignment module to achieve efficient cross-paradigm fusion. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Memory mechanism is a core component of LLM-based agents, enabling reasoning and knowledge discovery over long-horizon contexts. Existing agent memory systems are typically designed within isolated paradigms (e.g., explicit, parametric, or latent memory) with tightly coupled retrieval methods that hinder cross-paradigm generalization and fusion. In this work, we take a first step toward unifying heterogeneous memory paradigms within a single memory system. We propose MemAdapter, a memory retrieval framework that enables fast alignment across agent memory paradigms. MemAdapter adopts a two-stage training strategy: (1) training a generative subgraph retriever from the unified memory space, and (2) adapting the retriever to unseen memory paradigms by training a lightweight alignment module through contrastive learning. This design improves the flexibility for memory retrieval and substantially reduces alignment cost across paradigms. Comprehensive experiments on three public evaluation benchmarks demonstrate that the generative subgraph retriever consistently outperforms five strong agent memory systems across three memory paradigms and agent model scales. Notably, MemAdapter completes cross-paradigm alignment within 13 minutes on a single GPU, achieving superior performance over original memory retrievers with less than 5% of training compute. Furthermore, MemAdapter enables effective zero-shot fusion across memory paradigms, highlighting its potential as a plug-and-play solution for agent memory systems.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.08369" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.08369" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.08369" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods / representation learning, neural architectures, generative modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">45. Scavenger hunt: Selection of obscured active galactic nuclei combining multiband optical variability and colors
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Demetra De Cicco, Stefano Cavuoti, Maurizio Paolillo, Vincenzo Petrecca, Ylenia Maruccia, Paula Sánchez-Sáez</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Combining multi-band optical variability with color-based features in a random forest classifier significantly improves the detection and purity of obscured active galactic nuclei in wide-field survey data. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">As wide-field optical surveys such as Vera Rubin Observatory&#39;s Legacy Survey of Space and Time (LSST) begin operations, time-domain astronomy is facing a data revolution, paving the road for new, expanded variability studies. This work leverages the complementary power of optical variability and color selection to identify active galactic nuclei (AGN), focusing on optimizing the identification of obscured AGN, typically more challenging to distinguish from inactive galaxies based on optical variability alone. The analysis is designed to provide valuable insights in the context of performance preview for the LSST, albeit using a scaled-down version of the LSST dataset. We present the first combined AGN selection based on g+r+i band light curves from the VST-COSMOS survey, spanning 3.3 yr. We identify AGN candidates independently in each band using a random forest (RF) classifier trained on features mainly related to optical variability, along with six optical/infrared colors and a morphology indicator. We subsequently merge the three band-specific samples in order to enhance selection purity and reliability. We then focus on defining a subset of features that significantly improve the identification of obscured AGN. The RF classifiers yield a consistent performance across the three bands, highlighting the critical role of contamination. Using the combined three-band plus color selection we successfully recover $58^{+9}_{-8}\%$ of all AGN and $69^{+10}_{-8}\%$ of the known obscured AGN that have been independently confirmed in all three bands. When requiring confirmation in two out of the three bands, these fractions increase to $69^{+10}_{-8}\%$ and $80^{+10}_{-9}\%$, respectively. We also demonstrate that, while combining variability features with colors is crucial to improve obscured AGN selection, relying solely on color features returns a markedly higher contamination rate.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07109" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07109" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07109" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">46. Event-Chain Monte Carlo: The global-balance breakthrough
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>E. A. J. F. Peters</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Generalizing the principles of global balance and persistent dynamics from hard-sphere systems to continuous potentials provides a robust framework for rejection-free, event-driven Monte Carlo sampling. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The seminal 2009 paper by Bernard, Krauth, and Wilson marked a paradigm shift in Monte Carlo sampling. By abandoning the restrictive condition of detailed balance in favor of the more fundamental principle of global balance, they introduced the Event-Chain Monte Carlo (ECMC) algorithm, which achieves rejection-free, deterministic sampling for hard spheres. This breakthrough demonstrated that persistent, directional dynamics could dramatically accelerate equilibration in dense particle systems. In this commentary, we review this foundational work and elucidate its underlying mechanism using the broader Event-Driven Monte Carlo (EDMC) framework developed in subsequent years. We show how the original hard-sphere concept naturally generalizes to continuous potentials and modern lifted Markov chain formalisms, transforming a surprising specific result into a powerful general class of sampling algorithms.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07199" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07199" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07199" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">physics.comp-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">47. Interplay of Lorentz Invariance Violation and Earth&#39;s Matter Potential in High-Energy Neutrinos
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Simon Hilding-Nørkjær, Johann Ioannou-Nikolaides, D. Jason Koskinen, Thomas Stuttard</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Interplay between anisotropic Lorentz invariance violation and the Earth&#39;s matter potential generates unique resonant enhancements and symmetry-breaking signatures detectable by high-energy neutrino telescopes. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Searches for Lorentz invariance violation (LIV) in the neutrino sector have traditionally focused on non-standard neutrino oscillations induced by LIV in vacuum. In this work, however, we study anisotropic LIV in matter. First, we review vacuum LIV phenomenology, explaining the energy and direction dependence of sidereal modulations for anisotropic coefficients in the Standard Model Extension. We then demonstrate that for high-energy neutrinos, the interplay between anisotropic LIV operators and the Earth&#39;s matter potential produces, distinct, observable signatures absent in the vacuum case. We identify a crossover regime where the energy-dependent LIV Hamiltonian becomes comparable to the matter potential, leading to strong interference effects. By analyzing the propagation of neutrinos through a realistic Earth model, we establish three key phenomenological consequences: (1) direction-dependent resonant enhancements of oscillation probabilities, (2) a macroscopic breakdown of neutrino-antineutrino symmetry for CPT-even operators, and (3) a significant increase of the $ν_τ$ flux due to LIV-driven injection of high-energy neutrinos into the $τ$ regeneration cycle. These results highlight that accounting for the interplay between LIV and matter is essential for future LIV searches at large-scale neutrino telescopes.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.08076" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.08076" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.08076" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">hep-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">48. Self-resonance preheating in deformed attractor models: oscillon formation and evolution
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Bao-Min Gu, Yu-Peng Zhang, Fu-Wen Shu, Yu-Xiao Liu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Introducing Gaussian features into the inflaton potential alters self-resonance dynamics, leading to shorter-lived oscillons and modified high-frequency gravitational wave spectra during the reheating phase. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">It is well known that, in potentials that are quadratic near the minimum but shallower away, such as small-$α$ ($\ll M_P^2$) attractors, the inflaton condensate fragments into localized compact objects known as oscillons during self-resonance preheating. In this work we investigate the self-resonance in deformed $α$-attractor T-model with a Gaussian feature near the minimum, distant from inflation&#39;s end. Linear analysis reveals altered resonance bands and deformed Floquet charts dependent on feature parameters. In fully nonlinear lattice simulations, we find that the gradient energy transfer is largely independent of the potential feature parameter $h$. In contrast, after resonance terminates, the subsequent evolution of gradient energy becomes strongly dependent on $h$. Statistical analysis reveals that models with the potential feature produce larger number of smaller oscillons, with a reduced energy stored in these objects, increasingly suppressed as the magnitude of $h$ grows. By tracking the total energy and the gradient energy contained in oscillons, we find that in models with nonzero $h$ oscillons are systematically shorter-lived, with this effect strengthening for larger $h$. The gravitational wave emission is dominated by the resonance stage and is strongly suppressed once oscillons form. Potential features leave the low-frequency spectrum largely unchanged but significantly modify the high-frequency tail. Although a complete reheating description requires external couplings and higher-resolution simulations, clear qualitative differences of cosmic expansion history already emerge within our simulated time window. These results highlight the important role of potential features in shaping reheating dynamics and their cosmological implications, and provide a deeper understanding of preheating dynamics and the properties of oscillons.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07972" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07972" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07972" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">49. Dark Matter as Screened Ordinary Matter
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Colin D. Froggatt, Holger Bech Nielsen</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Phenomenological evidence from 3.5 keV X-ray emissions and DAMA results suggests a dark matter model where highly compressed ordinary matter within a novel vacuum state constitutes the primary mass component. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We look at our since long studied model for dark matter as being pearls of a speculated new vacuum containing highly compressed ordinary matter, with so much ordinary in it that the content of ordinary matter in the dark matter pearls dominate. Most dark matter models have the dark matter consisting mainly of new-physics-matter such as WIMPs being supersymmetric partners of possibly known particles or, as in Maxim Khlopovs model, a doubly negatively charged new-physics-particle with a helium nucleus attached. But usually the new-physics matter makes up weight-wise the major content. It is only in our model that the ordinary matter content in the dark matter dominates. We here expose some weak phenomenological evidence that, in truth, dark matter should be of the type with a dominant component of ordinary matter (weight-wise), thus favoring as the typical example our previously so much studied vacuum type 2 model. The main such evidence is that we manage a fit to data in which the 3.5 keV X-rays, presumed to result from dark matter, come both from collisions of dark matter with dark matter and from dark matter with ordinary matter! Both mechanisms are of so similar an order of magnitude that they are both seen, indicating that their similarity is due to a significant similarity between dark with ordinary matter. The fact that the amounts of ordinary and dark matter only deviate by a factor 6 points in the same direction. Using the information obtained from this fitting, we develop our speculation that the main content weight-wise of dark matter is ordinary matter to the very DAMA experiment. Actually we found three spots on the sky in which we fit the observed production of 3.5 keV X-rays with ordinary plus dark scattering.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.07902" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.07902" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.07902" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">hep-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: weak lensing, galaxy clustering, cosmology / weak lensing, galaxy clustering, halo connection</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">50. Cutting Through the Noise: On-the-fly Outlier Detection for Robust Training of Machine Learning Interatomic Potentials
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Terry C. W. Lam, Niamh O&#39;Neill, Christoph Schran, Lars L. Schaaf</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> An unsupervised, on-the-fly outlier detection scheme mitigates the impact of numerical noise in electronic-structure data, enabling the robust training of machine learning interatomic potentials without additional reference calculations. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The accuracy of machine learning interatomic potentials suffers from reference data that contains numerical noise. Often originating from unconverged or inconsistent electronic-structure calculations, this noise is challenging to identify. Existing mitigation strategies such as manual filtering or iterative refinement of outliers, require either substantial expert effort or multiple expensive retraining cycles, making them difficult to scale to large datasets. Here, we introduce an on-the-fly outlier detection scheme that automatically down-weights noisy samples, without requiring additional reference calculations. By tracking the loss distribution via an exponential moving average, this unsupervised method identifies outliers throughout a single training run. We show that this approach prevents overfitting and matches the performance of iterative refinement baselines with significantly reduced overhead. The method&#39;s effectiveness is demonstrated by recovering accurate physical observables for liquid water from unconverged reference data, including diffusion coefficients. Furthermore, we validate its scalability by training a foundation model for organic chemistry on the SPICE dataset, where it reduces energy errors by a factor of three. This framework provides a simple, automated solution for training robust models on imperfect datasets across dataset sizes.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.08849" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.08849" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.08849" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: cosmology, Bayesian inference, sampling methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
    </div>

    <footer class="footer">
        &copy; 2026 arXiv Daily · Powered by <a href="https://arxiv.org/" target="_blank">arXiv</a>
    </footer>
</body>
</html>