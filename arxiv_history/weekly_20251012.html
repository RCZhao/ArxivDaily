<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>arXiv Daily: Backfill (2025-10-05 to 2025-10-12)</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto:400,700&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Roboto', Arial, sans-serif; background: #f8f9fa; transition: background 0.3s; }
        .arxiv-card { margin: 2em auto; max-width: 900px; box-shadow: 0 2px 8px rgba(0,0,0,0.08); border-radius: 16px; transition: box-shadow 0.3s, transform 0.3s; }
        .arxiv-card:hover { box-shadow: 0 4px 12px rgba(0,0,0,0.1); transform: translateY(-1px); }
        .arxiv-title { background: linear-gradient(90deg, #0d6efd 60%, #6610f2 100%); color: #fff; border-radius: 16px 16px 0 0; padding: 1.5em; font-size: 1.5em; font-weight: 700; letter-spacing: 0.5px;}
        .arxiv-meta { font-size: 1em; color: #555; margin-bottom: 1em; }
        .arxiv-abstract { background: #fff; padding: 1.5em; border-radius: 0 0 16px 16px; font-size: 1.1em; line-height: 1.7;}
        .arxiv-links a { margin-right: 1em; }
        .arxiv-scores { margin-top: 1em; font-size: 1em; }
        .navbar { background: #fff; box-shadow: 0 2px 8px rgba(0,0,0,0.04);}
        .footer { text-align: center; color: #888; padding: 2em 0 1em 0; font-size: 0.95em;}
        @media (prefers-color-scheme: dark) {
            body { background: #181a1b; color: #e4e6eb; }
            .arxiv-card { background: #23272b; box-shadow: 0 2px 8px rgba(0,0,0,0.28);}
            .arxiv-card:hover { box-shadow: 0 5px 15px rgba(0,0,0,0.25); transform: translateY(-1px); }
            .arxiv-title { background: linear-gradient(90deg, #375a7f 60%, #6f42c1 100%);}
            .arxiv-abstract { background: #23272b; color: #e4e6eb;}
            .navbar { background: #23272b; color: #e4e6eb;}
            .text-muted { color: #adb5bd !important; }
        }
        /* Improved abstract readability */
        .arxiv-abstract-text {
            font-size: 1.1em;
            line-height: 1.7;
        }
        p.big {
            line-height: 1.6;
            font-size: large;
        }
    </style>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'] ],
          processEscapes: true
        }
      });
    </script>
    <script>
    function toggleAuthors(element) {
        const fullList = element.querySelector('.author-full');
        const shortList = element.querySelector('.author-short');
        const toggleText = element.querySelector('.author-toggle');
        
        fullList.style.display = fullList.style.display === 'none' ? 'inline' : 'none';
        shortList.style.display = shortList.style.display === 'none' ? 'inline' : 'none';
        toggleText.textContent = fullList.style.display === 'none' ? ' (show all)' : ' (show less)';
    }
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>
<body>
    <nav class="navbar navbar-expand-lg mb-4">
        <div class="container">
            <a class="navbar-brand fw-bold text-primary" href="#">arXiv Daily</a>
            <span class="navbar-text">Modern arXiv Reader</span>
        </div>
    </nav>

    
    <div class="container py-4">
        <header class="mb-4"><h2 class="display-6 text-center">Analysis of Your Favorites</h2></header>
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Word Cloud</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/word_cloud.png" class="img-fluid rounded" alt="Word Cloud of favorite paper topics">
                    </div>
                </div>
            </div>
        </div>
        
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Clusters</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/backfill_cluster_map.png" class="img-fluid rounded" alt="2D Cluster plot of favorite papers">
                    </div>
                </div>
            </div>
        </div>
        
    </div>
    

    <div class="container py-4">
        <header class="mb-4"><h1 class="display-5 text-center text-primary">arXiv: Backfill (2025-10-05 to 2025-10-12)</h1></header>

        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Score Distribution of All Papers Found Today</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/score_distribution.png" class="img-fluid rounded" alt="Score distribution of all papers found today">
                    </div>
                </div>
            </div>
        </div>
        

        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">1. Euclid preparation: Towards a DR1 application of higher-order weak lensing statistics
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Euclid Collaboration, S. Vinciguerra, F. Bouchè, N. Martinet, L. Castiblanco, C. Uhlemann, S. Pires, J. Harnois-Déraps, C. Giocoli, M. Baldi, et al.</span>
                                <span class="author-full" style="display: none;">Euclid Collaboration, S. Vinciguerra, F. Bouchè, N. Martinet, L. Castiblanco, C. Uhlemann, S. Pires, J. Harnois-Déraps, C. Giocoli, M. Baldi, V. F. Cardone, A. Vadalà, N. Dagoneau, L. Linke, E. Sellentin, P. L. Taylor, J. C. Broxterman, S. Heydenreich, V. Tinnaneri Sreekanth, N. Porqueres, L. Porth, M. Gatti, D. Grandón, A. Barthelemy, F. Bernardeau, A. Tersenov, H. Hoekstra, J. -L. Starck, S. Cheng, P. A. Burger, I. Tereno, R. Scaramella, B. Altieri, S. Andreon, N. Auricchio, C. Baccigalupi, S. Bardelli, A. Biviano, E. Branchini, M. Brescia, S. Camera, G. Cañas-Herrera, V. Capobianco, C. Carbone, J. Carretero, M. Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C. Colodro-Conde, G. Congedo, L. Conversi, Y. Copin, F. Courbin, H. M. Courtois, M. Cropper, A. Da Silva, H. Degaudenzi, S. de la Torre, G. De Lucia, H. Dole, F. Dubath, X. Dupac, S. Dusini, S. Escoffier, M. Farina, R. Farinelli, S. Farrens, F. Faustini, S. Ferriol, F. Finelli, M. Frailis, E. Franceschi, M. Fumana, S. Galeotta, K. George, B. Gillis, J. Gracia-Carpio, A. Grazian, F. Grupp, S. V. H. Haugan, W. Holmes, F. Hormuth, A. Hornstrup, P. Hudelot, K. Jahnke, M. Jhabvala, B. Joachimi, E. Keihänen, S. Kermiche, A. Kiessling, M. Kilbinger, B. Kubik, M. Kunz, H. Kurki-Suonio, A. M. C. Le Brun, S. Ligori, P. B. Lilje, V. Lindholm, I. Lloro, G. Mainetti, D. Maino, O. Mansutti, O. Marggraf, M. Martinelli, F. Marulli, R. J. Massey, E. Medinaceli, S. Mei, M. Melchior, Y. Mellier, M. Meneghetti, G. Meylan, A. Mora, M. Moresco, L. Moscardini, C. Neissner, S. -M. Niemi, C. Padilla, S. Paltani, F. Pasian, K. Pedersen, V. Pettorino, G. Polenta, M. Poncet, L. A. Popa, F. Raison, A. Renzi, J. Rhodes, G. Riccio, E. Romelli, M. Roncarelli, R. Saglia, Z. Sakr, A. G. Sánchez, D. Sapone, B. Sartoris, P. Schneider, T. Schrabback, A. Secroun, G. Seidel, S. Serrano, C. Sirignano, G. Sirri, A. Spurio Mancini, L. Stanco, J. Steinwagner, P. Tallada-Crespí, A. N. Taylor, N. Tessore, S. Toft, R. Toledo-Moreo, F. Torradeflot, I. Tutusaus, J. Valiviita, T. Vassallo, Y. Wang, J. Weller, A. Zacchei, G. Zamorani, F. M. Zerbi, E. Zucca, M. Ballardini, M. Bolzonella, A. Boucaud, E. Bozzo, C. Burigana, R. Cabanac, M. Calabrese, A. Cappi, J. A. Escartin Vigo, L. Gabarra, W. G. Hartley, R. Maoli, J. Martín-Fleitas, S. Matthew, N. Mauri, R. B. Metcalf, A. Pezzotta, M. Pöntinen, I. Risso, V. Scottez, M. Sereno, M. Tenti, M. Viel, M. Wiesmann, Y. Akrami, I. T. Andika, R. E. Angulo, S. Anselmi, M. Archidiacono, F. Atrio-Barandela, E. Aubourg, D. Bertacca, M. Bethermin, A. Blanchard, L. Blot, M. Bonici, S. Borgani, M. L. Brown, S. Bruton, A. Calabro, B. Camacho Quevedo, F. Caro, C. S. Carvalho, T. Castro, F. Cogato, S. Conseil, A. R. Cooray, G. Desprez, A. Díaz-Sánchez, J. J. Diaz, S. Di Domizio, J. M. Diego, M. Y. Elkhashab, Y. Fang, P. G. Ferreira, A. Finoguenov, A. Franco, K. Ganga, J. García-Bellido, T. Gasparetto, V. Gautard, R. Gavazzi, E. Gaztanaga, F. Giacomini, F. Gianotti, G. Gozaliasl, M. Guidi, C. M. Gutierrez, A. Hall, S. Hemmati, H. Hildebrandt, J. Hjorth, J. J. E. Kajava, Y. Kang, D. Karagiannis, K. Kiiveri, J. Kim, C. C. Kirkpatrick, S. Kruk, L. Legrand, M. Lembo, F. Lepori, G. Leroy, G. F. Lesci, J. Lesgourgues, T. I. Liaudat, J. Macias-Perez, M. Magliocchetti, F. Mannucci, C. J. A. P. Martins, L. Maurin, M. Miluzio, P. Monaco, C. Moretti, G. Morgante, S. Nadathur, K. Naidoo, A. Navarro-Alsina, S. Nesseris, D. Paoletti, F. Passalacqua, K. Paterson, L. Patrizii, A. Pisani, D. Potter, S. Quai, M. Radovich, S. Sacquegna, M. Sahlén, D. B. Sanders, E. Sarpa, A. Schneider, D. Sciotti, L. C. Smith, K. Tanidis, C. Tao, G. Testera, R. Teyssier, S. Tosi, A. Troja, M. Tucci, D. Vergani, G. Verza, N. A. Walton</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> With respect to our first paper, we develop a full tomographic analysis based on realistic photometric redshifts which allows us to derive Fisher forecasts in the ($\sigma_8$, $w_0$) plane for a \textit{Euclid}-like data release 1 (DR1) setup. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">This is the second paper in the HOWLS (higher-order weak lensing statistics) series exploring the usage of non-Gaussian statistics for cosmology inference within \textit{Euclid}. With respect to our first paper, we develop a full tomographic analysis based on realistic photometric redshifts which allows us to derive Fisher forecasts in the ($\sigma_8$, $w_0$) plane for a \textit{Euclid}-like data release 1 (DR1) setup. We find that the 5 higher-order statistics (HOSs) that satisfy the Gaussian likelihood assumption of the Fisher formalism (1-point probability distribution function, $\ell$1-norm, peak counts, Minkowski functionals, and Betti numbers) each outperform the shear 2-point correlation functions by a factor $2.5$ on the $w_0$ forecasts, with only marginal improvement when used in combination with 2-point estimators, suggesting that every HOS is able to retrieve both the non-Gaussian and Gaussian information of the matter density field. The similar performance of the different estimators\inlinecomment{, with a slight preference for Minkowski functionals and 1-point probability distribution function,} is explained by a homogeneous use of multi-scale and tomographic information, optimized to lower computational costs. These results hold for the $3$ mass mapping techniques of the \textit{Euclid} pipeline: aperture mass, Kaiser--Squires, and Kaiser--Squires plus, and are unaffected by the application of realistic star masks. Finally, we explore the use of HOSs with the Bernardeau--Nishimichi--Taruya (BNT) nulling scheme approach, finding promising results towards applying physical scale cuts to HOSs.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04953" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04953" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04953" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 14.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.99</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">2. Validation of the DESI-DR1 3x2-pt analysis: scale cut and shear ratio tests
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">N. Emas, A. Porredon, C. Blake, J. DeRose, J. Aguilar, S. Ahlen, D. Bianchi, D. Brooks, F. J. Castander, T. Claybaugh, et al.</span>
                                <span class="author-full" style="display: none;">N. Emas, A. Porredon, C. Blake, J. DeRose, J. Aguilar, S. Ahlen, D. Bianchi, D. Brooks, F. J. Castander, T. Claybaugh, A. Cuceu, A. de la Macorra, A. Dey, B. Dey, P. Doel, S. Ferraro, J. E. Forero-Romero, C. Garcia-Quintero, E. Gaztañaga, S. Gontcho A Gontcho, G. Gutierrez, S. Heydenreich, K. Honscheid, D. Huterer, M. Ishak, S. Joudaki, R. Joyce, E. Jullo, S. Juneau, R. Kehoe, D. Kirkby, T. Kisner, A. Kremin, A. Krolewski, O. Lahav, M. Landriau, J. U. Lange, L. Le Guillou, A. Leauthaud, M. Manera, R. Miquel, S. Nadathur, W. J. Percival, F. Prada, G. Rossi, R. Ruggeri, E. Sanchez, C. Saulder, A. Semenaite, H. Seo, J. Silber, D. Sprayberry, Z. Sun, G. Tarlé, B. A. Weaver, R. H. Wechsler, R. Zhou</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Combined survey analyses of galaxy clustering and weak gravitational lensing (3x2-pt studies) will allow new and accurate tests of the standard cosmological model. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Combined survey analyses of galaxy clustering and weak gravitational lensing (3x2-pt studies) will allow new and accurate tests of the standard cosmological model. However, careful validation is necessary to ensure that these cosmological constraints are not biased by uncertainties associated with the modelling of astrophysical or systematic effects. In this study we validate the combined 3x2-pt analysis of the Dark Energy Spectroscopic Instrument Data Release 1 (DESI-DR1) spectroscopic galaxy clustering and overlapping weak lensing datasets from the Kilo-Degree Survey (KiDS), the Dark Energy Survey (DES), and the Hyper-Suprime-Cam Survey (HSC). By propagating the modelling uncertainties associated with the non-linear matter power spectrum, non-linear galaxy bias and baryon feedback, we design scale cuts to ensure that measurements of the matter density and the amplitude of the matter power spectrum are biased by less than 30% of the statistical error. We also test the internal consistency of the data and weak lensing systematics by performing new measurements of the lensing shear ratio. We demonstrate that the DESI-DR1 shear ratios can be successfully fit by the same model used to describe cosmic shear correlations, and analyse the additional information that can be extracted about the source redshift distributions and intrinsic alignment parameters. This study serves as crucial preparation for the upcoming cosmological parameter analysis of these datasets.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.05539" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.05539" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.05539" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 11.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.32</span>
                        <span class="badge bg-primary">Semantic Score: 0.97</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">3. CURLING -- II. Improvement on the $H_{0}$ Inference from Pixelized Cluster Strong Lens Modeling
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Yushan Xie, Huanyuan Shan, Yiping Shu, Nan Li, Ji Yao, Ran Li, Xiaoyue Cao, Zizhao He, Yin Li, Eric Jullo, et al.</span>
                                <span class="author-full" style="display: none;">Yushan Xie, Huanyuan Shan, Yiping Shu, Nan Li, Ji Yao, Ran Li, Xiaoyue Cao, Zizhao He, Yin Li, Eric Jullo, Jean-Paul Kneib, Guoliang Li</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Our results indicate that in the next-generation observations (e.g., JWST), uncertainties from lens modeling dominate the error budget for $H_0$ inference, emphasizing the importance of incorporating the extended surface brightness of multiple images to fully leverage the potential of glSNe for cosmology. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Strongly lensed supernovae (glSNe) provide a powerful, independent method to measure the Hubble constant, $H_{0}$, through time delays between their multiple images. The accuracy of this measurement depends critically on both the precision of time delay estimation and the robustness of lens modeling. In many current cluster-scale modeling algorithms, all multiple images used for modeling are simplified as point sources to reduce computational costs. In the first paper of the CURLING program, we demonstrated that such a point-like approximation can introduce significant uncertainties and biases in both magnification reconstruction and cosmological inference. In this study, we explore how such simplifications affect $H_0$ measurements from glSNe. We simulate a lensed supernova at $z=1.95$, lensed by a galaxy cluster at $z=0.336$, assuming time delays are measured from LSST-like light curves. The lens model is constructed using JWST-like imaging data, utilizing both Lenstool and a pixelated method developed in CURLING. Under a fiducial cosmology with $H_0=70\rm \ km \ s^{-1}\ Mpc^{-1}$, the Lenstool model yields $H_0=69.91^{+6.27}_{-5.50}\rm \ km\ s^{-1}\ Mpc^{-1}$, whereas the pixelated framework improves the precision by over an order of magnitude, $H_0=70.39^{+0.82}_{-0.60}\rm \ km \ s^{-1}\ Mpc^{-1}$. Our results indicate that in the next-generation observations (e.g., JWST), uncertainties from lens modeling dominate the error budget for $H_0$ inference, emphasizing the importance of incorporating the extended surface brightness of multiple images to fully leverage the potential of glSNe for cosmology.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.07131" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.07131" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.07131" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.9</span>
                        <span class="badge bg-info text-dark">Author Score: 0.27</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">4. Robust Measurement of Stellar Streams Around the Milky Way: Correcting Spatially Variable Observational Selection Effects in Optical Imaging Surveys
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">K. Boone, P. S. Ferguson, M. Tabbutt, K. Bechtol, T. -Y. Cheng, A. Drlica-Wagner, C. E. Martínez-Vázquez, B. Mutlu-Pakdil, T. M. C. Abbott, O. Alves, et al.</span>
                                <span class="author-full" style="display: none;">K. Boone, P. S. Ferguson, M. Tabbutt, K. Bechtol, T. -Y. Cheng, A. Drlica-Wagner, C. E. Martínez-Vázquez, B. Mutlu-Pakdil, T. M. C. Abbott, O. Alves, F. Andrade-Oliveira, D. Bacon, S. Bocquet, D. Brooks, R. Camilleri, A. Carnero Rosell, L. N. da Costa, M. E. da Silva Pereira, T. M. Davis, J. De Vicente, S. Desai, P. Doel, S. Everett, B. Flaugher, J. Frieman, J. García-Bellido, D. Gruen, G. Gutierrez, S. R. Hinton, D. L. Hollowood, K. Honscheid, D. J. James, K. Kuehn, J. L. Marshall, J. Mena-Fernández, F. Menanteau, R. Miquel, J. Myles, R. L. C. Ogando, A. A. Plazas Malagón, A. Porredon, M. Rodríguez-Monroy, E. Sanchez, D. Sanchez Cid, I. Sevilla-Noarbe, M. Smith, E. Suchyta, M. E. C. Swanson, V. Vikram, N. Weaverdyck</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We also find that uncorrected power-spectrum results for LSST-like data can be around five times more biased, highlighting the need for such corrections in future ground based surveys. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Observations of density variations in stellar streams are a promising probe of low-mass dark matter substructure in the Milky Way. However, survey systematics such as variations in seeing and sky brightness can also induce artificial fluctuations in the observed densities of known stellar streams. These variations arise because survey conditions affect both object detection and star-galaxy misclassification rates. To mitigate these effects, we use Balrog synthetic source injections in the Dark Energy Survey (DES) Y3 data to calculate detection rate variations and classification rates as functions of survey properties. We show that these rates are nearly separable with respect to survey properties and can be estimated with sufficient statistics from the synthetic catalogs. Applying these corrections reduces the standard deviation of relative detection rates across the DES footprint by a factor of five, and our corrections significantly change the inferred linear density of the Phoenix stream when including faint objects. Additionally, for artificial streams with DES like survey properties we are able to recover density power spectra with reduced bias. We also find that uncorrected power-spectrum results for LSST-like data can be around five times more biased, highlighting the need for such corrections in future ground based surveys.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.07511" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.07511" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.07511" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.8</span>
                        <span class="badge bg-info text-dark">Author Score: 0.33</span>
                        <span class="badge bg-primary">Semantic Score: 0.91</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">5. Detection of supernova magnitude fluctuations induced by large-scale structure
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">A. Nguyen, C. Blake, R. J. Turner, V. Aronica, J. Bautista, J. Aguilar, S. Ahlen, S. BenZvi, D. Bianchi, D. Brooks, et al.</span>
                                <span class="author-full" style="display: none;">A. Nguyen, C. Blake, R. J. Turner, V. Aronica, J. Bautista, J. Aguilar, S. Ahlen, S. BenZvi, D. Bianchi, D. Brooks, A. Carr, T. Claybaugh, A. Cuceu, A. de la Macorra, B. Dey, P. Doel, K. Douglass, S. Ferraro, J. E. Forero-Romero, E. Gaztañaga, S. Gontcho A Gontcho, G. Gutierrez, J. Guy, K. Honscheid, C. Howlett, D. Huterer, M. Ishak, R. Joyce, R. Kehoe, A. G. Kim, A. Kremin, O. Lahav, M. Landriau, L. Le Guillou, A. Leauthaud, M. E. Levi, M. Manera, P. Martini, A. Meisner, R. Miquel, E. Mueller, S. Nadathur, N. Palanque-Delabrouille, W. J. Percival, C. Poppett, F. Prada, F. Qin, A. J. Ross, C. Ross, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, D. Sprayberry, G. Tarlé, B. A. Weaver, P. Zarrouk, R. Zhou, H. Zou</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> In this work, we measure the correlation statistics of the large-scale structure traced by the Dark Energy Spectroscopic Instrument Bright Galaxy Survey Data Release 1 sample, and magnitude fluctuations of type Ia supernova from the Pantheon+ compilation across redshifts z &lt; 0.1. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The peculiar velocities of supernovae and their host galaxies are correlated with the large-scale structure of the Universe, and can be used to constrain the growth rate of structure and test the cosmological model. In this work, we measure the correlation statistics of the large-scale structure traced by the Dark Energy Spectroscopic Instrument Bright Galaxy Survey Data Release 1 sample, and magnitude fluctuations of type Ia supernova from the Pantheon+ compilation across redshifts z &lt; 0.1. We find a detection of the cross-correlation signal between galaxies and type Ia supernova magnitudes. Fitting the normalised growth rate of structure f sigma_8 to the auto- and cross-correlation function measurements we find f sigma_8 = 0.384 +0.094 -0.157, which is consistent with the Planck LambdaCDM model prediction, and indicates that the supernova magnitude fluctuations are induced by peculiar velocities. Using a large ensemble of N-body simulations, we validate our methodology, calibrate the covariance of the measurements, and demonstrate that our results are insensitive to supernova selection effects. We highlight the potential of this methodology for measuring the growth rate of structure, and forecast that the next generation of type Ia supernova surveys will improve f sigma_8 constraints by a further order of magnitude.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.07673" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.07673" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.07673" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.7</span>
                        <span class="badge bg-info text-dark">Author Score: 0.26</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">6. The $M_{\rm BH}-M_{*}$ Relationship at $3&lt;z&lt;7$: Big Black Holes in Little Red Dots
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Brenda L. Jones, Dale D. Kocevski, Fabio Pacucci, Anthony J. Taylor, Steven L. Finkelstein, Johannes Buchner, Jonathan R. Trump, Rachel S. Somerville, Michaela Hirschmann, L. Y. Aaron Yung, et al.</span>
                                <span class="author-full" style="display: none;">Brenda L. Jones, Dale D. Kocevski, Fabio Pacucci, Anthony J. Taylor, Steven L. Finkelstein, Johannes Buchner, Jonathan R. Trump, Rachel S. Somerville, Michaela Hirschmann, L. Y. Aaron Yung, Guillermo Barro, Eric F. Bell, Laura Bisigello, Antonello Calabro, Nikko J. Cleri, Avishai Dekel, Mark Dickinson, Giovanni Gandolfi, Mauro Giavalisco, Norman A. Grogin, Kohei Inayoshi, Jeyhan S. Kartaltepe, Anton M. Koekemoer, Lorenzo Napolitano, Masafusa Onoue, Swara Ravindranath, Giulia Rodighiero, Stephen M. Wilkins</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> JWST has identified a large population of faint, broad-line active galactic nuclei (AGN) in the early universe that are powered by black holes (BHs) that often appear overmassive relative to their host galaxies. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">JWST has identified a large population of faint, broad-line active galactic nuclei (AGN) in the early universe that are powered by black holes (BHs) that often appear overmassive relative to their host galaxies. In this study, we examine the relationship between BH mass and galaxy stellar mass at $33\sigma$ above the relationship measured for local broad-line AGN. We derive an intrinsic scatter in this relationship of $0.9$ dex, which does not vary over the redshift range of our sample. We also find that the $M_{\rm BH}/M_{\star}$ ratio increases by $2.3$ dex from $z = 3.5$ and $z = 6.5$ with a confidence level of $ &gt; 3\sigma$. We attribute this trend with the increasing fraction of LRDs in our sample at $z&gt;4$ as their host masses are $\sim1$ dex lower than the non-LRD AGN in our sample. These results support a picture in which the BHs powering JWST&#39;s broad-line AGN are genuinely overmassive and become increasingly so with redshift. We discuss the implications of our findings on early BH growth relative to that of their host galaxies and the constraints it places on BH seeding models.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.07376" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.07376" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.07376" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.21</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">7. Beyond the stars: Linking H$α$ sizes, kinematics, and star formation in galaxies at $z\approx 4-6$ with JWST grism surveys and $\texttt{geko}$
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">A. Lola Danhaive, Sandro Tacchella, William McClymont, Brant Robertson, Stefano Carniani, Courtney Carreira, Eiichi Egami, Andrew J. Bunker, Emma Curtis-Lake, Daniel J. Eisenstein, et al.</span>
                                <span class="author-full" style="display: none;">A. Lola Danhaive, Sandro Tacchella, William McClymont, Brant Robertson, Stefano Carniani, Courtney Carreira, Eiichi Egami, Andrew J. Bunker, Emma Curtis-Lake, Daniel J. Eisenstein, Zhiyuan Ji, Benjamin D. Johnson, Marcia Rieke, Natalia C. Villanueva, Christopher N. A. Willmer, Chris Willot, Zihao Wu, Yongda Zhu</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Understanding how galaxies assemble their mass during the first billion years of cosmic time is a central goal of extragalactic astrophysics, yet joint constraints on their sizes and kinematics remain scarce. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Understanding how galaxies assemble their mass during the first billion years of cosmic time is a central goal of extragalactic astrophysics, yet joint constraints on their sizes and kinematics remain scarce. We present one of the first statistical studies of the $\mathrm{H}\alpha$ size-mass relation at high redshift with a sample of 213 galaxies at spectroscopic redshifts of $z\approx 4-6$ from the FRESCO and CONGRESS NIRCam grism surveys. We measure the $\mathrm{H}\alpha$ morphology and kinematics of our sample using the novel forward modelling Bayesian inference tool $\texttt{geko}$, and complement them with stellar continuum sizes in the rest-frame FUV, NUV, and optical, obtained from modelling of imaging data from the JADES survey with $\texttt{Pysersic}$. At $z\approx5$, we find that the average H$\alpha$ sizes are larger than the stellar continuum (FUV, NUV and optical), with $r_{\rm e, H\alpha}= 1.17 \pm 0.05$ kpc and $r_{\rm e,cont} \approx 0.9$ kpc for galaxies with $\log(M_{\star} ~\rm [M_{\odot}])= 9.5$. However, we find no significant differences between the stellar continuum sizes at different wavelengths, suggesting that galaxies are not yet steadily growing inside-out at these epochs. Instead, we find that the ratio $r_{\rm e, H\alpha}/r_{\rm e, NUV}$ increases with the distance above the star-forming main sequence ($\Delta \rm MS$), consistent with an expansion of H$\alpha$ sizes during episodes of enhanced star formation caused by an increase in ionising photons. As galaxies move above the star-forming main sequence, we find an increase of their rotational support $v/\sigma$, which could be tracing accreting gas illuminated by the \Ha\ emission. Finally, we find that about half of the elongated systems ($b/a &lt; 0.5$) are not rotationally supported, indicating a potential flattened/prolate galaxy population at high redshift.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.06315" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.06315" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.06315" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.1</span>
                        <span class="badge bg-info text-dark">Author Score: 0.17</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">8. Modeling gravitational wave sources in the MillenniumTNG simulations
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Federico Marinacci, Marco Baldi, Giuliano Iorio, M. Celeste Artale, Michela Mapelli, Volker Springel, Sownak Bose, Lars Hernquist</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We find that GW progenitor rates closely track simulated star formation histories and are generally consistent with current observational constraints at low redshift, aside from a factor of $\sim 4.5$ excess in binary black hole mergers. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">(Edited) We introduce a flexible framework for building gravitational wave (GW) event catalogs in hydrodynamic simulations of galaxy formation. Our framework couples the state-of-the-art binary population synthesis code SEVN with Arepo-GW -- a module fully integrated into the moving-mesh code Arepo -- to assign merger events of binary compact objects to stellar particles in simulations by stochastically sampling merger tables generated with SEVN. Arepo-GW supports both on-the-fly operation, producing event catalogs during simulations, and post-processing, using snapshots from existing runs. The algorithm is fully parallel and can be readily adapted to outputs from other simulation codes. To demonstrate the capabilities of our new framework, we applied Arepo-GW in post-processing to simulations from the MillenniumTNG suite, including its flagship box. We investigate key properties of the resulting GW event catalog, built on SEVN predictions, focusing on comoving merger rates, formation efficiencies, delay-time distributions, and progenitor mass and metallicity distributions. We also examine how these properties vary with simulated volume. We find that GW progenitor rates closely track simulated star formation histories and are generally consistent with current observational constraints at low redshift, aside from a factor of $\sim 4.5$ excess in binary black hole mergers. Moreover, our binary black hole merger rates decline more slowly with redshift than current observational estimates for $z \lesssim 1$. Finally, the analysis of progenitor mass functions across different formation channels reveals only mild redshift evolution, while the binary black hole mass function displays features compatible with current observational determinations. These findings highlight the potential of our novel framework to enable detailed predictions for upcoming GW surveys within a full cosmological context.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.06311" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.06311" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.06311" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.0</span>
                        <span class="badge bg-info text-dark">Author Score: 0.13</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">9. DAMA/LIBRA and dark matter: decisive tension or contrived cancellation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Giorgio Busoni, Jonathan M. Cornell, Will Handley, Felix Kahlhoefer, Anders Kvellestad, Masen Pitts, Lauren Street, Aaron C. Vincent, Martin White</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Lowering the tension to reasonable values requires significant tuning, such as overfitting with large numbers of free parameters, and opposite-sign modulation between recoil signals on sodium versus iodine. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We assess the tension between DAMA/LIBRA and the latest dark matter annual modulation results from the ANAIS-112 and COSINE-100 NaI experiments, under a range of hypotheses ranging from physical to general parameterisations. We find that, in the most physically-motivated cases, the tension between DAMA and these other NaI experiments exceeds 5$\sigma$. Lowering the tension to reasonable values requires significant tuning, such as overfitting with large numbers of free parameters, and opposite-sign modulation between recoil signals on sodium versus iodine.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.05216" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.05216" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.05216" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.9</span>
                        <span class="badge bg-info text-dark">Author Score: 0.13</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">hep-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">10. The Cosmic Infrared Background Experiment-2: An Intensity Mapping Optimized Sounding-rocket Payload to Understand the Near-IR Extragalactic Background Light
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Michael Zemcov, James J. Bock, Asantha Cooray, Shuji Matsuura, Dae-Hee Lee, Candice Fazar, Richard M. Feder, Grigory Heaton, Ryo Hashimoto, Phillip Korngut, et al.</span>
                                <span class="author-full" style="display: none;">Michael Zemcov, James J. Bock, Asantha Cooray, Shuji Matsuura, Dae-Hee Lee, Candice Fazar, Richard M. Feder, Grigory Heaton, Ryo Hashimoto, Phillip Korngut, Toshio Matsumoto, Chi H. Nguyen, Kazuma Noda, Won-Kee Park, Kei Sano, Kohji Takimoto, Toshiaki Arai, Seung-Cheol Bang, Priyadarshini Bangale, Masaki Furutani, Viktor Hristov, Yuya Kawano, Arisa Kida, Tomoya Kojima, Alicia Lanz, Chika Matsumi, Dale Mercado, Shunsuke Nakagawa, Tomoya Nakagawa, Shuta Nakahata, Ryo Ohta, Dorin Patru, Mai Shirahata, Hiroko Suzuki, Aoi Takahashi, Momoko Tamai, Serena Tramm, Kohji Tsumura, Yasuhiro Yamada, Shiang-Yu Wang</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> At near-infrared wavelengths, this extragalactic background light (EBL) is comprised of emission from galaxies stretching all the way back to the first-light objects present during the Epoch of Reionization. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The background light produced by emission from all sources over cosmic history is a powerful diagnostic of structure formation and evolution. At near-infrared wavelengths, this extragalactic background light (EBL) is comprised of emission from galaxies stretching all the way back to the first-light objects present during the Epoch of Reionization. The Cosmic Infrared Background Experiment 2 (CIBER-2) is a sounding-rocket experiment designed to measure both the absolute photometric brightness of the EBL over 0.5 - 2.0 microns and perform an intensity mapping measurement of EBL spatial fluctuations in six broad bands over the same wavelength range. CIBER-2 comprises a 28.5 cm, 80K telescope that images several square degrees to three separate cameras. Each camera is equipped with an HAWAII-2RG detector covered by an assembly that combines two broadband filters and a linear-variable filter, which perform the intensity mapping and absolute photometric measurements, respectively. CIBER-2 has flown three times: an engineering flight in 2021; a terminated launch in 2023; and a successful science flight in 2024. In this paper, we review the science case for the experiment; describe the factors motivating the instrument design; review the optical, mechanical, and electronic implementation of the instrument; present preflight laboratory characterization measurements; and finally assess the instrument&#39;s performance in flight.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.05210" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.05210" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.05210" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.9</span>
                        <span class="badge bg-info text-dark">Author Score: 0.14</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.IM</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">11. Gaussian Embeddings: How JEPAs Secretly Learn Your Data Density
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Randall Balestriero, Nicolas Ballas, Mike Rabbat, Yann LeCun</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Our findings are empirically validated across datasets (synthetic, controlled, and Imagenet) and across different Self Supervised Learning methods falling under the JEPA family (I-JEPA and DINOv2) and on multimodal models, such as MetaCLIP. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Joint Embedding Predictive Architectures (JEPAs) learn representations able to solve numerous downstream tasks out-of-the-box. JEPAs combine two objectives: (i) a latent-space prediction term, i.e., the representation of a slightly perturbed sample must be predictable from the original sample&#39;s representation, and (ii) an anti-collapse term, i.e., not all samples should have the same representation. While (ii) is often considered as an obvious remedy to representation collapse, we uncover that JEPAs&#39; anti-collapse term does much more--it provably estimates the data density. In short, any successfully trained JEPA can be used to get sample probabilities, e.g., for data curation, outlier detection, or simply for density estimation. Our theoretical finding is agnostic of the dataset and architecture used--in any case one can compute the learned probabilities of sample $x$ efficiently and in closed-form using the model&#39;s Jacobian matrix at $x$. Our findings are empirically validated across datasets (synthetic, controlled, and Imagenet) and across different Self Supervised Learning methods falling under the JEPA family (I-JEPA and DINOv2) and on multimodal models, such as MetaCLIP. We denote the method extracting the JEPA learned density as {\bf JEPA-SCORE}.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.05949" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.05949" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.05949" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.9</span>
                        <span class="badge bg-info text-dark">Author Score: 0.08</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">12. BASILISK III. Stress-testing the Conditional Luminosity Function model
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Kaustav Mitra, Frank C. van den Bosch</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The goal of this paper is to investigate whether this model is sufficient to fully characterize the small-scale data extracted from spectroscopic surveys and to gauge how adding or removing degrees of freedom impact the inference regarding the galaxy-halo connection. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The Conditional Luminosity Function (CLF) is an effective and flexible way of characterizing the galaxy-halo connection. However, it is subject to a particular choice for its parametrization, which acts as a prior assumption. Most studies have been restricted to what has become a standard CLF parametrization with little to no variation. The goal of this paper is to investigate whether this model is sufficient to fully characterize the small-scale data extracted from spectroscopic surveys and to gauge how adding or removing degrees of freedom impact the inference regarding the galaxy-halo connection. After extensive validation with realistic mock data, we use Basilisk, a highly constraining Bayesian hierarchical tool to model the kinematics and abundance of satellite galaxies, to test the standard CLF model against a slew of more flexible variants. In particular, we test whether the SDSS data favour any of these variants in terms of a goodness-of-fit improvement, and identify the models that are sufficiently flexible, beyond which additional model freedom is not demanded by the data. We show that some of these additional degrees of freedom, which have hitherto not been considered, result in a drastic improvement of the fit and cause significant changes in the inferred galaxy-halo connection. This highlights that an empirical model comes with an implicit prior about the parametrization form, which needs to be addressed to ensure that it is sufficiently flexible to capture the complexity of the data and to safeguard against a biased inference.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.08421" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.08421" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.08421" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.9</span>
                        <span class="badge bg-info text-dark">Author Score: 0.10</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">13. Comparing XRISM cluster velocity dispersions with predictions from cosmological simulations: are feedback models too ejective?
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">XRISM Collaboration, Marc Audard, Hisamitsu Awaki, Ralf Ballhausen, Aya Bamba, Ehud Behar, Rozenn Boissay-Malaquin, Laura Brenneman, Gregory V. Brown, Lia Corrales, et al.</span>
                                <span class="author-full" style="display: none;">XRISM Collaboration, Marc Audard, Hisamitsu Awaki, Ralf Ballhausen, Aya Bamba, Ehud Behar, Rozenn Boissay-Malaquin, Laura Brenneman, Gregory V. Brown, Lia Corrales, Elisa Costantini, Renata Cumbee, Maria Diaz Trigo, Chris Done, Tadayasu Dotani, Ken Ebisawa, Megan E. Eckart, Dominique Eckert, Satoshi Eguchi, Teruaki Enoto, Yuichiro Ezoe, Adam Foster, Ryuichi Fujimoto, Yutaka Fujita, Yasushi Fukazawa, Kotaro Fukushima, Akihiro Furuzawa, Luigi Gallo, Javier A. García, Liyi Gu, Matteo Guainazzi, Kouichi Hagino, Kenji Hamaguchi, Isamu Hatsukade, Katsuhiro Hayashi, Takayuki Hayashi, Natalie Hell, Edmund Hodges-Kluck, Ann Hornschemeier, Yuto Ichinohe, Daiki Ishi, Manabu Ishida, Kumi Ishikawa, Yoshitaka Ishisaki, Jelle Kaastra, Timothy Kallman, Erin Kara, Satoru Katsuda, Yoshiaki Kanemaru, Richard Kelley, Caroline Kilbourne, Shunji Kitamoto, Shogo Kobayashi, Takayoshi Kohmura, Aya Kubota, Maurice Leutenegger, Michael Loewenstein, Yoshitomo Maeda, Maxim Markevitch, Hironori Matsumoto, Kyoko Matsushita, Dan McCammon, Brian McNamara, François Mernier, Eric D. Miller, Jon M. Miller, Ikuyuki Mitsuishi, Misaki Mizumoto, Tsunefumi Mizuno, Koji Mori, Koji Mukai, Hiroshi Murakami, Richard Mushotzky, Hiroshi Nakajima, Kazuhiro Nakazawa, Jan-Uwe Ness, Kumiko Nobukawa, Masayoshi Nobukawa, Hirofumi Noda, Hirokazu Odaka, Shoji Ogawa, Anna Ogorzałek, Takashi Okajima, Naomi Ota, Stephane Paltani, Robert Petre, Paul Plucinsky, Frederick S. Porter, Katja Pottschmidt, Kosuke Sato, Toshiki Sato, Makoto Sawada, Hiromi Seta, Megumi Shidatsu, Aurora Simionescu, Randall Smith, Hiromasa Suzuki, Andrew Szymkowiak, Hiromitsu Takahashi, Mai Takeo, Toru Tamagawa, Keisuke Tamura, Takaaki Tanaka, Atsushi Tanimoto, Makoto Tashiro, Yukikatsu Terada, Yuichi Terashima, Yohko Tsuboi, Masahiro Tsujimoto, Hiroshi Tsunemi, Takeshi Tsuru, Aysegül Tümer, Hiroyuki Uchida, Nagomi Uchida, Yuusuke Uchida, Hideki Uchiyama, Shutaro Ueda, Yoshihiro Ueda, Shinichiro Uno, Jacco Vink, Shin Watanabe, Brian J. Williams, Satoshi Yamada, Shinya Yamada, Hiroya Yamaguchi, Kazutaka Yamaoka, Noriko Yamasaki, Makoto Yamauchi, Shigeo Yamauchi, Tahir Yaqoob, Tomokage Yoneyama, Tessei Yoshida, Mihoko Yukita, Irina Zhuravleva, Weiguang Cui, Stefano Ettori, Skylar Grayson, Annie Heinrich, Hannah McCall, Dylan Nelson, Nobuhiro Okabe, Yuki Omiya, Arnab Sarkar, Evan Scannapieco, Ming Sun, Keita Tanaka, Nhut Truong, Daniel R. Wik, Congyao Zhang, John ZuHone</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We compare XRISM measurements for nine galaxy clusters (Virgo, Perseus, Centaurus, Hydra A, PKS\,0745--19, A2029, Coma, A2319, Ophiuchus) with predictions from three state-of-the-art cosmological simulation suites, TNG-Cluster, The Three Hundred Project GADGET-X, and GIZMO-SIMBA, that employ different models of feedback. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The dynamics of the intra-cluster medium (ICM), the hot plasma that fills galaxy clusters, are shaped by gravity-driven cluster mergers and feedback from supermassive black holes (SMBH) in the cluster cores. XRISM measurements of ICM velocities in several clusters offer insights into these processes. We compare XRISM measurements for nine galaxy clusters (Virgo, Perseus, Centaurus, Hydra A, PKS\,0745--19, A2029, Coma, A2319, Ophiuchus) with predictions from three state-of-the-art cosmological simulation suites, TNG-Cluster, The Three Hundred Project GADGET-X, and GIZMO-SIMBA, that employ different models of feedback. In cool cores, XRISM reveals systematically lower velocity dispersions than the simulations predict, with all ten measurements below the median simulated values by a factor $1.5-1.7$ on average and all falling within the bottom $10\%$ of the predicted distributions. The observed kinetic-to-total pressure ratio is also lower, with a median value of $2.2\%$, compared to the predicted $5.0-6.5\%$ for the three simulations. Outside the cool cores and in non-cool-core clusters, simulations show better agreement with XRISM measurements, except for the outskirts of the relaxed, cool-core cluster A2029, which exhibits an exceptionally low kinetic pressure support ($&lt;1\%$), with none of the simulated systems in either of the three suites reaching such low levels. The non-cool-core Coma and A2319 exhibit dispersions at the lower end but within the simulated spread. Our comparison suggests that the three numerical models may overestimate the kinetic effects of SMBH feedback in cluster cores. Additional XRISM observations of non-cool-core clusters will clarify if there is a systematic tension in the gravity-dominated regime as well.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.06322" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.06322" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.06322" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.8</span>
                        <span class="badge bg-info text-dark">Author Score: 0.09</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">14. On the sensitivity of different galaxy properties to warm dark matter
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Belén Costanza, Bonny Y. Wang, Francisco Villaescusa-Navarro, Alex M. Garcia, Jonah C. Rose, Mark Vogelsberger, Paul Torrey, Arya Farahi, Xuejian Shen, Ilem Leisher</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We study the impact of warm dark matter (WDM) particle mass on galaxy properties using 1,024 state-of-the-art cosmological hydrodynamical simulations from the DREAMS project. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We study the impact of warm dark matter (WDM) particle mass on galaxy properties using 1,024 state-of-the-art cosmological hydrodynamical simulations from the DREAMS project. We begin by using a Multilayer Perceptron (MLP) coupled with a normalizing flow to explore global statistical descriptors of galaxy populations, such as the mean, standard deviation, and histograms of 14 galaxy properties. We find that subhalo gas mass is the most informative feature for constraining the WDM mass, achieving a determination coefficient of R^2 = 0.9. We employ symbolic regression to extract simple, interpretable relations with the WDM particle mass. Finally, we adopt a more localized approach by selecting individual dark matter halos and using a Graph Neural Network (GNN) with a normalizing flow to infer the WDM mass, incorporating subhalo properties as node features and global simulation statistics as graph-level features. The GNN approach yields only a residual improvement over MLP models based solely on global features, indicating that most of the predictive power resides in the global descriptors, with only marginal gains from halo-level information.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.05037" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.05037" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.05037" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.7</span>
                        <span class="badge bg-info text-dark">Author Score: 0.06</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">15. Photometric Redshift Estimation for Rubin Observatory Data Preview 1 with Redshift Assessment Infrastructure Layers (RAIL)
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">T. Zhang, E. Charles, J. F. Crenshaw, S. J. Schmidt, P. Adari, J. Gschwend, S. Mau, B. Andrews, E. Aubourg, Y. Bains, et al.</span>
                                <span class="author-full" style="display: none;">T. Zhang, E. Charles, J. F. Crenshaw, S. J. Schmidt, P. Adari, J. Gschwend, S. Mau, B. Andrews, E. Aubourg, Y. Bains, K. Bechtol, A. Boucaud, D. Boutigny, P. Burchat, J. Chevalier, J. Chiang, H. -F. Chiang, D. Clowe, J. Cohen-Tanugi, C. Combet, A. Connolly, S. Dagoret-Campagne, P. N. Daly, F. Daruich, G. Daubard, J. De Vicente, H. Drass, K. Fanning, E. Gawiser, M. Graham, L. P. Guy, Q. Hang, P. Ingraham, O. Ilbert, M. Jarvis, M. J. Jee, T. Jenness, A. Johnson, C. Juramy-Gilles, S. M. Kahn, J. B. Kalmbach, Y. Kang, A. Kannawadi, L. S. Kelvin, S. Liang, O. Lynn, N. B. Lust, M. Lutfi, A. Malz, R. Mandelbaum, S. Marshall, J. Meyers, M. Migliore, M. Moniez, J. Neveu, J. A. Newman, E. Nourbakhsh, D. Oldag, H. Park, S. Pelesky, A. A. Plazas Malagón, B. Quint, M. Rahman, A. Rasmussen, K. Reil, W. Roby, A. Roodman, C. Roucelle, M. Salvato, B. Sánchez, D. Sanmartim, R. H. Schindler, J. Scora, J. Sebag, N. Sedaghat, I. Sevilla-Noarbe, R. Shirley, A. Shugart, R. Solomon, D. Taranu, G. Thayer, L. Toribio San Cipriano, E. Urbach, Y. Utsumi, W. van Reeven, A. von der Linden, C. W. Walter, W. M. Wood-Vasey, J. Zuntz, LSST Dark Energy Science Collaboration</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Across the algorithms, we achieve per-galaxy photo-z scatter of $\sigma_{\rm NMAD} \sim 0.03$ and outlier fractions around 10% in the 6-band data, with performance degrading at faint magnitudes and z&gt;1.2. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present the first systematic analysis of photometric redshifts (photo-z) estimated from the Rubin Observatory Data Preview 1 (DP1) data taken with the Legacy Survey of Space and Time (LSST) Commissioning Camera. Employing the Redshift Assessment Infrastructure Layers (RAIL) framework, we apply eight photo-z algorithms to the DP1 photometry, using deep ugrizy coverage in the Extended Chandra Deep Field South (ECDFS) field and griz data in the Rubin_SV_38_7 field. In the ECDFS field, we construct a reference catalog from spectroscopic redshift (spec-z), grism redshift (grism-z), and multiband photo-z for training and validating photo-z. Performance metrics of the photo-z are evaluated using spec-zs from ECDFS and Dark Energy Spectroscopic Instrument Data Release 1 samples. Across the algorithms, we achieve per-galaxy photo-z scatter of $\sigma_{\rm NMAD} \sim 0.03$ and outlier fractions around 10% in the 6-band data, with performance degrading at faint magnitudes and z&gt;1.2. The overall bias and scatter of our machine-learning based photo-zs satisfy the LSST Y1 requirement. We also use our photo-z to infer the ensemble redshift distribution n(z). We study the photo-z improvement by including near-infrared photometry from the Euclid mission, and find that Euclid photometry improves photo-z at z&gt;1.2. Our results validate the RAIL pipeline for Rubin photo-z production and demonstrate promising initial performance.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.07370" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.07370" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.07370" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.7</span>
                        <span class="badge bg-info text-dark">Author Score: 0.12</span>
                        <span class="badge bg-primary">Semantic Score: 0.91</span>
                        
                            <span class="badge bg-secondary">astro-ph.IM</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">16. Attention Sinks and Compression Valleys in LLMs are Two Sides of the Same Coin
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Enrique Queipo-de-Llano, Álvaro Arroyo, Federico Barbero, Xiaowen Dong, Michael Bronstein, Yann LeCun, Ravid Shwartz-Ziv</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> This unified view motivates us to propose the Mix-Compress-Refine theory of information flow, as an attempt to explain how LLMs organize their computation in depth by controlling attention and representational compression via massive activations. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Attention sinks and compression valleys have attracted significant attention as two puzzling phenomena in large language models, but have been studied in isolation. In this work, we present a surprising connection between attention sinks and compression valleys, tracing both to the formation of massive activations in the residual stream. We prove theoretically that massive activations necessarily produce representational compression and establish bounds on the resulting entropy reduction. Through experiments across several models (410M-120B parameters), we confirm that when the beginning-of-sequence token develops extreme activation norms in the middle layers, both compression valleys and attention sinks emerge simultaneously. Targeted ablation studies validate our theoretical predictions. This unified view motivates us to propose the Mix-Compress-Refine theory of information flow, as an attempt to explain how LLMs organize their computation in depth by controlling attention and representational compression via massive activations. Specifically, we posit that Transformer-based LLMs process tokens in three distinct phases: (1) broad mixing in the early layers, (2) compressed computation with limited mixing in the middle layers, and (3) selective refinement in the late layers. Our framework helps explain why embedding tasks perform best at intermediate layers, whereas generation tasks benefit from full-depth processing, clarifying differences in task-dependent representations.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.06477" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.06477" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.06477" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.7</span>
                        <span class="badge bg-info text-dark">Author Score: 0.08</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">17. Enhancing Multiplet Alignment Measurements with Imaging
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Alexus Annika Kumwembe, Claire Lamman, Daniel Eisenstein, Jessica Nicole Aguilar, Steven Ahlen, Davide Bianchi, David Brooks, Todd Claybaugh, Andrei Cuceu, Axel de la Macorra, et al.</span>
                                <span class="author-full" style="display: none;">Alexus Annika Kumwembe, Claire Lamman, Daniel Eisenstein, Jessica Nicole Aguilar, Steven Ahlen, Davide Bianchi, David Brooks, Todd Claybaugh, Andrei Cuceu, Axel de la Macorra, Biprateep Dey, Peter Doel, Andreu Font-Ribera, Jaime E. Forero-Romero, Enrique Gaztanaga, Satya Gontcho A Gontcho, Gaston Gutierrez, Mustapha Ishak, Jorge Jimenez, Dick Joyce, Robert Kehoe, Theodore Kisner, Ofer Lahav, Martin Landriau, Marc Manera, Ramon Miquel, Seshadri Nadathur, Nathalie Palanque-Delabrouille, Ignasi Perez-Rafols, Francisco Prada, Graziano Rossi, Eusebio Sanchez, David Schlegel, Hee-Jong Seo, Joseph Harry Silber, David Sprayberry, Gregory Tarle, Benjamin Alan Weaver</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The average orientation of small groups of galaxies, or &#34;multiplets&#34; is correlated with large-scale structure and is used to measure the direction of tidal forces. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We demonstrate that measurements of the gravitational tidal field made with spectroscopic redshifts can be improved with information from imaging surveys. The average orientation of small groups of galaxies, or &#34;multiplets&#34; is correlated with large-scale structure and is used to measure the direction of tidal forces. Previously, multiplet intrinsic alignment has been measured in DESI using galaxies that have spectroscopic redshifts. The DESI Legacy Imaging catalog can be used to supplement multiplet catalogs. Our findings show that galaxy positions from the imaging catalog produce a measurement similar to the measurements made with only spectroscopic data. This demonstrates that imaging can improve our signal-to-noise ratio for multiplet alignment in DESI.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.08353" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.08353" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.08353" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.7</span>
                        <span class="badge bg-info text-dark">Author Score: 0.13</span>
                        <span class="badge bg-primary">Semantic Score: 0.90</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">18. Improving Reasoning for Diffusion Language Models via Group Diffusion Policy Optimization
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Kevin Rojas, Jiahe Lin, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, Molei Tao, Wei Deng</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Empirically, GDPO achieves consistent gains over pretrained checkpoints and outperforms diffu-GRPO, one of the state-of-the-art baselines, on the majority of math, reasoning, and coding benchmarks. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Diffusion language models (DLMs) enable parallel, order-agnostic generation with iterative refinement, offering a flexible alternative to autoregressive large language models (LLMs). However, adapting reinforcement learning (RL) fine-tuning to DLMs remains an open challenge because of the intractable likelihood. Pioneering work such as diffu-GRPO estimated token-level likelihoods via one-step unmasking. While computationally efficient, this approach is severely biased. A more principled foundation lies in sequence-level likelihoods, where the evidence lower bound (ELBO) serves as a surrogate. Yet, despite this clean mathematical connection, ELBO-based methods have seen limited adoption due to the prohibitive cost of likelihood evaluation. In this work, we revisit ELBO estimation and disentangle its sources of variance. This decomposition motivates reducing variance through fast, deterministic integral approximations along a few pivotal dimensions. Building on this insight, we introduce \textbf{Group Diffusion Policy Optimization (GDPO)}, a new RL algorithm tailored for DLMs. GDPO leverages simple yet effective Semi-deterministic Monte Carlo schemes to mitigate the variance explosion of ELBO estimators under vanilla double Monte Carlo sampling, yielding a provably lower-variance estimator under tight evaluation budgets. Empirically, GDPO achieves consistent gains over pretrained checkpoints and outperforms diffu-GRPO, one of the state-of-the-art baselines, on the majority of math, reasoning, and coding benchmarks.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.08554" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.08554" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.08554" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.06</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">19. QML-FAST - A Fast Code for low-$\ell$ Tomographic Maximum Likelihood Power Spectrum Estimation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yurii Kvasiuk, Anderson Lai, Moritz Münchmeyer, Kendrick M. Smith</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We implement a number of optimizations which make the estimator and associated covariance matrix computationally tractable for a low-$\ell$ analysis, suitable for example for kSZ velocity reconstruction or primordial non-Gaussianity from scale-dependent bias analyses. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present a novel implementation for the quadratic maximum likelihood (QML) power spectrum estimator for multiple correlated scalar fields on the sphere. Our estimator supports arbitrary binning in redshift and multipoles $\ell$ and includes cross-correlations of redshift bins. It implements a fully optimal analysis with a pixel-wise covariance model. We implement a number of optimizations which make the estimator and associated covariance matrix computationally tractable for a low-$\ell$ analysis, suitable for example for kSZ velocity reconstruction or primordial non-Gaussianity from scale-dependent bias analyses. We validate our estimator extensively on simulations and compare its features and precision with the common pseudo-$C_\ell$ method, showing significant gains at large scales. We make our code publicly available. In a companion paper, we apply the estimator to kSZ velocity reconstruction using data from ACT and DESI Legacy Survey and construct full set of QML estimators on 40 correlated fields up to $N_{\text{side}}= 32$ in timescale of an hour on a single 24-core CPU requiring $&lt;256\ \mathrm{Gb}$ RAM, demonstrating the performance of the code.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.05215" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.05215" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.05215" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">20. From Data to Rewards: a Bilevel Optimization Perspective on Maximum Likelihood Estimation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Abdelhakim Benechehab, Gabriel Singer, Corentin Léger, Youssef Attia El Hili, Giuseppe Paolo, Albert Thomas, Maurizio Filippone, Balázs Kégl</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> While Maximum Likelihood Estimation has traditionally served as the dominant training paradigm, recent work have highlighted its limitations, particularly in generalization and susceptibility to catastrophic forgetting compared to Reinforcement Learning techniques, such as Policy Gradient methods. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Generative models form the backbone of modern machine learning, underpinning state-of-the-art systems in text, vision, and multimodal applications. While Maximum Likelihood Estimation has traditionally served as the dominant training paradigm, recent work have highlighted its limitations, particularly in generalization and susceptibility to catastrophic forgetting compared to Reinforcement Learning techniques, such as Policy Gradient methods. However, these approaches depend on explicit reward signals, which are often unavailable in practice, leaving open the fundamental problem of how to align generative models when only high-quality datasets are accessible. In this work, we address this challenge via a Bilevel Optimization framework, where the reward function is treated as the optimization variable of an outer-level problem, while a policy gradient objective defines the inner-level. We then conduct a theoretical analysis of this optimization problem in a tractable setting and extract insights that, as we demonstrate, generalize to applications such as tabular classification and model-based reinforcement learning. We release the code at https://github.com/abenechehab/nll_to_po .</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.07624" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.07624" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.07624" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.96</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">21. Benchmarking AI-evolved cosmological structure formation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Xiaofeng Dong, Nesar Ramachandra, Salman Habib, Katrin Heitmann</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> This study provides an example of how a family of physically motivated benchmarks can, in turn, be used to fine-tune optimization schemes -- such as the density-weighted loss used here -- to significantly enhance the accuracy of scientific machine learning approaches by focusing attention on relevant features. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The potential of deep learning-based image-to-image translations has recently attracted significant attention. One possible application of such a framework is as a fast, approximate alternative to cosmological simulations, which would be particularly useful in various contexts, including covariance studies, investigations of systematics, and cosmological parameter inference. To investigate different aspects of learning-based cosmological mappings, we choose two approaches for generating suitable cosmological matter fields as datasets: a simple analytical prescription provided by the Zel&#39;dovich approximation, and a numerical N-body method using the Particle-Mesh approach. The evolution of structure formation is modeled using U-Net, a widely employed convolutional image translation framework. Because of the lack of a controlled methodology, validation of these learned mappings requires multiple benchmarks beyond simple visual comparisons and summary statistics. A comprehensive list of metrics is considered, including higher-order correlation functions, conservation laws, topological indicators, and statistical independence of density fields. We find that the U-Net approach performs well only for some of these physical metrics, and accuracy is worse at increasingly smaller scales, where the dynamic range in density is large. By introducing a custom density-weighted loss function during training, we demonstrate a significant improvement in the U-Net results at smaller scales. This study provides an example of how a family of physically motivated benchmarks can, in turn, be used to fine-tune optimization schemes -- such as the density-weighted loss used here -- to significantly enhance the accuracy of scientific machine learning approaches by focusing attention on relevant features.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.06731" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.06731" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.06731" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">22. DISCO: Diversifying Sample Condensation for Efficient Model Evaluation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Alexander Rubinstein, Benjamin Raible, Martin Gubri, Seong Joon Oh</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> $\textbf{DISCO}$ shows empirical gains over prior methods, achieving state-of-the-art results in performance prediction across MMLU, Hellaswag, Winogrande, and ARC. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Evaluating modern machine learning models has become prohibitively expensive. Benchmarks such as LMMs-Eval and HELM demand thousands of GPU hours per model. Costly evaluation reduces inclusivity, slows the cycle of innovation, and worsens environmental impact. The typical approach follows two steps. First, select an anchor subset of data. Second, train a mapping from the accuracy on this subset to the final test result. The drawback is that anchor selection depends on clustering, which can be complex and sensitive to design choices. We argue that promoting diversity among samples is not essential; what matters is to select samples that $\textit{maximise diversity in model responses}$. Our method, $\textbf{Diversifying Sample Condensation (DISCO)}$, selects the top-k samples with the greatest model disagreements. This uses greedy, sample-wise statistics rather than global clustering. The approach is conceptually simpler. From a theoretical view, inter-model disagreement provides an information-theoretically optimal rule for such greedy selection. $\textbf{DISCO}$ shows empirical gains over prior methods, achieving state-of-the-art results in performance prediction across MMLU, Hellaswag, Winogrande, and ARC. Code is available here: https://github.com/arubique/disco-public.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.07959" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.07959" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.07959" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">23. FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Heming Zou, Yunliang Zang, Wutong Xu, Yao Zhu, Xiangyang Ji</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Although Mixture-of-Experts (MoE)-based LoRA variants show promise in mitigating intra-task correlations in single-task instruction tuning, they introduce additional router parameters and remain ineffective in multi-task model merging where inter-task interference arises. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning method for foundation models, but it suffers from parameter interference, resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based LoRA variants show promise in mitigating intra-task correlations in single-task instruction tuning, they introduce additional router parameters and remain ineffective in multi-task model merging where inter-task interference arises. Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the up-projection matrix, and (2) an implicit router that unifies expert routing and down-projection, where a frozen sparse random projection matrix replaces the traditional dense trainable version. This design resolves the trade-off between intra-task decorrelation and computational efficiency by eliminating the need for an explicit router, while inherently mitigating inter-task interference due to the orthogonality property of random matrices. Extensive experiments across four domains -- general knowledge understanding, scientific question answering, mathematical reasoning, and code generation -- demonstrate consistent performance improvements over existing methods. Beyond empirical gains, FlyLoRA highlights how biological structures can inspire innovations in AI technologies. Code is available at https://github.com/gfyddha/FlyLoRA.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.08396" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.08396" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.08396" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">24. Analysis of Galaxies at the Extremes: Failed Galaxy Progenitors in the MAGNETICUM Simulations
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jonah S. Gannon, Lucas C. Kimmig, Duncan A. Forbes, Jean P. Brodie, Lucas M. Valenzuela, Rhea-Silvia Remus, Joel L. Pfeffer, Klaus Dolag</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We build a toy model of passive galaxy evolution within the stellar mass-halo mass relation to trace z = 0 observations of UDGs back to their z = 2 locations. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">There is increasing observational evidence for a failed galaxy formation pathway for some ultradiffuse galaxies (UDGs) at low redshift however they currently lack simulated counterparts. We attempt to identify dark matter halos at high redshift within the MAGNETICUM cosmological simulations that could plausibly be their progenitors. We build a toy model of passive galaxy evolution within the stellar mass-halo mass relation to trace z = 0 observations of UDGs back to their z = 2 locations. We identify a population of 443 galaxies that match these parameter space positions within the simulation. We build two comparison samples within the simulation that follow the stellar mass-halo mass relationship at z = 2, one of which is stellar mass matched (with varying smaller halo masses) and the other is halo mass matched (with varying larger stellar masses) to our sample. We identify that our failed galaxy progenitor candidates have 1) flatter, cored dark matter halos; 2) more extended stellar bodies; 3) a larger fraction of their gas in the outskirts of their halos; 4) lower metallicities and 5) higher star formation rates than the control samples. Findings 1) and 2) are similar to low redshift observations of UDGs. Finding 3) will aid the removal of gas and permanent quenching of star formation which is a requirement of the failed galaxy formation scenario. The low metallicities of finding 4) match those observed in low redshift failed galaxy UDGs. Comparing the high star formation rates of finding 5) to recent JWST observations suggests that a starburst would naturally explain the high globular cluster richness of the UDGs. Many of the properties we find for these failed galaxy progenitors can be explained by an assembly bias of their dark matter halo to later formation times. We conclude by proposing that the fraction of failed galaxy UDGs is expected to increase with environmental density.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04416" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04416" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04416" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.03</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">25. Carré du champ flow matching: better quality-generalisation tradeoff in generative models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jacob Bamberger, Iolo Jones, Dennis Duncan, Michael M. Bronstein, Pierre Vandergheynst, Adam Gosztolai</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Further, we provide an extensive experimental evaluation on diverse datasets (synthetic manifolds, point clouds, single-cell genomics, animal motion capture, and images) as well as various neural network architectures (MLPs, CNNs, and transformers). (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Deep generative models often face a fundamental tradeoff: high sample quality can come at the cost of memorisation, where the model reproduces training data rather than generalising across the underlying data geometry. We introduce Carr\&#39;e du champ flow matching (CDC-FM), a generalisation of flow matching (FM), that improves the quality-generalisation tradeoff by regularising the probability path with a geometry-aware noise. Our method replaces the homogeneous, isotropic noise in FM with a spatially varying, anisotropic Gaussian noise whose covariance captures the local geometry of the latent data manifold. We prove that this geometric noise can be optimally estimated from the data and is scalable to large data. Further, we provide an extensive experimental evaluation on diverse datasets (synthetic manifolds, point clouds, single-cell genomics, animal motion capture, and images) as well as various neural network architectures (MLPs, CNNs, and transformers). We demonstrate that CDC-FM consistently offers a better quality-generalisation tradeoff. We observe significant improvements over standard FM in data-scarce regimes and in highly non-uniformly sampled datasets, which are often encountered in AI for science applications. Our work provides a mathematical framework for studying the interplay between data geometry, generalisation and memorisation in generative models, as well as a robust and scalable algorithm that can be readily integrated into existing flow matching pipelines.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.05930" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.05930" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.05930" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">26. Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Arjun Krishnakumar, Rhea Sanjay Sukthanker, Hannan Javed Mahadik, Gabriela Kadlecová, Vladyslav Moroshan, Timur Carstensen, Frank Hutter, Aaron Klein</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Second, we use evolutionary search to automatically discover high-quality sub-network initializations, providing better starting points for pretraining. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Small Language models (SLMs) offer an efficient and accessible alternative to Large Language Models (LLMs), delivering strong performance while using far fewer resources. We introduce a simple and effective framework for pretraining SLMs that brings together three complementary ideas. First, we identify structurally sparse sub-network initializations that consistently outperform randomly initialized models of similar size under the same compute budget. Second, we use evolutionary search to automatically discover high-quality sub-network initializations, providing better starting points for pretraining. Third, we apply knowledge distillation from larger teacher models to speed up training and improve generalization. Together, these components make SLM pretraining substantially more efficient: our best model, discovered using evolutionary search and initialized with LLM weights, matches the validation perplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining tokens. We release all code and models at https://github.com/whittle-org/whittle/, offering a practical and reproducible path toward cost-efficient small language model development at scale.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.07227" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.07227" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.07227" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.CL</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">27. HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zheng Xiong, Kang Li, Zilin Wang, Matthew Jackson, Jakob Foerster, Shimon Whiteson</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Successfully training an HN-based VLA is nontrivial so HyperVLA contains several key algorithm design features that improve its performance, including properly utilizing the prior knowledge from existing vision foundation models, HN normalization, and an action generation strategy. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Built upon language and vision foundation models with strong generalization ability and trained on large-scale robotic data, Vision-Language-Action (VLA) models have recently emerged as a promising approach to learning generalist robotic policies. However, a key drawback of existing VLAs is their extremely high inference costs. In this paper, we propose HyperVLA to address this problem. Unlike existing monolithic VLAs that activate the whole model during both training and inference, HyperVLA uses a novel hypernetwork (HN)-based architecture that activates only a small task-specific policy during inference, while still retaining the high model capacity needed to accommodate diverse multi-task behaviors during training. Successfully training an HN-based VLA is nontrivial so HyperVLA contains several key algorithm design features that improve its performance, including properly utilizing the prior knowledge from existing vision foundation models, HN normalization, and an action generation strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even higher success rate for both zero-shot generalization and few-shot adaptation, while significantly reducing inference costs. Compared to OpenVLA, a state-of-the-art VLA model, HyperVLA reduces the number of activated parameters at test time by $90\times$, and accelerates inference speed by $120\times$. Code is publicly available at https://github.com/MasterXiong/HyperVLA</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04898" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04898" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04898" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.RO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">28. Symmetry-Aware Fully-Amortized Optimization with Scale Equivariant Graph Metanetworks
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Bart Kuipers, Freek Byrman, Daniel Uyterlinde, Alejandro García-Castellanos</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We demonstrate the effectiveness of this approach empirically and provide a theoretical result: the gauge freedom induced by scaling symmetries is strictly smaller in convolutional neural networks than in multi-layer perceptrons. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Amortized optimization accelerates the solution of related optimization problems by learning mappings that exploit shared structure across problem instances. We explore the use of Scale Equivariant Graph Metanetworks (ScaleGMNs) for this purpose. By operating directly in weight space, ScaleGMNs enable single-shot fine-tuning of existing models, reducing the need for iterative optimization. We demonstrate the effectiveness of this approach empirically and provide a theoretical result: the gauge freedom induced by scaling symmetries is strictly smaller in convolutional neural networks than in multi-layer perceptrons. This insight helps explain the performance differences observed between architectures in both our work and that of Kalogeropoulos et al. (2024). Overall, our findings underscore the potential of symmetry-aware metanetworks as a powerful approach for efficient and generalizable neural network optimization. Open-source code: https://github.com/daniuyter/scalegmn_amortization</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.08300" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.08300" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.08300" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">29. MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Lixuan He, Shikang Zheng, Linfeng Zhang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Autoregressive (AR) models have shown great promise in image generation, yet they face a fundamental inefficiency stemming from their core component: a vast, unstructured vocabulary of visual tokens. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Autoregressive (AR) models have shown great promise in image generation, yet they face a fundamental inefficiency stemming from their core component: a vast, unstructured vocabulary of visual tokens. This conventional approach treats tokens as a flat vocabulary, disregarding the intrinsic structure of the token embedding space where proximity often correlates with semantic similarity. This oversight results in a highly complex prediction task, which hinders training efficiency and limits final generation quality. To resolve this, we propose Manifold-Aligned Semantic Clustering (MASC), a principled framework that constructs a hierarchical semantic tree directly from the codebook&#39;s intrinsic structure. MASC employs a novel geometry-aware distance metric and a density-driven agglomerative construction to model the underlying manifold of the token embeddings. By transforming the flat, high-dimensional prediction task into a structured, hierarchical one, MASC introduces a beneficial inductive bias that significantly simplifies the learning problem for the AR model. MASC is designed as a plug-and-play module, and our extensive experiments validate its effectiveness: it accelerates training by up to 57% and significantly improves generation quality, reducing the FID of LlamaGen-XL from 2.87 to 2.58. MASC elevates existing AR frameworks to be highly competitive with state-of-the-art methods, establishing that structuring the prediction space is as crucial as architectural innovation for scalable generative modeling.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04220" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04220" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04220" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">30. Contrastive Weak-to-strong Generalization
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Houcheng Jiang, Junfeng Fang, Jiaxin Wu, Tianyu Zhang, Chen Gao, Yong Li, Xiang Wang, Xiangnan He, Yang Deng</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> This approach enables more reliable capability transfer, denoising, and improved robustness, substantially mitigating the limitations of traditional weak-to-strong methods. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Weak-to-strong generalization provides a promising paradigm for scaling large language models (LLMs) by training stronger models on samples from aligned weaker ones, without requiring human feedback or explicit reward modeling. However, its robustness and generalization are hindered by the noise and biases in weak-model outputs, which limit its applicability in practice. To address this challenge, we leverage implicit rewards, which approximate explicit rewards through log-likelihood ratios, and reveal their structural equivalence with Contrastive Decoding (CD), a decoding strategy shown to reduce noise in LLM generation. Building on this connection, we propose Contrastive Weak-to-Strong Generalization (ConG), a framework that employs contrastive decoding between pre- and post-alignment weak models to generate higher-quality samples. This approach enables more reliable capability transfer, denoising, and improved robustness, substantially mitigating the limitations of traditional weak-to-strong methods. Empirical results across different model families confirm consistent improvements, demonstrating the generality and effectiveness of ConG. Taken together, our findings highlight the potential of ConG to advance weak-to-strong generalization and provide a promising pathway toward AGI.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.07884" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.07884" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.07884" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CL</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">31. Rethinking Nonlinearity: Trainable Gaussian Mixture Modules for Modern Neural Architectures
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Weiguo Lu, Gangnan Yuan, Hong-kun Zhang, Shangyang Li</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> By relaxing probabilistic constraints and adopting a flexible parameterization of Gaussian projections, GMNM can be seamlessly integrated into diverse neural architectures and trained end-to-end with gradient-based methods. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Neural networks in general, from MLPs and CNNs to attention-based Transformers, are constructed from layers of linear combinations followed by nonlinear operations such as ReLU, Sigmoid, or Softmax. Despite their strength, these conventional designs are often limited in introducing non-linearity by the choice of activation functions. In this work, we introduce Gaussian Mixture-Inspired Nonlinear Modules (GMNM), a new class of differentiable modules that draw on the universal density approximation Gaussian mixture models (GMMs) and distance properties (metric space) of Gaussian kernal. By relaxing probabilistic constraints and adopting a flexible parameterization of Gaussian projections, GMNM can be seamlessly integrated into diverse neural architectures and trained end-to-end with gradient-based methods. Our experiments demonstrate that incorporating GMNM into architectures such as MLPs, CNNs, attention mechanisms, and LSTMs consistently improves performance over standard baselines. These results highlight GMNM&#39;s potential as a powerful and flexible module for enhancing efficiency and accuracy across a wide range of machine learning applications.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.06660" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.06660" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.06660" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">32. Reconstructing the local density field with combined convolutional and point cloud architecture
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Baptiste Barthe-Gold, Nhat-Minh Nguyen, Leander Thiele</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> This combination enables efficient use of small-scale information and improves reconstruction quality relative to a U-Net-only approach. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We construct a neural network to perform regression on the local dark-matter density field given line-of-sight peculiar velocities of dark-matter halos, biased tracers of the dark matter field. Our architecture combines a convolutional U-Net with a point-cloud DeepSets. This combination enables efficient use of small-scale information and improves reconstruction quality relative to a U-Net-only approach. Specifically, our hybrid network recovers both clustering amplitudes and phases better than the U-Net on small scales.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.08573" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.08573" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.08573" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.04</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">33. Transverse Velocities in Real-Time Cosmology: Position Drift in Relativistic N-Body Simulations
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Alexander Oestreicher, Chris Clarkson, Julian Adamek, Sofie Marie Koksbang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We calculate the position drift directly from the past light cone for ten different observers and compare the results to predictions from linear perturbation theory. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The era of real-time cosmology has begun. It is now possible to directly measure the apparent drift of high-redshift astronomical sources across the sky $\textit{in real time}$. This so-called $\textit{position drift}$ provides a valuable probe of the peculiar velocity field and cosmic structure formation by giving direct access to the transverse velocity that is otherwise currently not measurable and must be statistically reconstructed from the density field in a model-dependent way. To fully exploit this new window into the Universe, it is essential to understand how cosmological structures affect position drift measurements. Here we present the first position drift study based on the general relativistic N-body simulation code $\texttt{gevolution}$. We calculate the position drift directly from the past light cone for ten different observers and compare the results to predictions from linear perturbation theory. At linear order, the position drift is directly proportional to the transverse velocity on the sky. This linear approximation reproduces our non-linear simulation results to within about 5%. We calculate power spectra for the position drift, splitting the signal into an E- and B-mode and compare the former to linear expectations, finding good agreement. The B-mode is suppressed on linear scales, but has similar amplitude as the E-mode on non-linear scales. We further demonstrate that light-cone inhomogeneities induce biases in the dipole of the drift, introducing redshift dependence of both the amplitude and direction. Although our analysis is not yet sufficient for a firm conclusion, our results suggest that these effects alone cannot explain the possible redshift-dependent dipole in Gaia DR3 data reported in the literature.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.05956" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.05956" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.05956" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">34. GCPO: When Contrast Fails, Go Gold
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Hao Wu, Wei Liu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> In this paper, we introduce Group Contrastive Policy Optimization (GCPO), a method that incorporates external standard reference answers. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Reinforcement learning has been widely applied to enhance the reasoning capabilities of large language models. Extending the inference limits of smaller models has become a prominent research focus. However, algorithms such as Group Relative Policy Optimization (GRPO) suffer from a clear drawback: the upper bound of a model&#39;s rollout responses is entirely determined by the model itself, preventing the acquisition of knowledge from samples that are either all incorrect or all correct. In this paper, we introduce Group Contrastive Policy Optimization (GCPO), a method that incorporates external standard reference answers. When the model cannot solve a problem, the reference answer supplies the correct response, steering the model toward an unequivocally accurate update direction. This approach offers two main advantages: (1) it improves training efficiency by fully utilizing every sample; (2) it enables the model to emulate the problem solving strategy of the reference answer during training, thereby enhancing generalization in reasoning. GCPO achieves outstanding results across multiple benchmark datasets, yielding substantial improvements over the baseline model. Our code is available at: https://github.com/AchoWu/GCPO.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.07790" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.07790" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.07790" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">35. The Milky Way - Large Magellanic Cloud Interaction with Simulation Based Inference
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Richard A. N. Brooks, Jason L. Sanders, Vedant Chandra, Nicolás Garavito-Camargo, Adam M. Dillamore, Adrian M. Price-Whelan, Yuan-Sen Ting</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We train neural networks to estimate parameter posterior distributions using a set of $128,000$ rigid MW--LMC simulations conditioned upon velocity data from the Dark Energy Spectroscopic Instrument (DESI) and the combined H3+SEGUE+MagE outer halo surveys. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The infall of the Large Magellanic Cloud (LMC) into the Milky Way (MW) has displaced the MW&#39;s centre of mass, manifesting as an observed reflex motion in the velocities of outer halo stars. We use a Simulation Based Inference framework to constrain properties of the MW, LMC and the induced reflex motion using the dynamics of outer MW halo stars. Specifically, we use the mean radial and tangential velocities of outer halo stars calculated in a set of distance and on-sky bins. We train neural networks to estimate parameter posterior distributions using a set of $128,000$ rigid MW--LMC simulations conditioned upon velocity data from the Dark Energy Spectroscopic Instrument (DESI) and the combined H3+SEGUE+MagE outer halo surveys. We constrain the reflex motion velocity and the enclosed MW and LMC masses within $50 \, \rm kpc$ using the DESI or H3+SEGUE+MagE dataset while varying the survey sky coverage and depth. We find the most precise constraints by using the radial and tangential velocity data from the H3+SEGUE+MagE survey and on-sky quadrant sky coverages. We report a reflex motion velocity, the speed at which the MW lurches towards the LMC, of $v_{\rm{travel}} = 26.4^{+5.5}_{-4.4} \, \rm km \, \rm s^{-1}$, while simultaneously finding an enclosed LMC mass of $M_{\rm LMC}(&lt; 50 \, \rm kpc) = 9.2^{+1.9}_{-2.3} \times 10^{10}\, \rm M_{\odot}$ and enclosed MW mass of $M_{\rm MW}(&lt; 50 \, \rm kpc) = 4.4^{+0.7}_{-0.7} \times 10^{11}\, \rm M_{\odot}$. Our results suggest that the LMC&#39;s total mass is at least $\approx 10-15 \%$ of that of the MW. This inference framework is flexible such that it can provide rapid and reliable constraints when applied to any future survey measuring the velocities of outer halo stars.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04735" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04735" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04735" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">36. Glocal Information Bottleneck for Time Series Imputation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jie Yang, Kexin Zhang, Guibin Zhang, Philip S. Yu, Kaize Ding</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Time Series Imputation (TSI), which aims to recover missing values in temporal data, remains a fundamental challenge due to the complex and often high-rate missingness in real-world scenarios. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Time Series Imputation (TSI), which aims to recover missing values in temporal data, remains a fundamental challenge due to the complex and often high-rate missingness in real-world scenarios. Existing models typically optimize the point-wise reconstruction loss, focusing on recovering numerical values (local information). However, we observe that under high missing rates, these models still perform well in the training phase yet produce poor imputations and distorted latent representation distributions (global information) in the inference phase. This reveals a critical optimization dilemma: current objectives lack global guidance, leading models to overfit local noise and fail to capture global information of the data. To address this issue, we propose a new training paradigm, Glocal Information Bottleneck (Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework by introducing a Global Alignment loss, derived from a tractable mutual information approximation. This loss aligns the latent representations of masked inputs with those of their originally observed counterparts. It helps the model retain global structure and local details while suppressing noise caused by missing values, giving rise to better generalization under high missingness. Extensive experiments on nine datasets confirm that Glocal-IB leads to consistently improved performance and aligned latent representations under missingness. Our code implementation is available in https://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04910" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04910" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04910" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">37. Unsupervised Active Learning via Natural Feature Progressive Framework
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yuxi Liu, Catherine Lalman, Yimin Yang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Detailed ablation studies and qualitative visualizations provide compelling evidence for NFPF&#39;s superior performance, enhanced robustness, and improved data distribution coverage. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The effectiveness of modern deep learning models is predicated on the availability of large-scale, human-annotated datasets, a process that is notoriously expensive and time-consuming. While Active Learning (AL) offers a strategic solution by labeling only the most informative and representative data, its iterative nature still necessitates significant human involvement. Unsupervised Active Learning (UAL) presents an alternative by shifting the annotation burden to a single, post-selection step. Unfortunately, prevailing UAL methods struggle to achieve state-of-the-art performance. These approaches typically rely on local, gradient-based scoring for sample importance estimation, which not only makes them vulnerable to ambiguous and noisy data but also hinders their capacity to select samples that adequately represent the full data distribution. Moreover, their use of shallow, one-shot linear selection falls short of a true UAL paradigm. In this paper, we propose the Natural Feature Progressive Framework (NFPF), a UAL method that revolutionizes how sample importance is measured. At its core, NFPF employs a Specific Feature Learning Machine (SFLM) to effectively quantify each sample&#39;s contribution to model performance. We further utilize the SFLM to define a powerful Reconstruction Difference metric for initial sample selection. Our comprehensive experiments show that NFPF significantly outperforms all established UAL methods and achieves performance on par with supervised AL methods on vision datasets. Detailed ablation studies and qualitative visualizations provide compelling evidence for NFPF&#39;s superior performance, enhanced robustness, and improved data distribution coverage.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04939" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04939" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04939" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">38. A comprehensive look into the accuracy of SpEC binary black hole waveforms
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Taylor Knapp, Katerina Chatziioannou, Keefe Mitman, Mark A. Scheel, Michael Boyle, Lawrence E. Kidder, Harald Pfeiffer</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Numerical relativity simulations provide a full description of the dynamics of binary systems, including gravitational radiation. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Numerical relativity simulations provide a full description of the dynamics of binary systems, including gravitational radiation. The waveforms produced by these simulations have a number of applications in gravitational-wave detection and inference. In this work, we revisit the accuracy of the waveforms produced by the Spectral Einstein Code. Motivated by the wide range of waveform applications, we propose and explore three accuracy metrics between simulation resolutions: (i) the generalized frequency-weighted mismatch, (ii) the relative amplitude difference, and (iii) the phase difference at different times. We find that numerical errors accumulate over the binary evolution, but the error is not intrinsically larger during the latest, more dynamical stages. Studying errors across the parameter space, we identify a positive correlation between both the mismatch and the phase difference with precessing spin, but little correlation with aligned spin or eccentricity. Lastly, amplitude and phases differences are symmetric upon exchanging resolutions across the catalog, suggesting that there is no systematic error.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.06393" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.06393" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.06393" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">39. Physics-Informed Neural Networks with Fourier Features and Attention-Driven Decoding
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Rohan Arni, Carlos Blanco</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Further, we integrate Fourier feature embeddings to explicitly mitigate spectral bias, enabling adaptive encoding of multiscale behaviors in the frequency domain. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Physics-Informed Neural Networks (PINNs) are a useful framework for approximating partial differential equation solutions using deep learning methods. In this paper, we propose a principled redesign of the PINNsformer, a Transformer-based PINN architecture. We present the Spectral PINNSformer (S-Pformer), a refinement of encoder-decoder PINNSformers that addresses two key issues; 1. the redundancy (i.e. increased parameter count) of the encoder, and 2. the mitigation of spectral bias. We find that the encoder is unnecessary for capturing spatiotemporal correlations when relying solely on self-attention, thereby reducing parameter count. Further, we integrate Fourier feature embeddings to explicitly mitigate spectral bias, enabling adaptive encoding of multiscale behaviors in the frequency domain. Our model outperforms encoder-decoder PINNSformer architectures across all benchmarks, achieving or outperforming MLP performance while reducing parameter count significantly.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.05385" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.05385" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.05385" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">40. Heptapod: Language Modeling on Visual Signals
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yongxin Zhu, Jiawei Chen, Yuanzhe Chen, Zhuo Chen, Dongya Jia, Jian Cong, Xiaobin Zhuang, Yuping Wang, Yuxuan Wang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> On the ImageNet generation benchmark, Heptapod achieves an FID of $2.70$, significantly outperforming previous causal autoregressive approaches. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We introduce Heptapod, an image autoregressive model that adheres to the foundational principles of language modeling. Heptapod employs \textbf{causal attention}, \textbf{eliminates reliance on CFG}, and \textbf{eschews the trend of semantic tokenizers}. Our key innovation is \textit{next 2D distribution prediction}: a causal Transformer with reconstruction-focused visual tokenizer, learns to predict the distribution over the entire 2D spatial grid of images at each timestep. This learning objective unifies the sequential modeling of autoregressive framework with the holistic self-supervised learning of masked autoencoding, enabling the model to capture comprehensive image semantics via generative training. On the ImageNet generation benchmark, Heptapod achieves an FID of $2.70$, significantly outperforming previous causal autoregressive approaches. We hope our work inspires a principled rethinking of language modeling on visual signals and beyond.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.06673" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.06673" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.06673" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">41. Revisiting Long-context Modeling from Context Denoising Perspective
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zecheng Tang, Baibei Ji, Juntao Li, Lijun Wu, Haijia Gui, Min Zhang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Our findings reveal that even simple mitigation of detected context noise can substantially boost the model&#39;s attention on critical tokens and benefit subsequent predictions. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Long-context models (LCMs) have demonstrated great potential in processing long sequences, facilitating many real-world applications. The success of LCMs can be attributed to their ability to locate implicit critical information within the context for further prediction. However, recent research reveals that LCMs are often susceptible to contextual noise, i.e., irrelevant tokens, that can mislead model attention. In this paper, we conduct a fine-grained analysis of the context noise and propose an effective metric, the Integrated Gradient (IG) score, to detect and quantify the noise information within the context. Our findings reveal that even simple mitigation of detected context noise can substantially boost the model&#39;s attention on critical tokens and benefit subsequent predictions. Building on this insight, we propose Context Denoising Training (CDT), a straightforward yet effective training strategy that improves attention on critical tokens while reinforcing their influence on model predictions. Extensive experiments across four tasks, under both context window scaling and long-context alignment settings, demonstrate the superiority of CDT. Notably, when trained with CDT, an open-source 8B model can achieve performance (50.92) comparable to GPT-4o (51.00).</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.05862" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.05862" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.05862" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CL</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">42. DISCO-DJ II: a differentiable particle-mesh code for cosmology
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Florian List, Oliver Hahn, Thomas Flöss, Lukas Winkler</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> To control discreteness effects and achieve high accuracy, the code incorporates a suite of advanced techniques, for example a custom non-uniform FFT implementation for force evaluation. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The mildly non-linear regime of cosmic structure formation holds much of the information that upcoming large-scale structure surveys aim to exploit, making fast and accurate predictions on these scales essential. We present the $N$-body module of DISCO-DJ (DIfferentiable Simulations for COsmology - Done with Jax), designed to deliver high-fidelity, GPU-accelerated, and differentiable particle-mesh simulations tailored for cosmological inference. Theory-informed time integrators such as the recently introduced BullFrog method allow for accurate predictions already with few time steps (e.g. $6$ steps for per-cent-level accuracy in terms of the present-day power spectrum at $k \approx 0.2 \, h / \mathrm{Mpc}$ using $N = 512^3$ particles, which takes just a few seconds). To control discreteness effects and achieve high accuracy, the code incorporates a suite of advanced techniques, for example a custom non-uniform FFT implementation for force evaluation. Both forward- and reverse-mode differentiation are supported, with memory requirements independent of the number of time steps; in the reverse case, this is achieved through an adjoint formulation. We extensively study the effect of various numerical parameters on the accuracy. As an application of DISCO-DJ, we perform field-level inference by recovering $\sigma_8$ and the initial conditions from a noisy Gadget matter density field. Coupled with our recently introduced Einstein--Boltzmann solver, the DISCO-DJ ecosystem provides a self-consistent, fully differentiable pipeline for modelling the large-scale structure of the universe. The code is available at https://github.com/cosmo-sims/DISCO-DJ.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.05206" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.05206" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.05206" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">43. The radial acceleration relation at the EDGE of galaxy formation: testing its universality in low-mass dwarf galaxies
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Mariana P. Júlio, Justin I. Read, Marcel S. Pawlowski, Pengfei Li, Daniel Vaz, Jarle Brinchmann, Martin P. Rey, Oscar Agertz, Tom Holmes</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We use stellar line-of-sight velocities and the Jeans modelling code GravSphere to infer the mass distributions of these galaxies, allowing us to compute the RAR. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">A tight correlation between the baryonic and observed acceleration of galaxies has been reported over a wide range of mass ($10^8 &lt; M_{\rm bar}/{\rm M}_\odot &lt; 10^{11}$) - the Radial Acceleration Relation (RAR). This has been interpreted as evidence that dark matter is actually a manifestation of some modified weak-field gravity theory. In this paper, we study the radially resolved RAR of 12 nearby dwarf galaxies, with baryonic masses in the range $10^4 &lt; M_{\rm bar}/{\rm M}_\odot &lt; 10^{7.5}$, using a combination of literature data and data from the MUSE-Faint survey. We use stellar line-of-sight velocities and the Jeans modelling code GravSphere to infer the mass distributions of these galaxies, allowing us to compute the RAR. We compare the results with the EDGE simulations of isolated dwarf galaxies with similar stellar masses in a $\Lambda$CDM cosmology. We find that most of the observed dwarf galaxies lie systematically above the low-mass extrapolation of the RAR. Each galaxy traces a locus in the RAR space that can have a multi-valued observed acceleration for a given baryonic acceleration, while there is significant scatter from galaxy to galaxy. Our results indicate that the RAR does not apply to low-mass dwarf galaxies and that the inferred baryonic acceleration of these dwarfs does not contain enough information, on its own, to derive the observed acceleration. The simulated EDGE dwarfs behave similarly to the real data, lying systematically above the extrapolated RAR. We show that, in the context of modified weak-field gravity theories, these results cannot be explained by differential tidal forces from the Milky Way, nor by the galaxies being far from dynamical equilibrium, since none of the galaxies in our sample seems to experience strong tides. As such, our results provide further evidence for the need for invisible dark matter in the smallest dwarf galaxies.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.06905" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.06905" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.06905" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">44. AMAQ: Adaptive Mixed-bit Activation Quantization for Collaborative Parameter Efficient Fine-tuning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yurun Song, Zhuoyi Yang, Ian G. Harris, Sangeetha Abdu Jyothi</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Large Language Models (LLMs) are scaling rapidly, creating significant challenges for collaborative server client distributed training, particularly in terms of communication efficiency and computational overheads. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Large Language Models (LLMs) are scaling rapidly, creating significant challenges for collaborative server client distributed training, particularly in terms of communication efficiency and computational overheads. To address these challenges, we implement Parameter-efficient Split Learning, which effectively balances efficiency and performance for collaborative training on low-resource devices. To reduce communication overhead in collaborative training, we introduce Adaptive Mixed bit Activation Quantization (AMAQ), a strategy that progressively compresses activations and gradients from high precision (6 to 8 bits) to low precision (3 to 4 bits). AMAQ achieves this by effectively allocating bit budgets across channels based on feature wise and layer wise importance using bit regularization. Under the same bit budgets, AMAQ outperforms fixed-precision approaches, delivering about 2.5% higher generation accuracy and about 1.3% better classification accuracy for models like LLaMA3 8B and Qwen2.5 7B. In addition, it significantly enhances training stability and reducing ultra-low bit representation collapse during the training. Experiments demonstrate that AMAQ integrates effectively into practical multi-machine collaborative training setups, offering superior inference accuracy with only a modest communication overhead for bits adaptation during training. This trade off makes AMAQ a practical and effective solution for collaborative training with minimal communication cost.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.05468" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.05468" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.05468" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">45. TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant LoRA
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Chanjoo Jung, Jaehyung Kim</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these costs, but the adapted parameters are dependent on the base model and cannot be transferred across different backbones. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Large Language Models (LLMs) are widely applied in real world scenarios, but fine-tuning them comes with significant computational and storage costs. Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these costs, but the adapted parameters are dependent on the base model and cannot be transferred across different backbones. One way to address this issue is through knowledge distillation, but its effectiveness inherently depends on training data. Recent work such as TransLoRA avoids this by generating synthetic data, but this adds complexity because it requires training an additional discriminator model. In this paper, we propose TiTok, a new framework that enables effective LoRA Transplantation through Token-level knowledge transfer. Specifically, TiTok captures task-relevant information through a contrastive excess between a source model with and without LoRA. This excess highlights informative tokens and enables selective filtering of synthetic data, all without additional models or overhead. Through experiments on three benchmarks across multiple transfer settings, our experiments show that the proposed method is consistently effective, achieving average performance gains of +4~8% compared to baselines overall.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04682" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04682" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04682" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CL</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">46. A Mathematical Explanation of Transformers for Large Language Models and GPTs
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Xue-Cheng Tai, Hao Liu, Lingfeng Li, Raymond H. Chan</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> This operator-theoretic and variational perspective offers a unified and interpretable foundation for understanding the architecture&#39;s core components, including attention, feedforward layers, and normalization. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The Transformer architecture has revolutionized the field of sequence modeling and underpins the recent breakthroughs in large language models (LLMs). However, a comprehensive mathematical theory that explains its structure and operations remains elusive. In this work, we propose a novel continuous framework that rigorously interprets the Transformer as a discretization of a structured integro-differential equation. Within this formulation, the self-attention mechanism emerges naturally as a non-local integral operator, and layer normalization is characterized as a projection to a time-dependent constraint. This operator-theoretic and variational perspective offers a unified and interpretable foundation for understanding the architecture&#39;s core components, including attention, feedforward layers, and normalization. Our approach extends beyond previous theoretical analyses by embedding the entire Transformer operation in continuous domains for both token indices and feature dimensions. This leads to a principled and flexible framework that not only deepens theoretical insight but also offers new directions for architecture design, analysis, and control-based interpretations. This new interpretation provides a step toward bridging the gap between deep learning architectures and continuous mathematical modeling, and contributes a foundational perspective to the ongoing development of interpretable and theoretically grounded neural network models.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.03989" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.03989" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.03989" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">47. Optimal estimation of a factorizable density using diffusion models with ReLU neural networks
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jianqing Fan, Yihong Gu, Ximing Li</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> This is, to the best of our knowledge, the first in the literature showing that diffusion models with standard configurations can adapt to the low-dimensional factorizable structures. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">This paper investigates the score-based diffusion models for density estimation when the target density admits a factorizable low-dimensional nonparametric structure. To be specific, we show that when the log density admits a $d^*$-way interaction model with $\beta$-smooth components, the vanilla diffusion model, which uses a fully connected ReLU neural network for score matching, can attain optimal $n^{-\beta/(2\beta+d^*)}$ statistical rate of convergence in total variation distance. This is, to the best of our knowledge, the first in the literature showing that diffusion models with standard configurations can adapt to the low-dimensional factorizable structures. The main challenge is that the low-dimensional factorizable structure no longer holds for most of the diffused timesteps, and it is very challenging to show that these diffused score functions can be well approximated without a significant increase in the number of network parameters. Our key insight is to demonstrate that the diffused score functions can be decomposed into a composition of either super-smooth or low-dimensional components, leading to a new approximation error analysis of ReLU neural networks with respect to the diffused score function. The rate of convergence under the 1-Wasserstein distance is also derived with a slight modification of the method.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.03994" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.03994" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.03994" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">math.ST</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">48. ESS-Flow: Training-free guidance of flow-based models as inference in source space
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Adhithyan Kalaivanan, Zheng Zhao, Jens Sjölund, Fredrik Lindsten</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We demonstrate its effectiveness on designing materials with desired target properties and predicting protein structures from sparse inter-residue distance measurements. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Guiding pretrained flow-based generative models for conditional generation or to produce samples with desired target properties enables solving diverse tasks without retraining on paired data. We present ESS-Flow, a gradient-free method that leverages the typically Gaussian prior of the source distribution in flow-based models to perform Bayesian inference directly in the source space using Elliptical Slice Sampling. ESS-Flow only requires forward passes through the generative model and observation process, no gradient or Jacobian computations, and is applicable even when gradients are unreliable or unavailable, such as with simulation-based observations or quantization in the generation or observation process. We demonstrate its effectiveness on designing materials with desired target properties and predicting protein structures from sparse inter-residue distance measurements.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.05849" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.05849" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.05849" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">49. Overview of the latest developments in understanding the initial state and thermalization
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Kirill Boguslavski</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A proper description of the non-equilibrium matter preceding the quark-gluon plasma (QGP) in heavy-ion collisions and its observable consequences remain a major theoretical challenge, while at the same time offering new opportunities for experimental exploration. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">A proper description of the non-equilibrium matter preceding the quark-gluon plasma (QGP) in heavy-ion collisions and its observable consequences remain a major theoretical challenge, while at the same time offering new opportunities for experimental exploration. In these proceedings, I provide an overview of studies presented in talks and posters at Quark Matter 2025 on this topic. We will focus on the latest developments regarding the features and the numerical description of the non-equilibrium pre-QGP matter, as well as the potential to use hard probes as a means to study the hydrodynamization dynamics of the QCD plasma.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04661" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04661" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04661" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">hep-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">50. Closed-Form Last Layer Optimization
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Alexandre Galashov, Nathaël Da Costa, Liyuan Xu, Philipp Hennig, Arthur Gretton</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We show this is equivalent to alternating between gradient descent steps on the backbone and closed-form updates on the last layer. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Neural networks are typically optimized with variants of stochastic gradient descent. Under a squared loss, however, the optimal solution to the linear last layer weights is known in closed-form. We propose to leverage this during optimization, treating the last layer as a function of the backbone parameters, and optimizing solely for these parameters. We show this is equivalent to alternating between gradient descent steps on the backbone and closed-form updates on the last layer. We adapt the method for the setting of stochastic gradient descent, by trading off the loss on the current batch against the accumulated information from previous batches. Further, we prove that, in the Neural Tangent Kernel regime, convergence of this method to an optimal solution is guaranteed. Finally, we demonstrate the effectiveness of our approach compared with standard SGD on a squared loss in several supervised tasks -- both regression and classification -- including Fourier Neural Operators and Instrumental Variable Regression.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.04606" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.04606" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.04606" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
    </div>

    <footer class="footer">
        &copy; 2025 arXiv Daily · Powered by <a href="https://arxiv.org/" target="_blank">arXiv</a>
    </footer>
</body>
</html>