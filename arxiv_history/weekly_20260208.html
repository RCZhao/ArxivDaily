<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>arXiv Daily: Backfill (2026-02-01 to 2026-02-08)</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto:400,700&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Roboto', Arial, sans-serif; background: #f8f9fa; transition: background 0.3s; }
        .arxiv-card { margin: 2em auto; max-width: 900px; box-shadow: 0 2px 8px rgba(0,0,0,0.08); border-radius: 16px; transition: box-shadow 0.3s, transform 0.3s; }
        .arxiv-card:hover { box-shadow: 0 4px 12px rgba(0,0,0,0.1); transform: translateY(-1px); }
        .arxiv-title { background: linear-gradient(90deg, #0d6efd 60%, #6610f2 100%); color: #fff; border-radius: 16px 16px 0 0; padding: 1.5em; font-size: 1.5em; font-weight: 700; letter-spacing: 0.5px;}
        .arxiv-meta { font-size: 1em; color: #555; margin-bottom: 1em; }
        .arxiv-abstract { background: #fff; padding: 1.5em; border-radius: 0 0 16px 16px; font-size: 1.1em; line-height: 1.7;}
        .arxiv-links a { margin-right: 1em; }
        .arxiv-scores { margin-top: 1em; font-size: 1em; }
        .navbar { background: #fff; box-shadow: 0 2px 8px rgba(0,0,0,0.04);}
        .footer { text-align: center; color: #888; padding: 2em 0 1em 0; font-size: 0.95em;}
        @media (prefers-color-scheme: dark) {
            body { background: #181a1b; color: #e4e6eb; }
            .arxiv-card { background: #23272b; box-shadow: 0 2px 8px rgba(0,0,0,0.28);}
            .arxiv-card:hover { box-shadow: 0 5px 15px rgba(0,0,0,0.25); transform: translateY(-1px); }
            .arxiv-title { background: linear-gradient(90deg, #375a7f 60%, #6f42c1 100%);}
            .arxiv-abstract { background: #23272b; color: #e4e6eb;}
            .navbar { background: #23272b; color: #e4e6eb;}
            .text-muted { color: #adb5bd !important; }
        }
        /* Improved abstract readability */
        .arxiv-abstract-text {
            font-size: 1.1em;
            line-height: 1.7;
        }
        p.big {
            line-height: 1.6;
            font-size: large;
        }
    </style>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'] ],
          processEscapes: true
        }
      });
    </script>
    <script>
    function toggleAuthors(element) {
        const fullList = element.querySelector('.author-full');
        const shortList = element.querySelector('.author-short');
        const toggleText = element.querySelector('.author-toggle');
        
        fullList.style.display = fullList.style.display === 'none' ? 'inline' : 'none';
        shortList.style.display = shortList.style.display === 'none' ? 'inline' : 'none';
        toggleText.textContent = fullList.style.display === 'none' ? ' (show all)' : ' (show less)';
    }
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>
<body>
    <nav class="navbar navbar-expand-lg mb-4">
        <div class="container">
            <a class="navbar-brand fw-bold text-primary" href="#">arXiv Daily</a>
            <span class="navbar-text">Modern arXiv Reader</span>
        </div>
    </nav>

    
    <div class="container py-4">
        <header class="mb-4"><h2 class="display-6 text-center">Analysis of Your Favorites</h2></header>
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Word Cloud</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/word_cloud.png" class="img-fluid rounded" alt="Word Cloud of favorite paper topics">
                    </div>
                </div>
            </div>
        </div>
        
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Clusters</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/backfill_cluster_map.png" class="img-fluid rounded" alt="2D Cluster plot of favorite papers">
                    </div>
                </div>
            </div>
        </div>
        
    </div>
    

    <div class="container py-4">
        <header class="mb-4"><h1 class="display-5 text-center text-primary">arXiv: Backfill (2026-02-01 to 2026-02-08)</h1></header>

        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Score Distribution of All Papers Found Today</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/score_distribution.png" class="img-fluid rounded" alt="Score distribution of all papers found today">
                    </div>
                </div>
            </div>
        </div>
        

        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">1. pop-cosmos: Forward modeling KiDS-1000 redshift distributions using realistic galaxy populations
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Boris Leistedt, Hiranya V. Peiris, Anik Halder, Stephen Thorp, Daniel J. Mortlock, Arthur Loureiro, Justin Alsing, Gurjeet Jagwani, Madalina N. Tudorache, Sinan Deger, et al.</span>
                                <span class="author-full" style="display: none;">Boris Leistedt, Hiranya V. Peiris, Anik Halder, Stephen Thorp, Daniel J. Mortlock, Arthur Loureiro, Justin Alsing, Gurjeet Jagwani, Madalina N. Tudorache, Sinan Deger, Joel Leja, Benedict Van den Bussche, Angus H. Wright, Shun-Sheng Li, Konrad Kuijken, Hendrik Hildebrandt</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A new forward-modeling framework combining the pop-cosmos generative model and SKiLLS simulations successfully infers KiDS galaxy redshift distributions, providing an independent calibration method that reveals systematic differences (Δz ~ 0.05-0.1) compared to spectroscopic reweighting in extreme tomographic bins. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The accuracy of the cosmological constraints from Stage~IV galaxy surveys will be limited by how well the galaxy redshift distributions can be inferred. We have addressed this challenging problem for the Kilo-Degree Survey (KiDS) cosmic shear sample by developing a forward-modeling framework with two main ingredients: (1) the \texttt{pop-cosmos} generative model for the evolving galaxy population, calibrated on \textit{Spitzer} IRAC $\textit{Ch.\,1}&lt;26$ galaxies from COSMOS2020; and (2) a data model for noise and selection, machine-learned from the SURFS-based KiDS-Legacy-Like Simulations (SKiLLS). Applying KiDS tomographic binning to our synthetic photometric data, we infer redshift distributions in each of five bins directly from the population and data models, bypassing the need for spectroscopic reweighting. Keeping the data model fixed, we compare results using two different galaxy population models: \texttt{pop-cosmos}; and \texttt{shark}, the semi-analytic galaxy formation model used in SKiLLS. In the first ($0.1&lt;z&lt;0.3$) and last ($0.9&lt;z&lt;1.2$) tomographic bins we find systematic differences in the mean redshifts of $Δz\sim0.05$-$0.1$, comparable to the reported uncertainties from spectroscopic reweighting methods. This work paves the way for accurate redshift distribution calibration for Stage~IV surveys directly through forward modeling, thus providing an independent cross-check on spectroscopic-based calibrations which avoids their selection biases and incompleteness. We will use the \texttt{pop-cosmos} redshift distributions in an upcoming full KiDS cosmology reanalysis.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.03935" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.03935" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.03935" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 11.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.41</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">2. Emulating galaxy and peculiar velocity clustering on non-linear scales
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">T. Dumerchat, J. Bautista, C. Ravoux, J. Aguilar, S. Ahlen, S. BenZvi, D. Bianchi, D. Brooks, T. Claybaugh, A. de la Macorra, et al.</span>
                                <span class="author-full" style="display: none;">T. Dumerchat, J. Bautista, C. Ravoux, J. Aguilar, S. Ahlen, S. BenZvi, D. Bianchi, D. Brooks, T. Claybaugh, A. de la Macorra, P. Doel, S. Ferraro, J. E. Forero-Romero, E. Gaztañaga, S. Gontcho A Gontcho, G. Gutierrez, C. Hahn, C. Howlett, M. Ishak, R. Joyce, D. Kirkby, A. Kremin, C. Lamman, M. Landriau, L. Le Guillou, M. Manera, R. Miquel, S. Nadathur, W. J. Percival, F. Prada, I. Pérez-Ràfols, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, J. Silber, D. Sprayberry, G. Tarlé, B. A. Weaver, H. Zou</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Combining galaxy clustering and peculiar velocities on non-linear scales, modeled via HOD emulators trained on AbacusSummit, yields tighter cosmological constraints, improving fσ8 precision to 3.8%, but highlights the necessity of reducing velocity measurement uncertainties to maximize these gains. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We explore the potential of cross-correlating galaxies and peculiar velocities on non-linear scales to enhance cosmological constraints. Leveraging the \textsc{AbacusSummit} simulation suite and the halo occupation distribution (HOD) formalism, we train emulator models to describe the non-linear clustering of galaxies and velocities in redshift space. Our analysis demonstrates that combining galaxy and peculiar velocity clustering, provides tighter constraints on both HOD and cosmological parameters, particularly on $σ_8$ and $w_0$. We further apply our models to realistic mock catalogues, reproducing the expected density and peculiar velocity errors of type-Ia supernovae and Tully-Fisher/fundamental plane measurements for the combined ZTF and DESI measurements. While systematic biases arise in the HOD parameters, the cosmological constraints remain unbiased, yielding $3.8\%$ precision measurement on $fσ_8$ compared to $4.7\%$ using galaxy clustering alone. We demonstrate that, while combining tracers with realistic velocity measurements still yields improvement, the gains are diminished, highlighting the need for further efforts to reduce velocity measurement uncertainties and correct observational systematics on small scales.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.03382" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.03382" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.03382" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.17</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">3. Microlensing constraint on Primordial Black Hole abundance with Subaru Hyper Suprime-Cam observations of Andromeda
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Sunao Sugiyama, Masahiro Takada, Naoki Yasuda, Nozomu Tominaga</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Analysis of high-cadence Subaru HSC microlensing observations toward M31 identified 12 candidates, constraining primordial black hole dark matter parameters to an allowed region characterized by a mass scale of M_{\rm PBH}\sim10^{-7}--10^{-6}M_\odot and an abundance f_{\rm PBH}\sim \mathcal{O}(10^{-2}{\rm -}10^{-1}). (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present updated microlensing analysis results based on high-cadence ($\sim$2~min) Subaru Hyper Suprime-Cam (HSC) observations of the Andromeda Galaxy (M31) in 2014, 2017, and 2020, yielding a total of 39.3 hours of data. We use a point-lens finite-source model for the microlensing light curve model and employ multi-stage selection procedures to identify microlensing candidates. From more than 25,000 variable candidates detected across all nights, we identify 12 microlensing candidates with light-curve timescales shorter than 5~hours, and among them, 4 secure candidates with high-significance detections. We estimate detection efficiencies using light-curve-level simulations that account for observational conditions and finite-source effects. Using a hierarchical Bayesian framework that combines the light-curve fitting information for each candidate with the Poisson statistics of the number of candidates, we derive constraints on parameters that characterize the abundance and mass scale of primordial black hole (PBH) dark matter. First, we derive upper limits on the PBH abundance under the null hypothesis that all events are assumed to be false detections. Next, employing the PBH hypothesis in which all (or only secure) candidates are assumed to be due to PBH microlensing, we derive the allowed region of the PBH parameters; the inferred mass scale is $M_{\rm PBH}\sim10^{-7}$--$10^{-6}M_\odot$, and the PBH abundance to the total dark matter is $f_{\rm PBH}\sim \mathcal{O}(10^{-2}{\rm -}10^{-1})$. Our results demonstrate that HSC-M31 monitoring remains a uniquely powerful probe of PBHs, and highlight the need for further studies for example, using Rubin Observatory LSST observations of the Large Magellanic Cloud.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.05840" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.05840" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.05840" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.0</span>
                        <span class="badge bg-info text-dark">Author Score: 0.15</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">4. The FLAMINGO Project: Exploring the X-ray--cosmic-shear cross-correlation as a probe of large-scale structure
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>William McDonald, Joop Schaye, Konrad Kuijken, John Helly, Joey Braspenning, Matthieu Schaller</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Examining the X-ray--cosmic-shear cross-correlation using FLAMINGO simulations reveals its sensitivity to baryonic feedback strength and gas distribution in massive haloes, concluding that exploiting this probe requires resolving fainter Active Galactic Nuclei to break the degeneracy between feedback and cosmological parameters. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Baryonic feedback processes associated with galaxy formation directly influence the large-scale structure by redistributing gas. Recent measurements of the kinetic Sunyaev-Zel&#39;dovich effect and stacks of X-ray emission from optically selected galaxy clusters suggest that feedback from Active Galactic Nuclei (AGN) is more efficient at expelling gas from low-mass clusters than previously thought. The measurement of the cross-correlation between cosmic shear and diffuse X-ray emission provides a new probe of the distribution of gas in groups and clusters. We use the FLAMINGO cosmological, hydrodynamical simulations to examine the X-ray--cosmic-shear cross-correlation. The cross-correlation is most sensitive to the distribution of gas in haloes with masses $10^{14}\leq M_{200\mathrm{c}}/\mathrm{M}_{\odot}\leq10^{15}$. It is sensitive to the strength of feedback, but the effects of variations in cosmology and baryonic physics are largely degenerate. We compare the FLAMINGO predictions with the cross-correlation between cosmic shear from the Dark Energy Survey and ROSAT all-sky X-ray maps. We find that, if we neglect the X-ray emission from AGN that would remain unresolved by ROSAT, then the fiducial FLAMINGO model is in excellent agreement with the data, while models with stronger or weaker feedback are ruled out. However, if we account for unresolved AGN, either using the direct FLAMINGO predictions or by abundance matching to the observed (extrapolated) AGN luminosity function, then models with stronger feedback are preferred. We conclude that to exploit the potential of the X-ray--lensing cross-correlation, it will be necessary to resolve fainter AGN, and to use external constraints to break the degeneracy between baryonic feedback and cosmology.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.02484" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.02484" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.02484" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.9</span>
                        <span class="badge bg-info text-dark">Author Score: 0.10</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmic Shear, Cosmological Constraints / Gravitational Lensing, Galaxy Clustering, Halo Occupation</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">5. The Impact of Galaxy Formation on Galaxy Biasing, and Implications for Primordial non-Gaussianity Constraints
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Lucia A. Perez, Shy Genel, Elisabeth Krause, Rachel S. Somerville</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Leveraging the CAMELS-SAM pipeline and separate-universe simulations, researchers demonstrated that the evolution of the non-Gaussianity bias parameters b_1 and b_\phi is closely tracked by the stellar-to-halo mass relationship, confirming that specific star formation rate selections are particularly robust against variations in galaxy formation modeling. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The parameter $f_{\textrm{NL}}$ measures the local non-Gaussianity in the primordial energy fluctuations of the Universe, with any deviation from $f_{\textrm{NL}}=0$ providing key constraints on inflationary models. Galaxy clustering is sensitive to $f_{\textrm{NL}}$ at large scale modes and the next generation of galaxy surveys will approach a statistical error of $σ_{f_{\textrm{NL}}}\sim1$. However, the systematic errors on these constraints are dominated by the degeneracy of $f_{\textrm{NL}}$ with the galaxy bias parameters $b_1$ (galaxy overdensities caused by mass perturbations) and $b_φ$ (galaxy overdensities caused by primordial potential perturbations). It has been shown that the assumed scaling of $b_φ(z)=2δ_c (b_1(z)-1)$ is not accurate for realistically simulated galaxies, and depends both on the galaxy selection and the way that galaxies are modeled. To address this, we leverage the CAMELS-SAM pipeline to explore how varying parameters of galaxy formation affects $b_φ$ and $b_1$ for various galaxy selections. We run separate-universe N-body simulations of $L=205 h^{-1}$ cMpc and $N=1280^3$ to measure $b_φ$, and run 55 unique instances of the Santa Cruz semi-analytic model with varying parameters of stellar and AGN feedback. We find the behavior and evolution of a SC-SAM model&#39;s stellar-, SFR- and sSFR- to halo mass relationships track well with how $b_1$ and $b_φ(b_1)$ change across redshift and selection for the SC-SAM. We find our variations of the SC-SAM encapsulate the $b_φ$ behavior previously measured in IllustrisTNG, the Munich SAM, and Galacticus.Finally, we identify sSFR selections as particularly robust to varied galaxy modeling.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.04987" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.04987" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.04987" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.9</span>
                        <span class="badge bg-info text-dark">Author Score: 0.07</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmic Shear, Cosmological Constraints</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">6. A Lightweight Library for Energy-Based Joint-Embedding Predictive Architectures
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Basile Terver, Randall Balestriero, Megi Dervishi, David Fan, Quentin Garrido, Tushar Nagarajan, Koustuv Sinha, Wancong Zhang, Mike Rabbat, Yann LeCun, et al.</span>
                                <span class="author-full" style="display: none;">Basile Terver, Randall Balestriero, Megi Dervishi, David Fan, Quentin Garrido, Tushar Nagarajan, Koustuv Sinha, Wancong Zhang, Mike Rabbat, Yann LeCun, Amir Bar</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The open-source EB-JEPA library provides modular implementations of Joint-Embedding Predictive Architectures, demonstrating that these energy-based self-supervised learning techniques effectively scale from image representation learning to temporal video prediction and action-conditioned world models, achieving a 97% planning success rate in navigation tasks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present EB-JEPA, an open-source library for learning representations and world models using Joint-Embedding Predictive Architectures (JEPAs). JEPAs learn to predict in representation space rather than pixel space, avoiding the pitfalls of generative modeling while capturing semantically meaningful features suitable for downstream tasks. Our library provides modular, self-contained implementations that illustrate how representation learning techniques developed for image-level self-supervised learning can transfer to video, where temporal dynamics add complexity, and ultimately to action-conditioned world models, where the model must additionally learn to predict the effects of control inputs. Each example is designed for single-GPU training within a few hours, making energy-based self-supervised learning accessible for research and education. We provide ablations of JEA components on CIFAR-10. Probing these representations yields 91% accuracy, indicating that the model learns useful features. Extending to video, we include a multi-step prediction example on Moving MNIST that demonstrates how the same principles scale to temporal modeling. Finally, we show how these representations can drive action-conditioned world models, achieving a 97% planning success rate on the Two Rooms navigation task. Comprehensive ablations reveal the critical importance of each regularization component for preventing representation collapse. Code is available at https://github.com/facebookresearch/eb_jepa.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.03604" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.03604" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.03604" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.8</span>
                        <span class="badge bg-info text-dark">Author Score: 0.06</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">7. pop-cosmos: Redshifts and physical properties of KiDS-1000 galaxies
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Anik Halder, Hiranya V. Peiris, Stephen Thorp, Boris Leistedt, Daniel J. Mortlock, Gurjeet Jagwani, Madalina N. Tudorache, Sinan Deger, Benedict Van den Bussche, Joel Leja, et al.</span>
                                <span class="author-full" style="display: none;">Anik Halder, Hiranya V. Peiris, Stephen Thorp, Boris Leistedt, Daniel J. Mortlock, Gurjeet Jagwani, Madalina N. Tudorache, Sinan Deger, Benedict Van den Bussche, Joel Leja, Angus H. Wright</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Principled Bayesian inference using the pop-cosmos generative model and GPU-accelerated MCMC successfully constrained joint redshift and physical properties for 4 million KiDS-1000 galaxies, achieving low photometric redshift bias and enabling the definition of weak lensing samples based on physical characteristics rather than color cuts. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Principled Bayesian inference of galaxy properties has not previously been performed for wide-area weak lensing surveys with millions of sources. We address this gap by applying the pop-cosmos generative model to perform spectral energy distribution (SED) fitting for 4 million KiDS-1000 galaxies. Calibrated on deep COSMOS2020 photometric data, pop-cosmos specifies a physically-motivated prior over the galaxy population up to $z \simeq 6$ in stellar population synthesis (SPS) parameter space. Using the Speculator SPS emulator with GPU-accelerated MCMC sampling, we perform full posterior inference at 6.5 GPU seconds per galaxy, obtaining joint constraints on galaxy redshifts and physical properties. We validate photometric redshifts against $\sim\!185,\!000$ KiDS galaxies cross-matched to DESI DR1 spectroscopic samples, achieving low bias ($3\times10^{-3}$), scatter ($σ_{\mathrm{MAD}}=0.04$), and outlier fraction (3.7%) for the Bright Galaxy Survey, with comparable performance (bias $3\times10^{-2}$, $σ_{\mathrm{MAD}}=0.05$, 1.3% outliers) for luminous red galaxies (LRGs). Within the LRG sample, we identify massive, dusty, star-forming contaminants at $z \simeq 0.4$ satisfying standard colour selections for quenched populations. We infer trends in stellar mass, star formation, metallicity, and dust across five tomographic redshift bins consistent with established scaling relations. Using specific star formation rate constraints, we identify $\sim$10% of KiDS-1000 galaxies as quenched, versus 37% implied by conservative colour cuts. This enables the construction of weak lensing samples defined by physical properties while mitigating intrinsic alignment systematics and preserving statistical power. Our analysis validates pop-cosmos out-of-sample, establishing it as a scaleable approach for galaxy evolution and cosmological analyses in photometric surveys.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.03930" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.03930" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.03930" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.8</span>
                        <span class="badge bg-info text-dark">Author Score: 0.10</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">8. The Atacama Cosmology Telescope: Constraints on Local Non-Gaussianity from the ACT Cluster Catalog
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Leonid Sarieddine, J. Richard Bond, Matt Hilton, Raul Jimenez, Arthur Kosowsky, Kavilan Moodley, Bernardita Ried Guachalla, Cristóbal Sifón, Suzanne T. Staggs, Licia Verde, et al.</span>
                                <span class="author-full" style="display: none;">Leonid Sarieddine, J. Richard Bond, Matt Hilton, Raul Jimenez, Arthur Kosowsky, Kavilan Moodley, Bernardita Ried Guachalla, Cristóbal Sifón, Suzanne T. Staggs, Licia Verde, Edward J. Wollack</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Modeling the number counts of 1,201 clusters in the ACT DR6 Sunyaev–Zel&#39;dovich catalog yields constraints on local-type primordial non-Gaussianity of f_{\rm NL} = 55 \pm 125 (68% CL), consistent with Gaussian initial conditions while probing smaller comoving scales than complementary cosmic microwave background measurements. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We derive constraints on local-type primordial non-Gaussianity using the ACT DR6 Sunyaev--Zel&#39;dovich cluster catalog. Modeling the redshift- and mass-dependent number counts of 1,201 clusters in the 10,347~deg$^2$ Legacy region, and accounting for survey completeness, intrinsic SZ scatter, and a weak-lensing-calibrated mass bias, we compute theoretical abundances using the Log--Edgeworth halo mass function. Assuming $Λ$CDM with well-motivated external priors, we obtain $f_{\rm NL} = 55 \pm 125$ (68% CL), consistent with Gaussian initial conditions. These constraints probe comoving scales of $5$--$10~{\rm Mpc}~h^{-1}$, complementing CMB bispectrum and scale-dependent bias measurements, which do not reach such small scales. We also find evidence for a 16.4% residual mass bias, which, although heavily informed by our adopted priors, plays a key role in matching observed and predicted counts but has negligible effect on $f_{\rm NL}$ constraints. We briefly discuss robustness of the results under relaxed priors and the prospects for next-generation SZ and lensing surveys to strengthen cluster-based tests of primordial non-Gaussianity.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.03917" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.03917" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.03917" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.7</span>
                        <span class="badge bg-info text-dark">Author Score: 0.05</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmic Shear, Cosmological Constraints / Gravitational Lensing, Galaxy Clustering, Halo Occupation</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">9. Clustering of emission line galaxies with IllustrisTNG -- II. cosmology challenge with anisotropic correlation functions and ELG-halo connections
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Ken Osato, Teppei Okumura</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Mock Emission Line Galaxy samples generated from IllustrisTNG simulations show a suppressed anisotropic correlation function quadrupole moment, confirming a velocity bias due to ELG infalling motion that leads to an underestimation of the linear growth rate unless clustering analysis is restricted to scales larger than 15 \, h^{-1} \, \mathrm{Mpc}. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Emission line galaxies (ELGs) are the primary tracers of the large-scale structures of the Universe in ongoing Stage-IV cosmological spectroscopic surveys, which aim to measure the clustering statistics at higher redshifts $z \simeq 1.5 \text{--} 2$ with unprecedented precision. In this study, we construct realistic mock ELG samples with IllustrisTNG hydrodynamical simulations and stellar population synthesis framework. In order to validate the modelling of clustering, we measure the anisotropic correlation functions of mock ELGs and infer the linear growth rate, which is one of key cosmological parameters in galaxy clustering. As a control sample, we construct the mass-limited subhalo samples with the same number density as ELGs. The isotropic correlation functions in real space for both samples do not differ significantly. However, the quadrupole moment of the anisotropic correlation function, which is sensitive to the velocity of galaxies, is suppressed for ELGs, potentially due to the infalling motion of ELGs towards the centre of the hosting halos. The smaller amplitude leads to the underestimation of the linear growth rate and implies the velocity bias between ELGs and dark matter. When the analysis is limited to large scales $(\gtrsim 15 \, h^{-1} \, \mathrm{Mpc})$, the parameter bias vanishes. Next, we investigate the ELG-halo connection through the phase-space distribution of satellite ELGs within hosting halos and galactic conformity of star formation activity. The infalling motion is further confirmed by the phase-space distribution relative to the host halo, and this dynamics of ELGs challenges the assumption that the radial distribution of satellites follows that of dark matter.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.01925" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.01925" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.01925" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.05</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">10. Weak Lensing Low Multipoles
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Albert Bonnefous, Roya Mohayaee</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Analytical and simulation-based analysis confirms that the weak-lensing convergence field&#39;s low multipoles saturate at an amplitude of $10^{-4}$ in $\Lambda$CDM, finding that observed low multipoles reconstructed from 2MRS align with predictions for Milky Way-like observers and contribute minimally to the cosmic dipole anomaly. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We analyse the low-multipole components of the weak-lensing convergence field in a FLRW universe. The low-multipole convergence field, encodes the largest-angle coherent potential gradients, essential for assessment of large-angle features in data. To study this large angle signal, we perform a combined analytical, numerical and observational study. Starting from exact analytical expressions for the convergence power spectrum, we quantify how the dipole, quadrupole and octupole build up with source redshift and show that, in $Λ$CDM, they saturate at an amplitude of order $10^{-4}$. We then use full-sky, horizon-scale $N$-body simulations (Quijote) to explore the dependence of this signal on the observer&#39;s environment, comparing random observers to ``Milky Way-like&#39;&#39; observers. In parallel, we reconstruct the convergence field due to our local Universe with the 2MASS Redshift Survey (2MRS), with proper treatment of incompleteness and galaxy bias. We find that the observed low multipoles from observation is above the $Λ$CDM mean predictions, but in full agreement with Milky Way-like observers in the simulation. Finally, by converting the convergence dipole into a number-count dipole, we test whether weak lensing can contribute to the cosmic dipole anomaly, an idea motivated by its natural alignment with the CMB dipole and by the fact that lensing, unlike clustering, cannot be removed by cross-matching surveys and thus survives in all high-redshift catalogues. We show that weak lensing by local structure contributes at most a few percent to this observed anomaly.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.04504" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.04504" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.04504" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.96</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmic Shear, Cosmological Constraints</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">11. Reconstructing the largest scales of the Universe with field-level inference applied to the Quaia Quasar Catalogue
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Adam Andrews, Arthur Loureiro, Jens Jasche, Stuart McAlpine, Guilhem Lavaux, Florent Leclercq</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Applying the BORG field-level inference algorithm to the vast Quaia quasar catalogue, researchers generated the largest comoving volume 3D reconstruction of the Universe&#39;s initial conditions and matter distribution by incorporating detailed physical forward modeling, including light-cone and redshift-space effects. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The recently released Quaia quasar catalogue, with its broad redshift range and all-sky coverage, enables unprecedented three-dimensional reconstructions of matter across cosmic time. In this work, we apply the field-level inference algorithm BORG to the Quaia catalogues to reconstruct the initial conditions and present-day matter distribution of the Universe. We employ a physics-based forward model of large-scale structure using Lagrangian perturbation theory, incorporating light-cone effects, redshift-space distortions, quasar bias, and survey selection effects. This approach enables a detailed and physically motivated inference of the three-dimensional density field and initial conditions over the entire cosmic volume considered. We analyse both the G &lt; 20.0 (Quaia Clean) and G &lt; 20.5 (Quaia Deep) samples, where G denotes the Gaia broad optical-band magnitude, imposing conservative sky cuts to ensure robustness against foreground contamination. The resulting reconstructions span a comoving volume of (10h^{-1} Gpc)^3 with a maximum spatial resolution of 39.1 h^{-1}Mpc, making this the largest field-level reconstruction of the observable Universe in terms of comoving volume to date. We validate our reconstructions through a range of internal and external consistency checks, including the cross-correlation of the inferred density fields with Planck CMB lensing, where we detect a signal at ~4σsignificance. Beyond delivering high-fidelity data products, including posterior maps of initial conditions, present-day dark matter, and velocity fields, this work establishes a framework for exploiting quasar surveys in field-level cosmology.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.02363" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.02363" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.02363" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.06</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">12. A Statistical Theory of Gated Attention through the Lens of Hierarchical Mixture of Experts
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Viet Nguyen, Tuan Minh Pham, Thinh Cao, Tan Dinh, Huy Nguyen, Nhat Ho, Alessandro Rinaldo</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Rigorous theoretical analysis demonstrates that gated attention, modeled as a hierarchical mixture of experts, achieves superior sample efficiency compared to multi-head self-attention, requiring only a polynomial number of data points for expert estimation where the latter requires an exponential amount. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Self-attention has greatly contributed to the success of the widely used Transformer architecture by enabling learning from data with long-range dependencies. In an effort to improve performance, a gated attention model that leverages a gating mechanism within the multi-head self-attention has recently been proposed as a promising alternative. Gated attention has been empirically demonstrated to increase the expressiveness of low-rank mapping in standard attention and even to eliminate the attention sink phenomenon. Despite its efficacy, a clear theoretical understanding of gated attention&#39;s benefits remains lacking in the literature. To close this gap, we rigorously show that each entry in a gated attention matrix or a multi-head self-attention matrix can be written as a hierarchical mixture of experts. By recasting learning as an expert estimation problem, we demonstrate that gated attention is more sample-efficient than multi-head self-attention. In particular, while the former needs only a polynomial number of data points to estimate an expert, the latter requires exponentially many data points to achieve the same estimation error. Furthermore, our analysis also provides a theoretical justification for why gated attention yields higher performance when a gate is placed at the output of the scaled dot product attention or the value map rather than at other positions in the multi-head self-attention architecture.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.01468" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.01468" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.01468" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.96</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">13. Dynamical Systematics on Time Delay Lenses and the Impact on the Hubble Constant
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>R. Forés-Toribio, C. S. Kochanek, J. A. Muñoz</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Achieving the required 2% precision for $H_0$ from time-delay lenses is severely challenged by systematic uncertainties, particularly those arising from PSF characterization, the choice of anisotropy models, and discrepancies between true and modeled stellar mass distributions, which can bias velocity dispersion measurements by up to 40%. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">While time-delay lenses can be an independent probe of $H_0$ the estimates are degenerate with the convergence of the lens near the Einstein radius. Velocity dispersions, $σ$, can be used to break the degeneracy, with uncertainties $ΔH/H_0 \propto Δσ^2/σ^2$ ultimately limited by the systematic uncertainties in the kinematic measurements - measuring $H_0$ to 2% requires $Δσ^2/σ^2$ &lt; 2%. Here we explore a broad range of potential systematic uncertainties contributing to eight time-delay lenses used in cosmological analyses. We find that: (1) The characterization of the PSF both in absolute scale and in shape is important, with biases in $Δσ^2/σ^2$ up to 1-6%, depending on the lens system. Small slit miscenterings of the lens are less important. (2) The difference between the measured velocity dispersion and the mean square velocity needed for the Jeans equations is important, with up to $Δσ^2/σ^2 \sim$ 3-8%. (3) The choice of anisotropy models is important with maximum changes of $Δσ^2/σ^2 \sim$ 5-18% and the frequently used Osipkov-Merritt models do not produce $h_4$ velocity moments typical of early-type galaxies. (4) Small differences between the true stellar mass distribution and the model light profile matter ($Δσ^2/σ^2 \sim$ 5-40%), with radial color gradients further complicating the problem. Finally, we discuss how the homogeneity of the early-type galaxy population means that many dynamically related parameters must be marginalized over the lens sample as a whole and not over individual lenses.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.03934" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.03934" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.03934" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.02</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmic Shear, Cosmological Constraints / Gravitational Lensing, Galaxy Clustering, Halo Occupation</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">14. Large-scale halo velocity correlations and the impact of finite simulation volumes
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yao-Tsung Chuang, Teppei Okumura, Takahiro Nishimichi</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Finite simulation box sizes suppress the amplitude of large-scale halo velocity correlation functions, with the suppression depending on halo mass, necessitating the marginalization of the minimum wave number ($k_{\min}$) to accurately constrain the cosmological growth rate parameter $f\sigma_8$. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The velocity correlation functions directly measured from the peculiar velocity field of dark matter in numerical simulations are known to have an amplitude lower than that predicted by theoretical models at large scales. The trend persists for dark-matter halos or galaxies that are more closely related to the observables. We investigate the impact of the finite simulation box sizes on the measured velocity correlation functions of halos, utilizing N-body simulations with different box sizes. We measure the halo velocity correlations from N-body simulations with side lengths of $1{\rm Gpc}/h$ and $2{\rm Gpc}/h$, confirming the former is more suppressed compared to the linear theory prediction on large scales due to the lack of large-scale modes beyond the box size. In contrast, even though we subdivide the larger-box simulations into those with side lengths of $1{\rm Gpc}/h$, the amount of the suppression is the same as that from the original boxes, as the large-scale modes are already imprinted. Introducing the lower limit of the integral in the Hankel transform, $k_{\rm min}$, as a free parameter and marginalizing it over, we find that the constrained growth rate parameter, $f(z)σ_8(z)$, returns the correct value assumed in the simulations. However, when we ignore the effect and set $k_{\rm min}=0$, the constraint on $fσ_8$ is significantly biased if the correlation between different separation bins is also ignored. Furthermore, we find that the suppression of the velocity correlation amplitude on large scales depends on halo mass, with more massive halos exhibiting a systematically stronger suppression. These results highlight the importance of accounting for missing long-wavelength modes when developing simulation-based modeling of velocity statistics, such as emulators.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.04485" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.04485" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.04485" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.10</span>
                        <span class="badge bg-primary">Semantic Score: 0.91</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">15. Differentiable Stochastic Halo Occupation Distribution with Galaxy Intrinsic Alignments
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Sneh Pandya, Jonathan Blazek</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The new diffHOD-IA framework provides a fully differentiable implementation of the halo occupation distribution model incorporating galaxy intrinsic alignments, enabling end-to-end automatic differentiation for efficient parameter recovery and integration into field-level weak-lensing inference pipelines. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present diffHOD-IA, a fully differentiable implementation of a halo occupation distribution (HOD) model that incorporates galaxy intrinsic alignments (IA). Motivated by the diffHOD framework, we create a new implementation that extends differentiable galaxy population modeling to include orientation-dependent statistics crucial for weak gravitational lensing analyses. Our implementation combines this HOD formulation with an IA model, enabling end-to-end automatic differentiation from HOD and IA parameters through to the galaxy field. We additionally extend this framework to differentiably model two-point correlation functions, including galaxy clustering and IA statistics. We validate diffHOD-IA against the reference halotools-IA implementation using the Bolshoi-Planck simulation, demonstrating excellent agreement across both one-point and two-point statistics. We verify the accuracy of gradients computed via automatic differentiation by comparison with finite-difference estimates for both HOD and IA parameters. We present science use cases leveraging gradients in the simulations to recover the IA parameters of a galaxy field representative of the TNG300 simulation. Finally, we apply diffHOD-IA in a Hamiltonian Monte Carlo analysis and compare its performance with halotools-IA and a neural-network-based emulator, IAEmu. Unlike emulator-based approaches, diffHOD-IA provides differentiability at the catalog level, enabling integration into field-level inference pipelines and extension to arbitrary summary statistics for next-generation weak-lensing analyses. Our code is publicly available.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.04977" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.04977" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.04977" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">16. Degeneracies and modelling choices in double-plane time-delay cosmography
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Daniel Johnson, Pierre Fleury, Martin Millon</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Analyzing double-plane gravitational lensing, the mass-sheet degeneracy is generalized to account for line-of-sight effects and cosmological scaling factor uncertainty, showing that the constraint on $H_0$ simplifies to the familiar mass-sheet transformation of the first lens plane plus a line-of-sight contribution to the second. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Double-plane gravitational lensing is a rare but increasingly observed phenomenon in which the light from a distant source is lensed by two foreground objects at different redshifts. Such systems can be used to provide simultaneous constraints on the Hubble constant $H_0$ and the dark-energy equation of state, independent of and complementary to other probes. However, just as for single-plane gravitational lenses, the precision of these constraints is limited by the so-called mass-sheet degeneracy (MSD) -- a fundamental limit to the knowledge of the mass profiles of lens galaxies and the line of sight that can be obtained from imaging constraints alone. In this work, we show explicitly how contributions from the line of sight appear in double-plane systems. Because these contributions modify angular diameter distances, we argue that cosmological priors should not be used to simply fix the ``cosmological scaling factor&#39;&#39;, a ratio of angular diameter distances which is key to the modelling of double-plane lenses. Motivated by this fact, we generalise the double-plane MSD to account for this uncertainty in the scaling factor. While this complicates the time-delay function, we show that, using the ``unfolding relation&#39;&#39;, a geometric relation between distances which holds even in the presence of line-of-sight corrections, the uncertainty in the Hubble constant reduces to the familiar mass-sheet transformation of the first lens plane, and a line-of-sight contribution between the observer and the second lens plane. Our main message is therefore a prescription for reducing the degrees of freedom within double-plane models, while still safely accounting for the MSD in measurements of $H_0$.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.02697" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.02697" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.02697" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmic Shear, Cosmological Constraints</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">17. Mitigating half-wave plate systematics at the map-making level: calibration requirements for LiteBIRD
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">N. Raffuzzi, A. Carones, M. Monelli, S. Giardiello, L. Pagano, Y. Sakurai, H. Ishino, E. Allys, A. Anand, J. Aumont, et al.</span>
                                <span class="author-full" style="display: none;">N. Raffuzzi, A. Carones, M. Monelli, S. Giardiello, L. Pagano, Y. Sakurai, H. Ishino, E. Allys, A. Anand, J. Aumont, A. J. Banday, G. Barbieri Ripamonti, R. B. Barreiro, N. Bartolo, S. Basak, A. Basyrov, A. Besnard, M. Bortolami, T. Brinckmann, F. Cacciotti, E. Calabrese, P. Campeti, F. Carralot, F. J. Casas, J. Chandran, K. Cheung, M. Citran, L. Clermont, F. Columbro, A. Coppolecchia, F. Cuttaia, P. de Bernardis, T. de Haan, M. De Lucia, P. Diego-Palazuelos, H. K. Eriksen, F. Finelli, C. Franceschet, U. Fuskeland, G. Galloni, M. Galloway, M. Gerbino, M. Gervasi, T. Ghigna, C. Gimeno-Amo, A. Gruppuso, M. Hazumi, S. Henrot-Versillé, L. T. Hergt, E. Hivon, K. Kohri, L. Lamagna, M. Lattanzi, C. Leloup, F. Levrier, A. I. Lonappan, M. López-Caniego, G. Luzzi, J. Macias-Perez, V. Maranchery, E. Martínez-González, S. Masi, S. Matarrese, T. Matsumura, S. Micheli, M. Migliaccio, G. Morgante, L. Mousset, R. Nagata, T. Namikawa, P. Natoli, A. Novelli, F. Noviello, A. Occhiuzzi, A. Paiella, D. Paoletti, G. Pascual-Cisneros, G. Patanchon, F. Piacentini, G. Piccirilli, M. Pinchera, G. Polenta, L. Porcelli, M. Remazeilles, A. Ritacco, M. Ruiz-Granda, L. Salvati, J. Sanghavi, V. Sauvage, D. Scott, M. Shiraishi, G. Signorelli, R. M. Sullivan, Y. Takase, L. Terenzi, M. Tomasi, M. Tristram, L. Vacher, B. van Tent, P. Vielva, S. Vinzl, I. K. Wehus, G. Weymann-Despres, E. J. Wollack</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Discrepancies in the characterization of half-wave plate non-idealities introduce biases in the reconstructed Cosmic Microwave Background maps, necessitating precise measurement requirements to ensure the LiteBIRD satellite mission accurately constrains the tensor-to-scalar ratio $r$. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Although half-wave plates (HWPs) are becoming a popular choice of polarization modulators for cosmic microwave background (CMB) experiments, their non-idealities can introduce systematic effects that should be carefully characterized and mitigated. One possible mitigation strategy is to incorporate information about the non-idealities at the map-making level, which helps to reduce the HWP-induced distortions of the reconstructed CMB. Nevertheless, the non-idealities can only be known with finite precision. In this paper we investigate the consequences of discrepancies between their true frequency profiles and those assumed by the map-maker. We present an end-to-end framework, including a blind component-separation step, and use it to translate these discrepancies into a bias on the tensor-to-scalar ratio, $r$, for the LiteBIRD satellite mission. We subsequently derive realistic and conservative measurement requirements for accurately characterizing the HWP non-idealities to ensure they do not compromise LiteBIRD&#39;s ambitious scientific goals. We find that the obtained results are robust against sky models with varying complexity.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.02410" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.02410" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.02410" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.04</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">18. EvoMU: Evolutionary Machine Unlearning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Pawel Batorski, Paul Swoboda</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> EvoMU employs an evolutionary search procedure to automatically synthesize novel, task-specific unlearning loss functions, achieving state-of-the-art machine unlearning performance across various datasets and demonstrating efficient automatic scientific discovery even with limited computational resources. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Machine unlearning aims to unlearn specified training data (e.g. sensitive or copyrighted material). A prominent approach is to fine-tune an existing model with an unlearning loss that retains overall utility. The space of suitable unlearning loss functions is vast, making the search for an optimal loss function daunting. Additionally, there might not even exist a universally optimal loss function: differences in the structure and overlap of the forget and retain data can cause a loss to work well in one setting but over-unlearn or under-unlearn in another. Our approach EvoMU tackles these two challenges simultaneously. An evolutionary search procedure automatically finds task-specific losses in the vast space of possible unlearning loss functions. This allows us to find dataset-specific losses that match or outperform existing losses from the literature, without the need for a human-in-the-loop. This work is therefore an instance of automatic scientific discovery, a.k.a. an AI co-scientist. In contrast to previous AI co-scientist works, we do so on a budget: We achieve SotA results using a small 4B parameter model (Qwen3-4B-Thinking), showing the potential of AI co-scientists with limited computational resources. Our experimental evaluation shows that we surpass previous loss-based unlearning formulations on TOFU-5%, TOFU-10%, MUSE and WMDP by synthesizing novel unlearning losses. Our code is available at https://github.com/Batorskq/EvoMU.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.02139" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.02139" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.02139" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">19. Physics-Informed Neural Networks for Modeling Galactic Gravitational Potentials
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Charlotte Myers, Nathaniel Starkman, Lina Necib</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Combining data-driven learning with embedded physical constraints, a physics-informed neural framework accurately models static and time-dependent galactic gravitational potentials using a neural ODE approach, achieving sub-percent reconstruction errors and enhanced dynamical consistency. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We introduce a physics-informed neural framework for modeling static and time-dependent galactic gravitational potentials. The method combines data-driven learning with embedded physical constraints to capture complex, small-scale features while preserving global physical consistency. We quantify predictive uncertainty through a Bayesian framework, and model time evolution using a neural ODE approach. Applied to mock systems of varying complexity, the model achieves reconstruction errors at the sub-percent level ($0.14\%$ mean acceleration error) and improves dynamical consistency compared to analytic baselines. This method complements existing analytic methods, enabling physics-informed baseline potentials to be combined with neural residual fields to achieve both interpretable and accurate potential models.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.01806" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.01806" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.01806" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">20. Cosmological phase transitions: from particle physics to gravitational waves, semi-analytically
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>S. Pascoli, S. Rosauro-Alcaraz, M. Zandi</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> To efficiently connect particle physics models to cosmological predictions for the stochastic gravitational wave background, a semi-analytical method is proposed for predicting the gravitational wave spectrum generated by supercooled first-order phase transitions, avoiding computationally demanding simulations. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Motivated by the recent evidence of a stochastic gravitational wave background found by pulsar timing array experiments, we focus on one of the prime cosmological explanations, i.e. a supercooled first order phase transition. If confirmed, it would offer a unique opportunity to probe early Universe dynamics and the related physics beyond the Standard Model of particles and interactions. However, the prediction of the gravitational wave spectrum from a given particle physics scenario requires theoretically and computationally demanding methods. While several tools have been put forward to reduce uncertainties and automatize these computations, we study here the possibility to perform the full pipeline of computations semi-analytically in the $4D$ theory, thus avoiding computationally intensive simulations. Our approach yields accurate results that can be used in phenomenological studies and allow for an efficient exploration of the connection between the particle physics models and their cosmological predictions.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.02829" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.02829" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.02829" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">hep-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">21. Sparsely Supervised Diffusion
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Wenshuai Zhao, Zhiyuan Li, Yi Zhao, Mohammad Hassan Vali, Martin Trapp, Joni Pajarinen, Juho Kannala, Arno Solin</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A sparsely supervised learning approach, which involves masking up to 98% of pixels during training, effectively mitigates spatial inconsistency in diffusion models, yielding competitive FID scores and enhancing global contextual information use. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Diffusion models have shown remarkable success across a wide range of generative tasks. However, they often suffer from spatially inconsistent generation, arguably due to the inherent locality of their denoising mechanisms. This can yield samples that are locally plausible but globally inconsistent. To mitigate this issue, we propose sparsely supervised learning for diffusion models, a simple yet effective masking strategy that can be implemented with only a few lines of code. Interestingly, the experiments show that it is safe to mask up to 98\% of pixels during diffusion model training. Our method delivers competitive FID scores across experiments and, most importantly, avoids training instability on small datasets. Moreover, the masking strategy reduces memorization and promotes the use of essential contextual information during generation.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.02699" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.02699" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.02699" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">22. VEQ: Modality-Adaptive Quantization for MoE Vision-Language Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Guangshuo Qin, Zhiteng Li, Zheng Chen, Weihang Zhang, Linghe Kong, Yulun Zhang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Visual Expert Quantization (VEQ) introduces a dual-aware post-training quantization framework for Mixture-of-Experts Vision-Language Models, leveraging expert activation frequency and modality affinity to manage cross-modal and expert heterogeneity, thereby achieving substantial accuracy improvements during compression. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Mixture-of-Experts(MoE) Vision-Language Models (VLMs) offer remarkable performance but incur prohibitive memory and computational costs, making compression essential. Post-Training Quantization (PTQ) is an effective training-free technique to address the massive memory and computation overhead. Existing quantization paradigms fall short as they are oblivious to two critical forms of heterogeneity: the inherent discrepancy between vision and language tokens, and the non-uniform contribution of different experts. To bridge this gap, we propose Visual Expert Quantization (VEQ), a dual-aware quantization framework designed to simultaneously accommodate cross-modal differences and heterogeneity between experts. Specifically, VEQ incorporates 1)Modality-expert-aware Quantization, which utilizes expert activation frequency to prioritize error minimization for pivotal experts, and 2)Modality-affinity-aware Quantization, which constructs an enhanced Hessian matrix by integrating token-expert affinity with modality information to guide the calibration process. Extensive experiments across diverse benchmarks verify that VEQ consistently outperforms state-of-the-art baselines. Specifically, under the W3A16 configuration, our method achieves significant average accuracy gains of 2.04\% on Kimi-VL and 3.09\% on Qwen3-VL compared to the previous SOTA quantization methods, demonstrating superior robustness across various multimodal tasks. Our code will be available at https://github.com/guangshuoqin/VEQ.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.01037" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.01037" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.01037" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">23. Rethinking Multinomial Logistic Mixture of Experts with Sigmoid Gating Function
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Tuan Minh Pham, Thinh Cao, Viet Nguyen, Huy Nguyen, Nhat Ho, Alessandro Rinaldo</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Theoretical analysis of multinomial logistic Mixture-of-Experts models reveals that a modified sigmoid gate ensures convergence and provides lower sample complexity than the softmax gate for parameter and expert estimation, while a Euclidean score replacement mitigates the exponential complexity introduced by temperature scaling. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The sigmoid gate in mixture-of-experts (MoE) models has been empirically shown to outperform the softmax gate across several tasks, ranging from approximating feed-forward networks to language modeling. Additionally, recent efforts have demonstrated that the sigmoid gate is provably more sample-efficient than its softmax counterpart under regression settings. Nevertheless, there are three notable concerns that have not been addressed in the literature, namely (i) the benefits of the sigmoid gate have not been established under classification settings; (ii) existing sigmoid-gated MoE models may not converge to their ground-truth; and (iii) the effects of a temperature parameter in the sigmoid gate remain theoretically underexplored. To tackle these open problems, we perform a comprehensive analysis of multinomial logistic MoE equipped with a modified sigmoid gate to ensure model convergence. Our results indicate that the sigmoid gate exhibits a lower sample complexity than the softmax gate for both parameter and expert estimation. Furthermore, we find that incorporating a temperature into the sigmoid gate leads to a sample complexity of exponential order due to an intrinsic interaction between the temperature and gating parameters. To overcome this issue, we propose replacing the vanilla inner product score in the gating function with a Euclidean score that effectively removes that interaction, thereby substantially improving the sample complexity to a polynomial order.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.01466" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.01466" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.01466" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">24. PLATE: Plasticity-Tunable Efficient Adapters for Geometry-Aware Continual Learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Romain Cosentino</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Exploiting the geometric redundancy inherent in pretrained network weights allows for the construction of protected update subspaces, leading to 	extsc{PLATE}, a continual learning method that efficiently adapts foundation models without old-task data while providing explicit control over the plasticity-retention trade-off. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We develop a continual learning method for pretrained models that \emph{requires no access to old-task data}, addressing a practical barrier in foundation model adaptation where pretraining distributions are often unavailable. Our key observation is that pretrained networks exhibit substantial \emph{geometric redundancy}, and that this redundancy can be exploited in two complementary ways. First, redundant neurons provide a proxy for dominant pretraining-era feature directions, enabling the construction of approximately protected update subspaces directly from pretrained weights. Second, redundancy offers a natural bias for \emph{where} to place plasticity: by restricting updates to a subset of redundant neurons and constraining the remaining degrees of freedom, we obtain update families with reduced functional drift on the old-data distribution and improved worst-case retention guarantees. These insights lead to \textsc{PLATE} (\textbf{Pla}sticity-\textbf{T}unable \textbf{E}fficient Adapters), a continual learning method requiring no past-task data that provides explicit control over the plasticity-retention trade-off. PLATE parameterizes each layer with a structured low-rank update $ΔW = B A Q^\top$, where $B$ and $Q$ are computed once from pretrained weights and kept frozen, and only $A$ is trained on the new task. The code is available at https://github.com/SalesforceAIResearch/PLATE.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.03846" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.03846" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.03846" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">25. Efficient Estimation of Kernel Surrogate Models for Task Attribution
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zhenshuo Zhang, Minxuan Duan, Hongyang R. Zhang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> To accurately quantify how individual training tasks influence target performance, kernel surrogate models are developed to effectively represent nonlinear, second-order task interactions, demonstrating superior correlation with ground-truth leave-one-out results compared to linear methods. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Modern AI agents such as large language models are trained on diverse tasks -- translation, code generation, mathematical reasoning, and text prediction -- simultaneously. A key question is to quantify how each individual training task influences performance on a target task, a problem we refer to as task attribution. The direct approach, leave-one-out retraining, measures the effect of removing each task, but is computationally infeasible at scale. An alternative approach that builds surrogate models to predict a target task&#39;s performance for any subset of training tasks has emerged in recent literature. Prior work focuses on linear surrogate models, which capture first-order relationships, but miss nonlinear interactions such as synergy, antagonism, or XOR-type effects. In this paper, we first consider a unified task weighting framework for analyzing task attribution methods, and show a new connection between linear surrogate models and influence functions through a second-order analysis. Then, we introduce kernel surrogate models, which more effectively represent second-order task interactions. To efficiently learn the kernel surrogate, we develop a gradient-based estimation procedure that leverages a first-order approximation of pretrained models; empirically, this yields accurate estimates with less than $2\%$ relative error without repeated retraining. Experiments across multiple domains -- including math reasoning in transformers, in-context learning, and multi-objective reinforcement learning -- demonstrate the effectiveness of kernel surrogate models. They achieve a $25\%$ higher correlation with the leave-one-out ground truth than linear surrogates and influence-function baselines. When used for downstream task selection, kernel surrogate models yield a $40\%$ improvement in demonstration selection for in-context learning and multi-objective reinforcement learning benchmarks.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.03783" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.03783" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.03783" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">26. A Hitchhiker&#39;s Guide to Poisson Gradient Estimation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Michael Ibrahim, Hanqi Zhao, Eli Sennesh, Zhi Li, Anqi Wu, Jacob L. Yates, Chengrui Li, Hadi Vafaii</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A modified Exponential Arrival Time (EAT) simulation method is proposed for differentiating through Poisson latent variable models, guaranteeing an unbiased first moment and demonstrating superior gradient quality and robustness compared to existing EAT and Gumbel-SoftMax relaxation techniques. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Poisson-distributed latent variable models are widely used in computational neuroscience, but differentiating through discrete stochastic samples remains challenging. Two approaches address this: Exponential Arrival Time (EAT) simulation and Gumbel-SoftMax (GSM) relaxation. We provide the first systematic comparison of these methods, along with practical guidance for practitioners. Our main technical contribution is a modification to the EAT method that theoretically guarantees an unbiased first moment (exactly matching the firing rate), and reduces second-moment bias. We evaluate these methods on their distributional fidelity, gradient quality, and performance on two tasks: (1) variational autoencoders with Poisson latents, and (2) partially observable generalized linear models, where latent neural connectivity must be inferred from observed spike trains. Across all metrics, our modified EAT method exhibits better overall performance (often comparable to exact gradients), and substantially higher robustness to hyperparameter choices. Together, our results clarify the trade-offs between these methods and offer concrete recommendations for practitioners working with Poisson latent variable models.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.03896" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.03896" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.03896" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">27. TruKAN: Towards More Efficient Kolmogorov-Arnold Networks Using Truncated Power Functions
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Ali Bayeh, Samira Sadaoui, Malek Mouhoub</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The TruKAN architecture improves upon Kolmogorov-Arnold Networks by substituting B-spline bases with truncated power functions derived from spline theory, resulting in enhanced interpretability, superior accuracy, and greater computational efficiency across complex vision tasks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">To address the trade-off between computational efficiency and adherence to Kolmogorov-Arnold Network (KAN) principles, we propose TruKAN, a new architecture based on the KAN structure and learnable activation functions. TruKAN replaces the B-spline basis in KAN with a family of truncated power functions derived from k-order spline theory. This change maintains the KAN&#39;s expressiveness while enhancing accuracy and training time. Each TruKAN layer combines a truncated power term with a polynomial term and employs either shared or individual knots. TruKAN exhibits greater interpretability than other KAN variants due to its simplified basis functions and knot configurations. By prioritizing interpretable basis functions, TruKAN aims to balance approximation efficacy with transparency. We develop the TruKAN model and integrate it into an advanced EfficientNet-V2-based framework, which is then evaluated on computer vision benchmark datasets. To ensure a fair comparison, we develop various models: MLP-, KAN-, SineKAN and TruKAN-based EfficientNet frameworks and assess their training time and accuracy across small and deep architectures. The training phase uses hybrid optimization to improve convergence stability. Additionally, we investigate layer normalization techniques for all the models and assess the impact of shared versus individual knots in TruKAN. Overall, TruKAN outperforms other KAN models in terms of accuracy, computational efficiency and memory usage on the complex vision task, demonstrating advantages beyond the limited settings explored in prior KAN studies.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.03879" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.03879" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.03879" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">28. Constraining cosmological simulations with peculiar velocities: a forward-modeling approach
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Aurélien Valade, Noam Libeskind, Daniel Pomarède, Richard Stiskalek, Yehuda Hoffman, Stefan Gottlöber, R. Brent Tully</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Field-level forward modeling of the early-time density field, implemented in the Hamlet-PM method using late-time peculiar velocity data, successfully constrains the initial conditions of cosmological simulations, allowing for direct spatial comparison with the observed Local Universe. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Numerical simulations are a key tool to decipher the dynamics of gravitation. Yet, they fail to spatially reproduce the Universe we observe, limiting comparison between observations and simulations to a statistical level. This is highly problematic for rare, faint or well studied nearby objects that are observed in a single environment. The computational cost of recovering this environment in random simulations is prohibitive. We present Hamlet-PM, a method that enables the constraining of initial conditions for cosmological simulations so as to produce evolved numerical universes that can be directly compared to observations of the Local Universe: constrained simulations. Our method implements the field-level forward modeling of the early-time density field from sparse and noisy measurements of late-time peculiar velocities. The dynamics are integrated with a particle-mesh gravity solver, thus probing the mildly non-linear regime. The code is applied to the Cosmicflows-4 compilation of peculiar velocities up to z &lt; 0.05 (160 Mpc/h). The constrained ICs a re-simulated with a high precision N-body code. A series of one hundred dark-matter only cosmological constrained simulations with a resolution of 512^3 particles in a 500^3 [Mpc/h]3 box is presented. Special attention is given to twelve prominent nearby galaxy clusters, whose simulated counterparts are matched on criteria of mass and separation. We provide a mass estimate constrained by the dynamical environment for each cluster. Field-level forward modeling of the initial conditions produces highly constrained cosmological simulations. Currently, this method already overtakes in quality the pipeline in use in the peculiar-velocity community, although systematic biases still need to be addressed. Furthermore, improving the model is easy thanks to the inherent flexibility of the Bayesian approach.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.03699" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.03699" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.03699" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">29. IntraSlice: Towards High-Performance Structural Pruning with Block-Intra PCA for LLMs
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Meng Li, Peisong Wang, Yuantian Shao, Qinghao Hu, Hongjian Fang, Yifan Zhang, Zhihui Wei, Jian Cheng</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The IntraSlice framework achieves superior structured pruning for large language models by implementing block-wise module-intra PCA compression, which allows transformation matrices to be fully fused into the model without additional parameters, guided by a novel PCA-based global pruning ratio estimator. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Large Language Models (LLMs) achieve strong performance across diverse tasks but face deployment challenges due to their massive size. Structured pruning offers acceleration benefits but leads to significant performance degradation. Recent PCA-based pruning methods have alleviated this issue by retaining key activation components, but are only applied between modules in order to fuse the transformation matrix, which introduces extra parameters and severely disrupts activation distributions due to residual connections. To address these issues, we propose IntraSlice, a framework that applies block-wise module-intra PCA compression pruning. By leveraging the structural characteristics of Transformer modules, we design an approximate PCA method whose transformation matrices can be fully fused into the model without additional parameters. We also introduce a PCA-based global pruning ratio estimator that further considers the distribution of compressed activations, building on conventional module importance. We validate our method on Llama2, Llama3, and Phi series across various language benchmarks. Experimental results demonstrate that our approach achieves superior compression performance compared to recent baselines at the same compression ratio or inference speed.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.01975" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.01975" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.01975" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">30. Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Peter Holderrieth, Douglas Chen, Luca Eyring, Ishin Shah, Giri Anantharaman, Yutong He, Zeynep Akata, Tommi Jaakkola, Nicholas Matthew Boffi, Max Simchowitz</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Stochastic flow map models called Diamond Maps are introduced to enable efficient and accurate reward alignment for generative models by amortizing multiple simulation steps into a single-step sampler while preserving the necessary stochasticity for optimal alignment. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Flow and diffusion models produce high-quality samples, but adapting them to user preferences or constraints post-training remains costly and brittle, a challenge commonly called reward alignment. We argue that efficient reward alignment should be a property of the generative model itself, not an afterthought, and redesign the model for adaptability. We propose &#34;Diamond Maps&#34;, stochastic flow map models that enable efficient and accurate alignment to arbitrary rewards at inference time. Diamond Maps amortize many simulation steps into a single-step sampler, like flow maps, while preserving the stochasticity required for optimal reward alignment. This design makes search, sequential Monte Carlo, and guidance scalable by enabling efficient and consistent estimation of the value function. Our experiments show that Diamond Maps can be learned efficiently via distillation from GLASS Flows, achieve stronger reward alignment performance, and scale better than existing methods. Our results point toward a practical route to generative models that can be rapidly adapted to arbitrary preferences and constraints at inference time.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.05993" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.05993" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.05993" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">31. Model Specific Task Similarity for Vision Language Model Selection via Layer Conductance
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Wei Yang, Hong Xie, Tao Tan, Xin Li, Defu Lian, Enhong Chen</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A new asymmetric metric, Directional Conductance Divergence (DCD), leverages the internal functional dynamics of the visual encoder via layer-wise conductance to efficiently predict the optimal ranking of Vision-Language Models for specific downstream tasks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">While open sourced Vision-Language Models (VLMs) have proliferated, selecting the optimal pretrained model for a specific downstream task remains challenging. Exhaustive evaluation is often infeasible due to computational constraints and data limitations in few shot scenarios. Existing selection methods fail to fully address this: they either rely on data-intensive proxies or use symmetric textual descriptors that neglect the inherently directional and model-specific nature of transferability. To address this problem, we propose a framework that grounds model selection in the internal functional dynamics of the visual encoder. Our approach represents each task via layer wise conductance and derives a target-conditioned block importance distribution through entropy regularized alignment. Building on this, we introduce Directional Conductance Divergence (DCD), an asymmetric metric that quantifies how effectively a source task covers the target&#39;s salient functional blocks. This allows for predicting target model rankings by aggregating source task ranks without direct inference. Experimental results on 48 VLMs across 21 datasets demonstrate that our method outperforms state-of-the-art baselines, achieving a 14.7% improvement in NDCG@5 over SWAB.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.01346" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.01346" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.01346" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">32. Negentropy as Diagnostic of Cosmic Density Fields and Dynamical Dark Energy Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Suman Sarkar</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Employing negentropy as an information-theoretic measure of non-Gaussianity in the cosmic density field, researchers established diagnostics that effectively discriminate between various dynamical dark energy models, including thawing, freezing, and phantom scenarios, based on characteristic redshift evolution. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We employ negentropy ($J$), defined as the difference between the information content of a non-Gaussian probability distribution and a Gaussian with identical variance, as an information-theoretic probe of non-Gaussianity in the cosmic density field. We quantify its sensitivity to dynamical dark energy by studying the evolution of $J(a)$ and its derivatives $Γ_1(a)$ and $Γ_2(a)$ across three parameterisation schemes: CPL, JBP, and BA. We determine the characteristic redshift $z_{NG}$, marking the epoch of maximal non-Gaussian structure formation, and the turnaround redshift $z_{TA}$, when information production transitions due to dark-energy domination, finding $z_{NG}\sim0.81$ and $z_{TA}\sim0.18$ for $Λ$CDM. Our diagnostics clearly discriminate between thawing and freezing quintessence models and phantom dark energy at low redshifts. Thawing models show small departures from $Λ$CDM, freezing models display higher $z_{TA}$, while phantom models exhibit lower $z_{TA}$, reflecting late-time evolution. We provide a practical prescription for measuring negentropy from discrete galaxy distributions, establishing a framework that can be applied to simulations and observations. This information-theoretic approach offers a robust and complementary tool for probing dark energy dynamics, enabling sensitive discrimination between evolving and cosmological-constant scenarios.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.02339" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.02339" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.02339" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">33. Provable Target Sample Complexity Improvements as Pre-Trained Models Scale
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Kazuto Fukuchi, Ryuichiro Hataya, Kota Matsui</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A novel theoretical framework called &#34;caulking,&#34; inspired by parameter-efficient fine-tuning, provides the first theoretical justification for empirical scaling laws by proving that enhanced pre-trained models inherently reduce the sample complexity needed for subsequent downstream learning tasks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Pre-trained models have become indispensable for efficiently building models across a broad spectrum of downstream tasks. The advantages of pre-trained models have been highlighted by empirical studies on scaling laws, which demonstrate that larger pre-trained models can significantly reduce the sample complexity of downstream learning. However, existing theoretical investigations of pre-trained models lack the capability to explain this phenomenon. In this paper, we provide a theoretical investigation by introducing a novel framework, caulking, inspired by parameter-efficient fine-tuning (PEFT) methods such as adapter-based fine-tuning, low-rank adaptation, and partial fine-tuning. Our analysis establishes that improved pre-trained models provably decrease the sample complexity of downstream tasks, thereby offering theoretical justification for the empirically observed scaling laws relating pre-trained model size to downstream performance, a relationship not covered by existing results.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.04233" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.04233" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.04233" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">34. PromptSplit: Revealing Prompt-Level Disagreement in Generative Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Mehdi Lotfian, Mohammad Jalali, Farzan Farnia</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> PromptSplit, a kernel-based framework utilizing tensor-product embeddings and covariance matrix eigenspace analysis, accurately detects and isolates the specific textual prompts responsible for behavioral disagreement between pairs of generative AI models. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Prompt-guided generative AI models have rapidly expanded across vision and language domains, producing realistic and diverse outputs from textual inputs. The growing variety of such models, trained with different data and architectures, calls for principled methods to identify which types of prompts lead to distinct model behaviors. In this work, we propose PromptSplit, a kernel-based framework for detecting and analyzing prompt-dependent disagreement between generative models. For each compared model pair, PromptSplit constructs a joint prompt--output representation by forming tensor-product embeddings of the prompt and image (or text) features, and then computes the corresponding kernel covariance matrix. We utilize the eigenspace of the weighted difference between these matrices to identify the main directions of behavioral difference across prompts. To ensure scalability, we employ a random-projection approximation that reduces computational complexity to $O(nr^2 + r^3)$ for projection dimension $r$. We further provide a theoretical analysis showing that this approximation yields an eigenstructure estimate whose expected deviation from the full-dimensional result is bounded by $O(1/r^2)$. Experiments across text-to-image, text-to-text, and image-captioning settings demonstrate that PromptSplit accurately detects ground-truth behavioral differences and isolates the prompts responsible, offering an interpretable tool for detecting where generative models disagree.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.04009" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.04009" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.04009" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">35. Contribution-aware Token Compression for Efficient Video Understanding via Reinforcement Learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yinchao Ma, Qiang Zhou, Zhibin Wang, Xianing Chen, Hanqing Yang, Jun Song, Bo Zheng</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The Contribution-aware token Compression for VIDeo understanding (CaCoVID) algorithm utilizes a reinforcement learning framework to optimize token selection policies based on the explicit contribution of tokens to correct predictions, thereby overcoming the limitations of attention-score based compression in VLLMs. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Video large language models have demonstrated remarkable capabilities in video understanding tasks. However, the redundancy of video tokens introduces significant computational overhead during inference, limiting their practical deployment. Many compression algorithms are proposed to prioritize retaining features with the highest attention scores to minimize perturbations in attention computations. However, the correlation between attention scores and their actual contribution to correct answers remains ambiguous. To address the above limitation, we propose a novel \textbf{C}ontribution-\textbf{a}ware token \textbf{Co}mpression algorithm for \textbf{VID}eo understanding (\textbf{CaCoVID}) that explicitly optimizes the token selection policy based on the contribution of tokens to correct predictions. First, we introduce a reinforcement learning-based framework that optimizes a policy network to select video token combinations with the greatest contribution to correct predictions. This paradigm shifts the focus from passive token preservation to active discovery of optimal compressed token combinations. Secondly, we propose a combinatorial policy optimization algorithm with online combination space sampling, which dramatically reduces the exploration space for video token combinations and accelerates the convergence speed of policy optimization. Extensive experiments on diverse video understanding benchmarks demonstrate the effectiveness of CaCoVID. Codes will be released.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.01649" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.01649" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.01649" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">36. Limitations of SGD for Multi-Index Models Beyond Statistical Queries
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Daniel Barzilai, Ohad Shamir</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A new theoretical framework, independent of the Statistical Queries (SQ) approach, is developed to rigorously analyze the fundamental performance limitations of standard vanilla Stochastic Gradient Descent (SGD) when applied to single-index and multi-index models, including deep neural networks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Understanding the limitations of gradient methods, and stochastic gradient descent (SGD) in particular, is a central challenge in learning theory. To that end, a commonly used tool is the Statistical Queries (SQ) framework, which studies performance limits of algorithms based on noisy interaction with the data. However, it is known that the formal connection between the SQ framework and SGD is tenuous: Existing results typically rely on adversarial or specially-structured gradient noise that does not reflect the noise in standard SGD, and (as we point out here) can sometimes lead to incorrect predictions. Moreover, many analyses of SGD for challenging problems rely on non-trivial algorithmic modifications, such as restricting the SGD trajectory to the sphere or using very small learning rates. To address these shortcomings, we develop a new, non-SQ framework to study the limitations of standard vanilla SGD, for single-index and multi-index models (namely, when the target function depends on a low-dimensional projection of the inputs). Our results apply to a broad class of settings and architectures, including (potentially deep) neural networks.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.05704" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.05704" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.05704" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">37. Probing The Dark Matter Halo of High-redshift Quasar from Wide-Field Clustering Analysis
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Hao Meng, Huanian Zhang, Guangping Ye</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Wide-field clustering analysis of over 200,000 high-redshift quasar candidates yielded a dark matter halo mass estimate of $\log(M_h/M_{\odot}) \approx 12.2$, indicating a potentially more complex and non-monotonic evolutionary path for quasar host halos than previously suggested by lower mass measurements. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">High-redshift quasars have been an excellent tracer to study the astrophysics and cosmology at early Universe. Using 216,949 high-redshift quasar candidates ($5.0 \leq z &lt; 6.3$) selected via machine learning from the Legacy Survey Data Release 9 and the Wide-field Infrared Survey Explorer, we perform wide-field clustering analysis to investigate the large-scale environment of those high-redshift quasars. We construct the projected auto correlation function of those high-redshift quasars that is weighted by its predicted probability of being a true high-redshift quasar, from which we derive the bias parameter and the typical dark matter halo mass of those quasars. The dark matter halo mass of quasars estimated from the projected auto correlation function is $\log(M_h/M_{\odot})=12.2 ^{+0.2}_{-0.7}$ ($11.9^{+0.3}_{-0.7}$), with the bias parameter $b$ of $12.34 ^{+4.26}_{-4.37}$ ($11.52^{+4.02}_{-4.14}$) for the redshift interval of $5.0 \leq z &lt;5.7$ ($5.7 \leq z &lt;6.3$). Our results, combined with other measurements of dark matter halo masses for quasars or active galactic nucleus which obtain a lower dark matter halo mass of $\sim 10^{11.5}$ M$_\odot$ at similar redshift, suggest a more complex, and possibly non-monotonic evolution of quasar hosting dark matter halo. Moreover, we estimate the duty cycle of those quasars, which is $0.008^{+0.135}_{-0.007}$ ($0.003+^{+0.047}_{-0.003}$) for the redshift interval of $5.0 \leq z &lt;5.7$ ($5.7 \leq z &lt;6.3$).</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.02778" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.02778" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.02778" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmic Shear, Cosmological Constraints / Gravitational Lensing, Galaxy Clustering, Halo Occupation</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">38. Factorized neural posterior estimation for rapid and reliable inference of parameterized post-Einsteinian deviation parameters in gravitational waves
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yong-Xin Zhang, Tian-Yang Sun, Chun-Yu Xiong, Song-Tao Liu, Yu-Xin Wang, Shang-Jie Jin, Jing-Fei Zhang, Xin Zhang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A factorized neural posterior estimation framework, leveraging normalizing flows and a hybrid neural network, enables statistically reliable, millisecond-scale inference of nine parameterized post-Einsteinian deviation parameters, providing a viable solution for real-time General Relativity tests using next-generation gravitational wave detectors. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The direct detection of gravitational waves (GWs) by LIGO has strikingly confirmed general relativity (GR), but testing GR via GWs requires estimating parameterized post-Einsteinian (ppE) deviation parameters in waveform models. Traditional Bayesian inference methods like Markov chain Monte Carlo (MCMC) provide reliable estimates but suffer from prohibitive computational costs, failing to meet the real-time demands and surging data volume of future GW detectors. Here, we propose a factorized neural posterior estimation framework: we construct independent normalizing flow models for each of the nine ppE deviation parameters and effectively integrate prior information from other source parameters via a conditional embedding network. Leveraging a hybrid neural network with a convolutional neural network and a Residual Neural Network for feature extraction, our method performs rapid and statistically reliable posterior inference directly from binary black hole signals. Compared to conventional MCMC, our approach achieves millisecond-scale inference time with a speedup factor of $9 \times 10^4$. Comprehensive validations show that the posterior estimates pass the Kolmogorov-Smirnov test and achieve empirical coverage probabilities close to theoretical targets. This work demonstrates the great potential of deep learning for GW parameter estimation and provides a viable technical solution for real-time GR tests with next-generation detectors.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.01319" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.01319" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.01319" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.IM</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">39. Lotus: Efficient LLM Training by Randomized Low-Rank Gradient Projection with Adaptive Subspace Switching
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Tianhao Miao, Zhongyuan Bao, Lejun Zhang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The Lotus method resolves the efficiency trade-off in low-rank gradient training by modifying the projection process based on a unit gradient displacement criterion, resulting in simultaneous significant reductions in both training time and memory consumption compared to prior methods like GaLore. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Training efficiency in large-scale models is typically assessed through memory consumption, training time, and model performance. Current methods often exhibit trade-offs among these metrics, as optimizing one generally degrades at least one of the others. Addressing this trade-off remains a central challenge in algorithm design. While GaLore enables memory-efficient training by updating gradients in a low-rank subspace, it incurs a comparable extra training time cost due to the Singular Value Decomposition(SVD) process on gradients. In this paper, we propose Lotus, a method that resolves this trade-off by simply modifying the projection process. We propose a criterion that quantifies the displacement of the unit gradient to enable efficient transitions between low-rank gradient subspaces. Experimental results indicate that Lotus is the most efficient method, achieving a 30% reduction in training time and a 40% decrease in memory consumption for gradient and optimizer states. Additionally, it outperforms the baseline method in both pre-training and fine-tuning tasks.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.01233" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.01233" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.01233" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">40. Muon in Associative Memory Learning: Training Dynamics and Scaling Laws
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Binghui Li, Kaifei Wang, Han Zhong, Pinyan Lu, Liwei Wang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Theoretical analysis of the Muon optimizer in a linear associative memory model demonstrates that it achieves exponential speedup and superior scaling efficiency over Gradient Descent by mitigating the imbalanced learning rates of frequency components, acting effectively as an implicit matrix preconditioner. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Muon updates matrix parameters via the matrix sign of the gradient and has shown strong empirical gains, yet its dynamics and scaling behavior remain unclear in theory. We study Muon in a linear associative memory model with softmax retrieval and a hierarchical frequency spectrum over query-answer pairs, with and without label noise. In this setting, we show that Gradient Descent (GD) learns frequency components at highly imbalanced rates, leading to slow convergence bottlenecked by low-frequency components. In contrast, the Muon optimizer mitigates this imbalance, leading to faster and more uniform progress. Specifically, in the noiseless case, Muon achieves an exponential speedup over GD; in the noisy case with a power-decay frequency spectrum, we derive Muon&#39;s optimization scaling law and demonstrate its superior scaling efficiency over GD. Furthermore, we show that Muon can be interpreted as an implicit matrix preconditioner arising from adaptive task alignment and block-symmetric gradient structure. In contrast, the preconditioner with coordinate-wise sign operator could match Muon under oracle access to unknown task representations, which is infeasible for SignGD in practice. Experiments on synthetic long-tail classification and LLaMA-style pre-training corroborate the theory.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.05725" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.05725" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.05725" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">41. CoSA: Compressed Sensing-Based Adaptation of Large Language Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Songtao Wei, Yi Li, Bohan Zhang, Zhichun Guo, Ying Huang, Yuede Ji, Miao Yin, Guanpeng Li, Bingzhe Li</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Compressed Sensing-Based Adaptation (CoSA) is introduced as a novel parameter-efficient fine-tuning method that leverages random projection matrices and a compact core to enable expressive, multi-scale model adaptation, surpassing state-of-the-art low-rank techniques. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Parameter-Efficient Fine-Tuning (PEFT) has emerged as a practical paradigm for adapting large language models (LLMs) without updating all parameters. Most existing approaches, such as LoRA and PiSSA, rely on low-rank decompositions of weight updates. However, the low-rank assumption may restrict expressivity, particularly in task-specific adaptation scenarios where singular values are distributed relatively uniformly. To address this limitation, we propose CoSA (Compressed Sensing-Based Adaptation), a new PEFT method extended from compressed sensing theory. Instead of constraining weight updates to a low-rank subspace, CoSA expresses them through fixed random projection matrices and a compact learnable core. We provide a formal theoretical analysis of CoSA as a synthesis process, proving that weight updates can be compactly encoded into a low-dimensional space and mapped back through random projections. Extensive experimental results show that CoSA provides a principled perspective for efficient and expressive multi-scale model adaptation. Specifically, we evaluate CoSA on 10 diverse tasks, including natural language understanding and generation, employing 5 models of different scales from RoBERTa, Llama, and Qwen families. Across these settings, CoSA consistently matches or outperforms state-of-the-art PEFT methods.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.05148" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.05148" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.05148" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">42. Inverse Depth Scaling From Most Layers Being Similar
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yizhou Liu, Sara Kangaslahti, Ziming Liu, Jeff Gore</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Analysis of large language models reveals that loss scales inversely with depth, suggesting that residual layers primarily function through inefficient ensemble averaging rather than compositional learning, indicating a need for architectural innovation. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Neural scaling laws relate loss to model size in large language models (LLMs), yet depth and width may contribute to performance differently, requiring more detailed studies. Here, we quantify how depth affects loss via analysis of LLMs and toy residual networks. We find loss scales inversely proportional to depth in LLMs, probably due to functionally similar layers reducing error through ensemble averaging rather than compositional learning or discretizing smooth dynamics. This regime is inefficient yet robust and may arise from the architectural bias of residual networks and target functions incompatible with smooth dynamics. The findings suggest that improving LLM efficiency may require architectural innovations to encourage compositional use of depth.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.05970" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.05970" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.05970" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">43. When Shared Knowledge Hurts: Spectral Over-Accumulation in Model Merging
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yayuan Li, Ze Peng, Jian Zhang, Jintao Guo, Yue Duan, Yinghuan Shi</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Singular Value Calibration (SVC) is proposed as a training-free method for model merging that mitigates the over-counting of shared knowledge by quantifying subspace overlap and rescaling inflated singular values, leading to state-of-the-art performance across vision and language benchmarks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Model merging combines multiple fine-tuned models into a single model by adding their weight updates, providing a lightweight alternative to retraining. Existing methods primarily target resolving conflicts between task updates, leaving the failure mode of over-counting shared knowledge unaddressed. We show that when tasks share aligned spectral directions (i.e., overlapping singular vectors), a simple linear combination repeatedly accumulates these directions, inflating the singular values and biasing the merged model toward shared subspaces. To mitigate this issue, we propose Singular Value Calibration (SVC), a training-free and data-free post-processing method that quantifies subspace overlap and rescales inflated singular values to restore a balanced spectrum. Across vision and language benchmarks, SVC consistently improves strong merging baselines and achieves state-of-the-art performance. Furthermore, by modifying only the singular values, SVC improves the performance of Task Arithmetic by 13.0%. Code is available at: https://github.com/lyymuwu/SVC.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.05536" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.05536" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.05536" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">44. Quantized Evolution Strategies: High-precision Fine-tuning of Quantized LLMs at Low-precision Cost
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yinggan Xu, Risto Miikkulainen, Xin Qiu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Quantized Evolution Strategies (QES) enable full-parameter fine-tuning of non-differentiable, post-training quantized models by utilizing accumulated error feedback and stateless seed replay, significantly outperforming existing zeroth-order methods for arithmetic reasoning. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Post-Training Quantization (PTQ) is essential for deploying Large Language Models (LLMs) on memory-constrained devices, yet it renders models static and difficult to fine-tune. Standard fine-tuning paradigms, including Reinforcement Learning (RL), fundamentally rely on backpropagation and high-precision weights to compute gradients. Thus they cannot be used on quantized models, where the parameter space is discrete and non-differentiable. While Evolution Strategies (ES) offer a backpropagation-free alternative, optimization of the quantized parameters can still fail due to vanishing or inaccurate gradient. This paper introduces Quantized Evolution Strategies (QES), an optimization paradigm that performs full-parameter fine-tuning directly in the quantized space. QES is based on two innovations: (1) it integrates accumulated error feedback to preserve high-precision gradient signals, and (2) it utilizes a stateless seed replay to reduce memory usage to low-precision inference levels. QES significantly outperforms the state-of-the-art zeroth-order fine-tuning method on arithmetic reasoning tasks, making direct fine-tuning for quantized models possible. It therefore opens up the possibility for scaling up LLMs entirely in the quantized space. The source code is available at https://github.com/dibbla/Quantized-Evolution-Strategies .</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.03120" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.03120" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.03120" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">45. Universal One-third Time Scaling in Learning Peaked Distributions
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yizhou Liu, Ziming Liu, Cengiz Pehlevan, Jeff Gore</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The intrinsic properties of softmax and cross-entropy, when applied to peaked probability distributions, create a fundamental optimization bottleneck characterized by power-law vanishing losses and gradients, providing a mechanistic explanation for the universal 1/3 exponent observed in LLM scaling laws. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Training large language models (LLMs) is computationally expensive, partly because the loss exhibits slow power-law convergence whose origin remains debatable. Through systematic analysis of toy models and empirical evaluation of LLMs, we show that this behavior can arise intrinsically from the use of softmax and cross-entropy. When learning peaked probability distributions, e.g., next-token distributions, these components yield power-law vanishing losses and gradients, creating a fundamental optimization bottleneck. This ultimately leads to power-law time scaling of the loss with a universal exponent of $1/3$. Our results provide a mechanistic explanation for observed neural scaling and suggest new directions for improving LLM training efficiency.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.03685" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.03685" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.03685" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">46. Orthogonal Self-Attention
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Leo Zhang, James Martens</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Orthogonal Self-Attention (OSA) is introduced to stabilize skipless Transformer architectures by enforcing orthogonality on the attention matrix using a matrix exponential parametrization, achieving linear complexity and ensuring well-conditioned Jacobians. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Softmax Self-Attention (SSA) is a key component of Transformer architectures. However, when utilised within skipless architectures, which aim to improve representation learning, recent work has highlighted the inherent instability of SSA due to inducing rank collapse and poorly-conditioned Jacobians. In this work, we design a novel attention mechanism: Orthogonal Self-Attention (OSA), which aims to bypass these issues with SSA, in order to allow for (non-causal) Transformers without skip connections and normalisation layers to be more easily trained. In particular, OSA parametrises the attention matrix to be orthogonal via mapping a skew-symmetric matrix, formed from query-key values, through the matrix exponential. We show that this can be practically implemented, by exploiting the low-rank structure of our query-key values, resulting in the computational complexity and memory cost of OSA scaling linearly with sequence length. Furthermore, we derive an initialisation scheme for which we prove ensures that the Jacobian of OSA is well-conditioned.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.05996" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.05996" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.05996" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">47. Structuring Value Representations via Geometric Coherence in Markov Decision Processes
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zuyuan Zhang, Zeyu Fang, Tian Lan</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Geometric Coherence Regularized Reinforcement Learning (GCR-RL) leverages order theory to stabilize RL by learning value functions as a sequence of geometrically coherent super-poset refinements, resulting in improved sample efficiency and stable performance. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Geometric properties can be leveraged to stabilize and speed reinforcement learning. Existing examples include encoding symmetry structure, geometry-aware data augmentation, and enforcing structural restrictions. In this paper, we take a novel view of RL through the lens of order theory and recast value function estimates into learning a desired poset (partially ordered set). We propose \emph{GCR-RL} (Geometric Coherence Regularized Reinforcement Learning) that computes a sequence of super-poset refinements -- by refining posets in previous steps and learning additional order relationships from temporal difference signals -- thus ensuring geometric coherence across the sequence of posets underpinning the learned value functions. Two novel algorithms by Q-learning and by actor--critic are developed to efficiently realize these super-poset refinements. Their theoretical properties and convergence rates are analyzed. We empirically evaluate GCR-RL in a range of tasks and demonstrate significant improvements in sample efficiency and stable performance over strong baselines.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.02978" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.02978" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.02978" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">48. When pre-training hurts LoRA fine-tuning: a dynamical analysis via single-index models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Gibbs Nwemadji, Bruno Loureiro, Jean Barbier</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Theoretical analysis of LoRA fine-tuning dynamics shows that strong pre-training can paradoxically hinder convergence by inducing a prolonged search phase, even when tasks are aligned, thereby slowing down optimization on downstream problems. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Pre-training on a source task is usually expected to facilitate fine-tuning on similar downstream problems. In this work, we mathematically show that this naive intuition is not always true: excessive pre-training can computationally slow down fine-tuning optimization. We study this phenomenon for low-rank adaptation (LoRA) fine-tuning on single-index models trained under one-pass SGD. Leveraging a summary statistics description of the fine-tuning dynamics, we precisely characterize how the convergence rate depends on the initial fine-tuning alignment and the degree of non-linearity of the target task. The key take away is that even when the pre-training and down- stream tasks are well aligned, strong pre-training can induce a prolonged search phase and hinder convergence. Our theory thus provides a unified picture of how pre-training strength and task difficulty jointly shape the dynamics and limitations of LoRA fine-tuning in a nontrivial tractable model.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.02855" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.02855" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.02855" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">49. Discrete diffusion samplers and bridges: Off-policy algorithms and applications in latent spaces
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Arran Carter, Sanghyeok Choi, Kirill Tamogashev, Víctor Elvira, Nikolay Malkin</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Off-policy training techniques are introduced to enhance discrete diffusion samplers, generalizing them to the data-to-energy Schrödinger bridge problem in the discrete domain and enabling efficient data-free posterior sampling in latent spaces. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Sampling from a distribution $p(x) \propto e^{-\mathcal{E}(x)}$ known up to a normalising constant is an important and challenging problem in statistics. Recent years have seen the rise of a new family of amortised sampling algorithms, commonly referred to as diffusion samplers, that enable fast and efficient sampling from an unnormalised density. Such algorithms have been widely studied for continuous-space sampling tasks; however, their application to problems in discrete space remains largely unexplored. Although some progress has been made in this area, discrete diffusion samplers do not take full advantage of ideas commonly used for continuous-space sampling. In this paper, we propose to bridge this gap by introducing off-policy training techniques for discrete diffusion samplers. We show that these techniques improve the performance of discrete samplers on both established and new synthetic benchmarks. Next, we generalise discrete diffusion samplers to the task of bridging between two arbitrary distributions, introducing data-to-energy Schrödinger bridge training for the discrete domain for the first time. Lastly, we showcase the application of the proposed diffusion samplers to data-free posterior sampling in the discrete latent spaces of image generative models.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.05961" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.05961" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.05961" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">50. FlyPrompt: Brain-Inspired Random-Expanded Routing with Temporal-Ensemble Experts for General Continual Learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Hongwei Yan, Guanglong Sun, Kanglei Zhou, Qian Li, Liyuan Wang, Yi Zhong</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> FlyPrompt, a brain-inspired framework for general continual learning, utilizes a randomly expanded analytic router for expert allocation and a temporal ensemble of output heads to dynamically adapt to non-stationary data streams, significantly improving performance over existing continual parameter-efficient tuning methods. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">General continual learning (GCL) challenges intelligent systems to learn from single-pass, non-stationary data streams without clear task boundaries. While recent advances in continual parameter-efficient tuning (PET) of pretrained models show promise, they typically rely on multiple training epochs and explicit task cues, limiting their effectiveness in GCL scenarios. Moreover, existing methods often lack targeted design and fail to address two fundamental challenges in continual PET: how to allocate expert parameters to evolving data distributions, and how to improve their representational capacity under limited supervision. Inspired by the fruit fly&#39;s hierarchical memory system characterized by sparse expansion and modular ensembles, we propose FlyPrompt, a brain-inspired framework that decomposes GCL into two subproblems: expert routing and expert competence improvement. FlyPrompt introduces a randomly expanded analytic router for instance-level expert activation and a temporal ensemble of output heads to dynamically adapt decision boundaries over time. Extensive theoretical and empirical evaluations demonstrate FlyPrompt&#39;s superior performance, achieving up to 11.23%, 12.43%, and 7.62% gains over state-of-the-art baselines on CIFAR-100, ImageNet-R, and CUB-200, respectively. Our source code is available at https://github.com/AnAppleCore/FlyGCL.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2602.01976" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2602.01976" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2602.01976" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Analysis / Neural Architectures, Representation Learning, Advanced Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
    </div>

    <footer class="footer">
        &copy; 2026 arXiv Daily · Powered by <a href="https://arxiv.org/" target="_blank">arXiv</a>
    </footer>
</body>
</html>