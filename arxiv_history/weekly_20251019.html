<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>arXiv Daily: Backfill (2025-10-12 to 2025-10-19)</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto:400,700&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Roboto', Arial, sans-serif; background: #f8f9fa; transition: background 0.3s; }
        .arxiv-card { margin: 2em auto; max-width: 900px; box-shadow: 0 2px 8px rgba(0,0,0,0.08); border-radius: 16px; transition: box-shadow 0.3s, transform 0.3s; }
        .arxiv-card:hover { box-shadow: 0 4px 12px rgba(0,0,0,0.1); transform: translateY(-1px); }
        .arxiv-title { background: linear-gradient(90deg, #0d6efd 60%, #6610f2 100%); color: #fff; border-radius: 16px 16px 0 0; padding: 1.5em; font-size: 1.5em; font-weight: 700; letter-spacing: 0.5px;}
        .arxiv-meta { font-size: 1em; color: #555; margin-bottom: 1em; }
        .arxiv-abstract { background: #fff; padding: 1.5em; border-radius: 0 0 16px 16px; font-size: 1.1em; line-height: 1.7;}
        .arxiv-links a { margin-right: 1em; }
        .arxiv-scores { margin-top: 1em; font-size: 1em; }
        .navbar { background: #fff; box-shadow: 0 2px 8px rgba(0,0,0,0.04);}
        .footer { text-align: center; color: #888; padding: 2em 0 1em 0; font-size: 0.95em;}
        @media (prefers-color-scheme: dark) {
            body { background: #181a1b; color: #e4e6eb; }
            .arxiv-card { background: #23272b; box-shadow: 0 2px 8px rgba(0,0,0,0.28);}
            .arxiv-card:hover { box-shadow: 0 5px 15px rgba(0,0,0,0.25); transform: translateY(-1px); }
            .arxiv-title { background: linear-gradient(90deg, #375a7f 60%, #6f42c1 100%);}
            .arxiv-abstract { background: #23272b; color: #e4e6eb;}
            .navbar { background: #23272b; color: #e4e6eb;}
            .text-muted { color: #adb5bd !important; }
        }
        /* Improved abstract readability */
        .arxiv-abstract-text {
            font-size: 1.1em;
            line-height: 1.7;
        }
        p.big {
            line-height: 1.6;
            font-size: large;
        }
    </style>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'] ],
          processEscapes: true
        }
      });
    </script>
    <script>
    function toggleAuthors(element) {
        const fullList = element.querySelector('.author-full');
        const shortList = element.querySelector('.author-short');
        const toggleText = element.querySelector('.author-toggle');
        
        fullList.style.display = fullList.style.display === 'none' ? 'inline' : 'none';
        shortList.style.display = shortList.style.display === 'none' ? 'inline' : 'none';
        toggleText.textContent = fullList.style.display === 'none' ? ' (show all)' : ' (show less)';
    }
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>
<body>
    <nav class="navbar navbar-expand-lg mb-4">
        <div class="container">
            <a class="navbar-brand fw-bold text-primary" href="#">arXiv Daily</a>
            <span class="navbar-text">Modern arXiv Reader</span>
        </div>
    </nav>

    
    <div class="container py-4">
        <header class="mb-4"><h2 class="display-6 text-center">Analysis of Your Favorites</h2></header>
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Word Cloud</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/word_cloud.png" class="img-fluid rounded" alt="Word Cloud of favorite paper topics">
                    </div>
                </div>
            </div>
        </div>
        
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Clusters</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/backfill_cluster_map.png" class="img-fluid rounded" alt="2D Cluster plot of favorite papers">
                    </div>
                </div>
            </div>
        </div>
        
    </div>
    

    <div class="container py-4">
        <header class="mb-4"><h1 class="display-5 text-center text-primary">arXiv: Backfill (2025-10-12 to 2025-10-19)</h1></header>

        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Score Distribution of All Papers Found Today</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/score_distribution.png" class="img-fluid rounded" alt="Score distribution of all papers found today">
                    </div>
                </div>
            </div>
        </div>
        

        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">1. Euclid: Exploring observational systematics in cluster cosmology -- a comprehensive analysis of cluster counts and clustering
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">A. Fumagalli, M. Costanzi, T. Castro, A. Saro, S. Borgani, M. Romanello, F. Marulli, E. Tsaprazi, P. Monaco, B. Altieri, et al.</span>
                                <span class="author-full" style="display: none;">A. Fumagalli, M. Costanzi, T. Castro, A. Saro, S. Borgani, M. Romanello, F. Marulli, E. Tsaprazi, P. Monaco, B. Altieri, A. Amara, L. Amendola, S. Andreon, N. Auricchio, C. Baccigalupi, M. Baldi, A. Balestra, S. Bardelli, A. Biviano, E. Branchini, M. Brescia, S. Camera, G. Cañas-Herrera, V. Capobianco, C. Carbone, J. Carretero, S. Casas, M. Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C. Colodro-Conde, G. Congedo, L. Conversi, Y. Copin, F. Courbin, H. M. Courtois, A. Da Silva, H. Degaudenzi, S. de la Torre, G. De Lucia, A. M. Di Giorgio, H. Dole, M. Douspis, F. Dubath, C. A. J. Duncan, X. Dupac, S. Dusini, S. Escoffier, M. Farina, R. Farinelli, F. Faustini, S. Ferriol, F. Finelli, P. Fosalba, N. Fourmanoit, M. Frailis, E. Franceschi, M. Fumana, S. Galeotta, K. George, B. Gillis, C. Giocoli, J. Gracia-Carpio, A. Grazian, F. Grupp, L. Guzzo, S. V. H. Haugan, W. Holmes, F. Hormuth, A. Hornstrup, K. Jahnke, M. Jhabvala, B. Joachimi, E. Keihänen, S. Kermiche, A. Kiessling, B. Kubik, M. Kümmel, M. Kunz, H. Kurki-Suonio, A. M. C. Le Brun, S. Ligori, P. B. Lilje, V. Lindholm, I. Lloro, G. Mainetti, D. Maino, E. Maiorano, O. Mansutti, O. Marggraf, M. Martinelli, N. Martinet, R. J. Massey, E. Medinaceli, S. Mei, Y. Mellier, M. Meneghetti, E. Merlin, G. Meylan, J. J. Mohr, A. Mora, M. Moresco, L. Moscardini, E. Munari, R. Nakajima, C. Neissner, S. -M. Niemi, C. Padilla, S. Paltani, F. Pasian, K. Pedersen, V. Pettorino, S. Pires, G. Polenta, M. Poncet, L. A. Popa, L. Pozzetti, F. Raison, R. Rebolo, A. Renzi, J. Rhodes, G. Riccio, E. Romelli, M. Roncarelli, C. Rosset, R. Saglia, Z. Sakr, A. G. Sánchez, D. Sapone, B. Sartoris, P. Schneider, T. Schrabback, A. Secroun, E. Sefusatti, G. Seidel, M. Seiffert, S. Serrano, P. Simon, C. Sirignano, G. Sirri, A. Spurio Mancini, L. Stanco, J. Steinwagner, P. Tallada-Crespí, D. Tavagnacco, A. N. Taylor, I. Tereno, N. Tessore, S. Toft, R. Toledo-Moreo, F. Torradeflot, I. Tutusaus, L. Valenziano, J. Valiviita, T. Vassallo, G. Verdoes Kleijn, A. Veropalumbo, Y. Wang, J. Weller, G. Zamorani, F. M. Zerbi, E. Zucca, C. Burigana, L. Gabarra, M. Maturi, C. Porciani, V. Scottez, M. Sereno, M. Viel</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> RSDs and photo-$z$ uncertainties also influence the number count covariance, with a significant impact, of about 15--20\%, on the parameter constraints. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">This study explores the impact of observational and modelling systematic effects on cluster number counts and cluster clustering and provides model prescriptions for their joint analysis, in the context of the \Euclid survey. Using 1000 \Euclid-like cluster catalogues, we investigate the effect of systematic uncertainties on cluster summary statistics and their auto- and cross-covariance, and perform a likelihood analysis to evaluate their impact on cosmological constraints, with a focus on the matter density parameter $\Omega_{\rm m}$ and on the power spectrum amplitude $\sigma_8$. Combining cluster clustering with number counts significantly improves cosmological constraints, with the figure of merit increasing by over 300\% compared to number counts alone. We confirm that the two probes are uncorrelated, and the cosmological constraints derived from their combination are almost insensitive to the cosmology dependence of the covariance. We find that photometric redshift uncertainties broaden cosmological posteriors by 20--30\%, while secondary effects like redshift-space distortions (RSDs) have a smaller impact on the posteriors -- 5\% for clustering alone, 10\% when combining probes -- but can significantly bias the constraints if neglected. We show that clustering data below $60\,h^{-1}\,$Mpc provides additional constraining power, while scales larger than acoustic oscillation scale add almost no information on $\Omega_{\rm m}$ and $\sigma_8$ parameters. RSDs and photo-$z$ uncertainties also influence the number count covariance, with a significant impact, of about 15--20\%, on the parameter constraints.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.13509" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.13509" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.13509" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 14.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.99</span>
                        <span class="badge bg-primary">Semantic Score: 0.96</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">2. Ultra-Faint Milky Way Satellites Discovered in Carina, Phoenix, and Telescopium with DELVE Data Release 3
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">C. Y. Tan, W. Cerny, A. B. Pace, J. A. Sharp, K. Overdeck, A. Drlica-Wagner, J. D. Simon, B. Mutlu-Pakdil, D. J. Sand, A. M. Senkevich, et al.</span>
                                <span class="author-full" style="display: none;">C. Y. Tan, W. Cerny, A. B. Pace, J. A. Sharp, K. Overdeck, A. Drlica-Wagner, J. D. Simon, B. Mutlu-Pakdil, D. J. Sand, A. M. Senkevich, D. Erkal, P. S. Ferguson, F. Sobreira, K. R. Atzberger, J. L. Carlin, A. Chiti, D. Crnojević, A. P. Ji, L. C. Johnson, T. S. Li, G. Limberg, C. E. Martínez-Vázquez, G. E. Medina, V. M. Placco, A. H. Riley, E. J. Tollerud, A. K. Vivas, T. M. C. Abbott, M. Aguena, O. Alves, D. Bacon, S. Bocquet, D. Brooks, D. L. Burke, R. Camilleri, J. A. Carballo-Bello, A. Carnero Rosell, J. Carretero, T. -Y. Cheng, Y. Choi, L. N. da Costa, M. E. da Silva Pereira, T. M. Davis, J. De Vicente, S. Desai, P. Doel, S. Everett, B. Flaugher, J. Frieman, J. García-Bellido, D. Gruen, G. Gutierrez, S. R. Hinton, D. L. Hollowood, D. J. James, K. Kuehn, S. Lee, J. L. Marshall, J. Mena-Fernández, F. Menanteau, R. Miquel, J. Myles, M. Navabi, D. L. Nidever, R. L. C. Ogando, A. A. Plazas Malagón, A. Porredon, S. Samuroff, E. Sanchez, D. Sanchez Cid, I. Sevilla-Noarbe, M. Smith, E. Suchyta, M. E. C. Swanson, V. Vikram, A. R. Walker, A. Zenteno</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The candidate systems were identified by cross-matching results from two independent search algorithms. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We report the discovery of three Milky Way satellite candidates: Carina IV, Phoenix III, and DELVE 7, in the third data release of the DECam Local Volume Exploration survey (DELVE). The candidate systems were identified by cross-matching results from two independent search algorithms. All three are extremely faint systems composed of old, metal-poor stellar populations ($\tau \gtrsim 10$ Gyr, [Fe/H] $ \lesssim -1.4$). Carina IV ($M_V = -2.8;\ r_{1/2} = 40 {\rm pc}$) and Phoenix III ($M_V = -1.2;\ r_{1/2} = 19 {\rm pc}$) have half-light radii that are consistent with the known population of dwarf galaxies, while DELVE 7 ($M_V = 1.2;\ r_{1/2} = 2 {\rm pc}$) is very compact and seems more likely to be a star cluster, though its nature remains ambiguous without spectroscopic followup. The Gaia proper motions of stars in Carina IV ($M_* = 2250^{+1180}_{-830} {\rm M_\odot}$) indicate that it is unlikely to be associated with the LMC, while DECam CaHK photometry confirms that its member stars are metal-poor. Phoenix III ($M_* = 520^{+660}_{-290} {\rm M_\odot}$) is the faintest known satellite in the extreme outer stellar halo ($D_{\rm GC} &gt; 100$ kpc), while DELVE 7 ($M_* = 60^{+120}_{-40} {\rm M_\odot}$) is the faintest known satellite with $D_{\rm GC} &gt; 20$ kpc.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.11684" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.11684" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.11684" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.8</span>
                        <span class="badge bg-info text-dark">Author Score: 0.35</span>
                        <span class="badge bg-primary">Semantic Score: 0.90</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">3. Probing cosmic velocities with the pairwise kinematic Sunyaev-Zel&#39;dovich signal in DESI Bright Galaxy Sample DR1 and ACT DR6
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">B. Hadzhiyska, Y. Gong, Y. Hsu, P. A. Gallardo, J. Aguilar, S. Ahlen, D. Alonso, R. Bean, D. Bianchi, D. Brooks, et al.</span>
                                <span class="author-full" style="display: none;">B. Hadzhiyska, Y. Gong, Y. Hsu, P. A. Gallardo, J. Aguilar, S. Ahlen, D. Alonso, R. Bean, D. Bianchi, D. Brooks, F. J. Castander, T. Claybaugh, S. Cole, A. Cuceu, A. de la Macorra, Arjun Dey, S. Ferraro, A. Font-Ribera, J. E. Forero-Romero, S. Gontcho A Gontcho, G. Gutierrez, J. Guy, H. K. Herrera-Alcantar, C. Howlett, D. Huterer, M. Ishak, R. Joyce, T. Kisner, A. Kremin, M. Landriau, L. Le Guillou, M. E. Levi, M. Manera, A. Meisner, R. Miquel, K. Moodley, T. Mroczkowski, S. Nadathur, N. Palanque-Delabrouille, W. J. Percival, F. Prada, F. J. Qu, I. Perez-Rafols, B. Ried Guachalla, G. Rossi, E. Sanchez, E. Schaan, D. Schlegel, M. Schubnell, H. Seo, C. Sifon, J. Silber, D. Sprayberry, G. Tarle, E. M. Vavagiakis, B. A. Weaver, R. Zhou, H. Zou</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We stress that this is a proof-of-concept analysis; with BGS DR2 data we expect to improve the statistical precision by roughly a factor of two, paving the way toward robust tests of modified gravity with kSZ-informed velocity-field measurements at low redshift. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present a measurement of the pairwise kinematic Sunyaev-Zel&#39;dovich (kSZ) signal using the Dark Energy Spectroscopic Instrument (DESI) Bright Galaxy Sample (BGS) Data Release 1 (DR1) galaxy sample overlapping with the Atacama Cosmology Telescope (ACT) CMB temperature map. Our analysis makes use of $1.6$ million galaxies with stellar masses $\log M_\star/M_\odot &gt; 10$, and we explore measurements across a range of aperture sizes ($2.1&#39; 11$), we achieve a $5\sigma$ detection. Assuming that an estimate of the optical depth and galaxy bias of the sample exists via e.g., external observables, this measurement constrains the fundamental cosmological combination $H_0 f \sigma_8^2$. A key challenge is the degeneracy with the galaxy optical depth. We address this by combining CMB lensing, which allows us to infer the halo mass and galaxy population properties, with hydrodynamical simulation estimates of the mean optical depth, $\bar \tau$. We stress that this is a proof-of-concept analysis; with BGS DR2 data we expect to improve the statistical precision by roughly a factor of two, paving the way toward robust tests of modified gravity with kSZ-informed velocity-field measurements at low redshift.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.14135" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.14135" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.14135" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.7</span>
                        <span class="badge bg-info text-dark">Author Score: 0.23</span>
                        <span class="badge bg-primary">Semantic Score: 0.96</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">4. The dark side of early galaxies: $\texttt{geko}$ uncovers dark-matter fractions at $z\sim4-6$
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">A. Lola Danhaive, Sandro Tacchella, Andrew J. Bunker, Emma Curtis-Lake, Anna de Graaff, Francesco D&#39;Eugenio, Qiao Duan, Eiichi Egami, Daniel J. Eisenstein, Benjamin D. Johnson, et al.</span>
                                <span class="author-full" style="display: none;">A. Lola Danhaive, Sandro Tacchella, Andrew J. Bunker, Emma Curtis-Lake, Anna de Graaff, Francesco D&#39;Eugenio, Qiao Duan, Eiichi Egami, Daniel J. Eisenstein, Benjamin D. Johnson, Roberto Maiolino, William McClymont, Marcia Rieke, Brant Robertson, Fengwu Sun, Christopher N. A. Willmer, Zihao Wu, Yongda Zhu</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We model the H$\alpha$ morpho-kinematics of 163 galaxies at redshift $z\approx4$-6 from FRESCO and CONGRESS (with JADES imaging), using the $\texttt{geko}$ code, and infer rotational velocities and dispersions within $r_{\rm e}$. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">JWST/NIRCam slitless spectroscopy enables dynamical mass measurements for typical star-forming galaxies only a billion years after the Big Bang. We model the H$\alpha$ morpho-kinematics of 163 galaxies at redshift $z\approx4$-6 from FRESCO and CONGRESS (with JADES imaging), using the $\texttt{geko}$ code, and infer rotational velocities and dispersions within $r_{\rm e}$. Our sample spans $\log M_{\star}\approx7$-10 and $\log M_{\rm dyn}\approx9$-11. Gas masses are estimated via scaling relations, yielding baryonic masses and dark-matter (DM) fractions $f_{\rm DM}(r&lt;r_{\rm e})$ within the H$\alpha$ half-light radius. We find high median fractions of $\langle f_{\rm gas}\rangle=0.77$ and $\langle f_{\rm DM}\rangle=0.73$, where $f_{\rm gas}$ is measured with respect to the baryonic mass and $f_{\rm DM}$ with respect to the DM+baryonic mass. About two-thirds of systems are DM-dominated within $r_{\rm e}\sim0.5-1$ kpc. Both $f_{\rm gas}$ and $f_{\rm DM}$ decrease with stellar mass, consistent with simulations. The stellar Tully-Fisher relation shows a tentative offset to higher $v_{\rm circ}$ at fixed $M_{\star}$ and substantial intrinsic scatter, suggesting that the relation is only beginning to emerge at $z\sim5$. We measure a negative correlation between $f_{\rm DM}$ and baryonic surface density $\Sigma_{\rm bar}$, weaker but broadly consistent with trends at cosmic noon and at $z\sim0$. Qualitatively comparing with modified NFW profiles coupled to an empirical stellar-to-halo mass relation suggests that the lowest $f_{\rm DM}$ ($\lesssim0.4$) require cored inner DM profiles, while the highest fractions favour cuspier profiles, potentially reflecting adiabatic contraction. Overall, the elevated $f_{\rm gas}$ and $f_{\rm DM}$ at $z\gtrsim4$ are compatible with progenitors of baryon-dominated systems at $z\sim2$ and naturally anticipate overmassive black holes at fixed $M_{\star}$.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.14779" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.14779" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.14779" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.1</span>
                        <span class="badge bg-info text-dark">Author Score: 0.17</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">5. Fast radio bursts shed light on direct gravity test on cosmological scales
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Shuren Zhou, Pengjie Zhang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> It can be further mitigated to the $\lesssim 1\%$ level, based on the gastrophysics-agnostic behavior that the bias of total baryonic matter (ionized diffuse gas, stars, neutral hydrogen, etc) approaches unity at sufficiently large scales. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">A key measure of gravity is the relation between the Weyl potential $\Psi+\Phi$ and the matter overdensity $\delta_m$, capsulized as an effective gravitational constant $G_{\rm light}$ for light motion. Its value, together with the possible spatial and temporal variation, is essential in probing physics beyond Einstein gravity. However, the lack of an unbiased proxy of $\delta_m$ prohibits direct measurement of $G_{\rm light}$. We point out that the equivalence principle ensures the dispersion measure (DM) of localized fast radio bursts (FRBs) as a good proxy of $\delta_m$. We further propose a FRB-based method $F_G$ to directly measure $G_{\rm light}$, combining galaxy-DM of localized FRBs and galaxy-weak lensing cross-correlations. The measurement, with a conservative cut $k\leq 0.1h$/Mpc, can achieve a precision of $\lesssim 10\% \sqrt{10^5/N_{\rm FRB}}$ over 10 equal-width redshift bins at $z\lesssim 1$. The major systematic error, arising from the clustering bias of electrons traced by the FRB DM, is subdominant ($\sim 5\%$). It can be further mitigated to the $\lesssim 1\%$ level, based on the gastrophysics-agnostic behavior that the bias of total baryonic matter (ionized diffuse gas, stars, neutral hydrogen, etc) approaches unity at sufficiently large scales. Therefore, FRBs shed light on gravitational physics across spatial and temporal scales spanning over 20 orders of magnitude.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.11022" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.11022" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.11022" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.0</span>
                        <span class="badge bg-info text-dark">Author Score: 0.12</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">6. Eternal inflation bubble collision signature on CMB remote dipole and quadrupole fields
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Hongbo Cai, Pengjie Zhang, Yilun Guan</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We present the first analytic expression of the RQF signal induced by bubble collision and validate it against numerical calculations performed with $\texttt{RemoteField}$, a new public software tool we developed, finding excellent agreement between the two. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The remote dipole and quadrupole fields (RDF/RQF) encode information about the observable universe as seen from remote places within our past light cone. Sensitive to the superhorizon inhomogeneites, they provide a unique way to probe physics at the largest scales, bypassing the limitations of cosmic variance inherent in the primary cosmic microwave background (CMB). In this work, we focus on the bubble collision predicted by the eternal inflation theory, which can leave distinct azimuthally symmetric patterns on the superhorizon scales, potentially detectable through the RDF and RQF. We present the first analytic expression of the RQF signal induced by bubble collision and validate it against numerical calculations performed with $\texttt{RemoteField}$, a new public software tool we developed, finding excellent agreement between the two. Combining our new RQF calculation with the corresponding RDF signal calculated by prior work, we forecast the constraining power on bubble collision parameters using RDF/RQF reconstruction. We find that, for an CMB-S4-like and an LSST-like experiment, the RDF reconstruction can provide comparable constraining power as that from the primary CMB alone; and the RQF reconstruction can improve the constraining power by about an order of magnitude. We argue that these constraints can be improved further by including more RDF/RQF multipoles included and by using tomographic techniques to mitigate the standard $\Lambda$CDM signal. We anticipate the framework we developed in this work to be broadly applicable to probe other superhorizon-scale physics, such as cosmic topology and domain walls.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.12134" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.12134" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.12134" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.0</span>
                        <span class="badge bg-info text-dark">Author Score: 0.12</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">7. Simulation budgeting for hybrid effective field theories
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Alexa Bartlett, Joseph DeRose, Martin White</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> These results offer practical guidance for efficient emulator design and simulation budgeting in future cosmological analyses. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">In this work, we forecast the number of, and requirements on, N-body simulations needed to train hybrid effective field theory (HEFT) emulators for a range of use cases, using a hybrid of HMcode and perturbation theory as a surrogate model. Our accuracy goals, determined with careful consideration of statistical and systematic uncertainties, are $1\%$ accurate in the high-likelihood range of cosmological parameters, and $2\%$ accurate over a broader parameter space volume for $k&lt;1 h Mpc^{-1}$ and $z&lt;3$. Focusing in part on the 8-parameter $w_0w_a$CDM+$m_\nu$ cosmological model, we find that $&lt;225$ simulations are required to meet our error goals over our wide parameter space, including models with rapidly evolving dark energy, given our simulation and emulator recommendations. For a more restricted parameter space volume, as few as 80 simulations are sufficient. We additionally present simulation forecasts for example use cases, and make the code used in our analyses publicly available. These results offer practical guidance for efficient emulator design and simulation budgeting in future cosmological analyses.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.13962" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.13962" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.13962" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.0</span>
                        <span class="badge bg-info text-dark">Author Score: 0.12</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">8. Lyα Intensity Mapping in HETDEX: Galaxy-Lyα Intensity Cross-Power Spectrum
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Maja Lujan Niemeyer, Eiichiro Komatsu, José Luis Bernal, Chris Byrohl, Robin Ciardullo, Olivia Curtis, Daniel J. Farrow, Steven L. Finkelstein, Karl Gebhardt, Caryl Gronwall, et al.</span>
                                <span class="author-full" style="display: none;">Maja Lujan Niemeyer, Eiichiro Komatsu, José Luis Bernal, Chris Byrohl, Robin Ciardullo, Olivia Curtis, Daniel J. Farrow, Steven L. Finkelstein, Karl Gebhardt, Caryl Gronwall, Gary J. Hill, Matt J. Jarvis, Donghui Jeong, Erin Mentuch Cooper, Deeshani Mitra, Shiro Mukae, Julian B. Muñoz, Masami Ouchi, Shun Saito, Donald P. Schneider, Lutz Wisotzki</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> To produce a power spectrum model and its covariance matrix, we simulate the data using lognormal mocks for the LAE catalog and Ly$\alpha$ intensity in redshift space. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present a measurement of the Lyman-$\alpha$ (Ly$\alpha$) intensity mapping power spectrum from the Hobby-Eberly Telescope Dark Energy Experiment (HETDEX). We measure the cross-power spectrum of the Ly$\alpha$ intensity and Ly$\alpha$-emitting galaxies (LAEs) in a redshift range of $1.9 &lt; z &lt; 3.5$. We calculate the intensity from HETDEX spectra that do not contain any detected LAEs above a signal-to-noise ratio of $5.5$. To produce a power spectrum model and its covariance matrix, we simulate the data using lognormal mocks for the LAE catalog and Ly$\alpha$ intensity in redshift space. The simulations include the HETDEX sensitivity, selection function, and mask. The measurements yield the product of the LAE bias, the intensity bias, the mean intensity of undetected sources, and the ratio of the actual and fiducial redshift-space distortion parameters, $b_\mathrm{g} b_I \langle I \rangle \bar{F}_{\rm RSD} / \bar{F}^{\rm fid}_{\rm RSD}= (6.7 \pm 3.1)$, $(11.7 \pm 1.4)$, and $(8.3 \pm 1.5) \times 10^{-22} \, \text{erg}\, \text{s}^{-1} \, \text{cm}^{-2} \, \text{arcsec}^{-2} \, \text{{\AA}}^{-1}$ in three redshift bins centered at $\bar z=2.1$, 2.6, and 3.2, respectively. The results are reasonably consistent with cosmological hydrodynamical simulations that include Ly$\alpha$ radiative transfer. They are, however, significantly smaller than previous results from cross-correlations of quasars with Ly$\alpha$ intensity. These results demonstrate the statistical power of HETDEX for Ly$\alpha$ intensity mapping and pave the way for a more comprehensive analysis. They will also be useful for constraining models of Ly$\alpha$ emission from galaxies used in modern cosmological simulations of galaxy formation and evolution.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.11427" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.11427" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.11427" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.0</span>
                        <span class="badge bg-info text-dark">Author Score: 0.11</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">9. JADES Dark Horse: demonstrating high-multiplex observations with JWST/NIRSpec dense-shutter spectroscopy in the JADES Origins Field
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Francesco D&#39;Eugenio, Erica J. Nelson, Daniel J. Eisenstein, Roberto Maiolino, Stefano Carniani, Jan Scholtz, Mirko Curti, Christopher N. A. Willmer, Andrew J. Bunker, Jakob M. Helton, et al.</span>
                                <span class="author-full" style="display: none;">Francesco D&#39;Eugenio, Erica J. Nelson, Daniel J. Eisenstein, Roberto Maiolino, Stefano Carniani, Jan Scholtz, Mirko Curti, Christopher N. A. Willmer, Andrew J. Bunker, Jakob M. Helton, Ignas Juodžbalis, Fengwu Sun, Sandro Tacchella, Santiago Arribas, Alex J. Cameron, Stéphane Charlot, Emma Curtis-Lake, Kevin Hainline, Benjamin D. Johnson, Brant Robertson, Christina C. Williams, Chris Willott, William M. Baker, Jacopo Chevallard, A. Lola Danhaive, Yuki Isobe, Xihan Ji, Zhiyuan Ji, Gareth C. Jones, Nimisha Kumari, Tobias J. Looser, Jianwei Lyu, Eleonora Parlanti, Michele Perna, Dávid Puskás, Pierluigi Rinaldi, Charlotte Simmonds, Yang Sun, Giacomo Venturi, Joris Witstok, Zihao Wu, Yongda Zhu</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> This approach is immediately applicable wherever deep imaging enables robust pre-selection and astrometry, providing an efficient method to obtain large samples of faint emission-line galaxies, a compelling middle ground between the completeness of slitless surveys and the sensitivity and bandwidth of NIRSpec/MSA. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present JWST/NIRSpec dense-shutter spectroscopy (DSS). This novel observing strategy with the NIRSpec micro-shutter assembly (MSA) deliberately permits a high number of controlled spectral overlaps to reach extreme multiplex while retaining the low background of slit spectroscopy. In a single configuration over the JADES Origins Field we opened shutters on all faint (F444W3 candidates in the MSA, prioritising emission-line science and rejecting only bright continuum sources. Using 33.6 and 35.8 ks on-source in G235M and G395M, we observed a single mask with ~850 sources, obtaining secure spectroscopic redshifts for ~540 galaxies over 2.5&lt;z&lt;8.9. The per-configuration target density in DSS mode is 4-5x higher than standard no- and low-overlap MSA strategies (&lt;200 sources), with no loss in redshift precision or accuracy. Line-flux sensitivities are 30 percent lower at fixed exposure time, matching the expected increase in background noise, but the gain in survey speed is 5x in our setup, more than justifying the penalty. The measured line sensitivity exceeds NIRCam WFSS by a minimum factor of ~5 (i.e. ~25 in exposure time) at $\lambda$~4 $\mu$m, demonstrating that controlled overlap is a compelling method to gain deep, wide-band spectra for large samples. Crucially, we envisage the MSA could deliver even higher target allocation densities than what used here. We derive Balmer-line based SFRs, gas-phase metallicities (including a large sample suitable for strong-line calibrations), and identify rare sources (mini-quenched systems and broad-line AGN). This approach is immediately applicable wherever deep imaging enables robust pre-selection and astrometry, providing an efficient method to obtain large samples of faint emission-line galaxies, a compelling middle ground between the completeness of slitless surveys and the sensitivity and bandwidth of NIRSpec/MSA.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.11626" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.11626" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.11626" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.9</span>
                        <span class="badge bg-info text-dark">Author Score: 0.17</span>
                        <span class="badge bg-primary">Semantic Score: 0.90</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">10. Mergers lighting the early Universe: enhanced star formation, AGN triggering, and Ly$α$ emission in close pairs at $z=3-9$
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Dávid Puskás, Sandro Tacchella, Charlotte Simmonds, Gareth C. Jones, Ignas Juodžbalis, Jan Scholtz, William M. Baker, Andrew J. Bunker, Stefano Carniani, Emma Curtis-Lake, et al.</span>
                                <span class="author-full" style="display: none;">Dávid Puskás, Sandro Tacchella, Charlotte Simmonds, Gareth C. Jones, Ignas Juodžbalis, Jan Scholtz, William M. Baker, Andrew J. Bunker, Stefano Carniani, Emma Curtis-Lake, Qiao Duan, Daniel J. Eisenstein, Kevin Hainline, Benjamin D. Johnson, Roberto Maiolino, Marcia Rieke, Brant Robertson, Christina C. Williams, Joris Witstok</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We use JADES data to analyse a mass-complete sample of 2095 galaxies at $z=3-9$ with ${\rm log}(M_\star/{\rm M_\odot}) = [8, 10]$, identifying major merger pairs (projected separation of $5-100$ pkpc, mass ratio $\geq 1/4$) using a probabilistic method. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Galaxy mergers and interactions are often invoked to explain enhanced star formation, black hole growth, and mass build-up of galaxies at later cosmic times, but their effect is poorly understood at high redshift ($z&gt;2$). We use JADES data to analyse a mass-complete sample of 2095 galaxies at $z=3-9$ with ${\rm log}(M_\star/{\rm M_\odot}) = [8, 10]$, identifying major merger pairs (projected separation of $5-100$ pkpc, mass ratio $\geq 1/4$) using a probabilistic method. To look for signatures of enhancement in multiple physical properties, we carefully build a control sample of non-pairs that are simultaneously matched in redshift, stellar mass, isolation, and environment to the pair sample. We find a moderate enhancement in specific star formation rate (sSFR) of $1.12 \pm 0.05$ at separations $\lesssim 20$ kpc, which is weakly detectable out to $\sim50$ kpc. We find that at longer averaging timescales (50-100 Myr) the sSFR is more affected by interactions and environment, whereas at shorter timescales (5-10 Myr) it is dominated by internal feedback and burstiness. By averaging star formation histories, we find two distinct populations: pre-first passage/coalescence (monotonically rising SFR) and post-pericentre pairs (earlier peak in SFR). Finally, we find no significant excess of AGN in pairs, suggesting galaxy interactions are not effectively triggering black hole activity at separations $&gt;5$ kpc. Similarly, we also do not detect an excess in the fraction of Lyman-$\alpha$ emitters in pairs, implying that at the probed separations, galaxy interactions are not efficient at enhancing Lyman-$\alpha$ photon production and escape, which may only become important at the smallest scales.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.14743" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.14743" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.14743" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.7</span>
                        <span class="badge bg-info text-dark">Author Score: 0.17</span>
                        <span class="badge bg-primary">Semantic Score: 0.89</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">11. Mass dependence of halo baryon fractions from the kinetic Sunyaev-Zeldovich effect
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Finn A. Roper, Yan-Chuan Cai, John A. Peacock</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We develop a comprehensive forward model based on the AbacusSummit cosmological simulations: mock galaxy group catalogues and synthetic kSZ maps are generated, together with a reconstructed peculiar velocity field that allows for photo-$z$ errors, redshift-space distortions, and survey masks. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We detect the kinetic Sunyaev-Zeldovich imprint of peculiar motions of galaxy groups and clusters, using the photometric DESI Legacy Survey together with cosmic microwave background (CMB) maps from the Atacama Cosmology Telescope (ACT). We develop a comprehensive forward model based on the AbacusSummit cosmological simulations: mock galaxy group catalogues and synthetic kSZ maps are generated, together with a reconstructed peculiar velocity field that allows for photo-$z$ errors, redshift-space distortions, and survey masks. We investigate possible contamination from the cosmic infrared background (CIB), finding that CIB effects are subdominant to the kSZ signal in the relevant ACT frequency channel. We then predict the kSZ signal expected when stacking CMB temperature maps around groups, taking account of their estimated radial velocity. Comparing the model with observations, we are able to constrain the total baryon fraction within haloes, as well as their internal gas profiles. We find evidence for mass dependence of the halo baryon fraction within the virial radius. The gas fraction in massive groups is consistent with the universal baryon fraction, but low-mass groups ($10^{12.5} \lesssim M\,/h^{-1}\mathrm{M}_\odot \lesssim 10^{14}$) are depleted to $0.21 \pm 0.06$ times the universal baryon fraction. We find this low virial baryon fraction to be consistent with an extended gas profile, for which the total baryon content reaches the universal value well beyond the virial radius. This conclusion is consistent with previous analyses using X-ray, kSZ, and weak lensing, and plausibly reflects energetic feedback processes from the galaxies in these haloes.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.12553" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.12553" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.12553" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.7</span>
                        <span class="badge bg-info text-dark">Author Score: 0.03</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">12. Axial Neural Networks for Dimension-Free Foundation Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Hyunsu Kim, Jonggeon Park, Joan Bruna, Hongseok Yang, Juho Lee</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> However, training such models on physics data, including solutions to partial differential equations (PDEs), poses a unique challenge due to varying dimensionalities across different systems. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The advent of foundation models in AI has significantly advanced general-purpose learning, enabling remarkable capabilities in zero-shot inference and in-context learning. However, training such models on physics data, including solutions to partial differential equations (PDEs), poses a unique challenge due to varying dimensionalities across different systems. Traditional approaches either fix a maximum dimension or employ separate encoders for different dimensionalities, resulting in inefficiencies. To address this, we propose a dimension-agnostic neural network architecture, the Axial Neural Network (XNN), inspired by parameter-sharing structures such as Deep Sets and Graph Neural Networks. XNN generalizes across varying tensor dimensions while maintaining computational efficiency. We convert existing PDE foundation models into axial neural networks and evaluate their performance across three training scenarios: training from scratch, pretraining on multiple PDEs, and fine-tuning on a single PDE. Our experiments show that XNNs perform competitively with original models and exhibit superior generalization to unseen dimensions, highlighting the importance of multidimensional pretraining for foundation models.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.13665" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.13665" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.13665" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.03</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">13. Chimera: State Space Models Beyond Sequences
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Aakash Lahoti, Tanya Marwah, Ratish Puduppully, Albert Gu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The key idea is that state space models--which naturally do not require position embeddings--can be generalized to capture any graph topology. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Transformer-based deep learning methods have become the standard approach for modeling diverse data such as sequences, images, and graphs. These methods rely on self-attention, which treats data as an unordered set of elements. This ignores the neighborhood structure or graph topology of the data and requires inductive biases--such as position embeddings in sequences and images, or random walks in graphs--to incorporate topology. However, designing such task-specific biases requires significant effort and can introduce side effects that hinder generalization. We introduce Chimera, a unified model that directly incorporates data topology in a principled way, removing the need for domain-specific biases. The key idea is that state space models--which naturally do not require position embeddings--can be generalized to capture any graph topology. Our experiments show that Chimera achieves strong performance across language, vision, and graph domains, outperforming BERT on GLUE by 0.7 points, ViT on ImageNet-1k by 2.6%, and all baselines on the Long Range Graph Benchmark. We further propose algorithmic optimizations to improve Chimera&#39;s efficiency: (1) for Directed Acyclic Graphs, Chimera can be implemented as a linear-time recurrence; (2) for general graphs, a simple mathematical relaxation achieves Transformer&#39;s quadratic complexity without domain-specific heuristics. These results validate Chimera&#39;s core contribution and support the idea that data topology is a powerful inductive bias across modalities.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.12111" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.12111" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.12111" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.96</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">14. Islands in Simulated Cosmos: Probing the Hubble Flow around Groups and Clusters
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>David Benisty, Antonino Del Popolo</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> On large scales, galaxy velocities follow Hubble&#39;s law, but within groups and clusters local gravitational effects introduce significant departures from linearity. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The local Hubble flow offers a powerful laboratory to study the interplay between cosmic expansion and gravitational dynamics. On large scales, galaxy velocities follow Hubble&#39;s law, but within groups and clusters local gravitational effects introduce significant departures from linearity. Using the IllustrisTNG cosmological simulations, we investigate whether dark energy leaves detectable imprints on the local velocity-radius relation. We model the kinematics with extensions of the Lemaitre-Tolman framework and apply Bayesian inference to recover halo masses and the Hubble constant H0. The fits reveal systematic biases: halo masses are underestimated with a median ratio $M_{fit}/M_{true} = 0.95 \pm 0.28$, while the inferred Hubble constant clusters around $H_0 = 64 \pm 16 km/s/Mpc$, compared to the simulation input of 67.74. This corresponds to an average 25\% uncertainty in H0 recovery from the local flow method. While the mass and expansion rate can be constrained, different model variants whether including angular momentum, friction, or altered radial scaling-remain statistically indistinguishable. Our results highlight both the promise and the limitations of using local kinematics as a precision probe of dark energy.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.11382" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.11382" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.11382" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">15. The ASTRID Simulation at z=0: from Massive Black Holes to Large-scale Structure
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yihao Zhou, Tiziana Di Matteo, Simeon Bird, Rupert Croft, Yueying Ni, Yanhui Yang, Nianyi Chen, Patrick Lachance, Xiaowen Zhang, Fatemeh Hafezianzadeh</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Notably, ASTRID generates scatter in these relations that is more consistent with observations than previous simulations, indicating a more realistic MBH diversity. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present the $z=0$ results for the cosmological simulation ASTRID. Hosting $2\times 5500^3\approx$ 0.33 trillion particles in a box of $370\, {\rm Mpc}$ per side, ASTRID is one of the largest cosmological hydrodynamic simulations evolved to $z=0$. ASTRID features a large population of massive black holes (MBHs), covering a wide mass range $4\times10^{4}\sim 2\times 10^{11}\ M_{\odot}$. The adopted dynamical friction model provides a relatively accurate description of MBH dynamics, making ASTRID a powerful tool to study MBH growth and mergers in a cosmological context. ASTRID successfully captures the co-evolution of MBHs and their host galaxies, producing $M_{\rm BH}-M_{\star}$ and $M_{\rm BH}-\sigma$ relations in good agreement with observations. Notably, ASTRID generates scatter in these relations that is more consistent with observations than previous simulations, indicating a more realistic MBH diversity. The galaxy stellar mass function at $z=0$ is generally consistent with observational constraints. When dust attenuation is applied, the galaxy luminosity function also agrees well with observations, and the bimodality in galaxy colors is reproduced as well. ASTRID hosts a large population of massive galaxy groups and clusters: 7 halos have $M_{\rm 200c}&gt;10^{15}\ M_{\odot}$, and 9709 halos have $M_{\rm 200c}&gt;10^{13}\ M_{\odot}$. We quantify the stellar mass content in these halos, and find that the correlations between the stellar and halo mass match well with observational constraints. Finally, we present the $z=0$ power spectra of MBH and galaxies, as well as their bias with respect to the matter power spectrum. We find that MBHs with $M_{\rm BH}\geq 10^{8}\ M_{\odot}$ and galaxies with $M_{\star}\geq 10^{10.5}\ M_{\odot}$ serve as good tracers of large-scale structure.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.13976" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.13976" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.13976" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">16. First galaxy ultraviolet luminosity function limits on dark matter-proton scattering
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Hovav Lazare, Ely D. Kovetz, Kimberly K. Boddy, Julian B. Munoz</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Our results show that including lensed UVLF data, which probe fainter galaxies than the blank HST fields and thus smaller scales, leads to a substantial improvement in the constraints on $\sigma_0$ for $n&gt;0$, surpassing existing bounds from Milky-Way (MW) satellite abundance and CMB anisotropies. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Scattering between dark matter (DM) and protons leads to suppressed small-scale fluctuations, with implications for a variety of cosmological observables. In this work, we search for evidence of DM-proton scattering with an interaction cross section $\sigma\!=\!\sigma_0 (\frac{v}{c})^n$ for $n=0,2$ and $4$, corresponding e.g. to velocity-independent contact interactions from heavy mediators, velocity-dependent pseudoscalar-mediated scattering, and higher-order dipole interactions, respectively, using high-redshift ($z \sim4-10$) ultraviolet galaxy luminosity functions (UVLFs) observed by Hubble Space Telescope (HST). We employ an adjusted implementation of GALLUMI combined with the modified Boltzmann solver CLASS DMeff that accounts for interacting DM, and incorporate UVLF data from both blank and lensed HST fields, alongside Planck CMB data and the Pantheon supernova catalog in a Bayesian analysis framework to set constraints on $\sigma_0$. Our results show that including lensed UVLF data, which probe fainter galaxies than the blank HST fields and thus smaller scales, leads to a substantial improvement in the constraints on $\sigma_0$ for $n&gt;0$, surpassing existing bounds from Milky-Way (MW) satellite abundance and CMB anisotropies. For $m_{\chi} = 1\,\rm MeV $, for example, we set the upper bounds at $8.3\times 10^{-26} \, \rm cm^2$ for $n=2$ and $1.2\times 10^{-22} \, \rm cm^2$ for $n=4$. For $n=0$, our bound is within an order of magnitude of those from the Lyman-$\alpha$ forest and MW satellites.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.10757" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.10757" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.10757" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">17. Built-in precision: Improving cluster cosmology through the self-calibration of a galaxy cluster sample
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Junhao Zhan, Christian L. Reichardt</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The improvement in $w$ constraints from adding clustering information largely vanishes after tightening priors on the mass-observable relationship by a factor of two. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We examine the potential improvements in constraints on the dark energy equation of state parameter $w$ and matter density $\Omega_M$ from using clustering information along with number counts for future samples of thermal Sunyaev-Zel&#39;dovich selected galaxy clusters. We quantify the relative improvement from including the clustering power spectrum information for three cluster sample sizes from 33,000 to 140,000 clusters and for three assumed priors on the mass slope and redshift evolution of the mass-observable relation. As expected, clustering information has the largest impact when (i) there are more clusters and (ii) the mass-observable priors are weaker. For current knowledge of the cluster mass-observable relationship, we find the addition of clustering information reduces the uncertainty on the dark energy equation of state, $\sigma(w)$, by factors of $1.023\pm 0.007$ to $1.0790\pm 0.011$, with larger improvements observed with more clusters. Clustering information is more important for the matter density, with $\sigma(\Omega_M)$ reduced by factors of $1.068 \pm 007$ to $1.145 \pm 0.012$. The improvement in $w$ constraints from adding clustering information largely vanishes after tightening priors on the mass-observable relationship by a factor of two. For weaker priors, we find clustering information improves the determination of the cluster mass slope and redshift evolution by factors of $1.389 \pm 0.041$ and $1.340 \pm 0.039$ respectively. These findings highlight that, with the anticipated surge in cluster detections from next generation surveys, self-calibration through clustering information will provide an independent cross-check on the mass slope and redshift evolution of the mass-observable relationship as well as enhancing the precision achievable from cluster cosmology.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.14464" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.14464" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.14464" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">18. Quasinormal modes from numerical relativity with Bayesian inference
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Richard Dyer, Christopher J. Moore</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Numerical relativity (NR) enables the study of physics in strong and dynamical gravitational fields and provides predictions for the gravitational-wave signals produced by merging black holes. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Numerical relativity (NR) enables the study of physics in strong and dynamical gravitational fields and provides predictions for the gravitational-wave signals produced by merging black holes. Despite the impressive accuracy of modern codes, the resulting waveforms inevitably contain numerical uncertainties. Quantifying these uncertainties is important, especially for studies probing subdominant or nonlinear effects around the merger and ringdown. This paper describes a flexible Gaussian-process model for the numerical uncertainties in all the spherical-harmonic waveform modes across a state-of-the-art catalog of NR waveforms and a highly efficient procedure for sampling the posteriors of quasinormal mode models without the need for expensive Markov chain Monte Carlo. The Gaussian-process model is used to define a likelihood function which allows many Bayesian data analysis techniques - already widely used in the analysis of experimental gravitational wave data - to be applied to NR waveforms as well. The efficacy of this approach is demonstrated by applying it to the analysis of quasinormal modes in Cauchy-characteristic evolved waveforms.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.11783" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.11783" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.11783" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">19. EA4LLM: A Gradient-Free Approach to Large Language Model Optimization via Evolutionary Algorithms
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>WenTao Liu, Siyu Song, Hao Hao, Aimin Zhou</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> To address these limitations, we propose a method for optimizing LLMs using evolutionary algorithms (EA4LLM) and, for the first time, successfully demonstrate its capability to train a 1-billion-parameter LLM from the pre-trained stage. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">In recent years, large language models (LLMs) have made remarkable progress, with model optimization primarily relying on gradient-based optimizers such as Adam. However, these gradient-based methods impose stringent hardware requirements, demanding high-concurrency, high-memory GPUs. Moreover, they require all neural network operations to be differentiable, thereby excluding many promising non-differentiable architectures from practical use. To address these limitations, we propose a method for optimizing LLMs using evolutionary algorithms (EA4LLM) and, for the first time, successfully demonstrate its capability to train a 1-billion-parameter LLM from the pre-trained stage. We conduct extensive experiments and provide key insights into how evolutionary algorithms can effectively optimize neural networks. Our work challenges the prevailing assumption that gradient-based optimization is the only viable approach for training neural networks. It also holds significant potential to reduce the computational cost of training large language models, thereby enabling groups with limited computational resources to participate in deep learning research.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.10603" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.10603" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.10603" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">20. Cloudy with a chance of starshine: Possible photometric signatures of nebular-dominated emission in $1.5 &lt; z &lt; 8.5$ JADES galaxies
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">James A. A. Trussler, Alex J. Cameron, Daniel J. Eisenstein, Harley Katz, Nathan J. Adams, Duncan Austin, Andrew J. Bunker, Stefano Carniani, Christopher J. Conselice, Mirko Curti, et al.</span>
                                <span class="author-full" style="display: none;">James A. A. Trussler, Alex J. Cameron, Daniel J. Eisenstein, Harley Katz, Nathan J. Adams, Duncan Austin, Andrew J. Bunker, Stefano Carniani, Christopher J. Conselice, Mirko Curti, Emma Curtis-Lake, Kevin Hainline, Thomas Harvey, Benjamin D. Johnson, Qiong Li, Tobias J. Looser, Pierluigi Rinaldi, Brant Robertson, Fengwu Sun, Sandro Tacchella, Christina C. Williams, Christopher N. A. Willmer, Chris Willott, Zihao Wu</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The discovery of high-redshift galaxies exhibiting a steep spectral UV downturn potentially indicative of two-photon continuum emission marks a turning point in our search for signatures of top-heavy star formation in the early Universe. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The discovery of high-redshift galaxies exhibiting a steep spectral UV downturn potentially indicative of two-photon continuum emission marks a turning point in our search for signatures of top-heavy star formation in the early Universe. We develop a photometric search method for identifying further nebular-dominated galaxy candidates, whose nebular continuum dominates over the starlight, due to the high ionising photon production efficiencies $\xi_\mathrm{ion}$ associated with massive star formation. We utilise the extensive medium-band imaging from JADES, which enables the identification of Balmer jumps across a wide range of redshifts ($1.5 25.60$ to power the strong nebular continuum, together with a relatively non-blue UV slope indicating a lack of stellar continuum emission. Our nebular-dominated candidates, constituting ${\sim}$10% of galaxies at $z \sim 6$ (decreasing to ${\sim}$3% at $z \sim 2$, not completeness-corrected) are faint in the rest-frame optical (median $M_\mathrm{opt} = -17.95$) with extreme line emission (median $\mathrm{EW}_\mathrm{H\alpha,rest} = 1567$ \AA, $\mathrm{EW}_\mathrm{[O\ III] + H\beta,rest} = 2244$ \AA). However, hot H II region temperatures, collisionally-enhanced two-photon continuum emission, and strong UV lines are expected to accompany top-heavy star formation. Thus nebular-dominated galaxies do not necessarily exhibit the biggest Balmer jumps, nor the largest $\xi_\mathrm{ion, obs}$ or reddest UV slopes. Hence continuum spectroscopy is ultimately required to establish the presence of a two-photon downturn in our candidates, thus advancing our understanding of primordial star formation and AGN.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.12622" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.12622" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.12622" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.17</span>
                        <span class="badge bg-primary">Semantic Score: 0.87</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">21. Phantom Mirage from Axion Dark Energy
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Rayne Liu, Yijie Zhu, Wayne Hu, Vivian Miranda</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> With axions, raising the optical depth to reionization to $\tau \approx 0.1$ works essentially as well as $w_0-w_a$ phantom dark energy for all but the lowE CMB data, with a remaining $\Delta\chi^2\sim -16$ compared with $\Lambda$CDM, whereas a small spatial curvature of $\Omega_K \sim 0.003$ can largely relax the full SN-BAO-CMB tension with a total $\Delta\chi^2 \sim -12$. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Supernova (SN) and baryon acoustic oscillation (BAO) distance measures have recently provided hints that the dark energy is not only dynamical but apparently evolves from normal to phantom dark energy between redshifts $0&lt;z&lt;1$. A normal axion dark energy component in the mass range just below the Hubble scale can mimic a phantom component by appearing as dark energy at $z=1$ and dark matter at $z=0$, raising the possibility of a phantom mirage. We show that there is a wide range of axion dark energy contributions that can resolve the SN-BAO tension as well as thawing quintessence does, leaving BAO tension with the cosmic microwave background (CMB) for the distance measures from $z\sim 1$ to recombination to be resolved at high redshifts. With axions, raising the optical depth to reionization to $\tau \approx 0.1$ works essentially as well as $w_0-w_a$ phantom dark energy for all but the lowE CMB data, with a remaining $\Delta\chi^2\sim -16$ compared with $\Lambda$CDM, whereas a small spatial curvature of $\Omega_K \sim 0.003$ can largely relax the full SN-BAO-CMB tension with a total $\Delta\chi^2 \sim -12$.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.14957" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.14957" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.14957" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.17</span>
                        <span class="badge bg-primary">Semantic Score: 0.86</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">22. Diffusion-DFL: Decision-focused Diffusion Models for Stochastic Optimization
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zihao Zhao, Christopher Yeh, Lingkai Kong, Kai Wang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> To date, existing DFL methods typically rely on deterministic point predictions, which are often insufficient to capture the intrinsic stochasticity of real-world environments. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Decision-focused learning (DFL) integrates predictive modeling and optimization by training predictors to optimize the downstream decision target rather than merely minimizing prediction error. To date, existing DFL methods typically rely on deterministic point predictions, which are often insufficient to capture the intrinsic stochasticity of real-world environments. To address this challenge, we propose the first diffusion-based DFL approach, which trains a diffusion model to represent the distribution of uncertain parameters and optimizes the decision by solving a stochastic optimization with samples drawn from the diffusion model. Our contributions are twofold. First, we formulate diffusion DFL using the reparameterization trick, enabling end-to-end training through diffusion. While effective, it is memory and compute-intensive due to the need to differentiate through the diffusion sampling process. Second, we propose a lightweight score function estimator that uses only several forward diffusion passes and avoids backpropagation through the sampling. This follows from our results that backpropagating through stochastic optimization can be approximated by a weighted score function formulation. We empirically show that our diffusion DFL approach consistently outperforms strong baselines in decision quality. The source code for all experiments is available at the project repository: https://github.com/GT-KOALA/Diffusion_DFL.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.11590" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.11590" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.11590" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">23. Reconstructing and resampling: a guide to utilising posterior samples from gravitational wave observations
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Gregory Ashton</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The collaborations that build and operate these observatories release the interferometric strain data as well as a catalogue of observed signals with accompanying Bayesian posterior distributions. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The LIGO, Virgo, and KAGRA (LVK) gravitational-wave observatories have opened new scientific research in astrophysics, fundamental physics, and cosmology. The collaborations that build and operate these observatories release the interferometric strain data as well as a catalogue of observed signals with accompanying Bayesian posterior distributions. These posteriors, in the form of equally-weighted samples, form a dataset that is used by a multitude of further analyses seeking to constrain the population of merging black holes, identify lensed pairs of signals, and much more. However, many of these analyses rely, often implicitly, on the ability to reconstruct the likelihood and prior from the inputs to the analysis and apply resampling (a statistical technique to generate new samples varying the underlying analysis assumptions). In this work, we first provide a guide on how to reconstruct and modify the posterior density accurately from the inputs for analyses performed with the Bilby inference library. We then demonstrate and compare resampling techniques to produce new posterior sample sets and discuss Pareto-smoothing to improve the efficiency. Finally, we provide examples of how to use resampling to study observed gravitational-wave signals. We hope this guide provides a useful resource for those wishing to use open data products from the LVK for gravitational-wave astronomy.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.11197" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.11197" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.11197" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">24. Mapping the Perseus Galaxy Cluster with XRISM: Gas Kinematic Features and their Implications for Turbulence
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Congyao Zhang, Irina Zhuravleva, Annie Heinrich, Elena Bellomi, Nhut Truong, John ZuHone, Eugene Churazov, Megan E. Eckart, Yutaka Fujita, Julie Hlavacek-Larrondo, et al.</span>
                                <span class="author-full" style="display: none;">Congyao Zhang, Irina Zhuravleva, Annie Heinrich, Elena Bellomi, Nhut Truong, John ZuHone, Eugene Churazov, Megan E. Eckart, Yutaka Fujita, Julie Hlavacek-Larrondo, Yuto Ichinohe, Maxim Markevitch, Kyoko Matsushita, François Mernier, Eric D. Miller, Koji Mori, Hiroshi Nakajima, Anna Ogorzalek, Frederick S. Porter, Ayşegül Tümer, Shutaro Ueda, Norbert Werner</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> To date, Perseus remains the only cluster that has been extensively mapped out to ~0.7$r_{2500}$ by XRISM/Resolve, while simultaneously offering sufficient spatial resolution to resolve gaseous substructures driven by mergers and AGN feedback. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">In this paper, we present extended gas kinematic maps of the Perseus cluster by combining five new XRISM/Resolve pointings observed in 2025 with four Performance Verification datasets from 2024, totaling 745 ks net exposure. To date, Perseus remains the only cluster that has been extensively mapped out to ~0.7$r_{2500}$ by XRISM/Resolve, while simultaneously offering sufficient spatial resolution to resolve gaseous substructures driven by mergers and AGN feedback. Our observations cover multiple radial directions and a broad dynamical range, enabling us to characterize the intracluster medium kinematics up to the scale of ~500 kpc. In the measurements, we detect high velocity dispersions ($\simeq$300 km/s) in the eastern region of the cluster, corresponding to a nonthermal pressure fraction of $\simeq$7-13%. The velocity field outside the AGN-dominant region can be effectively described by a single, large-scale kinematic driver based on the velocity structure function, which statistically favors an energy injection scale of at least a few hundred kpc. The estimated turbulent dissipation energy is comparable to the gravitational potential energy released by a recent merger, implying a significant role of turbulent cascade in the merger energy conversion. In the bulk velocity field, we observe a dipole-like pattern along the east-west direction with an amplitude of $\simeq\pm$200-300 km/s, indicating rotational motions induced by the recent merger event. This feature constrains the viewing direction to ~30$^\circ$-50$^\circ$ relative to the normal of the merger plane. Our hydrodynamic simulations suggest that Perseus has experienced at least two energetic mergers since redshift z~1, the latest associated with the radio galaxy IC310. This study showcases exciting scientific opportunities for future missions with high-resolution spectroscopic capabilities (e.g., HUBS, LEM, and NewAthena).</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.12782" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.12782" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.12782" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.08</span>
                        <span class="badge bg-primary">Semantic Score: 0.91</span>
                        
                            <span class="badge bg-secondary">astro-ph.HE</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">25. Nonparametric Data Attribution for Diffusion Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yutian Zhao, Chao Du, Xiaosen Zheng, Tianyu Pang, Min Lin</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> In addition to producing spatially interpretable attributions, our framework uncovers patterns that reflect intrinsic relationships between training data and outputs, independent of any specific model. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Data attribution for generative models seeks to quantify the influence of individual training examples on model outputs. Existing methods for diffusion models typically require access to model gradients or retraining, limiting their applicability in proprietary or large-scale settings. We propose a nonparametric attribution method that operates entirely on data, measuring influence via patch-level similarity between generated and training images. Our approach is grounded in the analytical form of the optimal score function and naturally extends to multiscale representations, while remaining computationally efficient through convolution-based acceleration. In addition to producing spatially interpretable attributions, our framework uncovers patterns that reflect intrinsic relationships between training data and outputs, independent of any specific model. Experiments demonstrate that our method achieves strong attribution performance, closely matching gradient-based approaches and substantially outperforming existing nonparametric baselines. Code is available at https://github.com/sail-sg/NDA.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.14269" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.14269" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.14269" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">26. Evidence for the dynamical dark energy with evolving Hubble constant
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yi-Ying Wang, Yin-Jie Li, Yi-Zhong Fan</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Future BAO measurements from Euclid and next-generation CMB experiments will provide critical tests of these results and bring deeper insights into the nature of dark energy and the evolution of cosmic expansion. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Hubble constant tension, together with the recent indications of dynamical dark energy proposed from the Dark Energy Spectroscopic Instrument (DESI) baryon acoustic oscillation (BAO) measurements, poses significant challenges to the standard cosmological model. In this work, we perform a model-independent reconstruction of the dark-energy equation of state $w(z)$, jointly with an evolving Hubble constant $H_0(z)$. Using the DESI DR2 data combined with multiple type Ia supernova samples, we find that $w(z)$ varies with redshift and exhibits two potential phantom crossings at $z\sim0.5$ and $z\sim1.5$. Meanwhile, $H_0$ decreases continually from local to high redshift, alleviating the Hubble constant tension effectively. The joint $w(z)$-$H_0(z)$ model is strongly favored over the $w$CDM ($\Lambda$CDM) framework, with a logarithmic Bayes factor $\ln \boldsymbol{\mathcal B}= 5.04~(8.53)$. Across various prior assumptions and dataset combinations, we obtain consistent, data-driven reconstructions of both $w(z)$ and $H_0(z)$. Future BAO measurements from Euclid and next-generation CMB experiments will provide critical tests of these results and bring deeper insights into the nature of dark energy and the evolution of cosmic expansion.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.14390" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.14390" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.14390" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 2</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">27. Simple Projection Variants Improve ColBERT Performance
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Benjamin Clavié, Sean Lee, Rikiya Takehi, Aamir Shakir, Makoto P. Kato</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> In this study, we explore the implications of the MaxSim operator on the gradient flows of the training of multi-vector models and show that such a simple linear projection has inherent, if non-critical, limitations in this setting. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Multi-vector dense retrieval methods like ColBERT systematically use a single-layer linear projection to reduce the dimensionality of individual vectors. In this study, we explore the implications of the MaxSim operator on the gradient flows of the training of multi-vector models and show that such a simple linear projection has inherent, if non-critical, limitations in this setting. We then discuss the theoretical improvements that could result from replacing this single-layer projection with well-studied alternative feedforward linear networks (FFN), such as deeper, non-linear FFN blocks, GLU blocks, and skip-connections, could alleviate these limitations. Through the design and systematic evaluation of alternate projection blocks, we show that better-designed final projections positively impact the downstream performance of ColBERT models. We highlight that many projection variants outperform the original linear projections, with the best-performing variants increasing average performance on a range of retrieval benchmarks across domains by over 2 NDCG@10 points. We then conduct further exploration on the individual parameters of these projections block in order to understand what drives this empirical performance, highlighting the particular importance of upscaled intermediate projections and residual connections. As part of these ablation studies, we show that numerous suboptimal projection variants still outperform the traditional single-layer projection across multiple benchmarks, confirming our hypothesis. Finally, we observe that this effect is consistent across random seeds, further confirming that replacing the linear layer of ColBERT models is a robust, drop-in upgrade.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.12327" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.12327" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.12327" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.IR</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">28. Deep Edge Filter: Return of the Human-Crafted Layer in Deep Learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Dongkwan Lee, Junhoo Lee, Nojun Kwak</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Experimental results across diverse domains such as Vision, Text, 3D, and Audio demonstrate consistent performance improvements regardless of model architecture and data modality. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We introduce the Deep Edge Filter, a novel approach that applies high-pass filtering to deep neural network features to improve model generalizability. Our method is motivated by our hypothesis that neural networks encode task-relevant semantic information in high-frequency components while storing domain-specific biases in low-frequency components of deep features. By subtracting low-pass filtered outputs from original features, our approach isolates generalizable representations while preserving architectural integrity. Experimental results across diverse domains such as Vision, Text, 3D, and Audio demonstrate consistent performance improvements regardless of model architecture and data modality. Analysis reveals that our method induces feature sparsification and effectively isolates high-frequency components, providing empirical validation of our core hypothesis. The code is available at https://github.com/dongkwani/DeepEdgeFilter.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.13865" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.13865" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.13865" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">29. Probabilistic Inference of Cosmological Density Parameters from Synthetic Hubble Expansion Data of Varying SNR Using Diverse Artificial Neural Network Architectures
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zijian Jin, Jaehyon Rhee</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Then, this paper adds realistic noise of high, normal, and low SNR by sampling relative uncertainties from a Gaussian KDE on 47 real data observations compiled by A. Bouali et al. (2023). (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">This paper builds upon ParamANN&#39;s novel approach (S. Pal &amp; R. Saha 2024) of using ANNs to infer cosmological density parameters by determining optimal architecture for varying synthetic Hubble data SNRs in estimating the density parameters $\Omega_{m, 0}$ and $\Omega_{\Lambda, 0}$ across redshift values $z \in [0, 1]$. To generate the synthetic data, this study randomly sampled initial free parameter values at $z=0$ from theoretically motivated priors and evolved them backwards using the first Friedmann Equation to generate clean $H(z)$ curves. Then, this paper adds realistic noise of high, normal, and low SNR by sampling relative uncertainties from a Gaussian KDE on 47 real data observations compiled by A. Bouali et al. (2023). In the end, this study found that a RNN that uses BiLSTM is the most effective for high and normal SNR data across four quantitative metrics. On the other hand, a combination of convolution and recurrent layers that uses GRU performed the best for low SNR data across the same four metrics. A comparison between the results of this paper&#39;s ANN predictions and those of ParamANN shows that all architectures tested in this paper regardless of training SNR are statistically consistent within 1 standard deviation of ParamANN. However, most ANN results are not statistically consistent within 3 standard deviations of Planck Collaboration et al. (2020), showing a significant difference between ANN and the more traditional MCMC methods used by Planck collaboration.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.12865" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.12865" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.12865" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">30. GEFF: The Gradient Expansion Formalism Factory - A tool for inflationary gauge-field production
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Richard von Eckardstein</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The package provides a framework to implement and use the gradient expansion formalism (GEF), a numerical technique devised to study the nonlinear dynamics associated with inflationary gauge-field generation. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The GEFF - the Gradient Expansion Formalism Factory - is a new Python package designed to study gauge-field production during inflation. The package provides a framework to implement and use the gradient expansion formalism (GEF), a numerical technique devised to study the nonlinear dynamics associated with inflationary gauge-field generation. The GEF has already been applied in the context of axion inflation, and with the GEFF package, one can build on these results. The GEFF gives users access to ready-to-use model files for two scenarios of axion inflation: pure axion inflation, with the inflaton coupled to a pure Abelian gauge sector, and fermionic axion inflation, which assumes that the Standard Model (SM) hypercharge field is coupled to the inflaton, resulting in the production SM fermions via the Schwinger effect. The GEFF provides the user with methods to solve GEF equations, including an integrated error estimator and self-correction algorithm. Furthermore, users can implement their own GEF models, e.g., variations of axion inflation or related scenarios. The package also comes with tools to study the production of primordial gravitational waves induced by gauge fields. This is a starting guide for the GEFF, providing a high-level introduction to the GEF, installation instructions, and the basics for using this package.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.12644" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.12644" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.12644" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">31. Predicting the Subhalo Mass Functions in Simulations from Galaxy Images
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Andreas Filipp, Tri Nguyen, Laurence Perreault-Levasseur, Jonah Rose, Chris Lovell, Nicolas Payot, Francisco Villaescusa-Navarro, Yashar Hezaveh</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Strong gravitational lensing provides a powerful tool to directly infer the dark matter (DM) subhalo mass function (SHMF) in lens galaxies. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Strong gravitational lensing provides a powerful tool to directly infer the dark matter (DM) subhalo mass function (SHMF) in lens galaxies. However, comparing observationally inferred SHMFs to theoretical predictions remains challenging, as the predicted SHMF can vary significantly between galaxies - even within the same cosmological model - due to differences in the properties and environment of individual galaxies. We present a machine learning framework to infer the galaxy-specific predicted SHMF from galaxy images, conditioned on the assumed inverse warm DM particle mass $M^{-1}_{\rm DM}$. To train the model, we use 1024 high-resolution hydrodynamical zoom-in simulations from the DREAMS suite. Mock observations are generated using Synthesizer, excluding gas particle contributions, and SHMFs are computed with the Rockstar halo finder. Our neural network takes as input both the galaxy images and the inverse DM mass. This method enables scalable, image-based predictions for the theoretical DM SHMFs of individual galaxies, facilitating direct comparisons with observational measurements.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.14766" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.14766" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.14766" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.04</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">32. Iterative Amortized Inference: Unifying In-Context Learning and Learned Optimizers
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Sarthak Mittal, Divyat Mahajan, Guillaume Lajoie, Mohammad Pezeshki</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> To address this, we propose iterative amortized inference, a class of models that refine solutions step-by-step over mini-batches, drawing inspiration from stochastic optimization. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Modern learning systems increasingly rely on amortized learning - the idea of reusing computation or inductive biases shared across tasks to enable rapid generalization to novel problems. This principle spans a range of approaches, including meta-learning, in-context learning, prompt tuning, learned optimizers and more. While motivated by similar goals, these approaches differ in how they encode and leverage task-specific information, often provided as in-context examples. In this work, we propose a unified framework which describes how such methods differ primarily in the aspects of learning they amortize - such as initializations, learned updates, or predictive mappings - and how they incorporate task data at inference. We introduce a taxonomy that categorizes amortized models into parametric, implicit, and explicit regimes, based on whether task adaptation is externalized, internalized, or jointly modeled. Building on this view, we identify a key limitation in current approaches: most methods struggle to scale to large datasets because their capacity to process task data at inference (e.g., context length) is often limited. To address this, we propose iterative amortized inference, a class of models that refine solutions step-by-step over mini-batches, drawing inspiration from stochastic optimization. Our formulation bridges optimization-based meta-learning with forward-pass amortization in models like LLMs, offering a scalable and extensible foundation for general-purpose task adaptation.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.11471" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.11471" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.11471" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">33. Dynamic SBI: Round-free Sequential Simulation-Based Inference with Adaptive Datasets
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Huifang Lyu, James Alvey, Noemi Anau Montel, Mauro Pieroni, Christoph Weniger</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> This strategy is particularly well suited for high-precision inference in high-dimensional settings, which are commonplace in physics applications with growing data volumes and increasing model fidelity. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Simulation-based inference (SBI) is emerging as a new statistical paradigm for addressing complex scientific inference problems. By leveraging the representational power of deep neural networks, SBI can extract the most informative simulation features for the parameters of interest. Sequential SBI methods extend this approach by iteratively steering the simulation process towards the most relevant regions of parameter space. This is typically implemented through an algorithmic structure, in which simulation and network training alternate over multiple rounds. This strategy is particularly well suited for high-precision inference in high-dimensional settings, which are commonplace in physics applications with growing data volumes and increasing model fidelity. Here, we introduce dynamic SBI, which implements the core ideas of sequential methods in a round-free, asynchronous, and highly parallelisable manner. At its core is an adaptive dataset that is iteratively transformed during inference to resemble the target observation. Simulation and training proceed in parallel: trained networks are used both to filter out simulations incompatible with the data and to propose new, more promising ones. Compared to round-based sequential methods, this asynchronous structure can significantly reduce simulation costs and training overhead. We demonstrate that dynamic SBI achieves significant improvements in simulation and training efficiency while maintaining inference performance. We further validate our framework on two challenging astrophysical inference tasks: characterising the stochastic gravitational wave background and analysing strong gravitational lensing systems. Overall, this work presents a flexible and efficient new paradigm for sequential SBI.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.13997" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.13997" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.13997" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.IM</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">34. Context-Selective State Space Models: Feedback is All You Need
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Riccardo Zattra, Giacomo Baggio, Umberto Casti, Augusto Ferrante, Francesco Ticozzi</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Recent work has shown that state space models (SSMs) provide an efficient alternative, with the S6 module at the core of the Mamba architecture achieving state-of-the-art results on long-sequence benchmarks. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Transformers, powered by the attention mechanism, are the backbone of most foundation models, yet they suffer from quadratic complexity and difficulties in dealing with long-range dependencies in the input sequence. Recent work has shown that state space models (SSMs) provide an efficient alternative, with the S6 module at the core of the Mamba architecture achieving state-of-the-art results on long-sequence benchmarks. In this paper, we introduce the COFFEE (COntext From FEEdback) model, a novel time-varying SSM that incorporates state feedback to enable context-dependent selectivity, while still allowing for parallel implementation. Whereas the selectivity mechanism of S6 only depends on the current input, COFFEE computes it from the internal state, which serves as a compact representation of the sequence history. This shift allows the model to regulate its dynamics based on accumulated context, improving its ability to capture long-range dependencies. In addition to state feedback, we employ an efficient model parametrization that removes redundancies present in S6 and leads to a more compact and trainable formulation. On the induction head task, COFFEE achieves near-perfect accuracy with two orders of magnitude fewer parameters and training sequences compared to S6. On MNIST, COFFEE largely outperforms S6 within the same architecture, reaching 97% accuracy with only 3585 parameters. These results showcase the role of state feedback as a key mechanism for building scalable and efficient sequence models.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.14027" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.14027" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.14027" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">35. Semantic representations emerge in biologically inspired ensembles of cross-supervising neural networks
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Roy Urbach, Elad Schneidman</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Analogously, unsupervised training of artificial neural networks yields internal representations that allow for accurate stimulus classification or decoding, but typically rely on biologically-implausible implementations. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Brains learn to represent information from a large set of stimuli, typically by weak supervision. Unsupervised learning is therefore a natural approach for exploring the design of biological neural networks and their computations. Accordingly, redundancy reduction has been suggested as a prominent design principle of neural encoding, but its ``mechanistic&#39;&#39; biological implementation is unclear. Analogously, unsupervised training of artificial neural networks yields internal representations that allow for accurate stimulus classification or decoding, but typically rely on biologically-implausible implementations. We suggest that interactions between parallel subnetworks in the brain may underlie such learning: we present a model of representation learning by ensembles of neural networks, where each network learns to encode stimuli into an abstract representation space by cross-supervising interactions with other networks, for inputs they receive simultaneously or in close temporal proximity. Aiming for biological plausibility, each network has a small ``receptive field&#39;&#39;, thus receiving a fixed part of the external input, and the networks do not share weights. We find that for different types of network architectures, and for both visual or neuronal stimuli, these cross-supervising networks learn semantic representations that are easily decodable and that decoding accuracy is comparable to supervised networks -- both at the level of single networks and the ensemble. We further show that performance is optimal for small receptive fields, and that sparse connectivity between networks is nearly as accurate as all-to-all interactions, with far fewer computations. We thus suggest a sparsely interacting collective of cross-supervising networks as an algorithmic framework for representational learning and collective computation in the brain.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.14486" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.14486" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.14486" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">q-bio.NC</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">36. Joint Discriminative-Generative Modeling via Dual Adversarial Training
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Xuwang Yin, Claire Zhang, Julie Steele, Nir Shavit, Tony T. Wang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Our approach addresses key stability issues that have limited JEM scaling and demonstrates that adversarial training can serve as an effective foundation for unified frameworks capable of generating and robustly classifying visual data. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Simultaneously achieving robust classification and high-fidelity generative modeling within a single framework presents a significant challenge. Hybrid approaches, such as Joint Energy-Based Models (JEM), interpret classifiers as EBMs but are often limited by the instability and poor sample quality inherent in SGLD-based training. We address these limitations by proposing a novel training framework that integrates adversarial training (AT) principles for both discriminative robustness and stable generative learning. The proposed method introduces three key innovations: (1) the replacement of SGLD-based JEM learning with a stable, AT-based approach that optimizes the energy function by discriminating between real data and PGD-generated contrastive samples using the BCE loss; (2) synergistic adversarial training for the discriminative component that enhances classification robustness while eliminating the need for explicit gradient penalties; and (3) a two-stage training procedure to resolve the incompatibility between batch normalization and EBM training. Experiments on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our method substantially improves adversarial robustness over existing hybrid models while maintaining competitive generative performance. On ImageNet, when optimized for generative modeling, our model&#39;s generative fidelity surpasses that of BigGAN and approaches diffusion models, representing the first MCMC-based EBM approach to achieve high-quality generation on complex, high-resolution datasets. Our approach addresses key stability issues that have limited JEM scaling and demonstrates that adversarial training can serve as an effective foundation for unified frameworks capable of generating and robustly classifying visual data.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.13872" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.13872" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.13872" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">37. Towards Reversible Model Merging For Low-rank Weights
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Mohammadsajad Alipour, Mohammad Mohammadi Amiri</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> While prior work has shown that merging can approximate the performance of individual fine-tuned models for each task, it largely overlooks scenarios where models are compressed into low-rank representations, either through low-rank adaptation (LoRA) or post-training singular value decomposition (SVD). (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Model merging aims to combine multiple fine-tuned models into a single set of weights that performs well across all source tasks. While prior work has shown that merging can approximate the performance of individual fine-tuned models for each task, it largely overlooks scenarios where models are compressed into low-rank representations, either through low-rank adaptation (LoRA) or post-training singular value decomposition (SVD). We first demonstrate that applying conventional merging methods to low-rank weights leads to severe performance degradation in the merged model. Motivated by this phenomenon, we propose a fundamentally different approach: instead of collapsing all adapters into one set of weights, we construct a compact basis (e.g., an equivalent of holding two or more models) from which original task-specific models can be recovered via linear combination. This reframes merging as generating a reconstruction-capable model space rather than producing a single merged model. Crucially, this allows us to ``revert&#39;&#39; to each individual model when needed, recognizing that no merged model can consistently outperform one specialized for its task. Building on this insight, we introduce our method, Reversible Model Merging (RMM), an efficient, data-free, and flexible method that provides a closed-form solution for selecting the optimal basis of model weights and task-specific coefficients for linear combination. Extensive experiments across diverse datasets and model scales demonstrate that RMM consistently outperforms existing merging approaches, preserving the performance of low-rank compressed models by a significant margin.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.14163" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.14163" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.14163" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">38. Using gravitational lensing to probe for bright quintessential galaxies in the Epoch of Reionization
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Joshua Roberson, Matthew B. Bayliss, M. D. Gladders, Gourav Khullar, Keren Sharon, Keunho Kim, Dan Coe, Lindsey Bleem, Taweewat Somboonpanyakul</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The bright sources enable deep constraints on the dropout color which, in combination with flat continua measured in redder bands, require high-z solutions when searching the parameter space. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Understanding the properties of the first generation of galaxies is an ongoing challenge in observational astrophysics. While advances in deep field observation have led to the identification of large numbers of galaxies within the Epoch of Reionization, there are very few observed galaxies at this range that are sufficiently bright for high signal-to-noise spectroscopy. To this end, we analyse HST and ground-based photometry of five candidate strongly lensed galaxies, all projected behind the cores of massive clusters and with similarly red optical-NIR colors. All are characterized by a drop-off in their spectra between the near-infrared and optical wavelengths, corresponding to a Lyman-break that sets a lower bound on their redshifts. Using the open-source SED modeling software Prospector, we characterize two of these galaxies as high-z (z $\sim$ 6.5-7) while the other three are low-z (z $\sim$ 2) despite all five having similar apparent magnitudes at the observed wavelengths. We demonstrate that for the brightest dropout candidates we can distinguish high-z galaxies from red or dusty low-z galaxies using limited photometric data. The bright sources enable deep constraints on the dropout color which, in combination with flat continua measured in redder bands, require high-z solutions when searching the parameter space. At the time of writing this work significantly increases the number of m_AB &lt; 24 galaxies at or above a redshift of 6, and provides a path forward for future analysis on the early era of galaxy formation</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.12097" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.12097" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.12097" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 0 / Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">39. $\texttt{SBi3PCF:}$ Simulation-based inference with the integrated 3PCF
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>David Gebauer, Anik Halder, Stella Seitz, Dhayaa Anbajagane</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Having validated these measurements against theoretical predictions and thoroughly examined for potential systematic biases, we have found that the impact of source galaxy clustering and reduced shear on the i3PCF is negligible for Stage-III surveys. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present $\texttt{SBi3PCF}$, a simulation-based inference (SBI) framework for analysing a higher-order weak lensing statistic, the integrated 3-point correlation function (i3PCF). Our approach forward-models the cosmic shear field using the $\texttt{CosmoGridV1}$ suite of N-body simulations, including a comprehensive set of systematic effects such as intrinsic alignment, baryonic feedback, photometric redshift uncertainty, shear calibration bias, and shape noise. Using this, we have produced a set of DES Y3-like synthetic measurements for 2-point shear correlation functions $\xi_{\pm}$ (2PCFs) and i3PCFs $\zeta_{\pm}$ across 6 cosmological and 11 systematic parameters. Having validated these measurements against theoretical predictions and thoroughly examined for potential systematic biases, we have found that the impact of source galaxy clustering and reduced shear on the i3PCF is negligible for Stage-III surveys. Furthermore, we have tested the Gaussianity assumption for the likelihood of our data vector and found that while the sampling distribution of the 2PCF can be well approximated by a Gaussian function, the likelihood of the combined 2PCF + i3PCF data vector including filter sizes of $90&#39;$ and larger can deviate from this assumption. Our SBI pipeline employs masked autoregressive flows to perform neural likelihood estimation and is validated to give statistically accurate posterior estimates. On mock data, we find that including the i3PCF yields a substantial $63.8\%$ median improvement in the figure of merit for $\Omega_m - \sigma_8 - w_0$. These findings are consistent with previous works on the i3PCF and demonstrate that our SBI framework can achieve the accuracy and realism needed to analyse the i3PCF in wide-area weak lensing surveys.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.13805" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.13805" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.13805" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">40. Sculpting Latent Spaces With MMD: Disentanglement With Programmable Priors
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Quentin Fruytier, Akshay Malhotra, Shahab Hamidi-Rad, Aditya Sant, Aryan Mokhtari, Sujay Sanghavi</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Our framework allows practitioners to explicitly sculpt the latent space, achieving state-of-the-art mutual independence on complex datasets like CIFAR-10 and Tiny ImageNet without the common reconstruction trade-off. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Learning disentangled representations, where distinct factors of variation are captured by independent latent variables, is a central goal in machine learning. The dominant approach has been the Variational Autoencoder (VAE) framework, which uses a Kullback-Leibler (KL) divergence penalty to encourage the latent space to match a factorized Gaussian prior. In this work, however, we provide direct evidence that this KL-based regularizer is an unreliable mechanism, consistently failing to enforce the target distribution on the aggregate posterior. We validate this and quantify the resulting entanglement using our novel, unsupervised Latent Predictability Score (LPS). To address this failure, we introduce the Programmable Prior Framework, a method built on the Maximum Mean Discrepancy (MMD). Our framework allows practitioners to explicitly sculpt the latent space, achieving state-of-the-art mutual independence on complex datasets like CIFAR-10 and Tiny ImageNet without the common reconstruction trade-off. Furthermore, we demonstrate how this programmability can be used to engineer sophisticated priors that improve alignment with semantically meaningful features. Ultimately, our work provides a foundational tool for representation engineering, opening new avenues for model identifiability and causal reasoning.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.11953" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.11953" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.11953" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">41. Deep Attention-guided Adaptive Subsampling
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Sharath M Shankaranarayana, Soumava Kumar Roy, Prasad Sudhakar, Chandan Aladahalli</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> While some works, have proposed solutions using the Gumbel-max trick to overcome the problem of non-differentiability, they fall short in a crucial aspect: they are only task-adaptive and not inputadaptive. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Although deep neural networks have provided impressive gains in performance, these improvements often come at the cost of increased computational complexity and expense. In many cases, such as 3D volume or video classification tasks, not all slices or frames are necessary due to inherent redundancies. To address this issue, we propose a novel learnable subsampling framework that can be integrated into any neural network architecture. Subsampling, being a nondifferentiable operation, poses significant challenges for direct adaptation into deep learning models. While some works, have proposed solutions using the Gumbel-max trick to overcome the problem of non-differentiability, they fall short in a crucial aspect: they are only task-adaptive and not inputadaptive. Once the sampling mechanism is learned, it remains static and does not adjust to different inputs, making it unsuitable for real-world applications. To this end, we propose an attention-guided sampling module that adapts to inputs even during inference. This dynamic adaptation results in performance gains and reduces complexity in deep neural network models. We demonstrate the effectiveness of our method on 3D medical imaging datasets from MedMNIST3D as well as two ultrasound video datasets for classification tasks, one of them being a challenging in-house dataset collected under real-world clinical conditions.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.12376" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.12376" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.12376" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">42. Your VAR Model is Secretly an Efficient and Explainable Generative Classifier
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yi-Chung Chen, David I. Inouye, Jing Gao</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> In particular, due to its tractable likelihood, the VAR-based classifier enables visual explainability via token-wise mutual information and demonstrates inherent resistance to catastrophic forgetting in class-incremental learning tasks. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Generative classifiers, which leverage conditional generative models for classification, have recently demonstrated desirable properties such as robustness to distribution shifts. However, recent progress in this area has been largely driven by diffusion-based models, whose substantial computational cost severely limits scalability. This exclusive focus on diffusion-based methods has also constrained our understanding of generative classifiers. In this work, we propose a novel generative classifier built on recent advances in visual autoregressive (VAR) modeling, which offers a new perspective for studying generative classifiers. To further enhance its performance, we introduce the Adaptive VAR Classifier$^+$ (A-VARC$^+$), which achieves a superior trade-off between accuracy and inference speed, thereby significantly improving practical applicability. Moreover, we show that the VAR-based method exhibits fundamentally different properties from diffusion-based methods. In particular, due to its tractable likelihood, the VAR-based classifier enables visual explainability via token-wise mutual information and demonstrates inherent resistance to catastrophic forgetting in class-incremental learning tasks.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.12060" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.12060" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.12060" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">43. Offline Reinforcement Learning with Generative Trajectory Policies
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Xinsong Feng, Leshu Tang, Chenan Wang, Haipeng Chen</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Generative models have emerged as a powerful class of policies for offline reinforcement learning (RL) due to their ability to capture complex, multi-modal behaviors. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Generative models have emerged as a powerful class of policies for offline reinforcement learning (RL) due to their ability to capture complex, multi-modal behaviors. However, existing methods face a stark trade-off: slow, iterative models like diffusion policies are computationally expensive, while fast, single-step models like consistency policies often suffer from degraded performance. In this paper, we demonstrate that it is possible to bridge this gap. The key to moving beyond the limitations of individual methods, we argue, lies in a unifying perspective that views modern generative models, including diffusion, flow matching, and consistency models, as specific instances of learning a continuous-time generative trajectory governed by an Ordinary Differential Equation (ODE). This principled foundation provides a clearer design space for generative policies in RL and allows us to propose Generative Trajectory Policies (GTPs), a new and more general policy paradigm that learns the entire solution map of the underlying ODE. To make this paradigm practical for offline RL, we further introduce two key theoretically principled adaptations. Empirical results demonstrate that GTP achieves state-of-the-art performance on D4RL benchmarks - it significantly outperforms prior generative policies, achieving perfect scores on several notoriously hard AntMaze tasks.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.11499" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.11499" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.11499" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">44. CoLoR-GAN: Continual Few-Shot Learning with Low-Rank Adaptation in Generative Adversarial Networks
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Munsif Ali, Leonardo Rossi, Massimo Bertozzi</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We demonstrate the effectiveness of CoLoR-GAN through experiments on several benchmark CL and FS tasks and show that our model is efficient, reaching SOTA performance but with a number of resources enormously reduced. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Continual learning (CL) in the context of Generative Adversarial Networks (GANs) remains a challenging problem, particularly when it comes to learn from a few-shot (FS) samples without catastrophic forgetting. Current most effective state-of-the-art (SOTA) methods, like LFS-GAN, introduce a non-negligible quantity of new weights at each training iteration, which would become significant when considering the long term. For this reason, this paper introduces \textcolor{red}{\textbf{\underline{c}}}ontinual few-sh\textcolor{red}{\textbf{\underline{o}}}t learning with \textcolor{red}{\textbf{\underline{lo}}}w-\textcolor{red}{\textbf{\underline{r}}}ank adaptation in GANs named CoLoR-GAN, a framework designed to handle both FS and CL together, leveraging low-rank tensors to efficiently adapt the model to target tasks while reducing even more the number of parameters required. Applying a vanilla LoRA implementation already permitted us to obtain pretty good results. In order to optimize even further the size of the adapters, we challenged LoRA limits introducing a LoRA in LoRA (LLoRA) technique for convolutional layers. Finally, aware of the criticality linked to the choice of the hyperparameters of LoRA, we provide an empirical study to easily find the best ones. We demonstrate the effectiveness of CoLoR-GAN through experiments on several benchmark CL and FS tasks and show that our model is efficient, reaching SOTA performance but with a number of resources enormously reduced. Source code is available on \href{https://github.com/munsifali11/CoLoR-GAN}{Github.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.13869" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.13869" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.13869" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">45. Density reconstruction from biased tracers: Testing the equivalence principle through consistency relations
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Lawrence Dam, Omar Darwish</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> If the EP is violated, different tracers will undergo different accelerations in response to a uniform gravitational field, and this loss of universality manifests as a dipole with a characteristic $1/K$ scale dependence in the squeezed limit of the bispectrum. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Consistency relations of large-scale structure offer a unique and powerful test of the weak equivalence principle (EP) on cosmological scales. If the EP is violated, different tracers will undergo different accelerations in response to a uniform gravitational field, and this loss of universality manifests as a dipole with a characteristic $1/K$ scale dependence in the squeezed limit of the bispectrum. In this work we show that such a violation can be identified with a particular anti-symmetric {modulation} in the local cross-power spectrum of distinct tracers. Based on this observation, we propose to test the EP using quadratic estimators as a more practical alternative to the conventional approach of directly estimating the bispectrum. We apply our quadratic estimator to a DESI-like survey and forecast constraints on the overall amplitude of EP violation. Including mildly nonlinear scales in our reconstruction ($k_\mathrm{max}\simeq0.15\, h\,\mathrm{Mpc}^{-1}$), we find that our estimator is competitive with the more exhaustive direct bispectrum approach. This means surveys like DESI can already benefit from the quadratic estimator approach.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.13803" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.13803" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.13803" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">46. A Free Lunch in LLM Compression: Revisiting Retraining after Pruning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Moritz Wagner, Christophe Roux, Max Zimmer, Sebastian Pokutta</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We conduct an extensive computational study on state-of-the-art GPT architectures, and report several surprising findings that challenge common intuitions about retraining after pruning. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">While Neural Network pruning typically requires retraining the model to recover pruning-induced performance degradation, state-of-the-art Large Language Models (LLMs) pruning methods instead solve a layer-wise mask selection and reconstruction problem on a small set of calibration data to avoid full retraining, as it is considered computationally infeasible for LLMs. Reconstructing single matrices in isolation has favorable properties, such as convexity of the objective and significantly reduced memory requirements compared to full retraining. In practice, however, reconstruction is often implemented at coarser granularities, e.g., reconstructing a whole transformer block against its dense activations instead of a single matrix. In this work, we study the key design choices when reconstructing or retraining the remaining weights after pruning. We conduct an extensive computational study on state-of-the-art GPT architectures, and report several surprising findings that challenge common intuitions about retraining after pruning. In particular, we observe a free lunch scenario: reconstructing attention and MLP components separately within each transformer block is nearly the most resource-efficient yet achieves the best perplexity. Most importantly, this Pareto-optimal setup achieves better performance than full retraining, despite requiring only a fraction of the memory. Furthermore, we demonstrate that simple and efficient pruning criteria such as Wanda can outperform much more complex approaches when the reconstruction step is properly executed, highlighting its importance. Our findings challenge the narrative that retraining should be avoided at all costs and provide important insights into post-pruning performance recovery for LLMs.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.14444" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.14444" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.14444" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1 / Cluster 0</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">47. Hierarchical summaries for primordial non-Gaussianities
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>M. S. Cagliari, A. Bairagi, B. Wandelt</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The advent of Stage IV galaxy redshift surveys such as DESI and Euclid marks the beginning of an era of precision cosmology, with one key objective being the detection of primordial non-Gaussianities (PNG), potential signatures of inflationary physics. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The advent of Stage IV galaxy redshift surveys such as DESI and Euclid marks the beginning of an era of precision cosmology, with one key objective being the detection of primordial non-Gaussianities (PNG), potential signatures of inflationary physics. In particular, constraining the amplitude of local-type PNG, parameterised by $f_{\rm NL}$, with $\sigma_{f_{\rm NL}} \sim 1$, would provide a critical test of single versus multi-field inflation scenarios. While current large-scale structure and cosmic microwave background analyses have achieved $\sigma_{f_{\rm NL}} \sim 5$-$9$, further improvements demand novel data compression strategies. We propose a hybrid estimator that hierarchically combines standard $2$-point and $3$-point statistics with a field-level neural summary, motivated by recent theoretical work that shows that such a combination is nearly optimal, disentangling primordial from late-time non-Gaussianity. We employ PatchNet, a convolutional neural network that extracts small-scale information from sub-volumes (patches) of the halo number density field while large-scale information is retained via the power spectrum and bispectrum. Using Quijote-PNG simulations, we evaluate the Fisher information of this combined estimator across various redshifts, halo mass cuts, and scale cuts. Our results demonstrate that the inclusion of patch-based field-level compression always enhances constraints on $f_{\rm NL}$, reaching gains of $30$-$45\%$ at low $k_{\rm max}$ ($\sim 0.1 \, h \, \text{Mpc}^{-1}$), and capturing information beyond the bispectrum. This approach offers a computationally efficient and scalable pathway to tighten the PNG constraints from forthcoming survey data.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.12715" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.12715" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.12715" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">48. Constranits of dynamical dark energy models from different observational datasets
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Peiyuan Xu, Lu Chen, Guohao Li, Yang Han</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Existing work has achieved fruitful results in the dark energy models, exploring various parameterization forms, but it is relatively scattered and lacks systematic parameter constraints based on the latest dataset combinations. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">The measurements of baryon acoustic oscillation by the Dark Energy Spectroscopic Instrument Data Release 2 indicate that dark energy may be a dynamical quantity with a time-varying equation of state. This challenges the core assumptions of the $\Lambda$CDM model and has generated significant interest in dynamical dark energy models. Therefore, studying the parameterization of the equation of state for dynamical dark energy is crucial. Existing work has achieved fruitful results in the dark energy models, exploring various parameterization forms, but it is relatively scattered and lacks systematic parameter constraints based on the latest dataset combinations. We use the $\Lambda$CDM as a baseline model and carry out rigorous statistical constraints on key cosmological parameters for seven representative parameterization models. Planck PR4 and DESI DR2 observations are incorporated into our study. We use three dataset combinations: CMB+BAO+PantheonPlus, CMB+BAO+DES-Y5, and CMB+BAO+Union3. The ${H}_{0}$ and ${\sigma }_{8}$ values of all dynamical dark energy models are lower than the $\Lambda$CDM model, indicating that our results may not effectively alleviate ${H}_{0}$ tension, but can significantly reduce ${\sigma }_{8}$ tension. By comparing the $\chi^2$ and the Akaike Information Criterion obtained for each model, we demonstrate that the linear Chevallier-Polarski-Linder parameterization model is not the optimal choice in all cases. Specifically, when combined with the CMB+BAO+DES-Y5 dataset, the Barboza-Alcaniz, Logarithmic, and Exponential models demonstrate superior statistical fitting performance compared to the $\Lambda$CDM model. The Barboza-Alcaniz model shows a great advantage in fitting performance, leading to the most significant improvement.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.10439" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.10439" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.10439" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">49. Modeling nonlinear scales for dynamical dark energy cosmologies with COLA
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>João Rebouças, Victoria Lloyd, Jonathan Gordon, Guilherme Brando, Vivian Miranda</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Our results demonstrate that COLA emulators provide a computationally efficient path forward for modeling nonlinear structure in extended cosmologies, offering a practical alternative to full $N$-body suites. (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">Upcoming galaxy surveys will bring a wealth of information about the clustering of matter, but modeling small-scale structure beyond $\Lambda$CDM remains computationally challenging. While accurate $N$-body emulators exist to model the matter power spectrum for $\Lambda$CDM and some limited extensions, it&#39;s unfeasible to generate $N$-body simulation suites for all candidate models. Motivated by recent hints of an evolving dark energy equation of state, we assess the viability of employing the COmoving Lagrangian Acceleration (COLA) method to generate simulation suites for the $w_0w_a$ dark energy model. We combine COLA simulations with an existing high-precision $\Lambda$CDM emulator to extend its predictions into new regions of parameter space. We assess the precision of our emulator at the level of the matter power spectrum, finding that our emulator can reproduce the nonlinear boosts from EuclidEmulator2 at less than $2\%$ error. Moreover, we perform an analysis of a simulated cosmic shear survey akin to the Legacy Survey of Space and Time (LSST) first year of observations, assessing the differences in parameter constraints between our COLA-based emulator and the benchmark emulator. We find our emulator to be in excellent agreement with the benchmark, achieving less than $0.3\sigma$ shifts in cosmological parameters. We compare our emulator&#39;s performance to a commonly used approach: assuming the $\Lambda$CDM boost can be employed for extended parameter spaces without modification. We find that our emulator yields a significantly smaller $\Delta\chi^2$ distribution, parameter constraint biases, and a more accurate figure of merit compared to this second approach. Our results demonstrate that COLA emulators provide a computationally efficient path forward for modeling nonlinear structure in extended cosmologies, offering a practical alternative to full $N$-body suites.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.14888" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.14888" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.14888" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">50. Formation of protostars and the launching of stellar core outflows with moving-mesh radiation non-ideal magnetohydrodynamics
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Alexander C. Mayer, Rüdiger Pakmor, Thorsten Naab, Oliver Zier, Alexei V. Ivlev, Tommaso Grassi, Paola Caselli, Volker Springel</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> We present an implementation of radiative transfer with flux-limited diffusion (FLD) for the moving-mesh code {\small AREPO} and use the method in a physical model for the formation of protostars with non-ideal radiation-magnetohydrodynamics (RMHD). (sumy fallback)</p>
                </div>
                <div class="arxiv-abstract-text">We present an implementation of radiative transfer with flux-limited diffusion (FLD) for the moving-mesh code {\small AREPO} and use the method in a physical model for the formation of protostars with non-ideal radiation-magnetohydrodynamics (RMHD). We follow previous work in splitting the additional terms to the hydrodynamical equations arising from the inclusion of radiation into terms to be integrated explicitly and implicitly, as the diffusion and coupling terms would impose very restrictive timestep criteria. We validate the scheme with standard test problems for radiation diffusion, matter-gas coupling, and radiative shocks from the literature. Our implementation is compatible with local timestepping, which often presents problems for implicit schemes, and we found very good agreement with results obtained with global timesteps. We present an example application of the new implementation to the collapse of a $1\,{\rm M}_\odot$ molecular cloud core to a second Larson core modelled with radiation non-ideal magnetohydrodynamics. A high-velocity jet with v$_{\rm rad}&gt; 10\, {\rm km\,s^{-1}}$ is self-consistently launched from the second core, nested within the first core, which produces a lower-velocity magnetorotational outflow. We observe magnetic field amplification up to more than $\vert \mathbf{B}\vert_{\rm max}&gt;10^5$~G in the second core, which is surrounded by a small ($&lt;0.5$~au) disk. This application demonstrates the robustness of our scheme in multi-scale and high-resolution simulations on arbitrary meshes and, as such, the model can be readily used for further simulations of protostar formation at high resolution.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2510.12620" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2510.12620" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2510.12620" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.13</span>
                        <span class="badge bg-primary">Semantic Score: 0.87</span>
                        
                            <span class="badge bg-secondary">astro-ph.SR</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cluster 1</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
    </div>

    <footer class="footer">
        &copy; 2025 arXiv Daily · Powered by <a href="https://arxiv.org/" target="_blank">arXiv</a>
    </footer>
</body>
</html>