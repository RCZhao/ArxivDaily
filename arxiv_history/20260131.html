<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>arXiv Daily: 2026-01-31</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto:400,700&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Roboto', Arial, sans-serif; background: #f8f9fa; transition: background 0.3s; }
        .arxiv-card { margin: 2em auto; max-width: 900px; box-shadow: 0 2px 8px rgba(0,0,0,0.08); border-radius: 16px; transition: box-shadow 0.3s, transform 0.3s; }
        .arxiv-card:hover { box-shadow: 0 4px 12px rgba(0,0,0,0.1); transform: translateY(-1px); }
        .arxiv-title { background: linear-gradient(90deg, #0d6efd 60%, #6610f2 100%); color: #fff; border-radius: 16px 16px 0 0; padding: 1.5em; font-size: 1.5em; font-weight: 700; letter-spacing: 0.5px;}
        .arxiv-meta { font-size: 1em; color: #555; margin-bottom: 1em; }
        .arxiv-abstract { background: #fff; padding: 1.5em; border-radius: 0 0 16px 16px; font-size: 1.1em; line-height: 1.7;}
        .arxiv-links a { margin-right: 1em; }
        .arxiv-scores { margin-top: 1em; font-size: 1em; }
        .navbar { background: #fff; box-shadow: 0 2px 8px rgba(0,0,0,0.04);}
        .footer { text-align: center; color: #888; padding: 2em 0 1em 0; font-size: 0.95em;}
        @media (prefers-color-scheme: dark) {
            body { background: #181a1b; color: #e4e6eb; }
            .arxiv-card { background: #23272b; box-shadow: 0 2px 8px rgba(0,0,0,0.28);}
            .arxiv-card:hover { box-shadow: 0 5px 15px rgba(0,0,0,0.25); transform: translateY(-1px); }
            .arxiv-title { background: linear-gradient(90deg, #375a7f 60%, #6f42c1 100%);}
            .arxiv-abstract { background: #23272b; color: #e4e6eb;}
            .navbar { background: #23272b; color: #e4e6eb;}
            .text-muted { color: #adb5bd !important; }
        }
        /* Improved abstract readability */
        .arxiv-abstract-text {
            font-size: 1.1em;
            line-height: 1.7;
        }
        p.big {
            line-height: 1.6;
            font-size: large;
        }
    </style>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'] ],
          processEscapes: true
        }
      });
    </script>
    <script>
    function toggleAuthors(element) {
        const fullList = element.querySelector('.author-full');
        const shortList = element.querySelector('.author-short');
        const toggleText = element.querySelector('.author-toggle');
        
        fullList.style.display = fullList.style.display === 'none' ? 'inline' : 'none';
        shortList.style.display = shortList.style.display === 'none' ? 'inline' : 'none';
        toggleText.textContent = fullList.style.display === 'none' ? ' (show all)' : ' (show less)';
    }
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>
<body>
    <nav class="navbar navbar-expand-lg mb-4">
        <div class="container">
            <a class="navbar-brand fw-bold text-primary" href="#">arXiv Daily</a>
            <span class="navbar-text">Modern arXiv Reader</span>
        </div>
    </nav>

    
    <div class="container py-4">
        <header class="mb-4"><h2 class="display-6 text-center">Analysis of Your Favorites</h2></header>
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Word Cloud</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/word_cloud.png" class="img-fluid rounded" alt="Word Cloud of favorite paper topics">
                    </div>
                </div>
            </div>
        </div>
        
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Clusters</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/daily_cluster_map.png" class="img-fluid rounded" alt="2D Cluster plot of favorite papers">
                    </div>
                </div>
            </div>
        </div>
        
    </div>
    

    <div class="container py-4">
        <header class="mb-4"><h1 class="display-5 text-center text-primary">arXiv: 2026-01-31</h1></header>

        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Score Distribution of All Papers Found Today</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/score_distribution.png" class="img-fluid rounded" alt="Score distribution of all papers found today">
                    </div>
                </div>
            </div>
        </div>
        

        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">1. Euclid preparation. Decomposing components of the extragalactic background light using multi-band intensity mapping cross-correlations
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Euclid Collaboration, Y. Cao, A. R. Cooray, T. Li, Y. -T. Cheng, K. Tanidis, S. H. Lim, D. Scott, B. Altieri, A. Amara, et al.</span>
                                <span class="author-full" style="display: none;">Euclid Collaboration, Y. Cao, A. R. Cooray, T. Li, Y. -T. Cheng, K. Tanidis, S. H. Lim, D. Scott, B. Altieri, A. Amara, S. Andreon, N. Auricchio, C. Baccigalupi, M. Baldi, S. Bardelli, A. Biviano, E. Branchini, M. Brescia, S. Camera, G. Cañas-Herrera, V. Capobianco, C. Carbone, J. Carretero, S. Casas, M. Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C. Colodro-Conde, G. Congedo, C. J. Conselice, L. Conversi, Y. Copin, F. Courbin, H. M. Courtois, J. -C. Cuillandre, H. Degaudenzi, G. De Lucia, H. Dole, M. Douspis, F. Dubath, X. Dupac, M. Farina, R. Farinelli, F. Faustini, S. Ferriol, F. Finelli, M. Frailis, E. Franceschi, M. Fumana, S. Galeotta, K. George, B. Gillis, C. Giocoli, J. Gracia-Carpio, A. Grazian, F. Grupp, S. V. H. Haugan, W. Holmes, F. Hormuth, A. Hornstrup, K. Jahnke, M. Jhabvala, B. Joachimi, S. Kermiche, A. Kiessling, B. Kubik, M. Kunz, H. Kurki-Suonio, A. M. C. Le Brun, S. Ligori, P. B. Lilje, V. Lindholm, I. Lloro, G. Mainetti, D. Maino, E. Maiorano, O. Mansutti, S. Marcin, O. Marggraf, M. Martinelli, N. Martinet, F. Marulli, R. J. Massey, E. Medinaceli, S. Mei, Y. Mellier, M. Meneghetti, E. Merlin, G. Meylan, A. Mora, M. Moresco, L. Moscardini, C. Neissner, S. -M. Niemi, C. Padilla, S. Paltani, F. Pasian, K. Pedersen, W. J. Percival, V. Pettorino, G. Polenta, M. Poncet, L. A. Popa, F. Raison, A. Renzi, J. Rhodes, G. Riccio, E. Romelli, M. Roncarelli, R. Saglia, Z. Sakr, D. Sapone, P. Schneider, T. Schrabback, A. Secroun, G. Seidel, S. Serrano, E. Sihvola, C. Sirignano, G. Sirri, L. Stanco, J. Steinwagner, P. Tallada-Crespí, I. Tereno, N. Tessore, S. Toft, R. Toledo-Moreo, F. Torradeflot, I. Tutusaus, L. Valenziano, J. Valiviita, T. Vassallo, Y. Wang, J. Weller, G. Zamorani, F. M. Zerbi, E. Zucca, M. Ballardini, E. Bozzo, C. Burigana, R. Cabanac, M. Calabrese, A. Cappi, T. Castro, J. A. Escartin Vigo, L. Gabarra, J. Macias-Perez, R. Maoli, J. Martín-Fleitas, N. Mauri, R. B. Metcalf, P. Monaco, A. A. Nucita, A. Pezzotta, M. Pöntinen, I. Risso, V. Scottez, M. Sereno, M. Tenti, M. Tucci, M. Viel, M. Wiesmann, Y. Akrami, I. T. Andika, G. Angora, S. Anselmi, M. Archidiacono, E. Aubourg, L. Bazzanini, D. Bertacca, M. Bethermin, A. Blanchard, L. Blot, M. Bonici, S. Borgani, M. L. Brown, S. Bruton, A. Calabro, B. Camacho Quevedo, F. Caro, C. S. Carvalho, F. Cogato, S. Conseil, O. Cucciati, S. Davini, G. Desprez, A. Díaz-Sánchez, S. Di Domizio, J. M. Diego, V. Duret, M. Y. Elkhashab, A. Enia, A. Finoguenov, A. Fontana, A. Franco, K. Ganga, T. Gasparetto, E. Gaztanaga, F. Giacomini, F. Gianotti, G. Gozaliasl, A. Gruppuso, M. Guidi, C. M. Gutierrez, A. Hall, C. Hernández-Monteagudo, H. Hildebrandt, J. Hjorth, J. J. E. Kajava, Y. Kang, V. Kansal, D. Karagiannis, K. Kiiveri, J. Kim, C. C. Kirkpatrick, S. Kruk, M. Lattanzi, L. Legrand, F. Lepori, G. Leroy, G. F. Lesci, J. Lesgourgues, T. I. Liaudat, S. J. Liu, M. Magliocchetti, F. Mannucci, C. J. A. P. Martins, L. Maurin, M. Miluzio, C. Moretti, G. Morgante, K. Naidoo, P. Natoli, A. Navarro-Alsina, S. Nesseris, L. Pagano, D. Paoletti, F. Passalacqua, K. Paterson, L. Patrizii, A. Pisani, D. Potter, G. W. Pratt, S. Quai, M. Radovich, G. Rodighiero, K. Rojas, W. Roster, S. Sacquegna, M. Sahlén, D. B. Sanders, E. Sarpa, C. Scarlata, A. Schneider, D. Sciotti, E. Sellentin, L. C. Smith, J. G. Sorce, F. Tarsitano, G. Testera, R. Teyssier, S. Tosi, A. Troja, A. Venhola, D. Vergani, G. Verza, S. Vinciguerra, N. A. Walton, J. R. Weaver, A. H. Wright</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A joint halo-model framework integrating multi-band intensity mapping with cosmic shear and galaxy clustering effectively decomposes the extragalactic background light, significantly tightening constraints on intra-halo light parameters and star-formation rate density up to $z\sim 11$. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The extragalactic background light (EBL) fluctuations in the optical/near-IR encode the cumulative integrated galaxy light (IGL), diffuse intra-halo light (IHL), and high-$z$ sources from the epoch of reionisation (EoR), but they are difficult to disentangle with auto-spectra alone. We aim to decompose the EBL into its principal constituents using multi-band intensity mapping combined with cosmic shear and galaxy clustering. We develop a joint halo-model framework in which IHL follows a mass- and redshift-dependent luminosity scaling, IGL is set by an evolving Schechter luminosity function, and EoR emission is modelled with Pop II/III stellar emissivities and a binned star-formation efficiency. Using mock surveys in a flat $Λ$CDM cosmology with ten spectral bands spanning 0.75-5.0$\rm μm$ in the NEP deep fields over about 100$°^2$ with source detections down to AB=20.5 for masking, and six redshift bins to $z=2.5$, we fit auto- and cross-power spectra using a MCMC method. The combined SPHEREx$\times$Euclid analysis recovers all fiducial parameters within 1$σ$ and reduces 1$σ$ uncertainties on IHL parameters by 10-35% relative to SPHEREx EBL-only, while EoR star-formation efficiency parameters improve by 20-35%. Cross-correlations reveal a stronger coupling of IHL than IGL to the shear field, enhancing component separation; conversely, the EoR contribution shows negligible correlation with cosmic shear and galaxy clustering, aiding its isolation in the EBL. Relative to the SPHEREx EBL-only case, the inferred IHL fraction as a function of halo mass is significantly tightened over $10^{11}-10^{14} M_{\odot}$, with uncertainties reduced by 5-30%, and the resulting star-formation rate density constraints extend to $z\sim 11$, with uncertainty reductions of 22-31%.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21111" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21111" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21111" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 14.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.97</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmic Shear, Survey Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">2. Euclid: Early Release Observations -- The star-formation history of massive early-type galaxies in the Perseus cluster
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">S. Martocchia, A. Boselli, J. -C. Cuillandre, M. Mondelin, M. Bolzonella, C. Tortora, M. Fossati, C. Maraston, P. Amram, M. Baes, et al.</span>
                                <span class="author-full" style="display: none;">S. Martocchia, A. Boselli, J. -C. Cuillandre, M. Mondelin, M. Bolzonella, C. Tortora, M. Fossati, C. Maraston, P. Amram, M. Baes, S. Boissier, M. Boquien, H. Bouy, F. Durret, C. M. Gutierrez, M. Kluge, Y. Roehlly, T. Saifollahi, M. A. Taylor, D. Thomas, T. E. Woods, G. Zamorani, B. Altieri, S. Andreon, N. Auricchio, C. Baccigalupi, M. Baldi, S. Bardelli, P. Battaglia, R. Bender, A. Biviano, E. Branchini, M. Brescia, S. Camera, G. Cañas-Herrera, V. Capobianco, C. Carbone, J. Carretero, M. Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C. Colodro-Conde, G. Congedo, C. J. Conselice, L. Conversi, Y. Copin, A. Costille, F. Courbin, H. M. Courtois, H. Degaudenzi, G. De Lucia, F. Dubath, X. Dupac, S. Escoffier, M. Fabricius, M. Farina, R. Farinelli, F. Faustini, S. Ferriol, F. Finelli, M. Frailis, E. Franceschi, P. Franzetti, M. Fumana, S. Galeotta, K. George, B. Gillis, C. Giocoli, P. Gómez-Alvarez, J. Gracia-Carpio, A. Grazian, F. Grupp, S. V. H. Haugan, W. Holmes, I. M. Hook, F. Hormuth, A. Hornstrup, K. Jahnke, M. Jhabvala, B. Joachimi, S. Kermiche, A. Kiessling, B. Kubik, M. Kümmel, H. Kurki-Suonio, A. M. C. Le Brun, D. Le Mignant, S. Ligori, P. B. Lilje, V. Lindholm, I. Lloro, G. Mainetti, D. Maino, O. Mansutti, O. Marggraf, M. Martinelli, N. Martinet, F. Marulli, R. J. Massey, E. Medinaceli, Y. Mellier, M. Meneghetti, E. Merlin, G. Meylan, A. Mora, L. Moscardini, C. Neissner, S. -M. Niemi, C. Padilla, S. Paltani, F. Pasian, K. Pedersen, W. J. Percival, V. Pettorino, S. Pires, G. Polenta, M. Poncet, L. A. Popa, L. Pozzetti, A. Renzi, J. Rhodes, G. Riccio, E. Romelli, M. Roncarelli, R. Saglia, Z. Sakr, A. G. Sánchez, D. Sapone, B. Sartoris, P. Schneider, A. Secroun, G. Seidel, S. Serrano, E. Sihvola, P. Simon, C. Sirignano, G. Sirri, J. Steinwagner, P. Tallada-Crespí, A. N. Taylor, I. Tereno, N. Tessore, S. Toft, R. Toledo-Moreo, F. Torradeflot, I. Tutusaus, L. Valenziano, J. Valiviita, T. Vassallo, A. Veropalumbo, Y. Wang, J. Weller, I. A. Zinchenko, E. Zucca, J. García-Bellido, J. Martín-Fleitas, M. Maturi, V. Scottez</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Analyzing early-type galaxies in the Euclid ERO-Perseus cluster via FUV-to-NIR SED fitting confirms rapid stellar mass assembly ($\tau \lesssim 1500$ Myr) and supports the hypothesis that massive nearby ETGs are the descendants of high-redshift red quiescent galaxies seen by JWST. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The Euclid Early Release Observations (ERO) programme targeted the Perseus galaxy cluster in its central region over 0.7deg$^2$. We combined the exceptional image quality and depth of the ERO-Perseus with FUV and NUV observations from GALEX and AstroSat/UVIT, as well as $ugrizHα$ data from MegaCam at the CFHT, to deliver FUV-to-NIR magnitudes of the 87 brightest galaxies within the Perseus cluster. We reconstructed the star-formation history (SFH) of 59 early-type galaxies (ETGs) within the sample, through the spectral energy distribution (SED) fitting code CIGALE and state-of-the-art stellar population (SP) models to reproduce the galactic UV emission from hot, old, low-mass stars (i.e. the UV upturn). In addition, for the six most massive ETGs in Perseus [stellar masses $\log_{10}(M_{\ast}/M_{\odot}) \geq 10.3$], we analysed their spatially resolved SP through a radial SED fitting. In agreement with our previous work on Virgo ETGs, we found that (i) the majority of ETGs needs the presence of an UV upturn to explain their FUV emission, with temperatures $\langle T_{\rm UV}\rangle$~33800 K; (ii) ETGs have grown their stellar masses quickly, with SF timescales $τ\lesssim 1500$ Myr. We found that all ETGs in the sample have formed more than about 30% of their stellar masses at z~5, up to ~100%. At z~5, the stellar masses of the most massive nearby ETGs, which have present-day stellar masses $\log_{10}(M_{\ast}/M_{\odot})\gtrsim 10.8$, are then found to be comparable to those of the red quiescent galaxies observed by JWST at similar redshifts (z&gt;4.6). This study can be extended to ETGs in the 14000 deg$^2$ extragalactic sky that will soon be observed by Euclid, in combination with those from other major upcoming surveys (e.g. Rubin/LSST), and UV observations, to ultimately assess whether the nearby massive ETGs represent the progeny of the massive high-z JWST red quiescent galaxies.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.20948" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.20948" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.20948" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 13.9</span>
                        <span class="badge bg-info text-dark">Author Score: 0.94</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmic Shear, Survey Cosmology / Galaxy Clustering, Weak Lensing, Halo Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">3. Cosmological analysis of the DESI DR1 Lyman alpha 1D power spectrum
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">J. Chaves-Montero, A. Font-Ribera, P. McDonald, E. Armengaud, D. Chebat, C. Garcia-Quintero, N. G. Karaçaylı, C. Ravoux, S. Satyavolu, N. Schöneberg, et al.</span>
                                <span class="author-full" style="display: none;">J. Chaves-Montero, A. Font-Ribera, P. McDonald, E. Armengaud, D. Chebat, C. Garcia-Quintero, N. G. Karaçaylı, C. Ravoux, S. Satyavolu, N. Schöneberg, M. Walther, J. Aguilar, S. Ahlen, S. Bailey, D. Bianchi, D. Brooks, T. Claybaugh, A. Cuceu, A. de la Macorra, P. Doel, S. Ferraro, J. E. Forero-Romero, E. Gaztañaga, S. Gontcho A Gontcho, A. X. Gonzalez-Morales, G. Gutierrez, J. Guy, C. Hahn, H. K. Herrera-Alcantar, K. Honscheid, M. Ishak, R. Joyce, S. Juneau, D. Kirkby, A. Kremin, O. Lahav, C. Lamman, M. Landriau, J. M. Le Goff, L. Le Guillou, A. Leauthaud, M. E. Levi, M. Manera, P. Martini, A. Meisner, R. Miquel, J. Moustakas, S. Nadathur, G. Niz, N. Palanque-Delabrouille, W. J. Percival, F. Prada, I. Pérez-Ràfols, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, H. Seo, J. Silber, D. Sprayberry, T. Tan, G. Tarlé, B. A. Weaver, C. Yèche, R. Zhou, H. Zou</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Cosmological analysis of the DESI Lyman-$\alpha$ flux power spectrum, leveraging an emulator trained on hydrodynamical simulations, provides robust constraints on the linear matter power spectrum at $z=3$ and sharpens limits on $N_{\mathrm{eff}}$ and spectral index running when combined with other cosmological probes. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present the cosmological analysis of the one-dimensional Lyman-$α$ flux power spectrum from the first data release of the Dark Energy Spectroscopic Instrument (DESI). We capture the dependence of the signal on cosmology and intergalactic medium physics using an emulator trained on a cosmological suite of hydrodynamical simulations, and we correct its predictions for the impact of astrophysical contaminants and systematics, many of these not considered in previous analyses. We employ this framework to constrain the amplitude and logarithmic slope of the linear matter power spectrum at $k_\star=0.009\,\mathrm{km^{-1}s}$ and redshift $z=3$, obtaining $Δ^2_\star=0.379\pm0.032$ and $n_\star=-2.309\pm0.019$. The robustness of these constraints is validated through the analysis of mocks and a large number of alternative data analysis variations, with cosmological parameters kept blinded throughout the validation process. We then combine our results with constraints from DESI BAO and temperature, polarization, and lensing measurements from Planck, ACT, and SPT-3G to set constraints on $Λ$CDM extensions. While our measurements do not significantly tighten the limits on the sum of neutrino masses from the combination of these probes, they sharpen the constraints on the effective number of relativistic species, $N_\mathrm{eff}=3.02\pm0.10$, the running of the spectral index, $α_\mathrm{s}=0.0014\pm0.0041$, and the running of the running, $β_\mathrm{s}=-0.0006\pm0.0048$, by a factor of 1.18, 1.27, and 1.90, respectively. We conclude by outlining the improvements needed to fully reach the level of confidence implied by these uncertainties.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21432" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21432" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21432" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.20</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmic Shear, Survey Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">4. Cross-correlating galaxies and cosmic dispersion measures: Constraints on the gas-to-halo mass relation from 2MASS galaxies and 133 localized fast radio bursts
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Masato Shirasaki, Ryuichi Takahashi, Ken Osato, Kunihito Ioka</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Cross-correlation measurements between 2MASS galaxies and fast radio burst dispersion measures are inconsistent with IllustrisTNG-300 predictions, suggesting that the hot-gas mass fraction in $10^{12-13}\, M_\odot$ halos is below $\sim 10\%$ of the global baryon fraction, thereby constraining feedback mechanisms in galaxy formation. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We conduct a cross-correlation analysis between large-scale structures traced by the Two Micron All Sky Survey (2MASS) galaxy catalog and the cosmic dispersion measures of 133 localized fast radio bursts (FRBs). The cross-correlation signal is measured as a function of the comoving separation $R$ between 2MASS galaxies and background FRB sightlines, making full use of the available redshift information for both datasets. Our measurements are consistent with a null detection over the range $0.01 &lt; R\, [h^{-1}\mathrm{Mpc}] &lt; 1$. Using a halo-based model in which free-electron density profiles are drawn from the hydrodynamical simulation IllustrisTNG-300 (TNG300), we show that the null signal at $R \sim 0.01\, h^{-1}\mathrm{Mpc}$ is inconsistent with the TNG300 prediction. This discrepancy indicates that the hot-gas mass fraction in halos with masses of $10^{12-13}\, M_\odot$ hosting 2MASS galaxies must be lower than that predicted by TNG300. A simple phenomenological modification of the TNG300 model suggests that the hot-gas mass fraction in halos of $10^{12-13}\, M_\odot$ should be below $\sim 10\%$ of the global baryon fraction in the nearby universe, implying the need for stronger feedback in this mass range. Our constraints are consistent with those inferred from X-ray emission and Sunyaev-Zel&#39;dovich measurements in galaxies, while providing a direct estimate of the hot-gas mass fraction that does not rely on electron-temperature measurements. These results demonstrate that galaxy-FRB cross correlations offer a powerful probe of feedback processes in galaxy formation.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21336" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21336" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21336" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.8</span>
                        <span class="badge bg-info text-dark">Author Score: 0.06</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmic Shear, Survey Cosmology / Galaxy Clustering, Weak Lensing, Halo Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">5. Bayesian-LoRA: Probabilistic Low-Rank Adaptation of Large Language Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Moule Lin, Shuhao Guan, Andrea Patane, David Gregg, Goetz Botterweck</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Bayesian-LoRA reformulates LoRA updates as a probabilistic low-rank representation, inspired by Sparse Gaussian Processes, achieving significant improvements in Large Language Model calibration (up to 84% ECE reduction) across various architectures and reasoning benchmarks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Large Language Models usually put more emphasis on accuracy and therefore, will guess even when not certain about the prediction, which is especially severe when fine-tuned on small datasets due to the inherent tendency toward miscalibration. In this work, we introduce Bayesian-LoRA, which reformulates the deterministic LoRA update as a probabilistic low-rank representation inspired by Sparse Gaussian Processes. We identify a structural isomorphism between LoRA&#39;s factorization and Kronecker-factored SGP posteriors, and show that LoRA emerges as a limiting case when posterior uncertainty collapses. We conduct extensive experiments on various LLM architectures across commonsense reasoning benchmarks. With only approximately 0.42M additional parameters and ${\approx}1.2{\times}$ training cost relative to standard LoRA, Bayesian-LoRA significantly improves calibration across models up to 30B, achieving up to 84% ECE reduction and 76% NLL reduction while maintaining competitive accuracy for both in-distribution and out-of-distribution (OoD) evaluations.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21003" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21003" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21003" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">6. A Diffusive Classification Loss for Learning Energy-based Generative Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Louis Grenioux, RuiKang OuYang, José Miguel Hernández-Lobato</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Diffusive Classification (DiffCLF) introduces an efficient, non-mode-blind objective that reframes Energy-Based Model learning as a supervised classification problem across noise levels, resulting in higher fidelity models applicable to compositional sampling and Boltzmann Generators. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Score-based generative models have recently achieved remarkable success. While they are usually parameterized by the score, an alternative way is to use a series of time-dependent energy-based models (EBMs), where the score is obtained from the negative input-gradient of the energy. Crucially, EBMs can be leveraged not only for generation, but also for tasks such as compositional sampling or building Boltzmann Generators via Monte Carlo methods. However, training EBMs remains challenging. Direct maximum likelihood is computationally prohibitive due to the need for nested sampling, while score matching, though efficient, suffers from mode blindness. To address these issues, we introduce the Diffusive Classification (DiffCLF) objective, a simple method that avoids blindness while remaining computationally efficient. DiffCLF reframes EBM learning as a supervised classification problem across noise levels, and can be seamlessly combined with standard score-based objectives. We validate the effectiveness of DiffCLF by comparing the estimated energies against ground truth in analytical Gaussian mixture cases, and by applying the trained models to tasks such as model composition and Boltzmann Generator sampling. Our results show that DiffCLF enables EBMs with higher fidelity and broader applicability than existing approaches.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21025" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21025" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21025" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">7. Leveraging rapid parameter estimates for efficient gravitational-wave Bayesian inference via posterior repartitioning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Metha Prathaban, Charlie Hoy, Michael J. Williams</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Leveraging low-latency constraints with posterior repartitioning accelerates gravitational wave Bayesian inference, providing statistically rigorous speedups up to $2.2\times$ for high signal-to-noise ratio binaries without compromising the unbiased nature of the final analysis. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Gravitational wave astronomy typically relies on rigorous, computationally expensive Bayesian analyses. Several methods have been developed to perform rapid Bayesian inference, but they are not yet used to inform our full analyses. We present a novel approach for doing this whilst ensuring that the Bayesian prior remains independent of the data, providing a statistically rigorous way to leverage low-latency information to accelerate the final inference. By combining the fast constraints from the simple-pe algorithm with the nested sampling acceleration technique of posterior repartitioning, we demonstrate that our method can guide the nested sampler towards the most probable regions of parameter space more efficiently for signal-to-noise ratios (SNR) greater than 20, while mathematically guaranteeing that the final inference is identical to that of a standard, uninformed analysis. We validate the method through an injection study, demonstrating that it produces statistically robust and unbiased results, whilst providing speedups of up to $2.2\times$ for binaries with SNRs $&lt; 150$. Importantly, we show that the performance gain provided by our method scales with SNR, establishing it as a powerful technique to mitigate the cost of analysing signals from current and future gravitational-wave observatories.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21630" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21630" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21630" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">8. Disk Wind Feedback from High-mass Protostars. V. Application of Multi-Modal Machine Learning to Characterize Outflow Properties
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Duo Xu, Ioana A. Stelea, Joshua S. Speagle, Yichen Zhang, Jonathan C. Tan</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A multi-modal deep learning framework uses cross-attention fusion to jointly analyze spatial and spectral CO observations, accurately inferring protostellar outflow mass, inclination, and position angle, thereby providing an interpretable method to mitigate projection biases in high-mass star formation studies. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Characterizing protostellar outflows is fundamental to understanding star formation feedback, yet traditional methods are often hindered by projection effects and complex morphologies. We present a multi-modal deep learning framework that jointly leverages spatial and spectral information from CO observations to infer outflow mass, inclination, and position angle ($PA$). Our model, trained on synthetic ALMA observations generated from 3D magnetohydrodynamic simulations, utilizes a cross-attention fusion mechanism to integrate morphological and kinematic features with probabilistic uncertainty estimation. Our results demonstrate that Vision Transformer architectures significantly outperform convolutional networks, showing remarkable robustness to reduced spatial resolution. Interpretability analysis reveals a physically consistent hierarchy: spatial features dominate across all parameters, whereas spectral profiles provide secondary constraints for mass and inclination. Applied to observational ALMA data, the framework delivers stable mass and $PA$ estimates with exceptionally tightly constrained inclination angles. This study establishes multi-modal deep learning as a powerful, interpretable tool for overcoming projection biases in high-mass star formation studies.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21100" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21100" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21100" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.04</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">9. Multilevel and Sequential Monte Carlo for Training-Free Diffusion Guidance
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Aidan Gleich, Scott C. Schmidler</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Training-free guidance for conditional diffusion models is improved by a sequential Monte Carlo framework that uses Multi-Level Monte Carlo variance reduction to construct an unbiased posterior score estimator, achieving state-of-the-art accuracy with substantially reduced computational cost. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We address the problem of accurate, training-free guidance for conditional generation in trained diffusion models. Existing methods typically rely on point-estimates to approximate the posterior score, often resulting in biased approximations that fail to capture multimodality inherent to the reverse process of diffusion models. We propose a sequential Monte Carlo (SMC) framework that constructs an unbiased estimator of $p_θ(y|x_t)$ by integrating over the full denoising distribution via Monte Carlo approximation. To ensure computational tractability, we incorporate variance-reduction schemes based on Multi-Level Monte Carlo (MLMC). Our approach achieves new state-of-the-art results for training-free guidance on CIFAR-10 class-conditional generation, achieving $95.6\%$ accuracy with $3\times$ lower cost-per-success than baselines. On ImageNet, our algorithm achieves $1.5\times$ cost-per-success advantage over existing methods.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21104" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21104" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21104" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">10. Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Grzegorz Stefanski, Alberto Presta, Michal Byra</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Routing the Lottery (RTL) proposes an adaptive pruning framework that discovers specialized subnetworks tailored to data heterogeneity, achieving superior balanced accuracy and recall with significantly fewer parameters than independent models, and enabling diagnosis of subnetwork collapse via a similarity score. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.22141" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.22141" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.22141" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">11. It&#39;s More Complicated Than You Think: A Forward Model to Infer the Recent Star Formation History, Bursty or Not, of Galaxy Populations
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Emilie Burnham, Bingjie Wang, Joel Leja, Owen Gonzales, Jenny E. Greene, Kartheik G. Iyer, Abby Mintz, David J. Setton, Sarah Wellons, Rachel Bezanson, et al.</span>
                                <span class="author-full" style="display: none;">Emilie Burnham, Bingjie Wang, Joel Leja, Owen Gonzales, Jenny E. Greene, Kartheik G. Iyer, Abby Mintz, David J. Setton, Sarah Wellons, Rachel Bezanson, Olivia Curtis, Robert Feldmann, Tim B. Miller, Themiya Nanayakkara, Joshua S. Speagle, Katherine A. Suess, Guochao Sun</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A novel population-level simulation-based inference framework accurately constrains the power and timescales of stochastic star formation rate fluctuations in early galaxies observed by JWST, effectively distinguishing between cosmological models. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Observations of the early Universe (z &gt; 4) with the James Webb Space Telescope reveal galaxy populations with a wide range of intrinsic luminosities and colors. Bursty star formation histories (SFHs), characterized by short-term fluctuations in the star formation rate (SFR), may explain this diversity, but constraining burst timescales and amplitudes in individual galaxies is challenging due to degeneracies and sensitivity limits. We introduce a population-level simulation-based inference framework that recovers the power and timescales of SFR fluctuations by forward-modeling galaxy populations and distributions of rest-UV to rest-optical spectral features sensitive to star formation timescales. We adopt a stochastic SFH model based on a power spectral density formalism spanning 1 Myr-10 Gyr. Using simulated samples of N=500 galaxies at z~4 with typical JWST/NIRSpec uncertainties, we demonstrate that: (i) the power of SFR fluctuations can be measured with sufficient precision to distinguish between simulations (e.g., FIRE-2-like vs. Illustris-like populations at &gt;99% confidence for timescales &lt; 100 Myr); (ii) simultaneously modeling stochastic fluctuations and the recent (t_L &lt; 500 Myr) average SFH slope is essential, as secular trends otherwise mimic burstiness in common diagnostics; (iii) frequent, intense bursts impose an outshining limit, and bias inference toward underestimating burstiness due to the obscuration of long-timescale power; and (iv) the power of SFR fluctuations can be inferred to 95% confidence across all timescales in both smooth and bursty populations. This framework establishes a novel and robust method for placing quantitative constraints on the feedback physics regulating star formation using large, uniformly selected spectroscopic samples.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.20930" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.20930" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.20930" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.04</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">12. Sparsity for isotropic spherical random fields
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Giacomo Greco, Domenico Marinucci</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A new, simple representation for isotropic spherical random fields enables the construction of computationally efficient sparse approximations that accurately reproduce the angular power spectrum and polyspectra of complex non-Gaussian fields. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We introduce a simple representation for isotropic spherical random fields and we discuss how it allows to discuss different notions of sparsity under isotropy. We also show how a suitable construction of sparse fields can mimic well the angular power spectrum and the polyspectra of some popular non-Gaussian fields, at the same time allowing for computationally efficient simulation algorithms. Using related ideas we also show how it is possible to obtain sparse approximations of spherical random fields which preserve isotropy, thus addressing an issue which has been raised in the Cosmological literature.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21535" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21535" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21535" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">math.PR</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">13. PERTURB-c: Correlation Aware Perturbation Explainability for Regression Techniques to Understand Retrieval Black-boxes
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jools D. Clarke, Gordon Yip, Nikolaos Nikolaou</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The PERTURB-c framework offers a correlation-aware, computationally efficient method for interpreting black-box regression models, successfully addressing input interaction issues in machine learning-based exoplanet transit spectroscopy retrievals. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">In this paper we introduce PERTURB-c, a correlation-aware framework for interpreting black box regression models with one-dimensional structured inputs. We demonstrate this framework on a simulated case study with machine learning based transit spectroscopy retrievals of exoplanet WASP-107b. Characterising many exoplanet atmospheres can answer important questions about planetary populations, but traditional retrievals are very resource intensive; machine learning based methods offer a fast alternative however (i) they require high volumes data (only obtainable through simulations) to train and (ii) their complexity renders them black-boxes. Better understanding how they reach predictions can allow us to inspect for biases, which is especially important with simulated data, and verify that predictions are made on the basis of physically plausible features. This ultimately improves the ease of adoption of machine learning techniques. The most used methods to explain machine learning model predictions (such as SHAP and other methods that rely on stochastic sample generation) suffer from high computational complexity and struggle to account for interactions between inputs. PERTURB-c addresses these issues by leveraging physical knowledge of the known spectral correlation. For visualisation of this analysis, we propose a heat-map-based representation which is better suited to large numbers of input features along a single dimension, and that is more intuitive to those who are already familiar with retrieval methods. Note that while we chose this exoplanet retrieval context to demonstrate our methodologies, the PERTURB-c framework is model agnostic and in a broader context has potential value across a plethora of adjacent regression problems.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21685" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21685" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21685" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.EP</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">14. Do Reasoning Models Enhance Embedding Models?
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Wun Yu Chan, Shaojin Chen, Huihao Jing, Kwun Hang Lau, Elton Chun-Chai Li, Zihao Wang, Haoran Li, Yangqiu Song</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Reinforcement Learning with Verifiable Rewards (RLVR) tuning provides no consistent performance advantage for downstream embedding models because subsequent contrastive learning induces &#34;Manifold Realignment,&#34; aligning the latent spaces of RLVR-tuned and base models. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold&#39;s local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21192" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21192" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21192" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">15. MAR: Efficient Large Language Models via Module-aware Architecture Refinement
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Junhong Cai, Guiqin Wang, Kejie Zhao, Jianxiong Tang, Xiang Wang, Luziwei Leng, Ran Cheng, Yuxin Ma, Qinghai Guo</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The Module-aware Architecture Refinement (MAR) framework integrates State Space Models and activation sparsification, utilizing novel spiking neural network components to substantially reduce the inference energy consumption of Large Language Models while preserving performance. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Large Language Models (LLMs) excel across diverse domains but suffer from high energy costs due to quadratic attention and dense Feed-Forward Network (FFN) operations. To address these issues, we propose Module-aware Architecture Refinement (MAR), a two-stage framework that integrates State Space Models (SSMs) for linear-time sequence modeling and applies activation sparsification to reduce FFN costs. In addition, to mitigate low information density and temporal mismatch in integrating Spiking Neural Networks (SNNs) with SSMs, we design the Adaptive Ternary Multi-step Neuron (ATMN) and the Spike-aware Bidirectional Distillation Strategy (SBDS). Extensive experiments demonstrate that MAR effectively restores the performance of its dense counterpart under constrained resources while substantially reducing inference energy consumption. Furthermore, it outperforms efficient models of comparable or even larger scale, underscoring its potential for building efficient and practical LLMs.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21503" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21503" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21503" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">16. Statsformer: Validated Ensemble Learning with LLM-Derived Semantic Priors
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Erica Zhang, Naomi Sagan, Danny Tse, Fangzhao Zhang, Mert Pilanci, Jose Blanchet</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Statsformer, a guardrailed ensemble architecture, integrates Large Language Model knowledge into supervised statistical learning by adaptively weighting LLM-derived feature priors, ensuring performance gains while automatically mitigating the negative impact of potential LLM hallucinations. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We introduce Statsformer, a principled framework for integrating large language model (LLM)-derived knowledge into supervised statistical learning. Existing approaches are limited in adaptability and scope: they either inject LLM guidance as an unvalidated heuristic, which is sensitive to LLM hallucination, or embed semantic information within a single fixed learner. Statsformer overcomes both limitations through a guardrailed ensemble architecture. We embed LLM-derived feature priors within an ensemble of linear and nonlinear learners, adaptively calibrating their influence via cross-validation. This design yields a flexible system with an oracle-style guarantee that it performs no worse than any convex combination of its in-library base learners, up to statistical error. Empirically, informative priors yield consistent performance improvements, while uninformative or misspecified LLM guidance is automatically downweighted, mitigating the impact of hallucinations across a diverse range of prediction tasks.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21410" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21410" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21410" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">17. Galaxy-Galaxy Blending in SPHEREx Survey Data
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Kim Dachan, Hyunmi Song, Yigon Kim, Minjin Kim, Hyunjin Shim, Dohyeong Kim, Yongjung Kim, Bomee Lee, Jeong Hwan Lee, Woong-Seob Jeong, et al.</span>
                                <span class="author-full" style="display: none;">Kim Dachan, Hyunmi Song, Yigon Kim, Minjin Kim, Hyunjin Shim, Dohyeong Kim, Yongjung Kim, Bomee Lee, Jeong Hwan Lee, Woong-Seob Jeong, Yujin Yang</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Galaxy-galaxy blending in the SPHEREx survey, particularly severe in deep fields, moderately overestimates galaxy number counts and marginally alters the luminosity function, necessitating careful consideration in future analyses. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The Spectro-Photometer for the History of the Universe, Epoch of Reionization and Ices Explorer (SPHEREx) will provide all-sky spectral survey data covering optical to mid-infrared wavelengths with a spatial resolution of 6\farcs2, which can be widely used to study galaxy formation and evolution. We investigate the galaxy-galaxy blending in SPHEREx datasets using the mock galaxy catalogs generated from cosmological simulations and observational data. Only $\sim0.7\%$ of the galaxies will be blended with other galaxies in all-sky survey data with a limiting magnitude of 19 AB mag. However, the fraction of blended galaxies dramatically increases to $\sim7$--$9\%$ in the deep survey area around the ecliptic poles, where the depth reaches $\sim22$ AB mag. We examine the impact of the blending in the number count and luminosity function analyses using the SPHEREx data. We find that the number count can be overestimated by up to $10$--$20\%$ in the deep regions due to the flux boosting, suggesting that the impact of galaxy-galaxy blending on the number count is moderate. However, galaxy-galaxy blending can marginally change the luminosity function by up to 50\%\ over a wide range of redshifts. As we only employ the magnitude limit at $K_s$-band for the source detection, the blending fractions determined in this study should be regarded as lower limits.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21627" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21627" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21627" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmic Shear, Survey Cosmology / Galaxy Clustering, Weak Lensing, Halo Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">18. Earth-Density Effects in LBL Experiments: A Comprehensive Review of Theory, Observations, and Future Directions
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Tia Pandit, Bipin Singh Koranga</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Realistic spatial variations in Earth matter density introduce complex, energy-dependent structures in neutrino oscillation probabilities that, if mismodeled, significantly degrade the sensitivity and bias the inference of leptonic CP violation parameters. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Earth matter density uncertainties play a non trivial role in three flavor neutrino oscillations in matter, particularly for the muon to electron appearance channel that underpins CP violation measurements in long baseline experiments. We demonstrate that when realistic spatial variations of the Earths density are taken into account, the oscillation probabilities acquire additional, energy dependent structures that cannot be captured by path-averaged density approximations. We show that mismodeling of the matter density profile can introduce degeneracies that obscure genuine leptonic CP violating effects, thereby degrading parameter sensitivity and biasing the inference of the CP phase. Identifying energy regions in which CP sensitivity remains robust against matter density uncertainties is therefore essential. These considerations indicate that marginalization over a single effective density parameter is insufficient for next generation precision measurements and motivate the incorporation of spatially resolved Earth density profiles in the analysis frameworks of future long-baseline neutrino oscillation experiments.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21256" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21256" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21256" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">hep-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">19. Optimal cross-correlation technique to search for strongly lensed gravitational waves
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Anirban Kopty, Sanjit Mitra, Anupreeta More</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Optimal Cross-Correlation Analysis for Multiplets (OCCAM) provides an efficient, low-latency method for rapidly identifying strongly lensed Compact Binary Coalescence events in gravitational wave strain data, significantly narrowing the search space for high-latency algorithms. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">As the number of detected gravitational wave (GW) events increases with the improved sensitivity of the observatories, detecting strongly lensed pairs of events is becoming a real possibility. Identifying such lensed pairs, however, remains challenging due to the computational cost and/or the reliance on prior knowledge of source parameters in existing methods. This study investigates a novel approach, Optimal Cross-Correlation Analysis for Multiplets (OCCAM), applied to strain data from one or more detectors for Compact Binary Coalescence (CBC) events identified by GW searches, using an optimal, mildly model-dependent, low computation cost approach to identify strongly lensed candidates. This technique efficiently narrows the search space, allowing for more sensitive, but (much) higher latency, algorithms to refine the results further. We demonstrate that our method performs significantly better than other computationally inexpensive methods. In particular, we achieve 97 percent (80 percent) lensed event detection at a pairwise false positive probability of approximately 13 percent (7 percent) for a single detector with LIGO design sensitivity, assuming an SNR greater than or equal to 10 astrophysically motivated lensed and unlensed populations. Thus, this method, using a network of detectors and in conjunction with sky-localisation information, can enormously reduce the false positive probability, making it highly viable to efficiently and quickly search for lensing pairs among thousands of events, including the sub-threshold candidates.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.22138" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.22138" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.22138" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">20. A redshift survey of the nearby galaxy cluster Abell 2199 : No upturn of the faint-end slope of galaxy luminosity function
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jong-In Park, Hyunmi Song, Ho Seong Hwang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> New spectroscopic data for the Abell 2199 cluster establishes a faint-end slope of $\alpha \approx -1.23$ for the galaxy luminosity function, contradicting claims of a steep upturn and suggesting that dense cluster cores do not enhance the population of low-mass galaxies. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We determine the galaxy luminosity function of cluster galaxies in the nearby galaxy cluster Abell 2199 (A2199), focusing on the faint-end slope down to $M_r \sim -14.5$. To achieve this, we augment the existing dataset by adding redshift data from our deep MMT/Hectospec survey and from the Dark Energy Spectroscopic Instrument (DESI), significantly improving the spectroscopic completeness down to $r_{\mathrm{petro},0} = 20.8$ within the central $30^\prime$ region. The resulting luminosity function is well described by a Schechter function with a characteristic magnitude $M^* = -21.30 \pm 0.27$ and a faint-end slope $α= -1.23 \pm 0.05$. This faint-end slope is consistent with those measured in the nearby Coma and Virgo clusters and in a cluster from the TNG50 cosmological simulation, and is slightly shallower than that of field galaxies. These findings indicate that the previously claimed steep faint-end upturn (with $α\sim -2$) in nearby galaxy clusters is not supported. Instead, they indicate that environmental processes in dense cluster cores does not seem to trigger the formation or survival of low-mass galaxies, thereby preventing a steep faint-end upturn in the luminosity function.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21329" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21329" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21329" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmic Shear, Survey Cosmology / Galaxy Clustering, Weak Lensing, Halo Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">21. Hebbian Learning with Global Direction
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Wenjia Hua, Kejie Zhao, Luziwei Leng, Ran Cheng, Yuxin Ma, Qinghai Guo</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A novel Global-guided Hebbian Learning (GHL) framework integrates local Oja&#39;s rule with a global sign-based signal to achieve competitive performance against backpropagation on large-scale networks like ImageNet, overcoming the scalability limits of purely local Hebbian methods. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Backpropagation algorithm has driven the remarkable success of deep neural networks, but its lack of biological plausibility and high computational costs have motivated the ongoing search for alternative training methods. Hebbian learning has attracted considerable interest as a biologically plausible alternative to backpropagation. Nevertheless, its exclusive reliance on local information, without consideration of global task objectives, fundamentally limits its scalability. Inspired by the biological synergy between neuromodulators and local plasticity, we introduce a novel model-agnostic Global-guided Hebbian Learning (GHL) framework, which seamlessly integrates local and global information to scale up across diverse networks and tasks. In specific, the local component employs Oja&#39;s rule with competitive learning to ensure stable and effective local updates. Meanwhile, the global component introduces a sign-based signal that guides the direction of local Hebbian plasticity updates. Extensive experiments demonstrate that our method consistently outperforms existing Hebbian approaches. Notably, on large-scale network and complex datasets like ImageNet, our framework achieves the competitive results and significantly narrows the gap with standard backpropagation.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21367" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21367" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21367" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">22. Language-based Trial and Error Falls Behind in the Era of Experience
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Haoyu Wang, Guozheng Ma, Shugang Cui, Yilun Kong, Haotian Luo, Li Shen, Mengya Gao, Yichao Wu, Xiaogang Wang, Dacheng Tao</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The SCOUT framework enhances LLM performance in nonlinguistic environments by decoupling costly exploration using lightweight &#34;scouts&#34; and subsequently bootstrapping the LLM via supervised fine-tuning and reinforcement learning, achieving superior results compared to larger proprietary models with significant computational savings. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight &#34;scouts&#34; (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21754" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21754" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21754" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">23. Efficient Causal Structure Learning via Modular Subgraph Integration
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Haixiang Sun, Pengchao Tian, Zihan Zhou, Jielei Zhang, Peiyi Li, Andrew L. Liu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> VISTA, a modular and parallelizable framework for high-dimensional causal structure learning, decomposes the problem into local Markov Blanket subgraphs and integrates them using a weighted voting mechanism and an acyclicity constraint, yielding significant improvements in accuracy and efficiency across various base learners. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Learning causal structures from observational data remains a fundamental yet computationally intensive task, particularly in high-dimensional settings where existing methods face challenges such as the super-exponential growth of the search space and increasing computational demands. To address this, we introduce VISTA (Voting-based Integration of Subgraph Topologies for Acyclicity), a modular framework that decomposes the global causal structure learning problem into local subgraphs based on Markov Blankets. The global integration is achieved through a weighted voting mechanism that penalizes low-support edges via exponential decay, filters unreliable ones with an adaptive threshold, and ensures acyclicity using a Feedback Arc Set (FAS) algorithm. The framework is model-agnostic, imposing no assumptions on the inductive biases of base learners, is compatible with arbitrary data settings without requiring specific structural forms, and fully supports parallelization. We also theoretically establish finite-sample error bounds for VISTA, and prove its asymptotic consistency under mild conditions. Extensive experiments on both synthetic and real datasets consistently demonstrate the effectiveness of VISTA, yielding notable improvements in both accuracy and efficiency over a wide range of base learners.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21014" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21014" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21014" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">24. A Comprehensive Network for the Discovery and Characterization of Interstellar Objects
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Oem Trivedi, Abraham Loeb</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The Comprehensive ISO Network (CISON) proposes a coordinated, end-to-end observational strategy—combining wide-field discovery, rapid characterization, and potential interceptor missions—to transform Interstellar Object astronomy into a scalable discipline capable of predictive classification and risk assessment. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Interstellar object (ISO) astronomy has rapidly emerged over the past decade as a new frontier in planetary astrophysics, yet current observations remain limited by short visibility windows, inference degeneracies and fragmented follow-up capabilities. We argue that these constraints are structural rather than incidental and motivate a coordinated, end-to-end observational strategy for future ISO studies. We propose the Comprehensive ISO Network (CISON) which combines dual hemisphere wide-field discovery with rapid high resolution characterization and selective escalation to interceptor missions. By coupling this architecture to the differential formulation of the Loeb Scale, ISO classification and risk assessment become predictive rather than reactive. This framework transforms ISO astronomy into a mature, scalable discipline capable of maximizing scientific return and informing planetary defense in the coming decades.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21184" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21184" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21184" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.EP</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">25. Beyond Imitation: Reinforcement Learning for Active Latent Planning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zhi Zheng, Wee Sun Lee</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Active Latent Planning (ATP-Latent) improves dense chain-of-thought reasoning by utilizing a conditional VAE to create a smoother latent space and employing reinforcement learning guided by an auxiliary coherence reward based on VAE-decoded consistency, resulting in higher accuracy and token efficiency. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the \underline{A}c\underline{t}ive Latent \underline{P}lanning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\% accuracy and -3.3\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21598" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21598" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21598" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">26. CORE: Collaborative Reasoning via Cross Teaching
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Kshitij Mishra, Mirat Aubakirov, Martin Takac, Nils Lukas, Salem Lahlou</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Collaborative Reasoning (CORE) leverages the complementary errors of multiple LLMs through a training-time cross-teaching protocol, where failed models are rescued by successful peers, resulting in substantial performance gains and high Pass@2 scores for small model pairs on complex reasoning benchmarks.</p>
                </div>
                <div class="arxiv-abstract-text">Large language models exhibit complementary reasoning errors: on the same instance, one model may succeed with a particular decomposition while another fails. We propose Collaborative Reasoning (CORE), a training-time collaboration framework that converts peer success into a learning signal via a cross-teaching protocol. Each problem is solved in two stages: a cold round of independent sampling, followed by a contexted rescue round in which models that failed receive hint extracted from a successful peer. CORE optimizes a combined reward that balances (i) correctness, (ii) a lightweight DPP-inspired diversity term to reduce error overlap, and (iii) an explicit rescue bonus for successful recovery. We evaluate CORE across four standard reasoning datasets GSM8K, MATH, AIME, and GPQA. With only 1,000 training examples, a pair of small open source models (3B+4B) reaches Pass@2 of 99.54% on GSM8K and 92.08% on MATH, compared to 82.50% and 74.82% for single-model training. On harder datasets, the 3B+4B pair reaches Pass@2 of 77.34% on GPQA (trained on 348 examples) and 79.65% on AIME (trained on 792 examples), using a training-time budget of at most 1536 context tokens and 3072 generated tokens. Overall, these results show that training-time collaboration can reliably convert model complementarity into large gains without scaling model size.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21600" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21600" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21600" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">27. Drive-KD: Multi-Teacher Distillation for VLMs in Autonomous Driving
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Weitong Lian, Zecong Tang, Haoran Li, Tianjian Gao, Yifei Wang, Zixu Wang, Lingyi Meng, Tengju Ru, Zhejun Cui, Yichen Zhu, et al.</span>
                                <span class="author-full" style="display: none;">Weitong Lian, Zecong Tang, Haoran Li, Tianjian Gao, Yifei Wang, Zixu Wang, Lingyi Meng, Tengju Ru, Zhejun Cui, Yichen Zhu, Hangshuo Cao, Qi Kang, Tianxing Chen, Yusen Qin, Kaixuan Wang, Yu Zhang</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Drive-KD is a knowledge distillation framework that efficiently transfers autonomous driving capabilities (perception, reasoning, planning) from multiple large teachers to a small student model, enabling the distilled InternVL3-1B to outperform its 78B counterpart in performance while achieving massive gains in memory efficiency and throughput. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Autonomous driving is an important and safety-critical task, and recent advances in LLMs/VLMs have opened new possibilities for reasoning and planning in this domain. However, large models demand substantial GPU memory and exhibit high inference latency, while conventional supervised fine-tuning (SFT) often struggles to bridge the capability gaps of small models. To address these limitations, we propose Drive-KD, a framework that decomposes autonomous driving into a &#34;perception-reasoning-planning&#34; triad and transfers these capabilities via knowledge distillation. We identify layer-specific attention as the distillation signal to construct capability-specific single-teacher models that outperform baselines. Moreover, we unify these single-teacher settings into a multi-teacher distillation framework and introduce asymmetric gradient projection to mitigate cross-capability gradient conflicts. Extensive evaluations validate the generalization of our method across diverse model families and scales. Experiments show that our distilled InternVL3-1B model, with ~42 times less GPU memory and ~11.4 times higher throughput, achieves better overall performance than the pretrained 78B model from the same family on DriveBench, and surpasses GPT-5.1 on the planning dimension, providing insights toward efficient autonomous driving VLMs.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21288" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21288" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21288" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">28. Probing the Sound Speed of Dark Energy with a Lunar Laser Interferometer
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Alfredo Gurrola, Robert J. Scherrer, Oem Trivedi</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A proposed lunar-based laser interferometer, LILA, can serve as a novel and powerful probe of cosmic acceleration by measuring ultra-low-frequency gravitational strain signatures sourced by dark energy dynamics, enabling the detection or exclusion of clustering dark energy models with unprecedented sensitivity. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The sound speed of dark energy encodes fundamental information about the microphysics underlying cosmic acceleration, yet remains essentially unconstrained by existing observations. We demonstrate that a lunar-based laser interferometer, such as the proposed Laser Interferometer Lunar Antenna (LILA), can directly probe the sound speed of dark energy by measuring the real-time evolution of horizon-scale gravitational potentials. Operating in the ultra-low-frequency gravitational band inaccessible from Earth, LILA is sensitive to scalar metric perturbations sourced by dark energy dynamics. Using both fluid and effective field theory descriptions, we develop a complete framework linking dark energy sound speed to observable strain signatures. We construct a likelihood pipeline and Fisher forecasts, showing that LILA can either detect clustering dark energy or exclude broad classes of models with unprecedented sensitivity. This establishes lunar interferometry as a novel and powerful probe of the physics driving cosmic acceleration.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.22084" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.22084" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.22084" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">29. Forecasting Constraints on Cosmology and Modified Gravitational-wave Propagation by Strongly Lensed Gravitational Waves Associating with Galaxy Surveys
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Anson Chen, Jun Zhang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Time-delay cosmography utilizing doubly lensed gravitational wave events, especially those detected by the future ET+CE network, offers highly precise constraints on the Hubble constant (sub-percent level) and provides informative limits on dynamical dark energy models and modified gravitational wave propagation. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Gravitational lensing of gravitational wave (GW) will become the next frontier in studying cosmology and gravity. While time-delay cosmography using quadruply lensed GW events associated with optical images of the lens systems can provide precise measurement of the Hubble constant ($H_0$), they are considered to be much rarer than doubly lensed events. In this work, we analyze time-delay cosmography with doubly lensed GW events for the first time. We generate mock doubly lensed GW events with designed sensitivity of the LIGO-Virgo-KAGRA (LVK) O5 network, with LIGO post-O5 upgrade, and with Einstein Telescope (ET) + Cosmic Explorer (CE) respectively, and select the events that can be associated with future galaxy surveys. Over 1000 realizations, we find an average of 0.2(2.4) qualified events with the LVK O5(post-O5) network. Whereas with the ET+CE network, we find an average of 73.2 qualified events over 100 realizations. Using the Singular Isothermal Sphere (SIS) lens model, we jointly estimate waveform parameters and the impact parameter with doubly lensed GW signals, and then forecast the constraints on cosmological parameters and modified GW propagation by combining time-delay cosmography and the standard siren approach. The average posterior gives a constraint on $H_0$ with a relative uncertainty of $14\%$, $10\%$ and $0.42\%$ in the $Λ$CDM model for the LVK O5, LVK post-O5, and ET+CE network, respectively. While the LVK network gives uninformative constraints on the $(w_0,w_a)$ dynamical dark energy model, the ET+CE network yields a moderate constraint of $w_0=-1.02^{+0.31}_{-0.22}$ and $w_a=0.48^{+0.99}_{-1.54}$. In addition, our method can provide precise constraints on modified GW propagation effects jointly with $H_0$.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21820" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21820" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21820" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">30. DropoutTS: Sample-Adaptive Dropout for Robust Time Series Forecasting
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Siru Zhong, Yiqiu Liu, Zhiqing Cui, Zezhi Shao, Fei Wang, Qingsong Wen, Yuxuan Liang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> DropoutTS enhances the robustness of deep time series models against noisy data by introducing a Sample-Adaptive Dropout mechanism that dynamically calibrates learning capacity based on instance-level noise quantified efficiently via spectral sparsity and reconstruction residuals. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Deep time series models are vulnerable to noisy data ubiquitous in real-world applications. Existing robustness strategies either prune data or rely on costly prior quantification, failing to balance effectiveness and efficiency. In this paper, we introduce DropoutTS, a model-agnostic plugin that shifts the paradigm from &#34;what&#34; to learn to &#34;how much&#34; to learn. DropoutTS employs a Sample-Adaptive Dropout mechanism: leveraging spectral sparsity to efficiently quantify instance-level noise via reconstruction residuals, it dynamically calibrates model learning capacity by mapping noise to adaptive dropout rates - selectively suppressing spurious fluctuations while preserving fine-grained fidelity. Extensive experiments across diverse noise regimes and open benchmarks show DropoutTS consistently boosts superior backbones&#39; performance, delivering advanced robustness with negligible parameter overhead and no architectural modifications. Our code is available at https://github.com/CityMind-Lab/DropoutTS.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21726" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21726" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21726" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">31. A UV-Luminous Galaxy at z=11 with Surprisingly Weak Star Formation Activity
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Yuichi Harikane, Pablo G. Perez-Gonzalez, Javier Alvarez-Marquez, Masami Ouchi, Yurina Nakazato, Yoshiaki Ono, Kimihiko Nakajima, Hiroya Umeda, Yuki Isobe, Yi Xu, et al.</span>
                                <span class="author-full" style="display: none;">Yuichi Harikane, Pablo G. Perez-Gonzalez, Javier Alvarez-Marquez, Masami Ouchi, Yurina Nakazato, Yoshiaki Ono, Kimihiko Nakajima, Hiroya Umeda, Yuki Isobe, Yi Xu, Yechi Zhang</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> JWST/MIRI observations of the z=11.04 galaxy CEERS2-588 detected the first Balmer break at z&gt;10, indicating a massive (log(M_*/M_odot)=9.1), near-solar metallicity system that underwent rapid quenching, challenging current models of early universe galaxy formation. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">One of the major discoveries by the James Webb Space Telescope (JWST) is the identification of a large population of luminous galaxies at $z&gt;10$, challenging theoretical models for early galaxy formation. The unexpectedly high number density of these systems has triggered intense debate about potential differences in the physical properties of galaxies at such extreme redshifts and those at lower redshift. However, progress has been limited by the lack of rest-frame optical diagnostics, which are critical for constraining the key properties. Here we present deep JWST/MIRI observations of a UV-luminous galaxy at $z=11.04$, CEERS2-588, only 400 Myr after the Big Bang. CEERS2-588 is detected in the MIRI F560W and F770W bands, while deep MIRI/MRS spectroscopy yields no detection of H$α$ or [OIII]$\lambda5007$ line, revealing a prominent Balmer break detected for the first time at $z&gt;10$. Spectral energy distribution (SED) fitting indicates an extended star formation history possibly reaching $z&gt;15$, followed by rapid quenching within the recent $\sim10$ Myr, in stark contrast to other $z&gt;10$ galaxies. The MIRI detections also significantly improve our stellar mass estimate to $\mathrm{log}(M_*/M_\odot)=9.1^{+0.1}_{-0.1}$, making CEERS2-588 the most massive galaxy securely confirmed at $z&gt;10$. Remarkably, the inferred gas-phase metallicity is near solar, exceeding predictions from current theoretical models. These results suggest that efficient starbursts play a key role in producing the abundant luminous galaxy population in the early universe.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21833" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21833" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21833" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmic Shear, Survey Cosmology / Galaxy Clustering, Weak Lensing, Halo Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">32. Generalized ModMax and the Early Universe
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>M. Sabido, V. Sierra</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Utilizing Generalized ModMax nonlinear electrodynamics, a cosmological model is constructed that successfully resolves the initial singularity and generates an inflationary epoch matching current constraints on cosmological parameters. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">In this work, we study a cosmological model driven by Generalized ModMax nonlinear electrodynamics. We find that, with an appropriate choice of the theory&#39;s parameters, the universe&#39;s initial singularity can be avoided. Moreover, we also find that this model has an inflationary epoch that is consistent with the current values for $N$, $n_s$ and $r$. Therefore, using Generalized ModMax, we can construct a non singular Universe with an inflationary epoch.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.20908" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.20908" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.20908" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">33. Diffusion-based Annealed Boltzmann Generators : benefits, pitfalls and hopes
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Louis Grenioux, Maxence Noble</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Analyzing Diffusion Model-based Boltzmann Generators using Annealed Monte Carlo (aMC), researchers found that first-order stochastic integration fails systematically, while performance hinges on using second-order kernels and overcoming the critical bottleneck of inaccurate log-density estimation in learned models. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Sampling configurations at thermodynamic equilibrium is a central challenge in statistical physics. Boltzmann Generators (BGs) tackle it by combining a generative model with a Monte Carlo (MC) correction step to obtain asymptotically unbiased samples from an unnormalized target. Most current BGs use classic MC mechanisms such as importance sampling, which both require tractable likelihoods from the backbone model and scale poorly in high-dimensional, multi-modal targets. We study BGs built on annealed Monte Carlo (aMC), which is designed to overcome these limitations by bridging a simple reference to the target through a sequence of intermediate densities. Diffusion models (DMs) are powerful generative models and have already been incorporated into aMC-based recalibration schemes via the diffusion-induced density path, making them appealing backbones for aMC-BGs. We provide an empirical meta-analysis of DM-based aMC-BGs on controlled multi-modal Gaussian mixtures (varying mode separation, number of modes, and dimension), explicitly disentangling inference effects from learning effects by comparing (i) a perfectly learned DM and (ii) a DM trained from data. Even with a perfect DM, standard integrations using only first-order stochastic denoising kernels fail systematically, whereas second-order denoising kernels can substantially improve performance when covariance information is available. We further propose a deterministic aMC integration based on first-order transport maps derived from DMs, which outperforms the stochastic first-order variant at higher computational cost. Finally, in the learned-DM setting, all DM-aMC variants struggle to produce accurate BGs; we trace the main bottleneck to inaccurate DM log-density estimation.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21026" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21026" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21026" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">34. Depth-Recurrent Attention Mixtures: Giving Latent Reasoning the Attention it Deserves
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jonas Knupp, Jan Hendrik Metzen, Jeremias Bohn, Georg Groh, Kristian Kersting</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Introducing Dreamer, a modular framework of depth-recurrent attention mixtures, significantly improves efficiency and effectiveness in latent reasoning by mitigating the hidden-size bottleneck, leading to 2 to 8x fewer required training tokens compared to state-of-the-art models. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Depth-recurrence facilitates latent reasoning by sharing parameters across depths. However, prior work lacks combined FLOP-, parameter-, and memory-matched baselines, underutilizes depth-recurrence due to partially fixed layer stacks, and ignores the bottleneck of constant hidden-sizes that restricts many-step latent reasoning. To address this, we introduce a modular framework of depth-recurrent attention mixtures (Dreamer), combining sequence attention, depth attention, and sparse expert attention. It alleviates the hidden-size bottleneck through attention along depth, decouples scaling dimensions, and allows depth-recurrent models to scale efficiently and effectively. Across language reasoning benchmarks, our models require 2 to 8x fewer training tokens for the same accuracy as FLOP-, parameter-, and memory-matched SOTA, and outperform ca. 2x larger SOTA models with the same training tokens. We further present insights into knowledge usage across depths, e.g., showing 2 to 11x larger expert selection diversity than SOTA MoEs.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21582" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21582" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21582" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">35. All-order prescription for facet regions in massless wide-angle scattering
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yao Ma</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> An all-order momentum-space prescription for facet regions in the Expansion-by-Regions technique is derived for generic massless wide-angle scattering using graph theory and convex geometry, simultaneously uncovering the algebraic structure of momentum modes. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We take a step toward answering a long-standing question in the asymptotic expansion of Feynman integrals: how to systematically determine the regions in the Expansion-by-Regions technique for multiscale processes? Focusing on generic massless wide-angle scattering, we provide an all-order momentum-space prescription for facet regions, which generally dominate -- and in most cases exhaust -- the contributions in a given asymptotic expansion. This extends the Euclidean-space picture, where regions correspond to specific subgraphs, to the complexities of Minkowski space. Our results are derived from a novel analytical approach combining graph theory and convex geometry; as a key byproduct, we uncover for the first time the algebraic structure underlying momentum modes (collinear, soft, and their hierarchies).</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.22144" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.22144" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.22144" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">hep-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">36. When should I search more: Adaptive Complex Query Optimization with Reinforcement Learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Wei Wen, Sihang Deng, Tianjun Wei, Keyu Chen, Ruizhi Qiao, Xing Sun</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The Adaptive Complex Query Optimization (ACQO) framework leverages Curriculum Reinforcement Learning, dynamic query decomposition, and robust result fusion to achieve state-of-the-art efficiency and performance in handling complex, multi-strategy queries within Retrieval-Augmented Generation (RAG) systems. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Query optimization is a crucial component for the efficacy of Retrieval-Augmented Generation (RAG) systems. While reinforcement learning (RL)-based agentic and reasoning methods have recently emerged as a promising direction on query optimization, most existing approaches focus on the expansion and abstraction of a single query. However, complex user queries are prevalent in real-world scenarios, often requiring multiple parallel and sequential search strategies to handle disambiguation and decomposition. Directly applying RL to these complex cases introduces significant hurdles. Determining the optimal number of sub-queries and effectively re-ranking and merging retrieved documents vastly expands the search space and complicates reward design, frequently leading to training instability. To address these challenges, we propose a novel RL framework called Adaptive Complex Query Optimization (ACQO). Our framework is designed to adaptively determine when and how to expand the search process. It features two core components: an Adaptive Query Reformulation (AQR) module that dynamically decides when to decompose a query into multiple sub-queries, and a Rank-Score Fusion (RSF) module that ensures robust result aggregation and provides stable reward signals for the learning agent. To mitigate training instabilities, we adopt a Curriculum Reinforcement Learning (CRL) approach, which stabilizes the training process by progressively introducing more challenging queries through a two-stage strategy. Our comprehensive experiments demonstrate that ACQO achieves state-of-the-art performance on three complex query benchmarks, significantly outperforming established baselines. The framework also showcases improved computational efficiency and broad compatibility with different retrieval architectures, establishing it as a powerful and generalizable solution for next-generation RAG systems.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21208" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21208" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21208" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">37. Chain Of Thought Compression: A Theoritical Analysis
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Juncai Li, Ru Li, Yuxiang Zhou, Boxiang Ma, Jeff Z. Pan</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Theoretical findings demonstrate that learning implicit Chain-of-Thought reasoning is hindered by exponential signal decay from high-order interaction barriers, which the ALiCoT framework overcomes by aligning latent token distributions, resulting in a 54.4x speedup while maintaining performance comparable to explicit CoT. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Chain-of-Thought (CoT) has unlocked advanced reasoning abilities of Large Language Models (LLMs) with intermediate steps, yet incurs prohibitive computational costs due to generation of extra tokens. Recent studies empirically show that compressing reasoning steps into latent states, or implicit CoT compression, offers a token-efficient alternative. However, the mechanism behind CoT compression remains unclear. In this paper, we provide the first theoretical analysis of the difficulty of learning to internalize intermediate reasoning steps. By introducing Order-r Interaction, we prove that the learning signal for high-order logical dependencies exponentially decays to solve irreducible problem, where skipping intermediate steps inevitably leads to high-order interaction barriers. To empirically validate this, we introduce NatBool-DAG, a challenging benchmark designed to enforce irreducible logical reasoning and eliminate semantic shortcuts. Guided by our theoretical findings, we propose ALiCoT (Aligned Implicit CoT), a novel framework that overcomes the signal decay by aligning latent token distributions with intermediate reasoning states. Experimental results demonstrate that ALiCoT successfully unlocks efficient reasoning: it achieves a 54.4x speedup while maintaining performance comparable to explicit CoT.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21576" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21576" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21576" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">38. Ostrakon-VL: Towards Domain-Expert MLLM for Food-Service and Retail Stores
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Zhiyong Shen, Gongpeng Zhao, Jun Zhou, Li Yu, Guandong Kou, Jichen Li, Chuanlei Dong, Zuncheng Li, Kaimao Li, Bingkun Wei, et al.</span>
                                <span class="author-full" style="display: none;">Zhiyong Shen, Gongpeng Zhao, Jun Zhou, Li Yu, Guandong Kou, Jichen Li, Chuanlei Dong, Zuncheng Li, Kaimao Li, Bingkun Wei, Shicheng Hu, Wei Xia, Wenguo Duan</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Ostrakon-VL, a Multimodal Large Language Model optimized for Food-Service and Retail Stores (FSRS) using the QUAD data curation pipeline, sets a new state-of-the-art on the novel ShopBench benchmark, exhibiting improved parameter efficiency and robust perception capabilities. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Multimodal Large Language Models (MLLMs) have recently achieved substantial progress in general-purpose perception and reasoning. Nevertheless, their deployment in Food-Service and Retail Stores (FSRS) scenarios encounters two major obstacles: (i) real-world FSRS data, collected from heterogeneous acquisition devices, are highly noisy and lack auditable, closed-loop data curation, which impedes the construction of high-quality, controllable, and reproducible training corpora; and (ii) existing evaluation protocols do not offer a unified, fine-grained and standardized benchmark spanning single-image, multi-image, and video inputs, making it challenging to objectively gauge model robustness. To address these challenges, we first develop Ostrakon-VL, an FSRS-oriented MLLM based on Qwen3-VL-8B. Second, we introduce ShopBench, the first public benchmark for FSRS. Third, we propose QUAD (Quality-aware Unbiased Automated Data-curation), a multi-stage multimodal instruction data curation pipeline. Leveraging a multi-stage training strategy, Ostrakon-VL achieves an average score of 60.1 on ShopBench, establishing a new state of the art among open-source MLLMs with comparable parameter scales and diverse architectures. Notably, it surpasses the substantially larger Qwen3-VL-235B-A22B (59.4) by +0.7, and exceeds the same-scale Qwen3-VL-8B (55.3) by +4.8, demonstrating significantly improved parameter efficiency. These results indicate that Ostrakon-VL delivers more robust and reliable FSRS-centric perception and decision-making capabilities. To facilitate reproducible research, we will publicly release Ostrakon-VL and the ShopBench benchmark.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21342" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21342" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21342" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">39. $f$-Mode oscillations and the gravitational response of compact stars with analytic equations of state
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Kilar Zhang, Alessandro Parisi, C. Vásquez Flores, Chian-Shu Chen</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Applying analytical equations of state allows for the investigation of global observable properties, including mass, radius, tidal deformability, and fundamental mode oscillations, of neutron stars and dark stars, enabling constraints via multi-messenger observations. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We apply analytical models to study the property of neutron stars and dark stars. With the aim of exploring the global observable properties of those compact stars, we investigate the total masses and radii, the tidal deformabilities and especially the fundamental (f -) mode oscillations. While we choose two typical models in this work, this method applies to any analytical equations of state. By comparing with the multi-messenger observations, one can constrain the corresponding parameters in those models.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21911" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21911" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21911" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">hep-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">40. Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Shuo Liu, Tianle Chen, Ryan Amiri, Christopher Amato</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Multi-Agent Actor-Critic (MAAC) methods are proposed for optimizing decentralized LLM collaboration, where the CoLLM-CC approach with a centralized critic significantly outperforms Monte Carlo methods and decentralized critics on complex, long-horizon tasks requiring stable training signals. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \textbf{CoLLM-CC} with a \textbf{C}entralized \textbf{C}ritic and \textbf{CoLLM-DC} with \textbf{D}ecentralized \textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21972" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21972" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21972" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">41. LION: A Clifford Neural Paradigm for Multimodal-Attributed Graph Learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Xunkai Li, Zhengyu Wu, Zekai Chen, Henan Sun, Daohan Su, Guang Zeng, Hongchao Qin, Rong-Hua Li, Guoren Wang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The LION neural paradigm leverages Clifford algebra to construct a geometric manifold for efficient modality interaction and alignment in multimodal-attributed graphs, using adaptive holographic aggregation for superior fusion performance across various downstream tasks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Recently, the rapid advancement of multimodal domains has driven a data-centric paradigm shift in graph ML, transitioning from text-attributed to multimodal-attributed graphs. This advancement significantly enhances data representation and expands the scope of graph downstream tasks, such as modality-oriented tasks, thereby improving the practical utility of graph ML. Despite its promise, limitations exist in the current neural paradigms: (1) Neglect Context in Modality Alignment: Most existing methods adopt topology-constrained or modality-specific operators as tokenizers. These aligners inevitably neglect graph context and inhibit modality interaction, resulting in suboptimal alignment. (2) Lack of Adaptation in Modality Fusion: Most existing methods are simple adaptations for 2-modality graphs and fail to adequately exploit aligned tokens equipped with topology priors during fusion, leading to poor generalizability and performance degradation. To address the above issues, we propose LION (c\underline{LI}ff\underline{O}rd \underline{N}eural paradigm) based on the Clifford algebra and decoupled graph neural paradigm (i.e., propagation-then-aggregation) to implement alignment-then-fusion in multimodal-attributed graphs. Specifically, we first construct a modality-aware geometric manifold grounded in Clifford algebra. This geometric-induced high-order graph propagation efficiently achieves modality interaction, facilitating modality alignment. Then, based on the geometric grade properties of aligned tokens, we propose adaptive holographic aggregation. This module integrates the energy and scale of geometric grades with learnable parameters to improve modality fusion. Extensive experiments on 9 datasets demonstrate that LION significantly outperforms SOTA baselines across 3 graph and 3 modality downstream tasks.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21453" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21453" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21453" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">42. Acquiring Human-Like Mechanics Intuition from Scarce Observations via Deep Reinforcement Learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jingruo Peng, Shuze Zhu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Robust artificial mechanics intuition, generalizing accurately beyond training data, is acquired in a reinforcement learning framework by training agents with episodic switching, which theoretically enforces Bellman consistency across neighboring tasks to capture the underlying low-dimensional solution manifold. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Humans can infer accurate mechanical outcomes from only a few observations, a capability known as mechanics intuition. The mechanisms behind such data-efficient learning remain unclear. Here, we propose a reinforcement learning framework in which an agent encodes continuous physical observation parameters into its state and is trained via episodic switching across closely related observations. With merely two or three observations, the agent acquires robust mechanics intuition that generalizes accurately over wide parameter ranges, substantially beyond the training data, as demonstrated on the brachistochrone and a large-deformation elastic plate. We explain this generalization through a unified theoretical view: it emerges when the learned value function enforces Bellman consistency across neighboring task parameters, rendering the Bellman residual stationary with respect to physical variations. This induces a smooth policy that captures a low-dimensional solution manifold underlying the continuum of tasks. Our work establishes episodic switching as a principled route to artificial mechanics intuition and offers a theoretical link to similar generalization abilities in biological learners.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21881" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21881" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21881" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">physics.comp-ph</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">43. Stellar Populations in the Extreme Outer Halo of the Spiral Galaxy M96
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>J. Christopher Mihos, Patrick R. Durrell, Brian Malkan, Aaron E. Watkins</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Deep Hubble Space Telescope imaging of the M96 spiral galaxy halo reveals a stellar population with a median metallicity of [M/H] = -1.36 and a high stellar halo mass fraction, positioning M96 as distinctly metal-poor for its mass and suggesting a potentially unique accretion history. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We use deep Hubble Space Telescope imaging to study stellar populations in the outer halo of the spiral galaxy M96, located in the dynamically active Leo I galaxy group. Our imaging targets two fields at a projected distance of 50 kpc from the galaxy&#39;s center, with a 50% photometric completeness limit of F814W = 28.0, nearly two magnitudes below the tip of the red giant branch. In both fields we find a clear detection of red giant stars in M96&#39;s halo, with a space density that corresponds to an equivalent broadband surface brightness of $μ_V \approx $ 31.7 mag arcsec$^{-2}$. We find little evidence for any difference in the spatial density or color of the RGB stars in the two fields. Using isochrone matching we derive a median metallicity for the red giants of [M/H] = -1.36 with an interquartile spread of $\pm$0.75 dex. Adopting a power-law radial density profile, we also derive a total halo mass of $M_h = 7.8^{+17.4}_{-4.9}\times10^9$ M$_\odot$, implying a stellar halo mass fraction of $M_{*,halo}/M_{*,tot} = 15^{+33}_{-9}$%, on the high end for spiral galaxies, but with significant uncertainty. Finally, we find that M96 appears offset from the stellar halo mass-metallicity relationship for spirals, with a halo that is distinctly metal-poor for its halo mass. While a variety of systematic effects could have conspired to drive M96 off this relationship, if confirmed our results may argue for a markedly different accretion history for M96 compared to other spirals in the nearby universe.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21987" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21987" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21987" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Cosmic Shear, Survey Cosmology / Galaxy Clustering, Weak Lensing, Halo Modeling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">44. From Meta-Thought to Execution: Cognitively Aligned Post-Training for Generalizable and Reliable LLM Reasoning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Shaojie Wang, Liang Zhang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A cognitively-inspired two-stage framework, combining Chain-of-Meta-Thought (CoMT) for abstract strategy acquisition and Confidence-Calibrated Reinforcement Learning (CCRL) for reliable task adaptation, significantly improves LLM generalization and training efficiency compared to traditional outcome-based methods. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Current LLM post-training methods optimize complete reasoning trajectories through Supervised Fine-Tuning (SFT) followed by outcome-based Reinforcement Learning (RL). While effective, a closer examination reveals a fundamental gap: this approach does not align with how humans actually solve problems. Human cognition naturally decomposes problem-solving into two distinct stages: first acquiring abstract strategies (i.e., meta-knowledge) that generalize across problems, then adapting them to specific instances. In contrast, by treating complete trajectories as basic units, current methods are inherently problem-centric, entangling abstract strategies with problem-specific execution. To address this misalignment, we propose a cognitively-inspired framework that explicitly mirrors the two-stage human cognitive process. Specifically, Chain-of-Meta-Thought (CoMT) focuses supervised learning on abstract reasoning patterns without specific executions, enabling acquisition of generalizable strategies. Confidence-Calibrated Reinforcement Learning (CCRL) then optimizes task adaptation via confidence-aware rewards on intermediate steps, preventing overconfident errors from cascading and improving execution reliability. Experiments across four models and eight benchmarks show 2.19\% and 4.63\% improvements in-distribution and out-of-distribution respectively over standard methods, while reducing training time by 65-70% and token consumption by 50%, demonstrating that aligning post-training with human cognitive principles yields not only superior generalization but also enhanced training efficiency.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21909" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21909" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21909" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">45. Provably Reliable Classifier Guidance through Cross-entropy Error Control
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Sharan Sahu, Arisina Banerjee, Yuchen Wu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A theoretical analysis establishes that, given mild smoothness, controlling the cross-entropy error in classifier training directly controls the mean squared error of the resulting guidance vectors in diffusion models, providing the first quantitative link between classifier training and guidance alignment. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Classifier-guided diffusion models generate conditional samples by augmenting the reverse-time score with the gradient of a learned classifier, yet it remains unclear whether standard classifier training procedures yield effective diffusion guidance. We address this gap by showing that, under mild smoothness assumptions on the classifiers, controlling the cross-entropy error at each diffusion step also controls the error of the resulting guidance vectors: classifiers achieving conditional KL divergence $\varepsilon^2$ from the ground-truth conditional label probabilities induce guidance vectors with mean squared error $\widetilde{O}(d \varepsilon )$. Our result yields an upper bound on the sampling error under classifier guidance and bears resemblance to a reverse log-Sobolev-type inequality. Moreover, we show that the classifier smoothness assumption is essential, by constructing simple counterexamples demonstrating that, without it, control of the guidance vector can fail for almost all distributions. To our knowledge, our work establishes the first quantitative link between classifier training and guidance alignment, yielding both a theoretical foundation for classifier guidance and principled guidelines for classifier selection.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21200" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21200" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21200" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">stat.ML</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">46. Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jiecong Wang, Hao Peng, Chunyang Liu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The PLaT framework decouples reasoning from verbalization by modeling latent thoughts as a deterministic planning trajectory, enabling dynamic termination and achieving superior scalability and diversity in the solution space for complex mathematical problems. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21358" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21358" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21358" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">47. WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yao Zhang, Shijie Tang, Zeyu Li, Zhen Han, Volker Tresp</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> WebArbiter, a reasoning-first Process Reward Model (PRM) for web agents, generates structured textual justifications and preference verdicts to provide fine-grained supervision, achieving superior performance over state-of-the-art models in complex web navigation and trajectory search tasks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Web agents hold great potential for automating complex computer tasks, yet their interactions involve long-horizon, sequential decision-making with irreversible actions. In such settings, outcome-based supervision is sparse and delayed, often rewarding incorrect trajectories and failing to support inference-time scaling. This motivates the use of Process Reward Models (WebPRMs) for web navigation, but existing approaches remain limited: scalar WebPRMs collapse progress into coarse, weakly grounded signals, while checklist-based WebPRMs rely on brittle template matching that fails under layout or semantic changes and often mislabels superficially correct actions as successful, providing little insight or interpretability. To address these challenges, we introduce WebArbiter, a reasoning-first, principle-inducing WebPRM that formulates reward modeling as text generation, producing structured justifications that conclude with a preference verdict and identify the action most conducive to task completion under the current context. Training follows a two-stage pipeline: reasoning distillation equips the model with coherent principle-guided reasoning, and reinforcement learning corrects teacher biases by directly aligning verdicts with correctness, enabling stronger generalization. To support systematic evaluation, we release WebPRMBench, a comprehensive benchmark spanning four diverse web environments with rich tasks and high-quality preference annotations. On WebPRMBench, WebArbiter-7B outperforms the strongest baseline, GPT-5, by 9.1 points. In reward-guided trajectory search on WebArena-Lite, it surpasses the best prior WebPRM by up to 7.2 points, underscoring its robustness and practical value in real-world complex web tasks.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21872" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21872" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21872" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">48. ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zhao Wang, Ziliang Zhao, Zhicheng Dou</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> ProRAG, a process-supervised reinforcement learning framework, utilizes an MCTS-based Process Reward Model and a dual-granularity advantage mechanism to integrate fine-grained step-level supervision, effectively mitigating reward sparsity and process hallucinations in Retrieval-Augmented Generation for complex reasoning tasks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Reinforcement learning (RL) has become a promising paradigm for optimizing Retrieval-Augmented Generation (RAG) in complex reasoning tasks. However, traditional outcome-based RL approaches often suffer from reward sparsity and inefficient credit assignment, as coarse-grained scalar rewards fail to identify specific erroneous steps within long-horizon trajectories. This ambiguity frequently leads to &#34;process hallucinations&#34;, where models reach correct answers through flawed logic or redundant retrieval steps. Although recent process-aware approaches attempt to mitigate this via static preference learning or heuristic reward shaping, they often lack the on-policy exploration capabilities required to decouple step-level credit from global outcomes. To address these challenges, we propose ProRAG, a process-supervised reinforcement learning framework designed to integrate learned step-level supervision into the online optimization loop. Our framework consists of four stages: (1) Supervised Policy Warmup to initialize the model with a structured reasoning format; (2) construction of an MCTS-based Process Reward Model (PRM) to quantify intermediate reasoning quality; (3) PRM-Guided Reasoning Refinement to align the policy with fine-grained process preferences; and (4) Process-Supervised Reinforcement Learning with a dual-granularity advantage mechanism. By aggregating step-level process rewards with global outcome signals, ProRAG provides precise feedback for every action. Extensive experiments on five multi-hop reasoning benchmarks demonstrate that ProRAG achieves superior overall performance compared to strong outcome-based and process-aware RL baselines, particularly on complex long-horizon tasks, validating the effectiveness of fine-grained process supervision. The code and model are available at https://github.com/lilinwz/ProRAG.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21912" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21912" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21912" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">49. Balancing Variety and Sample Size: Optimal Parameter Sampling for Ariel Target Selection
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Emilie Panek, Alexander Roman, Katia Matcheva, Konstantin T. Matchev, Nicolas B. Cowan</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Evaluating exoplanet target selection strategies for the Ariel mission, leverage-based criteria proved most effective by successfully balancing the need for sample diversity with maximizing the total number of observed targets, outperforming purely time-greedy or variance-greedy methods. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Targeted astrophysical surveys are limited by the amount of telescope time available, which makes it impossible to observe every single object of interest. In order to maximize the scientific return, we need a well thought strategy for selecting the observational targets, in our case exoplanets. This study evaluates various strategies for selecting exoplanet targets within limited observation windows, focusing specifically on the selection of exoplanet targets for Tier 2 transit spectroscopy with ESA&#39;s upcoming Ariel mission. We define three distinct selection criteria -- sample size, variance, and leverage -- and translate them into objective functions compatible with modern optimization algorithms. Specifically, we test five heuristics for maximizing sample leverage: leverage greedy, simulated annealing, K-means clustering, regular classes, and quantile classes. The performance of these methods is demonstrated through three practical exercises across one, two, and three parameters of diversity. Each criterion represents a unique trade-off between sample size, diversity, and total observation time. While a time-greedy approach maximizes the quantity of planets, it fails to capture diversity. Conversely, variance-greedy selection prioritizes diversity but introduces significant drawbacks: it oversamples rare cases and undersamples typical planets, ultimately reducing the total number of targets observed. Leverage-based selections emerge as the most effective middle ground, successfully balancing sample diversity with a robust sample size. This work supports the broader community effort to ensure that Ariel delivers the most diverse and scientifically valuable sample of exoplanet atmospheres within mission limits.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21020" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21020" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21020" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.EP</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">50. BrainStack: Neuro-MoE with Functionally Guided Expert Routing for EEG-Based Language Decoding
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Ziyi Zhao, Jinzhao Zhou, Xiaowei Jiang, Beining Cao, Wenhao Ma, Yang Shen, Ren Li, Yu-Kai Wang, Chin-teng Lin</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> BrainStack, a functionally guided Neuro-Mixture-of-Experts framework, utilizes anatomically partitioned regional experts, a global transformer, and adaptive routing with cross-regional distillation to achieve superior accuracy and generalization in decoding linguistic information from EEG signals. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Decoding linguistic information from electroencephalography (EEG) remains challenging due to the brain&#39;s distributed and nonlinear organization. We present BrainStack, a functionally guided neuro-mixture-of-experts (Neuro-MoE) framework that models the brain&#39;s modular functional architecture through anatomically partitioned expert networks. Each functional region is represented by a specialized expert that learns localized neural dynamics, while a transformer-based global expert captures cross-regional dependencies. A learnable routing gate adaptively aggregates these heterogeneous experts, enabling context-dependent expert coordination and selective fusion. To promote coherent representation across the hierarchy, we introduce cross-regional distillation, where the global expert provides top-down regularization to the regional experts. We further release SilentSpeech-EEG (SS-EEG), a large-scale benchmark comprising over 120 hours of EEG recordings from 12 subjects performing 24 silent words, the largest dataset of its kind. Experiments demonstrate that BrainStack consistently outperforms state-of-the-art models, achieving superior accuracy and generalization across subjects. Our results establish BrainStack as a functionally modular, neuro-inspired MoE paradigm that unifies neuroscientific priors with adaptive expert routing, paving the way for scalable and interpretable brain-language decoding.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2601.21148" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2601.21148" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2601.21148" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.2</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Bayesian Inference, Cosmological Sampling / Neural Architectures, Representation Learning, Generative Models</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
    </div>

    <footer class="footer">
        &copy; 2026 arXiv Daily · Powered by <a href="https://arxiv.org/" target="_blank">arXiv</a>
    </footer>
</body>
</html>