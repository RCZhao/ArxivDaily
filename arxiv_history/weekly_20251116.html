<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>arXiv Daily: Backfill (2025-11-09 to 2025-11-16)</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto:400,700&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Roboto', Arial, sans-serif; background: #f8f9fa; transition: background 0.3s; }
        .arxiv-card { margin: 2em auto; max-width: 900px; box-shadow: 0 2px 8px rgba(0,0,0,0.08); border-radius: 16px; transition: box-shadow 0.3s, transform 0.3s; }
        .arxiv-card:hover { box-shadow: 0 4px 12px rgba(0,0,0,0.1); transform: translateY(-1px); }
        .arxiv-title { background: linear-gradient(90deg, #0d6efd 60%, #6610f2 100%); color: #fff; border-radius: 16px 16px 0 0; padding: 1.5em; font-size: 1.5em; font-weight: 700; letter-spacing: 0.5px;}
        .arxiv-meta { font-size: 1em; color: #555; margin-bottom: 1em; }
        .arxiv-abstract { background: #fff; padding: 1.5em; border-radius: 0 0 16px 16px; font-size: 1.1em; line-height: 1.7;}
        .arxiv-links a { margin-right: 1em; }
        .arxiv-scores { margin-top: 1em; font-size: 1em; }
        .navbar { background: #fff; box-shadow: 0 2px 8px rgba(0,0,0,0.04);}
        .footer { text-align: center; color: #888; padding: 2em 0 1em 0; font-size: 0.95em;}
        @media (prefers-color-scheme: dark) {
            body { background: #181a1b; color: #e4e6eb; }
            .arxiv-card { background: #23272b; box-shadow: 0 2px 8px rgba(0,0,0,0.28);}
            .arxiv-card:hover { box-shadow: 0 5px 15px rgba(0,0,0,0.25); transform: translateY(-1px); }
            .arxiv-title { background: linear-gradient(90deg, #375a7f 60%, #6f42c1 100%);}
            .arxiv-abstract { background: #23272b; color: #e4e6eb;}
            .navbar { background: #23272b; color: #e4e6eb;}
            .text-muted { color: #adb5bd !important; }
        }
        /* Improved abstract readability */
        .arxiv-abstract-text {
            font-size: 1.1em;
            line-height: 1.7;
        }
        p.big {
            line-height: 1.6;
            font-size: large;
        }
    </style>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'] ],
          processEscapes: true
        }
      });
    </script>
    <script>
    function toggleAuthors(element) {
        const fullList = element.querySelector('.author-full');
        const shortList = element.querySelector('.author-short');
        const toggleText = element.querySelector('.author-toggle');
        
        fullList.style.display = fullList.style.display === 'none' ? 'inline' : 'none';
        shortList.style.display = shortList.style.display === 'none' ? 'inline' : 'none';
        toggleText.textContent = fullList.style.display === 'none' ? ' (show all)' : ' (show less)';
    }
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>
<body>
    <nav class="navbar navbar-expand-lg mb-4">
        <div class="container">
            <a class="navbar-brand fw-bold text-primary" href="#">arXiv Daily</a>
            <span class="navbar-text">Modern arXiv Reader</span>
        </div>
    </nav>

    
    <div class="container py-4">
        <header class="mb-4"><h2 class="display-6 text-center">Analysis of Your Favorites</h2></header>
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Word Cloud</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/word_cloud.png" class="img-fluid rounded" alt="Word Cloud of favorite paper topics">
                    </div>
                </div>
            </div>
        </div>
        
        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Interest Clusters</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/backfill_cluster_map.png" class="img-fluid rounded" alt="2D Cluster plot of favorite papers">
                    </div>
                </div>
            </div>
        </div>
        
    </div>
    

    <div class="container py-4">
        <header class="mb-4"><h1 class="display-5 text-center text-primary">arXiv: Backfill (2025-11-09 to 2025-11-16)</h1></header>

        
        <div class="row justify-content-center">
            <div class="col-lg-8 mb-4">
                <div class="card">
                    <div class="card-header fw-bold">Score Distribution of All Papers Found Today</div>
                    <div class="card-body text-center p-2">
                        <img src="analysis_results/score_distribution.png" class="img-fluid rounded" alt="Score distribution of all papers found today">
                    </div>
                </div>
            </div>
        </div>
        

        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">1. Euclid preparation. Non-Gaussianity of 2-point statistics likelihood: Precise analysis of the matter power spectrum distribution
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Euclid Collaboration, J. Bel, S. Gouyou Beauchamps, P. Baratta, L. Blot, C. Carbone, P. -S. Corasaniti, E. Sefusatti, S. Escoffier, W. Gillard, et al.</span>
                                <span class="author-full" style="display: none;">Euclid Collaboration, J. Bel, S. Gouyou Beauchamps, P. Baratta, L. Blot, C. Carbone, P. -S. Corasaniti, E. Sefusatti, S. Escoffier, W. Gillard, A. Amara, S. Andreon, N. Auricchio, C. Baccigalupi, M. Baldi, S. Bardelli, P. Battaglia, A. Biviano, E. Branchini, M. Brescia, J. Brinchmann, S. Camera, G. Cañas-Herrera, V. Capobianco, V. F. Cardone, J. Carretero, S. Casas, M. Castellano, G. Castignani, S. Cavuoti, K. C. Chambers, A. Cimatti, C. Colodro-Conde, G. Congedo, C. J. Conselice, L. Conversi, Y. Copin, A. Costille, F. Courbin, H. M. Courtois, A. Da Silva, H. Degaudenzi, S. de la Torre, G. De Lucia, F. Dubath, C. A. J. Duncan, X. Dupac, M. Farina, R. Farinelli, F. Faustini, S. Ferriol, F. Finelli, N. Fourmanoit, M. Frailis, E. Franceschi, M. Fumana, S. Galeotta, K. George, B. Gillis, C. Giocoli, J. Gracia-Carpio, A. Grazian, F. Grupp, L. Guzzo, S. V. H. Haugan, W. Holmes, F. Hormuth, A. Hornstrup, K. Jahnke, M. Jhabvala, B. Joachimi, E. Keihänen, S. Kermiche, B. Kubik, M. Kunz, H. Kurki-Suonio, A. M. C. Le Brun, S. Ligori, P. B. Lilje, V. Lindholm, I. Lloro, G. Mainetti, D. Maino, E. Maiorano, O. Mansutti, O. Marggraf, K. Markovic, M. Martinelli, N. Martinet, F. Marulli, R. Massey, E. Medinaceli, Y. Mellier, M. Meneghetti, E. Merlin, G. Meylan, A. Mora, M. Moresco, L. Moscardini, C. Neissner, S. -M. Niemi, C. Padilla, S. Paltani, F. Pasian, K. Pedersen, W. J. Percival, V. Pettorino, S. Pires, G. Polenta, M. Poncet, L. A. Popa, F. Raison, A. Renzi, J. Rhodes, G. Riccio, F. Rizzo, E. Romelli, M. Roncarelli, R. Saglia, Z. Sakr, A. G. Sánchez, D. Sapone, B. Sartoris, P. Schneider, T. Schrabback, M. Scodeggio, A. Secroun, G. Seidel, M. Seiffert, S. Serrano, P. Simon, C. Sirignano, G. Sirri, L. Stanco, J. Steinwagner, P. Tallada-Crespí, A. N. Taylor, I. Tereno, N. Tessore, S. Toft, R. Toledo-Moreo, F. Torradeflot, I. Tutusaus, L. Valenziano, J. Valiviita, T. Vassallo, A. Veropalumbo, Y. Wang, J. Weller, G. Zamorani, E. Zucca, M. Ballardini, E. Bozzo, C. Burigana, R. Cabanac, M. Calabrese, D. Di Ferdinando, J. A. Escartin Vigo, L. Gabarra, J. Martín-Fleitas, S. Matthew, N. Mauri, R. B. Metcalf, A. Pezzotta, M. Pöntinen, C. Porciani, I. Risso, V. Scottez, M. Sereno, M. Tenti, M. Viel, M. Wiesmann, Y. Akrami, S. Alvi, I. T. Andika, S. Anselmi, M. Archidiacono, F. Atrio-Barandela, D. Bertacca, M. Bethermin, A. Blanchard, S. Borgani, M. L. Brown, S. Bruton, A. Calabro, B. Camacho Quevedo, F. Caro, C. S. Carvalho, T. Castro, F. Cogato, S. Conseil, S. Contarini, A. R. Cooray, S. Davini, G. Desprez, A. Díaz-Sánchez, J. J. Diaz, S. Di Domizio, J. M. Diego, A. Enia, Y. Fang, A. G. Ferrari, A. Finoguenov, A. Franco, K. Ganga, J. García-Bellido, T. Gasparetto, V. Gautard, E. Gaztanaga, F. Giacomini, F. Gianotti, G. Gozaliasl, M. Guidi, C. M. Gutierrez, A. Hall, C. Hernández-Monteagudo, H. Hildebrandt, J. Hjorth, J. J. E. Kajava, Y. Kang, V. Kansal, D. Karagiannis, K. Kiiveri, C. C. Kirkpatrick, S. Kruk, M. Lattanzi, J. Le Graet, L. Legrand, M. Lembo, F. Lepori, G. Leroy, G. F. Lesci, J. Lesgourgues, L. Leuzzi, T. I. Liaudat, J. Macias-Perez, G. Maggio, M. Magliocchetti, F. Mannucci, R. Maoli, C. J. A. P. Martins, L. Maurin, M. Miluzio, P. Monaco, C. Moretti, G. Morgante, S. Nadathur, K. Naidoo, A. Navarro-Alsina, S. Nesseris, L. Pagano, F. Passalacqua, K. Paterson, L. Patrizii, A. Pisani, D. Potter, S. Quai, M. Radovich, P. Reimberg, P. -F. Rocci, G. Rodighiero, S. Sacquegna, M. Sahlén, D. B. Sanders, E. Sarpa, A. Schneider, D. Sciotti, E. Sellentin, L. C. Smith, J. G. Sorce, K. Tanidis, C. Tao, G. Testera, R. Teyssier, S. Tosi, A. Troja, M. Tucci, C. Valieri, A. Venhola, D. Vergani, F. Vernizzi, G. Verza, P. Vielzeuf, N. A. Walton</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Non-Gaussian features in the matter power spectrum distribution on nonlinear scales are dominated by the pentaspectrum contribution, with survey geometry and integral constraints further amplifying the skewness. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We investigate the non-Gaussian features in the distribution of the matter power spectrum multipoles. Using the COVMOS method, we generate 100\,000 mock realisations of dark matter density fields in both real and redshift space across multiple redshifts and cosmological models. We derive an analytical framework linking the non-Gaussianity of the power spectrum distribution to higher-order statistics of the density field, including the trispectrum and pentaspectrum. We explore the effect of redshift-space distortions, the geometry of the survey, the Fourier binning, the integral constraint, and the shot noise on the skewness of the distribution of the power spectrum measurements. Our results demonstrate that the likelihood of the estimated matter power spectrum deviates significantly from a Gaussian assumption on nonlinear scales, particularly at low redshift. This departure is primarily driven by the pentaspectrum contribution, which dominates over the trispectrum at intermediate scales. We also examine the impact of the finiteness of the survey geometry in the context of the Euclid mission and find that both the shape of the survey and the integral constraint amplify the skewness.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.08266" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.08266" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.08266" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 14.1</span>
                        <span class="badge bg-info text-dark">Author Score: 0.99</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">2. The Dark Energy Survey Supernova Program: A Reanalysis Of Cosmology Results And Evidence For Evolving Dark Energy With An Updated Type Ia Supernova Calibration
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">B. Popovic, P. Shah, W. D. Kenworthy, R. Kessler, T. M. Davis, A. Goobar, D. Scolnic, M. Vincenzi, P. Wiseman, R. Chen, et al.</span>
                                <span class="author-full" style="display: none;">B. Popovic, P. Shah, W. D. Kenworthy, R. Kessler, T. M. Davis, A. Goobar, D. Scolnic, M. Vincenzi, P. Wiseman, R. Chen, E. Charleton, M. Acevedo, P. Armstrong, B. M. Boyd, D. Brout, R. Camilleri, J. Frieman, L. Galbany, M. Grayling, L. Kelsey, B. Rose, B. Sánchez, J. Lee, A. Möller, M. Smith, M. Sullivan, N. Shiamtanis, A. Alarcon, S. S. Allam, F. Andrade-Oliveira, S. Avila, D. Bacon, J. Blazek, S. Bocquet, D. Brooks, D. L. Burke, A. Carnero Rosell, J. Carretero, R. Cawthon, L. N. da Costa, M. E. da Silva Pereira, H. T. Diehl, S. Dodelson, P. Doel, S. Everett, C. Frohmaier, J. García-Bellido, D. Gruen, G. Gutierrez, K. Herner, S. R. Hinton, D. L. Hollowood, K. Honscheid, D. Huterer, D. J. James, N. Jeffrey, K. Kuehn, O. Lahav, S. Lee, C. Lidman, J. L. Marshall, J. Mena-Fernández, F. Menanteau, R. Miquel, J. Muir, J. Myles, R. L. C. Ogando, M. Paterno, A. A. Plazas Malagón, A. Porredon, J. Prat, R. C. Nichol, A. K. Romer, A. Roodman, E. Sanchez, D. Sanchez Cid, I. Sevilla-Noarbe, E. Suchyta, M. E. C. Swanson, C. To, D. L. Tucker, A. R. Walker, N. Weaverdyck</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Recalibrating the DES 5-year Type Ia supernova sample (DES-Dovekie) resulted in updated cosmological constraints, including $\Omega_{\rm m} = 0.330 \pm 0.015$, and reduced the evidence for evolving dark energy to a weak preference of $3.2\sigma$. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present improved cosmological constraints from a re-analysis of the Dark Energy Survey (DES) 5-year sample of Type Ia supernovae (DES-SN5YR). This re-analysis includes an improved photometric cross-calibration, recent white dwarf observations to cross-calibrate between DES and low redshift surveys, retraining the SALT3 light curve model and fixing a numerical approximation in the host galaxy colour law. Our fully recalibrated sample, which we call DES-Dovekie, comprises $\sim$1600 likely Type Ia SNe from DES and $\sim$200 low-redshift SNe from other surveys. With DES-Dovekie, we obtain $Ω_{\rm m} = 0.330 \pm 0.015$ in Flat $Λ$CDM which changes $Ω_{\rm m}$ by $-0.022$ compared to DES-SN5YR. Combining DES-Dovekie with CMB data from Planck, ACT and SPT and the DESI DR2 measurements in a Flat $w_0 w_a$CDM cosmology, we find $w_0 = -0.803 \pm 0.054$, $w_a = -0.72 \pm 0.21$. Our results hold a significance of $3.2σ$, reduced from $4.2σ$ for DES-SN5YR, to reject the null hypothesis that the data are compatible with the cosmological constant. This significance is equivalent to a Bayesian model preference odds of approximately 5:1 in favour of the Flat $w_0 w_a$CDM model. Using generally accepted thresholds for model preference, our updated data exhibits only a weak preference for evolving dark energy.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.07517" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.07517" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.07517" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 11.3</span>
                        <span class="badge bg-info text-dark">Author Score: 0.40</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Galaxy Clustering, Cosmology / Weak Lensing, Galaxy Clustering, Halo Model</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">3. Baryonic Feedback across Halo Mass: Impact on the Matter Power Spectrum
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Kyle Miller, Surhud More, Bhuvnesh Jain</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Baryonic suppression of the matter power spectrum on nonlinear scales is primarily driven by group-scale halos ($10^{13}-10^{14} M_{sun}$), necessitating the inclusion of matter redistribution beyond the virial radius for accurate modeling and making stacked group lensing a key observational probe. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Upcoming weak-lensing surveys will probe the matter distribution at a few percent level on nonlinear scales (k &gt; 1 h/Mpc) where baryonic feedback from galaxy formation modifies the clustering of matter. Using the IllustrisTNG hydrodynamical simulations, we quantify the mass and radial dependence of baryonic suppression of the matter power spectrum by selectively replacing halos in the collisionless run with their full-physics counterparts. We find that group-scale halos with log $M_{200m}/h^{-1} M_{sun}$ in [13, 14] dominate the suppression, contributing a large fraction of the total reduction in power at k ~ 5-30 h/Mpc. The suppression is smaller on either side of this mass bin. Correctly reproducing the full suppression of the power spectrum requires accounting for matter redistribution (while enforcing mass conservation) beyond the virial radius of each halo. Crucially, the same group-scale regime produces the strongest and most detectable deviations in group-galaxy lensing, making stacked group lensing a powerful observational test of feedback models. Our results motivate emulators that jointly predict the matter power spectrum and halo-matter correlations including baryonic effects, enabling unbiased cosmological inference from small scales.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.10634" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.10634" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.10634" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.6</span>
                        <span class="badge bg-info text-dark">Author Score: 0.30</span>
                        <span class="badge bg-primary">Semantic Score: 0.91</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">4. Mitigating Nonlinear Systematics in Weak Lensing Surveys II: Stability and Diagnostics with Intrinsic Alignment
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Shiming Gu, Ludovic van Waerbeke, Francis Bernardeau, Sebastien Fabbro</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Implementing the Bernardeau-Nishimichi-Taruya (BNT) transform with a $k$-cut framework yields robust and unbiased $S_8$ constraints in weak-lensing surveys even under minor intrinsic alignment (IA) model inaccuracies, while detecting major IA model mis-specifications. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The Bernardeau-Nishimichi-Taruya (BNT) transform provides a powerful framework for analysing tomographic cosmic shear data by improving the localization of shear correlations in physical scale. It operates by performing a linear combination of the shear data vector in $\ell$-space, yielding a transformed vector that is better localized in both redshift and $k$-space. BNT is particularly useful for estimating cosmological parameters while minimizing the impact of poorly understood nonlinear physics, without discarding large amounts of information as is typically done with simple scale cuts. In our previous work, we showed that BNT outperforms traditional weak-lensing analyses; however, that study did not include intrinsic alignments (IA). In the present work, we assess the robustness of our BNT-based $k$-cut framework in the presence of realistic IA models. We consider two cases: (i) when the assumed IA model used in sampling is close to, but not identical to, the true one, and (ii) when the assumed IA model is significantly mis-specified. In the first case, the $k$-cut framework yields precise and unbiased $S_8$ constraints even with limited knowledge of large-scale modes. In the second, the BNT transform serves as a powerful diagnostic tool, revealing internal inconsistencies in the assumed IA model across $k$- and redshift-space, indicating that the corresponding cosmological constraints cannot be trusted until a more accurate IA model is adopted.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.09544" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.09544" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.09544" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.0</span>
                        <span class="badge bg-info text-dark">Author Score: 0.11</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">5. Changing-Look Active Galactic Nuclei in SDSS-V: Host-Galaxy Properties and Black-Hole Scaling Relations
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Grisha Zeltyn, Benny Trakhtenbrot, Michael Eracleous, Scott F. Anderson, Claudio Ricci, Andrea Merloni, Jessie Runnoe, Mirko Krumpe, James Aird, Roberto J. Assef, et al.</span>
                                <span class="author-full" style="display: none;">Grisha Zeltyn, Benny Trakhtenbrot, Michael Eracleous, Scott F. Anderson, Claudio Ricci, Andrea Merloni, Jessie Runnoe, Mirko Krumpe, James Aird, Roberto J. Assef, Catarina Aydar, Franz E. Bauer, W. N. Brandt, Joel R. Brownstein, Johannes Buchner, Kaushik Chatterjee, Laura Duffy, Lorena Hernández-García, Héctor Hernández-Toledo, Anton M. Koekemoer, Sean Morrison, Castalia Alenka Negrete Peñaloza, Mara Salvato, Donald P. Schneider, Yue Shen, Marzena Śniegowska</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Intermediate-resolution spectroscopy of 23 Changing-Look AGNs confirms that their extreme variability is not due to obscuration and that they reside in typical AGN host galaxies, suggesting they represent a phase of normal AGN activity rather than a distinct population. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Changing-look active galactic nuclei (CL-AGNs) exhibit dramatic spectral variability on unexpectedly short timescales, challenging standard accretion flow models. Despite growing samples, the physical drivers of this extreme variability, and the potential link to host-galaxy properties, remain unknown. Regardless of the underlying mechanism, the transition between AGN-dominated and host-dominated spectra offers a unique opportunity to study relations between AGNs and their hosts within the same objects. We present intermediate-resolution spectroscopy of 23 CL-AGNs identified by the Sloan Digital Sky Survey V (SDSS-V), obtained with VLT/X-shooter and Gemini-N/GMOS. An analysis of the Mgii emission line observed in the spectra demonstrates that the majority of these sources cannot be driven by variable obscuration. Our CL-AGNs roughly follow the M_BH-sigma_* and M_BH-M_* relations of inactive galaxies, with a median black hole-to-stellar mass ratio of 0.38%, although they show hints of a shallower slope. We find no evidence that the stellar population properties of our CL-AGNs, including stellar mass, age, young stellar fraction, and star-formation rate differ from those of Type 2 AGNs in SDSS. These results suggest that CL-AGNs reside in typical AGN host galaxies and that their extreme variability is likely unrelated to host-galaxy environment, supporting the idea that CL-AGNs are not a distinct population, but rather represent a phase of normal AGN activity. This result, in turn, implies that CL-AGNs can serve as useful probes of the AGN-host connection, providing access to both AGN-dominated and host-dominated spectra of the same systems.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.07532" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.07532" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.07532" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 10.0</span>
                        <span class="badge bg-info text-dark">Author Score: 0.21</span>
                        <span class="badge bg-primary">Semantic Score: 0.90</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">6. CSST Strong Lensing Preparation: Cosmological Constraints Forecast from CSST Galaxy-Scale Strong Lensing
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Hengyu Wu, Yun Chen, Tonghua Liu, Xiaoyue Cao, Tian Li, Hui Li, Nan Li, Ran Li, Tengpeng Xu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Forecasting the cosmological constraining power of the China Space Station Telescope&#39;s (CSST) projected 100,000 strong lenses demonstrates that this massive sample will reduce the uncertainty on the dark energy equation of state ($w$) to 0.04, providing constraints twice as tight as current DESI BAO data. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Strong gravitational lensing by galaxies is a powerful tool for studying cosmology and galaxy structure. The China Space Station Telescope (CSST) will revolutionize this field by discovering up to $\sim$100,000 galaxy-scale strong lenses, a huge increase over current samples. To harness the statistical power of this vast dataset, we forecast its cosmological constraining power using the gravitational-dynamical mass combination method. We create a realistic simulated lens sample and test how uncertainties in redshift and velocity dispersion measurements affect results under ideal, optimistic, and pessimistic scenarios. We find that increasing the sample size from 100 to 10,000 systems dramatically improves precision: in the $Λ$CDM model, the uncertainty on the matter density parameter, $Ω_m$, drops from 0.2 to 0.01; in the $w$CDM model, the uncertainty on the dark energy equation of state, $w$, decreases from 0.3 to 0.04. With 10,000 lenses, our constraints on dark energy are twice as tight as those from the latest DESI BAO measurements. We also compare two parameter estimation techniques -- MultiNest sampling and Bayesian Hierarchical Modeling (BHM). While both achieve similar precision, BHM provides more robust estimates of intrinsic lens parameters, whereas MultiNest is about twice as fast. This work establishes an efficient and scalable framework for cosmological analysis with next-generation strong lensing surveys.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.08030" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.08030" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.08030" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.9</span>
                        <span class="badge bg-info text-dark">Author Score: 0.05</span>
                        <span class="badge bg-primary">Semantic Score: 0.97</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Galaxy Clustering, Cosmology / Large-Scale Structure, Weak Lensing, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">7. Mock Observations for the CSST Mission: Multi-Channel Imager--The Cluster Field
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Yushan Xie, Xiaokai Chen, Shuai Feng, Zhaojun Yan, Nan Li, Huanyuan Shan, Yin Li, Chengliang Wei, Weiwei Xu, Zhenya Zheng, et al.</span>
                                <span class="author-full" style="display: none;">Yushan Xie, Xiaokai Chen, Shuai Feng, Zhaojun Yan, Nan Li, Huanyuan Shan, Yin Li, Chengliang Wei, Weiwei Xu, Zhenya Zheng, Ran Li, Wei Chen, Zhenlei Chen, Chunyan Jiang, Dezi Liu, Lin Nie, Xiyan Peng, Lei Wang, Maochun Wu, Chun Xu, Fangting Yuan, Shen Zhang, Jing Zhong</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A comprehensive simulation framework was developed for the CSST Multi-Channel Imager (MCI) to model strong lensing galaxy clusters, incorporating ray-tracing, intra-cluster light, and SEDs to benchmark data pipelines and maximize the potential for combined strong and weak lensing analyses. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The Multi-Channel Imager (MCI), one of the instruments aboard the China Survey Space Telescope (CSST), is designed to simultaneously observe the sky in three filters, covering wavelengths from the near-ultraviolet (NUV) to the near-infrared (NIR). With its large field of view ($7.5^{\prime}\times7.5^{\prime}$), MCI is particularly well-suited for observing galaxy clusters, providing a powerful tool for investigating galaxy evolution, dark matter and dark energy through gravitational lensing. Here we present a comprehensive simulation framework of a strong lensing cluster as observed by MCI, aiming to fully exploit its capabilities in capturing lensing features. The framework simulates a strong lensing cluster from the CosmoDC2 catalog, calculating the gravitational potential and performing ray-tracing to derive the true positions, shapes and light distribution of galaxies within the cluster field. Additionally, the simulation incorporates intra-cluster light (ICL) and spectral energy distributions (SEDs), enabling further strong lensing analyses, such as ICL seperation from galaxy light and mass reconstruction combining strong and weak lensing measurements. This framework provides a critical benchmark for testing the MCI data pipeline and maximizing its potential in galaxy cluster research.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.06928" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.06928" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.06928" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.8</span>
                        <span class="badge bg-info text-dark">Author Score: 0.11</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.IM</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">8. LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Randall Balestriero, Yann LeCun</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The theoretically grounded LeJEPA framework for Joint-Embedding Predictive Architectures (JEPAs) introduces Sketched Isotropic Gaussian Regularization (SIGReg) to enforce optimal embedding distribution, resulting in a stable, scalable, and simple self-supervised objective that achieves high performance across various architectures and domains. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Learning manipulable representations of the world and its dynamics is central to AI. Joint-Embedding Predictive Architectures (JEPAs) offer a promising blueprint, but lack of practical guidance and theory has led to ad-hoc R&amp;D. We present a comprehensive theory of JEPAs and instantiate it in {\bf LeJEPA}, a lean, scalable, and theoretically grounded training objective. First, we identify the isotropic Gaussian as the optimal distribution that JEPAs&#39; embeddings should follow to minimize downstream prediction risk. Second, we introduce a novel objective--{\bf Sketched Isotropic Gaussian Regularization} (SIGReg)--to constrain embeddings to reach that ideal distribution. Combining the JEPA predictive loss with SIGReg yields LeJEPA with numerous theoretical and practical benefits: (i) single trade-off hyperparameter, (ii) linear time and memory complexity, (iii) stability across hyper-parameters, architectures (ResNets, ViTs, ConvNets) and domains, (iv) heuristics-free, e.g., no stop-gradient, no teacher-student, no hyper-parameter schedulers, and (v) distributed training-friendly implementation requiring only $\approx$50 lines of code. Our empirical validation covers 10+ datasets, 60+ architectures, all with varying scales and domains. As an example, using imagenet-1k for pretraining and linear evaluation with frozen backbone, LeJEPA reaches 79\% with a ViT-H/14. We hope that the simplicity and theory-friendly ecosystem offered by LeJEPA will reestablish self-supervised pre-training as a core pillar of AI research (\href{https://github.com/rbalestr-lab/lejepa}{GitHub repo}).</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.08544" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.08544" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.08544" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.8</span>
                        <span class="badge bg-info text-dark">Author Score: 0.07</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling / Generative Models, Representation Learning, Attention</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">9. The Atacama Cosmology Telescope. CMB Lensing from Daytime Data: A First Demonstration
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Irene Abril-Cabezas, Frank J. Qu, Joshua Kim, Mathew S. Madhavacheril, Karen Perez-Sarmiento, Zachary Atkins, Erminia Calabrese, Anthony Challinor, Mark J. Devlin, Adriaan J. Duivenvoorden, et al.</span>
                                <span class="author-full" style="display: none;">Irene Abril-Cabezas, Frank J. Qu, Joshua Kim, Mathew S. Madhavacheril, Karen Perez-Sarmiento, Zachary Atkins, Erminia Calabrese, Anthony Challinor, Mark J. Devlin, Adriaan J. Duivenvoorden, Jo Dunkley, Alexander van Engelen, Simone Ferraro, Emily Finson, Carlos Hervías-Caimapo, Matt Hilton, Arthur Kosowsky, Aleksandra Kusiak, Thibaut Louis, Niall MacCrann, Kavilan Moodley, Toshiya Namikawa, Sigurd Naess, Lyman A. Page, Adrien La Posta, Emmanuel Schaan, Neelima Sehgal, Blake D. Sherwin, Carlos E. Sierra, Cristóbal Sifón, Suzanne T. Staggs, Emilie Storer, Edward J. Wollack</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Robust analysis of challenging daytime data from the Atacama Cosmology Telescope (ACT DR6) successfully detected the CMB lensing power spectrum at $17\sigma$ significance, yielding a constraint on the amplitude of matter fluctuations $\sigma_8 = 0.826 \pm 0.027$ when combined with DESI BAO data. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present a cosmic microwave background (CMB) lensing power spectrum analysis using daytime data (11am-11pm UTC) gathered by the Atacama Cosmology Telescope (ACT) over the period 2017-2022 (ACT Data Release 6). This dataset is challenging to analyze because the Sun heats and deforms the telescope mirror, complicating the characterization of the telescope. We perform more than one hundred null and consistency checks to ensure the robustness of our measurement and its compatibility with nighttime observations. We detect the CMB lensing power spectrum at 17$σ$ significance, with an amplitude $A_\textrm{lens} = 1.045 \pm 0.063$ with respect to the prediction from the best-fit Planck-ACT CMB power spectrum $Λ$CDM cosmology. In combination with the Dark Energy Spectroscopic Instrument (DESI) Baryon Acoustic Oscillation (BAO) data, this corresponds to a constraint on the amplitude of matter fluctuations $σ_8 = 0.826 \pm 0.027$. The analysis presented here is especially relevant for ground-based millimeter-wave CMB experiments, paving the way for future analyses making use of both nighttime and daytime data to place tight constraints on cosmological parameters.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.10620" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.10620" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.10620" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.8</span>
                        <span class="badge bg-info text-dark">Author Score: 0.07</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling / Cosmological Inference, Structure Modeling, Bayesian Methods</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">10. Unbiased analysis of primordial non-Gaussianity: the multipoles of the full relativistic power spectrum
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Chris Addis, Sêcloka L. Guedezounme, Jessie Hammond, Chris Clarkson, Federico Montano, Stefano Camera, Sheean Jolicoeur, Roy Maartens</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Ignoring general relativistic effects in galaxy power spectrum multipoles introduces significant bias (up to $20\sigma$) on the measurement of primordial non-Gaussianity ($f_{\rm NL}$), a problem partially mitigated by implementing a bright-faint multi-tracer analysis that improves constraints by 15-20%. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">A major goal of ongoing and future cosmological surveys of the large-scale structure is to measure local type primordial non-Gaussianity in the galaxy power spectrum through the scale-dependent bias. General relativistic effects have been shown to be degenerate with this measurement, therefore requiring a non-Newtonian approach. In this work, we develop a consistent framework to compute integrated effects, including lensing convergence, time delay, and integrated Sachs--Wolfe, along with the local relativistic projection and wide-separation corrections in the multipoles of the power spectrum. We show that, for a \textit{Euclid}-like H$α$-line galaxy survey and a MegaMapper-like Lyman-break galaxy survey, ignoring these effects leads to a bias on the best fit measurement of the amplitude of primordial non-Gaussianity, $f_{\rm NL}$, of around $ 3\,σ$ and $ 20 \, σ$ respectively. When we include these corrections, the uncertainty in our knowledge of the luminosity function leads to further uncertainty in our measurement of $f_{\rm NL}$. In this work, we show that this degeneracy can be partly mitigated by using a bright-faint multi-tracer analysis, where the observed galaxy sample is subdivided into two separate populations based on luminosity, which provides a $15$--$20\%$ improvement on the forecasted constraints of local type $f_{\rm NL}$. In addition, we present a novel calculation of the full multi-tracer covariance with the inclusion of wide-separation corrections~-- all of these results are implemented in the \textit{Python} code \textsc{CosmoWAP}.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.09466" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.09466" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.09466" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.7</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.97</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Galaxy Clustering, Cosmology / Large-Scale Structure, Weak Lensing, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">11. JWST&#39;s GLIMPSE: an overview of the deepest probe of early galaxy formation and cosmic reionization
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Hakim Atek, John Chisholm, Vasily Kokorev, Ryan Endsley, Richard Pan, Lukas Furtak, Iryna Chemerynska, Johan Richard, Adélaïde Claeyssens, Pascal Oesch, et al.</span>
                                <span class="author-full" style="display: none;">Hakim Atek, John Chisholm, Vasily Kokorev, Ryan Endsley, Richard Pan, Lukas Furtak, Iryna Chemerynska, Johan Richard, Adélaïde Claeyssens, Pascal Oesch, Seiji Fujimoto, Rohan Naidu, Damien Korber, Daniel Schaerer, Jeremy Blaizot, Joki Rosdahl, Angela Adamo, Yoshihisa Asada, Arghyadeep Basu, Benjamin Beauchesne, Danielle Berg, Rachel Bezanson, Rychard Bouwens, Gabriel Brammer, Miroslava Dessauges-Zavadsky, Amaël Ellien, Meriam Ezziati, Qinyue Fei, Ilias Goovaerts, Sylvain Heurtier, Tiger Yu-Yang Hsiao, Michelle Jecmen, Gourav Khullar, Jean-Paul Kneib, Ivo Labbé, Floriane Leclercq, Rui Marques-Chaves, Charlotte Mason, Kristen B. W. McQuinn, Julian B. Muñoz, Priyamvada Natarajan, Alberto Saldana-Lopez, Mabel G. Stephenson, Maxime Trebitsch, Marta Volonteri, Andrea Weibel, Adi Zitrin</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Utilizing ultra-deep JWST imaging and the strong lensing of Abell S1063, the GLIMPSE program identified hundreds of faint galaxy candidates at $z=6-16$, significantly constraining the extreme faint end of the UV luminosity function. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present an overview of the JWST GLIMPSE program, highlighting its survey design, primary science goals, gravitational lensing models, and first results. GLIMPSE provides ultra-deep JWST/NIRCam imaging across seven broadband filters (F090W, F115W, F200W, F277W, F356W, F444W) and two medium-band filters (F410M, F480M), with exposure times ranging from 20 to 40 hours per filter. This yields a 5$σ$ limiting magnitude of 30.9 AB (measured in a 0.2 arcsec diameter aperture). The field is supported by extensive ancillary data, including deep HST imaging from the Hubble Frontier Fields program, VLT/MUSE spectroscopy, and deep JWST/NIRSpec medium-resolution multi-object spectroscopy. Exploiting the strong gravitational lensing of the galaxy cluster Abell S1063, GLIMPSE probes intrinsic depths beyond 33 AB magnitudes and covers an effective source-plane area of approximately 4.4 arcmin$^2$ at $z \sim 6$. The program&#39;s central aim is to constrain the abundance of the faintest galaxies from $z \sim 6$ up to the highest redshifts, providing crucial benchmarks for galaxy formation models, which have so far been tested primarily on relatively bright systems. We present an initial sample of $\sim 540$ galaxy candidates identified at $6 &lt; z &lt; 16$, with intrinsic UV magnitudes spanning $M_{\mathrm UV}$ = $-$20 to $-$12. This enables unprecedented constraints on the extreme faint end of the UV luminosity function at these epochs. In addition, GLIMPSE opens new windows for spatially resolved studies of star clusters in early galaxies and the detection and characterization of faint high-$z$ active galactic nuclei. This paper accompanies the first public data release, which includes reduced JWST and HST mosaics, photometric catalogs, and gravitational lensing models.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.07542" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.07542" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.07542" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.7</span>
                        <span class="badge bg-info text-dark">Author Score: 0.05</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Galaxy Clustering, Cosmology / Weak Lensing, Galaxy Clustering, Halo Model</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">12. Measuring the Dark Matter Self-Interaction Cross-Section with Deep Compact Clustering for Robust Machine Learning Inference
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Ethan Tregidga, David Harvey, Luca Biggio, Felix Vecchi</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A novel machine learning approach uses deep compact clustering to map galaxy clusters into a latent space, allowing for robust detection of out-of-domain data and providing trustworthy confidence estimates for cosmological parameters like the dark matter self-interaction cross-section. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We have developed a machine learning algorithm capable of detecting ``out-of-domain data&#39;&#39; for trustworthy cosmological inference. By using data from two separate suites of cosmological simulations, we show that our algorithm is able to determine whether ``observed&#39;&#39; data is consistent with its training domain, returning confidence estimates as well as accurate parameter estimations. We apply our algorithm to two-dimensional images of galaxy clusters from the BAHAMAS-SIDM and DARKSKIES simulations with the aim to measure the self-interaction cross-section of dark matter. Through deep compact clustering we construct an informative latent space where galaxy clusters are mapped to the latent space forming ``latent-clusters&#39;&#39; for each simulation, with the location of the latent-cluster corresponding to the macroscopic parameters, such as the cross-section, $σ_{\rm DM}/m$. We then pass through mock observations, where the location of the observed latent-cluster informs us of which properties are shared with the training data. If the observed latent-cluster shares no similarities with latent-clusters from the known simulations, we can conclude that our simulations do not represent the observations and discard any parameter estimations, thus providing us with a method to measure machine learning confidence. This method serves as a blueprint for transparent and robust inference that is in demand in scientific machine learning.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.09660" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.09660" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.09660" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">13. A new multiprobe analysis of modified gravity and evolving dark energy
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Zhiyu Lu, Théo Simon, Yi-Fu Cai</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Combining full-shape BOSS data analysis with tomographic angular power spectra and other cosmological datasets significantly improves constraints on the dark energy equation of state parameters, strengthening the evidence for evolving dark energy to $4.6\sigma$ while remaining compatible with General Relativity within $2\sigma$. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We study the $(w_0, \, w_a)$ parametrization of the dark energy (DE) equation of state, with and without the effective field theory of dark energy (EFTofDE) framework to describe the DE perturbations, parametrized here by the braiding parameter $α_B$ and the running of the Planck mass $α_M$. We combine the EFTofLSS full-shape analysis of the power spectrum and bispectrum of BOSS data with the tomographic angular power spectra $C_\ell^{gg}$, $C_\ell^{κg}$, $C_\ell^{Tg}$ and $C_\ell^{Tκ}$, where $g$, $κ$ and $T$ stand for the DESI luminous red galaxy map, Planck PR4 lensing map and Planck PR4 temperature map, respectively. To analyze these angular power spectra, we go beyond the Limber approximation, allowing us to include large-scales data in $C_\ell^{gg}$. The combination of all these probes with Planck PR4, DESI DR2 BAO and DES Y5 improves the constraint on the 2D posterior distribution of $\{w_0, \, w_a\}$ by $\sim 50 \%$ and increases the preference for evolving dark energy over $Λ$ from $3.8 σ$ to $4.6 σ$. When we remove BAO and supernovae data, we obtain a hint for evolving dark energy at $2.3 σ$. Regarding the EFTofDE parameters, we improve the constraints on $α_B$ and $α_M$ by $\sim 40 \%$ and $50 \%$ respectively, finding results compatible with general relativity at $\sim 2 σ$. We show that these constraints do not depend on the choice of the BAO and supernovae likelihoods.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.10616" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.10616" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.10616" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Galaxy Clustering, Cosmology</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">14. Accelerated inference of microlensed gravitational waves with machine learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Marienza Caldarola, Srashti Goyal, Nihar Gupte, Stephen R. Green, Miguel Zumalacárregui</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Employing the DINGO machine learning framework with neural posterior estimation enables efficient parameter estimation for microlensed gravitational wave signals, providing a rapid method for identifying lensed events and calculating background Bayes factors, though care is needed when sampling foreground events. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Gravitational waves (GWs) propagating through the universe can be microlensed by stellar and intermediate-mass objects. Lensing induces frequency-dependent amplification of GWs, which can be computed using \texttt{GLoW}, an accurate code suitable for evaluating this factor for generic lens models and arbitrary impact parameters depending on the lens configuration. For parameter inference, we employ the DINGO algorithm, a machine learning framework based on neural posterior estimation, a simulation-based inference method that uses normalizing flows to efficiently approximate posterior distributions of the physical parameters. As a proof-of-principle, we demonstrate that it enables efficient parameter estimation of diffracted GW signals using an isolated point mass lens model. This method can be useful for rapidly identifying microlensed events within large GW catalogs and for conducting population studies of compact binaries. Compared to traditional parameter estimation techniques, we find that combining DINGO with importance sampling can provide efficient estimation of the background Bayes-factor distribution, which is required in evaluating the significance of candidate lensed events. However, for foreground (lensed) events, care must be taken, as sampling efficiency can decrease when the lensed data lie outside the distribution learned by the unlensed DINGO network. Our framework can be naturally extended to more complex and realistic lens models, allowing detailed analyses of the microlensed GWs.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.08486" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.08486" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.08486" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">15. Mock Observations for the CSST Mission: Main Surveys--An Overview of Framework and Simulation Suite
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Cheng-Liang Wei, Guo-Liang Li, Yue-Dong Fang, Xin Zhang, Yu Luo, Hao Tian, De-Zi Liu, Xian-Ming Meng, Zhang Ban, Xiao-Bo Li, et al.</span>
                                <span class="author-full" style="display: none;">Cheng-Liang Wei, Guo-Liang Li, Yue-Dong Fang, Xin Zhang, Yu Luo, Hao Tian, De-Zi Liu, Xian-Ming Meng, Zhang Ban, Xiao-Bo Li, Zun Luo, Jing-Tian Xian, Wei Wang, Xi-Yan Peng, Nan Li, Ran Li, Li Shao, Tian-Meng Zhang, Jing Tang, Yang Chen, Zhao-Xiang Qi, Zi-Huang Cao, Huan- Yuan Shan, Lin Nie, Lei Wang, Zizhao He, Rui-Biao Luo, Quan-Yu Liu, Zhao-Jun Yan</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A comprehensive, high-fidelity simulation pipeline based on GalSim was developed using the latest instrumental specifications to generate accurate pixel-level mock observations for the Chinese Space Station Survey Telescope (CSST), optimizing survey strategies and ensuring robust cosmological measurements. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The Chinese Space Station Survey Telescope (CSST) is a flagship space-based observatory. Its main survey camera is designed to conduct high spatial resolution near-ultraviolet to near-infrared imaging and low-resolution spectroscopic surveys. To maximize the scientific output of CSST, we have developed a comprehensive, high-fidelity simulation pipeline for reproducing both imaging and spectroscopic observations. This paper presents an overview of the simulation framework, detailing its implementation and components. Built upon the GalSim package and incorporating the latest CSST instrumental specifications, our pipeline generates pixel-level mock observations that closely replicate the expected instrumental and observational conditions. The simulation suite integrates realistic astrophysical object catalogs, instrumental effects, point spread function (PSF) modeling, and observational noises to produce accurate synthetic data. We describe the key processing stages of the simulation, from constructing the input object catalogs to modeling the telescope optics and detector responses. Furthermore, we introduce the most recent release of simulated datasets, which provide a crucial testbed for data processing pipeline developments, calibration strategies, and scientific analyses, ensuring that CSST will meet its stringent requirements. Our pipeline serves as a vital tool for optimizing CSST main survey strategies and ensuring robust cosmological measurements.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.06970" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.06970" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.06970" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.05</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.IM</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">16. JWST lensed quasar dark matter survey IV: Stringent warm dark matter constraints from the joint reconstruction of extended lensed arcs and quasar flux ratios
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Daniel Gilman, A. M. Nierenberg, T. Treu, C. Gannon, X. Du, H. Paugnat, S. Birrer, A. J. Benson, P. Mozumdar, K. C. Wong, et al.</span>
                                <span class="author-full" style="display: none;">Daniel Gilman, A. M. Nierenberg, T. Treu, C. Gannon, X. Du, H. Paugnat, S. Birrer, A. J. Benson, P. Mozumdar, K. C. Wong, D. Williams, R. E. Keeley, K. N. Abazajian, T. Anguita, V. N. Bennert, S. G. Djorgovski, A. Kusenko, M. Malkan, T. Morishita, V. Motta, L. A. Moustakas, W. Sheu, D. Sluse, M. Stiavelli</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> By simultaneously modeling lensed arcs and flux ratios in 28 quadruple strong lenses observed by JWST MIRI, researchers derived the strongest constraints on warm dark matter free-streaming, setting the half-mode mass $m_{\rm{hm}} &lt; 10^{7.2} \mathrm{M}_{\odot}$ and precisely measuring subhalo abundance. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present a measurement of the free-streaming length of dark matter (DM) and subhalo abundance around 28 quadruple image strong lenses using observations from JWST MIRI presented in Paper III of this series. We improve on previous inferences on DM properties from lensed quasars by simultaneously reconstructing extended lensed arcs with image positions and relative magnifications (flux ratios). Our forward modeling framework generates full populations of subhalos, line-of-sight halos, and globular clusters, uses an accurate model for subhalo tidal evolution, and accounts for free-streaming effects on halo abundance and concentration. Modeling lensed arcs leads to more-precise model-predicted flux ratios, breaking covariance between subhalo abundance and the free-streaming scale parameterized by the half-mode mass $m_{\rm{hm}}$. Assuming subhalo abundance predicted by the semi-analytic model {\tt{galacticus}} (N-body simulations), we infer (Bayes factor of 10:1) $m_{\rm{hm}} &lt; 10^{7.4} \mathrm{M}_{\odot}$ ($m_{\rm{hm}} &lt; 10^{7.2} \mathrm{M}_{\odot}$), a 0.4 dex (0.3 dex) improvement relative to omitting lensed arcs. These bounds correspond to lower limits on thermal relic DM particle masses of $7.4$ and $8.4$ keV, respectively. Conversely, assuming DM is cold, we infer a projected mass in subhalos ($10^6 &lt; m/M_{\odot}&lt;10^{10.7}$) of $1.6_{-1.1}^{+2.4} \times 10^7 \ \mathrm{M}_{\odot} \ \rm{kpc^{-2}}$ at $95 \%$ confidence. This is consistent with {\tt{galacticus}} predictions ($0.6 \times 10^7 \mathrm{M}_{\odot} \ \rm{kpc^{-2}}$), but in tension with recent N-body simulations ($0.3 \times 10^7 \mathrm{M}_{\odot} \ \rm{kpc^{-2}}$). Our results are the strongest limits on WDM, and the most precise measurement of subhalo abundance around strong lenses. Further improvements will follow from the large sample of lenses to be discovered by Euclid, Rubin, and Roman.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.07513" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.07513" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.07513" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Galaxy Clustering, Cosmology / Weak Lensing, Galaxy Clustering, Halo Model</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">17. Covariance Scattering Transforms
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Andrea Cavallo, Ayushman Raghuvanshi, Sundeep Prabhakar Chepuri, Elvin Isufi</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Introducing Covariance Scattering Transforms (CSTs), deep untrained networks that utilize covariance wavelets to capture spectral patterns, provides stable and expressive hierarchical data representations that are less sensitive to finite-sample estimation errors than PCA, proving effective in low-data settings. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Machine learning and data processing techniques relying on covariance information are widespread as they identify meaningful patterns in unsupervised and unlabeled settings. As a prominent example, Principal Component Analysis (PCA) projects data points onto the eigenvectors of their covariance matrix, capturing the directions of maximum variance. This mapping, however, falls short in two directions: it fails to capture information in low-variance directions, relevant when, e.g., the data contains high-variance noise; and it provides unstable results in low-sample regimes, especially when covariance eigenvalues are close. CoVariance Neural Networks (VNNs), i.e., graph neural networks using the covariance matrix as a graph, show improved stability to estimation errors and learn more expressive functions in the covariance spectrum than PCA, but require training and operate in a labeled setup. To get the benefits of both worlds, we propose Covariance Scattering Transforms (CSTs), deep untrained networks that sequentially apply filters localized in the covariance spectrum to the input data and produce expressive hierarchical representations via nonlinearities. We define the filters as covariance wavelets that capture specific and detailed covariance spectral patterns. We improve CSTs&#39; computational and memory efficiency via a pruning mechanism, and we prove that their error due to finite-sample covariance estimations is less sensitive to close covariance eigenvalues compared to PCA, improving their stability. Our experiments on age prediction from cortical thickness measurements on 4 datasets collecting patients with neurodegenerative diseases show that CSTs produce stable representations in low-data settings, as VNNs but without any training, and lead to comparable or better predictions w.r.t. more complex learning models.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.08878" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.08878" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.08878" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling / Generative Models, Representation Learning, Attention</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">18. Compensating Distribution Drifts in Class-incremental Learning of Pre-trained Vision Transformers
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Xuan Rao, Simian Xu, Zheng Li, Bo Zhao, Derong Liu, Mingming Ha, Cesare Alippi</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Sequential Learning with Drift Compensation (SLDC) utilizes latent space transition operators and knowledge distillation to align feature distributions across tasks, successfully mitigating representation drift in sequential fine-tuning and significantly improving performance in class-incremental learning. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Recent advances have shown that sequential fine-tuning (SeqFT) of pre-trained vision transformers (ViTs), followed by classifier refinement using approximate distributions of class features, can be an effective strategy for class-incremental learning (CIL). However, this approach is susceptible to distribution drift, caused by the sequential optimization of shared backbone parameters. This results in a mismatch between the distributions of the previously learned classes and that of the updater model, ultimately degrading the effectiveness of classifier performance over time. To address this issue, we introduce a latent space transition operator and propose Sequential Learning with Drift Compensation (SLDC). SLDC aims to align feature distributions across tasks to mitigate the impact of drift. First, we present a linear variant of SLDC, which learns a linear operator by solving a regularized least-squares problem that maps features before and after fine-tuning. Next, we extend this with a weakly nonlinear SLDC variant, which assumes that the ideal transition operator lies between purely linear and fully nonlinear transformations. This is implemented using learnable, weakly nonlinear mappings that balance flexibility and generalization. To further reduce representation drift, we apply knowledge distillation (KD) in both algorithmic variants. Extensive experiments on standard CIL benchmarks demonstrate that SLDC significantly improves the performance of SeqFT. Notably, by combining KD to address representation drift with SLDC to compensate distribution drift, SeqFT achieves performance comparable to joint training across all evaluated datasets. Code: https://github.com/raoxuan98-hash/sldc.git.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.09926" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.09926" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.09926" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling / Generative Models, Representation Learning, Attention</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">19. TuckA: Hierarchical Compact Tensor Experts for Efficient Fine-Tuning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Qifeng Lei, Zhiyong Yang, Qianqian Xu, Cong Hua, Peisong Wen, Qingming Huang</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Tucker Adaptation (TuckA) introduces a parameter-efficient fine-tuning technique that leverages Tucker decomposition to create a compact, multi-expert structure with hierarchical organization and efficient batch-level routing, enabling the model to capture complex data diversity better than single-expert adapters. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Efficiently fine-tuning pre-trained models for downstream tasks is a key challenge in the era of foundation models. Parameter-efficient fine-tuning (PEFT) presents a promising solution, achieving performance comparable to full fine-tuning by updating only a small number of adaptation weights per layer. Traditional PEFT methods typically rely on a single expert, where the adaptation weight is a low-rank matrix. However, for complex tasks, the data&#39;s inherent diversity poses a significant challenge for such models, as a single adaptation weight cannot adequately capture the features of all samples. To address this limitation, we explore how to integrate multiple small adaptation experts into a compact structure to defeat a large adapter. Specifically, we propose Tucker Adaptation (TuckA), a method with four key properties: (i) We use Tucker decomposition to create a compact 3D tensor where each slice naturally serves as an expert. The low-rank nature of this decomposition ensures that the number of parameters scales efficiently as more experts are added. (ii) We introduce a hierarchical strategy that organizes these experts into groups at different granularities, allowing the model to capture both local and global data patterns. (iii) We develop an efficient batch-level routing mechanism, which reduces the router&#39;s parameter size by a factor of $L$ compared to routing at every adapted layer (where $L$ is the number of adapted layers) (iv) We propose data-aware initialization to achieve loss-free expert load balancing based on theoretical analysis. Extensive experiments on benchmarks in natural language understanding, image classification, and mathematical reasoning speak to the efficacy of TuckA, offering a new and effective solution to the PEFT problem.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.06859" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.06859" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.06859" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling / Generative Models, Representation Learning, Attention</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">20. Galactification: painting galaxies onto dark matter only simulations using a transformer-based model
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Shivam Pandey, Christopher C. Lovell, Chirag Modi, Benjamin D. Wandelt</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A multi-modal, transformer-based accelerated forward model rapidly generates realistic mock galaxy catalogs conditioned on dark matter simulations, accurately reproducing the full spatial distribution, physical properties, and conditional dependencies of galaxies across varying cosmological parameters. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Connecting the formation and evolution of galaxies to the large-scale structure is crucial for interpreting cosmological observations. While hydrodynamical simulations accurately model the correlated properties of galaxies, they are computationally prohibitive to run over volumes that match modern surveys. We address this by developing a framework to rapidly generate mock galaxy catalogs conditioned on inexpensive dark-matter-only simulations. We present a multi-modal, transformer-based model that takes 3D dark matter density and velocity fields as input, and outputs a corresponding point cloud of galaxies with their physical properties. We demonstrate that our trained model faithfully reproduces a variety of galaxy summary statistics and correctly captures their variation with changes in the underlying cosmological and astrophysical parameters, making it the first accelerated forward model to capture all the relevant galaxy properties, their full spatial distribution, and their conditional dependencies in hydrosimulations.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.08438" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.08438" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.08438" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.02</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">21. Extreme Model Compression with Structured Sparsity at Low Precision
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Dan Liu, Nikita Dvornik, Xue Liu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The SLOPE framework successfully integrates structured sparsity and low-precision quantization in deep neural networks using angular alignment regularization, yielding up to $20\times$ model size reduction while preserving high accuracy across various vision tasks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Deep neural networks (DNNs) are used in many applications, but their large size and high computational cost make them hard to run on devices with limited resources. Two widely used techniques to address this challenge are weight quantization, which lowers the precision of all weights, and structured sparsity, which removes unimportant weights while retaining the important ones at full precision. Although both are effective individually, they are typically studied in isolation due to their compounded negative impact on model accuracy when combined. In this work, we introduce SLOPE Structured Sparsity at Low Precision), a unified framework, to effectively combine structured sparsity and low-bit quantization in a principled way. We show that naively combining sparsity and quantization severely harms performance due to the compounded impact of both techniques. To address this, we propose a training-time regularization strategy that minimizes the discrepancy between full-precision weights and their sparse, quantized counterparts by promoting angular alignment rather than direct matching. On ResNet-18, SLOPE achieves $\sim20\times$ model size reduction while retaining $\sim$99% of the original accuracy. It consistently outperforms state-of-the-art quantization and structured sparsity methods across classification, detection, and segmentation tasks on models such as ResNet-18, ViT-Small, and Mask R-CNN.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.08360" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.08360" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.08360" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling / Generative Models, Representation Learning, Attention</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">22. Test-time Diverse Reasoning by Riemannian Activation Steering
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Ly Tran Ho Khanh, Dongxuan Zhu, Man-Chung Yue, Viet Anh Nguyen</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> To overcome the output diversity limit in Best-of-$N$ reasoning, a novel unsupervised test-time Riemannian activation steering strategy maximizes the volume spanned by intervened activations, significantly improving generative diversity and solution accuracy on complex tasks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Best-of-$N$ reasoning improves the accuracy of language models in solving complex tasks by sampling multiple candidate solutions and then selecting the best one based on some criteria. A critical bottleneck for this strategy is the output diversity limit, which occurs when the model generates similar outputs despite stochastic sampling, and hence recites the same error. To address this lack of variance in reasoning paths, we propose a novel unsupervised activation steering strategy that simultaneously optimizes the steering vectors for multiple reasoning trajectories at test time. At any synchronization anchor along the batch generation process, we find the steering vectors that maximize the total volume spanned by all possible intervened activation subsets. We demonstrate that these steering vectors can be determined by solving a Riemannian optimization problem over the product of spheres with a log-determinant objective function. We then use a Riemannian block-coordinate descent algorithm with a well-tuned learning rate to obtain a stationary point of the problem, and we apply these steering vectors until the generation process reaches the subsequent synchronization anchor. Empirical evaluations on popular mathematical benchmarks demonstrate that our test-time Riemannian activation steering strategy outperforms vanilla sampling techniques in terms of generative diversity and solution accuracy.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.08305" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.08305" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.08305" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling / Generative Models, Representation Learning, Attention</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">23. Gravity-Selected Galaxy Clusters: a Tight Mass-Richness relation and an unclear Compton $Y$-richness trend
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>S. Andreon, M. Radovich</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Analysis of 13 gravity-selected galaxy clusters reveals an exceptionally tight richness-mass scaling relation with scatter of only $\sim0.05$ dex, demonstrating that red-sequence richness is a more effective mass proxy than Compton $Y$ for this sample. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">This paper, the third in a series, investigates the scaling relations between optical richness, weak-lensing mass, and Compton $Y$ for a sample of galaxy clusters selected purely by the effect of their gravitational potential on the shapes of background galaxies. This selection method is uncommon, as most cluster samples in the literature are selected based on signals originating from cluster baryons. We analize a complete sample of 13 gravity-selected clusters at intermediate redshifts (with $0.12 \leq z_{phot} \leq 0.40$) with weak-lensing signal-to-noise ratios exceeding 7. We measured cluster richness by counting red-sequence galaxies, identifying two cases of line-of-sight projections in the process, later confirmed by spectroscopic data. Both clusters are sufficiently separated in redshift that contamination in richness can be straighforwardly dealt because the two red sequences do not blend each other. We find an exceptionally tight richness--mass relation using our red-sequence-based richness estimator, with a scatter of just $\sim0.05$ dex, smaller than the intrinsic scatter of Compton Y with mass for the same sample. The lower scatter highlights the effectiveness of richness compared to Compton $Y$. No outliers are found in the richness-mass scaling, whether or not one cluster with a mass likely affected by projection effects is included in the sample. In the Compton $Y$-richness plane, the data do not delineate a clear trend. The limited sample size is not the sole reason for the unclear relation between Compton $Y$ and richness, since the same sample, with identical richness values, exhibits a highly significant and tight mass-richness correlation.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.08693" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.08693" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.08693" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Galaxy Clustering, Cosmology / Weak Lensing, Galaxy Clustering, Halo Model</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">24. Bayesian and Machine-Learning Analyses of Nonminimal $f(Q)$ Gravity and $H_0$ Tension
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Simran Arora, Mridul Patel</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Statistical analysis and machine learning evaluation of a nonminimally coupled $f(Q)$ gravity model, using comprehensive cosmological data sets, confirms its flexibility for late-time cosmology and demonstrates its ability to partially alleviate the $H_0$ tension. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">In this study, the cosmological implications of nonminimally coupled $f(Q)$ gravity are examined within the metric-affine formalism, in which the nonmetricity scalar $Q$ couples directly to the matter Lagrangian. Within the symmetric teleparallel framework, a representative $f(Q)$ model is constructed, and the corresponding background cosmological equations are derived. The analysis aims to test whether this geometric formulation yields more consistent realizations of nonminimal matter-geometry couplings. A comprehensive statistical MCMC analysis is performed using cosmic chronometers, DESI BAO DR2, and Type Ia supernovae from the Pantheon+, DESY5, and Union3 samples. To complement the statistical study, we employ machine learning methods, such as linear regression, support vector regression (SVR), and random forest algorithms, to evaluate the predictive performance and robustness of the data. The results indicate that a partial alleviation of the $H_0$ tension can be achieved for a broad range of parameter choices. Nonetheless, $f(Q)$ gravity emerges as a promising and flexible framework for late-time cosmology, motivating further exploration of extended models consistent with all observations.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.06332" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.06332" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.06332" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.95</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">25. JWST Lensed Quasar Dark Matter Survey III: Dark Matter Sensitive Flux Ratios and Warm Dark Matter Constraint from the Full Sample
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">R. E. Keeley, A. M. Nierenberg, D. Gilman, T. Treu, X. Du, C. Gannon, P. Mozumdar, K. C. Wong, H. Paugnat, S. Birrer, et al.</span>
                                <span class="author-full" style="display: none;">R. E. Keeley, A. M. Nierenberg, D. Gilman, T. Treu, X. Du, C. Gannon, P. Mozumdar, K. C. Wong, H. Paugnat, S. Birrer, M. Malkan, A. J. Benson, K. N. Abazajian, T. Anguita, V. N. Bennert, S. G. Djorgovski, S. F. Hoenig, A. Kusenko, T. Morishita, V. Motta, L. A. Moustakas, W. Sheu, D. Sluse, D. Stern, M. Stiavelli, D. Williams</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Measurements of warm dust flux ratios from 31 strongly-lensed quasars using JWST MIRI provide a factor of two improvement in sensitivity, yielding one of the strongest constraints on the halo mass function turnover and inferring a half-mode mass $m_{\rm{hm}} \gtrsim 5.6$ keV for thermal dark matter. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present the full sample of measurements of the warm dust emission of 31 strongly-lensed, multiply imaged quasars, observed with JWST MIRI multiband imaging, which we use to constrain the particle properties of dark matter. The strongly lensed warm dust region of quasars is compact and statistically sensitive to a population of dark matter halos down to masses of $10^6$ M$_\odot$. The high spatial resolution and infrared sensitivity of MIRI make it uniquely suited to measure multiply imaged warm dust emission from quasars and thus to infer the properties of low-mass dark halos. We use the measured flux ratios to test for a warm dark matter turnover in the halo mass function. To infer the dark matter parameters, we use a forward modeling pipeline which explores dark matter parameters while also accounting for tidal stripping effects on subhalos, globular clusters, and complex deflector macromodels with $m=1, m=3, \text{ and } m=4$ elliptical multipole moments. Adopting a comparable prior on the projected density of substructure to our previous analyses, the data presented here provide a factor of 2 improvement in sensitivity to a turnover in the halo mass function. Assuming subhalo abundance predicted by the semi-analytic model galacticus we infer with a Bayes factor of 10:1, a half-mode mass $m_{\rm{hm}} 5.6 keV for a thermally produced dark matter particle). If instead we use a prior from N-body simulations, we infer $m_{\rm{hm}} 6.9 keV). This is one of the strongest constraints to date on a turnover on the halo mass function, and the flux ratios and inference methodology presented here can be used to test a broad range of dark matter physics.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.07765" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.07765" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.07765" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.5</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Weak Lensing, Galaxy Clustering, Cosmology / Weak Lensing, Galaxy Clustering, Halo Model</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">26. Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Valentino F. Foit, David W. Hogg, Soledad Villar</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Applying inexpensive group averaging at test time to machine learning models of differential equations consistently improves prediction accuracy, reducing evaluation loss by up to 37% by enforcing exact symmetries without requiring changes to model structure or training. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Many machine learning tasks in the natural sciences are precisely equivariant to particular symmetries. Nonetheless, equivariant methods are often not employed, perhaps because training is perceived to be challenging, or the symmetry is expected to be learned, or equivariant implementations are seen as hard to build. Group averaging is an available technique for these situations. It happens at test time; it can make any trained model precisely equivariant at a (often small) cost proportional to the size of the group; it places no requirements on model structure or training. It is known that, under mild conditions, the group-averaged model will have a provably better prediction accuracy than the original model. Here we show that an inexpensive group averaging can improve accuracy in practice. We take well-established benchmark machine learning models of differential equations in which certain symmetries ought to be obeyed. At evaluation time, we average the models over a small group of symmetries. Our experiments show that this procedure always decreases the average evaluation loss, with improvements of up to 37\% in terms of the VRMSE. The averaging produces visually better predictions for continuous dynamics. This short paper shows that, under certain common circumstances, there are no disadvantages to imposing exact symmetries; the ML4PS community should consider group averaging as a cheap and simple way to improve model accuracy.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.09573" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.09573" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.09573" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.04</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">27. Finding the boundary: Using galaxy membership to inform galaxy cluster extent through machine learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Christine Hao, Stephanie O&#39;Neil, Mark Vogelsberger, Vinh Tran, Lamiya Mowla, Joshua S. Speagle</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A data-driven neural network classifier applied to IllustrisTNG simulations maps the cluster-field galaxy transition region as a broad, scattered area of mixed populations, revealing that dynamical changes dominate the inner regions while gas properties transition in the outermost regions of high-mass clusters. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The spatial extent of the environment&#39;s impact on galaxies marks a transitional region between cluster and field galaxies. We present a data-driven method to identify this region in galaxy clusters with masses $M_{200\rm ,mean}&gt;10^{13} M_{\odot}$ at $z = 0$. Using resolved galaxy samples from the largest simulation volume of IllustrisTNG (TNG300-1), we examine how galaxy properties vary as a function of distance to the closest cluster. We train neural networks to classify galaxies into cluster and field galaxies based on their intrinsic properties. Using this classifier, we present the first quantitative and probabilistic map of the transition region. It is represented as a broad and intrinsically scattered region near cluster outskirts, rather than a sharp physical boundary. This is the physical detection of a mixed population. In order to determine transition regions of different physical processes by training property-specific models, we categorise galaxy properties based on their underlying physics, i.e. gas, stellar, and dynamical. Changes to the dynamical properties dominate the innermost regions of the clusters of all masses. Stellar properties and gas properties, on the other hand, exhibit transitions at similar locations for low mass clusters, yet gas properties have transitions in the outermost regions for high mass clusters. These results have implications for cluster environmental studies in both simulations and observations, particularly in refining the definition of cluster boundaries while considering environmental preprocessing and how galaxies evolve under the effect of the cluster environment.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.07516" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.07516" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.07516" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.06</span>
                        <span class="badge bg-primary">Semantic Score: 0.91</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">28. Belief Net: A Filter-Based Framework for Learning Hidden Markov Models from Observations
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Reginald Zhiyan Chen, Heng-Sheng Chang, Prashant G. Mehta</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Belief Net, a novel, fully interpretable framework, learns Hidden Markov Model parameters via gradient-based optimization by structuring the forward filter as a decoder-only neural network, demonstrating faster convergence than Baum-Welch and robustness where spectral methods fail. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Hidden Markov Models (HMMs) are fundamental for modeling sequential data, yet learning their parameters from observations remains challenging. Classical methods like the Baum-Welch (EM) algorithm are computationally intensive and prone to local optima, while modern spectral algorithms offer provable guarantees but may produce probability outputs outside valid ranges. This work introduces Belief Net, a novel framework that learns HMM parameters through gradient-based optimization by formulating the HMM&#39;s forward filter as a structured neural network. Unlike black-box Transformer models, Belief Net&#39;s learnable weights are explicitly the logits of the initial distribution, transition matrix, and emission matrix, ensuring full interpretability. The model processes observation sequences using a decoder-only architecture and is trained end-to-end with standard autoregressive next-observation prediction loss. On synthetic HMM data, Belief Net achieves superior convergence speed compared to Baum-Welch, successfully recovering parameters in both undercomplete and overcomplete settings where spectral methods fail. Comparisons with Transformer-based models are also presented on real-world language data.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.10571" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.10571" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.10571" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling / Generative Models, Representation Learning, Attention</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">29. Adaptive Regularization for Large-Scale Sparse Feature Embedding Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Mang Li, Wei Lyu</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Theoretical analysis identifies large-scale sparse categorical features as the fundamental cause of one-epoch overfitting in CTR/CVR models, leading to the proposal of an adaptive regularization method that prevents performance degradation during multi-epoch training and improves single-epoch results. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The one-epoch overfitting problem has drawn widespread attention, especially in CTR and CVR estimation models in search, advertising, and recommendation domains. These models which rely heavily on large-scale sparse categorical features, often suffer a significant decline in performance when trained for multiple epochs. Although recent studies have proposed heuristic solutions, they have not clearly identified the fundamental cause of this phenomenon. In this work, we provide a theoretical analysis that explains why overfitting occurs in models that use large-scale sparse categorical features. Based on this analysis, we propose an adaptive regularization method to address it. Our approach not only prevents the severe performance degradation observed during multi-epoch training, but also improves model performance within a single epoch. This method has already been deployed in online production systems.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.06374" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.06374" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.06374" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling / Generative Models, Representation Learning, Attention</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">30. Towards a Machine Learning Solution for Hubble Tension: Physics-Informed Neural Network (PINN) Analysis of Tsallis Holographic Dark Energy in Presence of Neutrinos
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Muhammad Yarahmadi, Amin Salehi</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A Physics-Informed Neural Network framework successfully reconstructs the Tsallis Holographic Dark Energy model with massive neutrinos (THDE+$ν$), significantly reducing the Hubble tension from $5\sigma$ down to $0.5\sigma-2.2\sigma$ and constraining the total neutrino mass to $\Sigma m_\nu &lt; 0.11\,\text{eV}$. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We present a Physics-Informed Neural Network (PINN) framework for reconstructing the redshift-dependent Hubble parameter \(H(z)\) within the Tsallis Holographic Dark Energy (THDE) model extended by massive neutrinos. In this approach, the modified Friedmann equation is incorporated into the neural network loss function, enabling training on Cosmic Chronometers data up to \(z \leq 2\). The framework allows for the simultaneous estimation of the Hubble constant \(H_0\), the neutrino density parameter \(Ω_ν\), and the Tsallis non-extensivity index \(δ\). Uncertainty quantification is performed through dropout simulations, resulting in statistically consistent \(1σ\) confidence bands. Our results show that the THDE+$ν$ model, reconstructed via PINN, alleviates the statistical Hubble tension from the canonical \(\sim 5σ\) level down to a range of \(0.5σ\leq T \leq 2.2σ\), depending on the redshift sampling. Additionally, we constrain the total neutrino mass to \(Σm_ν&lt; 0.11\,\text{eV}\). A detailed comparison with the traditional Markov Chain Monte Carlo (MCMC) analysis demonstrates the consistency of both methods, while highlighting the competitiveness of the PINN-based THDE framework as a robust, data-driven approach for non-parametric cosmological inference within generalized thermodynamics.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.09706" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.09706" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.09706" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">31. Automatic Grid Updates for Kolmogorov-Arnold Networks using Layer Histograms
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jamison Moody, James Usevitch</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> AdaptKAN improves Kolmogorov-Arnold Networks by autonomously updating the domain grid via a histogram algorithm, achieving superior or equivalent performance to existing KANs and MLPs on symbolic regression, classification, and out-of-distribution detection. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Kolmogorov-Arnold Networks (KANs) are a class of neural networks that have received increased attention in recent literature. In contrast to MLPs, KANs leverage parameterized, trainable activation functions and offer several benefits including improved interpretability and higher accuracy on learning symbolic equations. However, the original KAN architecture requires adjustments to the domain discretization of the network (called the &#34;domain grid&#34;) during training, creating extra overhead for the user in the training process. Typical KAN layers are not designed with the ability to autonomously update their domains in a data-driven manner informed by the changing output ranges of previous layers. As an added benefit, this histogram algorithm may also be applied towards detecting out-of-distribution (OOD) inputs in a variety of settings. We demonstrate that AdaptKAN exceeds or matches the performance of prior KAN architectures and MLPs on four different tasks: learning scientific equations from the Feynman dataset, image classification from frozen features, learning a control Lyapunov function, and detecting OOD inputs on the OpenOOD v1.5 benchmark.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.08570" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.08570" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.08570" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling / Generative Models, Representation Learning, Attention</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">32. Sampling and Loss Weights in Multi-Domain Training
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Mahdi Salmani, Pratik Worah, Meisam Razaviyayn, Vahab Mirrokni</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Analyzing data mixing in deep learning, sampling weights and loss weights are shown to play complementary roles in linear regression by reducing gradient variance and enhancing generalization performance. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">In the training of large deep neural networks, there is a need for vast amounts of training data. To meet this need, data is collected from multiple domains, such as Wikipedia and GitHub. These domains are heterogeneous in both data quality and the diversity of information they provide. This raises the question of how much we should rely on each domain. Several methods have attempted to address this issue by assigning sampling weights to each data domain using heuristics or approximations. As a first step toward a deeper understanding of the role of data mixing, this work revisits the problem by studying two kinds of weights: sampling weights, which control how much each domain contributes in a batch, and loss weights, which scale the loss from each domain during training. Through a rigorous study of linear regression, we show that these two weights play complementary roles. First, they can reduce the variance of gradient estimates in iterative methods such as stochastic gradient descent (SGD). Second, they can improve generalization performance by reducing the generalization gap. We provide both theoretical and empirical support for these claims. We further study the joint dynamics of sampling weights and loss weights, examining how they can be combined to capture both contributions.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.06913" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.06913" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.06913" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling / Generative Models, Representation Learning, Attention</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">33. Ginnungagap -- a massively parallel cosmological initial conditions generator
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Sergey Pilipenko, Gustavo Yepes, Stefan Gottlöber, Steffen Knollmann</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Ginnungagap is a new, fully parallel (MPI+OpenMP) and modular open-source code designed for generating cosmological initial conditions, supporting large-scale simulations with uniform, zoom-in, or resolution-extended particle setups. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Ginnungagap is a fully parallel (MPI+OpenMP) code designed to generate cosmological initial conditions for simulations involving very large numbers of particles. It operates in several modes, including the creation of initial conditions with either uniform or spatially varying resolution (for &#34;zoom-in&#34; simulations). The initial conditions can be fully random or derived by extending the resolution of existing ones while preserving the large-scale structures. Ginnungagap is open source and modular, consisting of a collection of independent tools that can be used for a variety of tasks. In this paper, we describe the main features of Ginnungagap and present test results for different types of simulations prepared with it.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.10353" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.10353" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.10353" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.03</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.IM</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">34. Carbox: an end-to-end differentiable astrochemical simulation framework
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Gijs Vermariën, Tommaso Grassi, Marie Van de Sande, Serena Viti, Stefano Bovino, Alessandro Lupi, Alexander Ruf, Lorenzo Branca, Catherine Walsh</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Carbox, a new astrochemical simulation code built on the Jax framework, provides computational efficiency, GPU acceleration, and differentiability crucial for modeling complex chemical networks and integrating with Scientific Machine Learning techniques. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Since the first observations of interstellar molecules, astrochemical simulations have been employed to model and understand its formation and destruction path- ways. With the advent of high-resolution telescopes such as JWST and ALMA, the number of detected molecules has increased significantly, thereby creating a need for increasingly complex chemical reaction networks. To model such complex systems, we have developed Carbox, a new astrochemical simulation code that leverages the modern high-performance transformation framework Jax. With Jax enabling computational efficiency and differentiability, Carbox can easily utilize GPU acceleration, be used to study sensitivity and uncertainty, and interface with advances in Scientific Machine Learning. All of these features are crucial for modeling the molecules observed by current and next-generation telescopes.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.10558" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.10558" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.10558" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">35. Hybrid Autoencoders for Tabular Data: Leveraging Model-Based Augmentation in Low-Label Settings
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Erel Naor, Ofir Lindenbaum</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A hybrid autoencoder utilizes complementary neural and oblivious soft decision tree encoders, guided by feature-selecting gating networks, to implement model-based augmentation that effectively captures high-frequency tabular structure, significantly improving performance in low-label settings. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Deep neural networks often under-perform on tabular data due to their sensitivity to irrelevant features and a spectral bias toward smooth, low-frequency functions. These limitations hinder their ability to capture the sharp, high-frequency signals that often define tabular structure, especially under limited labeled samples. While self-supervised learning (SSL) offers promise in such settings, it remains challenging in tabular domains due to the lack of effective data augmentations. We propose a hybrid autoencoder that combines a neural encoder with an oblivious soft decision tree (OSDT) encoder, each guided by its own stochastic gating network that performs sample-specific feature selection. Together, these structurally different encoders and model-specific gating networks implement model-based augmentation, producing complementary input views tailored to each architecture. The two encoders, trained with a shared decoder and cross-reconstruction loss, learn distinct yet aligned representations that reflect their respective inductive biases. During training, the OSDT encoder (robust to noise and effective at modeling localized, high-frequency structure) guides the neural encoder toward representations more aligned with tabular data. At inference, only the neural encoder is used, preserving flexibility and SSL compatibility. Spectral analysis highlights the distinct inductive biases of each encoder. Our method achieves consistent gains in low-label classification and regression across diverse tabular datasets, outperforming deep and tree-based supervised baselines.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.06961" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.06961" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.06961" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling / Generative Models, Representation Learning, Attention</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">36. How to evaluate the sufficiency and complementarity of summary statistics for cosmic fields: an information-theoretic perspective
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Ce Sui, Yi Mao, Xiaosheng Zhao, Tao Jing, Benjamin D. Wandelt</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Mutual Information offers a robust information-theoretic framework for quantifying the sufficiency and complementarity of summary statistics in cosmic fields, successfully evaluating information extraction from both Gaussian (CMB) and non-Gaussian (21cm brightness temperature) maps. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The advent of increasingly advanced surveys and cosmic tracers has motivated the development of new inference techniques and novel approaches to extracting information from cosmic fields. A central challenge in this endeavor is to quantify the information content carried by these summary statistics in cosmic fields. In particular, how should we assess which statistics are more informative than others and assess the exact degree of complementarity of the information from each statistic? Here, we introduce mutual information (MI) that provides, from an information-theoretic perspective, a natural framework for assessing the sufficiency and complementarity of summary statistics in cosmological data. We demonstrate how MI can be applied to typical inference tasks to make information-theoretic evaluations, using two representative examples: the cosmic microwave background map, from which the power spectrum extracts almost all information as is expected for a Gaussian random field, and the 21~cm brightness temperature map, from which the scattering transform extracts the most non-Gaussian information but is complementary to power spectrum and bispectrum. Our results suggest that MI offers a robust theoretical foundation for evaluating and improving summaries, thereby enabling a deeper understanding of cosmic fields from an information-theoretic perspective.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.08716" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.08716" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.08716" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">37. Identification and characterization of distorted gravitational waves by lensing using deep learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Juno C. L. Chan, Lorena Magaña Zertuche, Jose María Ezquiaga, Rico K. L. Lo, Luka Vujeva, Joey Bowman</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> DINGO-lensing, a neural posterior estimation method, drastically accelerates Bayesian inference for gravitationally lensed gravitational waves, reducing parameter estimation time from weeks to seconds while accurately recovering posterior distributions and evidence ratios. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Gravitational waves (GWs) can be distorted by intervening mass distributions while propagating, leading to frequency-dependent modulations that imprint a distinct signature on the observed waveforms. Bayesian inference for GW lensing with conventional sampling methods is costly, and the problem is exacerbated by the rapidly growing GW catalog. Moreover, assessing the statistical significance of lensed candidates requires thousands, if not millions, of simulations to estimate the background from noise fluctuations and waveform systematics, which is infeasible with standard samplers. We present a novel method, \texttt{DINGO-lensing}, for performing inference on lensed GWs, extending the neural posterior estimation framework \texttt{DINGO}. By comparing our results with those using conventional samplers, we show that the compute time of parameter estimation of lensed GWs can be reduced from weeks to seconds, while preserving accuracy both in the posterior distributions and the evidence ratios. We train our neural networks with LIGO detector noise at design sensitivity and a lens model that accommodates two overlapping chirps with opposite parity. We show that the lensing parameters are recovered with millisecond precision for the time delays. We also demonstrate that our network can identify signals diffracted by point masses, highlighting its flexibility for searches. By simulating thousands of lensed and nonlensed events, we determine how the detectability changes with different source properties. \texttt{DINGO-lensing} provides a scalable and efficient avenue for identifying and characterizing gravitationally lensed GW events in the upcoming observing runs.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.07186" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.07186" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.07186" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">38. Rank-1 LoRAs Encode Interpretable Reasoning Signals
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Jake Ward, Paul Riechers, Adam Shai</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Reasoning performance in large language models stems primarily from minimal, interpretable, single-rank parameter changes, evidenced by a rank-1 LoRA adapter that recovers most benchmark performance and exhibits monosemantic features tied to reasoning behaviors. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Reasoning models leverage inference-time compute to significantly enhance the performance of language models on difficult logical tasks, and have become a dominating paradigm in frontier LLMs. Despite their wide adoption, the mechanisms underpinning the enhanced performance of these reasoning models are not well understood. In this work, we show that the majority of new capabilities in reasoning models can be elicited by small, single-rank changes to base model parameters, with many of these changes being interpretable. Specifically, we use a rank-1 LoRA to create a minimal parameter adapter for Qwen-2.5-32B-Instruct which recovers 73-90% of reasoning-benchmark performance compared to a full parameter finetune. We find that the activations of this LoRA are as interpretable as MLP neurons, and fire for reasoning-specific behaviors. Finally, we train a sparse autoencoder on the entire activation state of this LoRA and identify fine-grained and monosemantic features. Our findings highlight that reasoning performance can arise largely from minimal changes to base model parameters, and explore what these changes affect. More broadly, our work shows that parameter-efficient training methods can be used as a targeted lens for uncovering fundamental insights about language model behavior and dynamics.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.06739" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.06739" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.06739" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling / Generative Models, Representation Learning, Attention</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">39. Transolver is a Linear Transformer: Revisiting Physics-Attention through the Lens of Linear Attention
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Wenjie Hu, Sidun Liu, Peng Qiao, Zhenglun Sun, Yong Dou</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Linear Attention Neural Operator (LinearNO) reformulates the Physics-Attention mechanism in Transformer-based PDE solvers into an efficient canonical linear attention, achieving state-of-the-art performance across multiple benchmarks while substantially reducing model complexity and computational overhead. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Recent advances in Transformer-based Neural Operators have enabled significant progress in data-driven solvers for Partial Differential Equations (PDEs). Most current research has focused on reducing the quadratic complexity of attention to address the resulting low training and inference efficiency. Among these works, Transolver stands out as a representative method that introduces Physics-Attention to reduce computational costs. Physics-Attention projects grid points into slices for slice attention, then maps them back through deslicing. However, we observe that Physics-Attention can be reformulated as a special case of linear attention, and that the slice attention may even hurt the model performance. Based on these observations, we argue that its effectiveness primarily arises from the slice and deslice operations rather than interactions between slices. Building on this insight, we propose a two-step transformation to redesign Physics-Attention into a canonical linear attention, which we call Linear Attention Neural Operator (LinearNO). Our method achieves state-of-the-art performance on six standard PDE benchmarks, while reducing the number of parameters by an average of 40.0% and computational cost by 36.2%. Additionally, it delivers superior performance on two challenging, industrial-level datasets: AirfRANS and Shape-Net Car.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.06294" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.06294" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.06294" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.LG</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling / Generative Models, Representation Learning, Attention</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">40. From Structure to Detail: Hierarchical Distillation for Efficient Diffusion Model
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Hanbo Cheng, Peng Wang, Kaixiang Lei, Qi Li, Zhen Zou, Pengfei Hu, Jun Du</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Hierarchical Distillation (HD) improves single-step diffusion model inference by combining trajectory distillation for structural initialization with distribution-based refinement, employing an Adaptive Weighted Discriminator (AWD) to focus on local imperfections and achieve state-of-the-art fidelity. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The inference latency of diffusion models remains a critical barrier to their real-time application. While trajectory-based and distribution-based step distillation methods offer solutions, they present a fundamental trade-off. Trajectory-based methods preserve global structure but act as a &#34;lossy compressor&#34;, sacrificing high-frequency details. Conversely, distribution-based methods can achieve higher fidelity but often suffer from mode collapse and unstable training. This paper recasts them from independent paradigms into synergistic components within our novel Hierarchical Distillation (HD) framework. We leverage trajectory distillation not as a final generator, but to establish a structural ``sketch&#34;, providing a near-optimal initialization for the subsequent distribution-based refinement stage. This strategy yields an ideal initial distribution that enhances the ceiling of overall performance. To further improve quality, we introduce and refine the adversarial training process. We find standard discriminator structures are ineffective at refining an already high-quality generator. To overcome this, we introduce the Adaptive Weighted Discriminator (AWD), tailored for the HD pipeline. By dynamically allocating token weights, AWD focuses on local imperfections, enabling efficient detail refinement. Our approach demonstrates state-of-the-art performance across diverse tasks. On ImageNet $256\times256$, our single-step model achieves an FID of 2.26, rivaling its 250-step teacher. It also achieves promising results on the high-resolution text-to-image MJHQ benchmark, proving its generalizability. Our method establishes a robust new paradigm for high-fidelity, single-step diffusion models.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.08930" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.08930" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.08930" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling / Generative Models, Representation Learning, Attention</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">41. Tests of General Relativity with Einstein Telescope
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Andrea Begnoni, Walter Del Pozzo, Matteo Pegorin, Joachim Pomper, Angelo Ricciardone</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Utilizing Fisher matrix and Bayesian hierarchical methods, forecasts predict that the Einstein Telescope can constrain post-Newtonian coefficients, achieving an accuracy of $\mathcal{O}(10^{-7})$ on the dipole radiation term, thereby providing stringent future tests of General Relativity using binary black hole mergers. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Gravitational wave signals from compact binary coalescences offer a powerful and reliable probe of General Relativity. To date, the LIGO-Virgo-KAGRA collaboration has provided stringent consistency tests of General Relativity predictions. In this work, we present forecasts for the accuracy with which General Relativity can be tested using third-generation ground-based interferometers, focusing on Einstein Telescope (ET) and binary black hole mergers. Given the expected high detection rate, performing full Bayesian analyses for each event becomes computationally challenging. To overcome this, we adopt a Fisher matrix approach, simulating parameter estimation in an idealized observation scenario, which allows us to study large populations of compact binary coalescences with feasible computational efforts. Within this framework, we investigate the constraints that ET, in its different configurations, can impose on inspiral post-Newtonian coefficients, by jointly analyzing events using a Bayesian hierarchical methodology. Our results indicate that ET could in principle achieve an accuracy of $\mathcal{O}(10^{-7})$ on the dipole radiation term and $\mathcal{O}(10^{-3})$ on higher-order post-Newtonian coefficients, for both the triangular and the two L-shaped designs, with $10^4$ catalog events. We also assess the number of detections required to confidently identify deviations from General Relativity at various post-Newtonian orders and for different detector configurations.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.07520" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.07520" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.07520" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">gr-qc</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">42. Identification of Candidate Halos Hosting Massive Black Hole Seeds in the \textit{Renaissance} Simulations with Support Vector Machines
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Brandon Pries, John H. Wise</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A Support Vector Machine model, optimized using features related to star formation like metallicity and Lyman-Werner radiation flux, successfully identifies and provides probabilistic seeding prescriptions for halos likely to form Direct Collapse Black Holes in cosmological simulations. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The nature of the origins of supermassive black holes remains uncertain. Multiple possible seeding pathways have been proposed across a variety of mass scales, each with their own strengths and weaknesses. One such channel is a direct collapse black hole (DCBH), thought to form from the deaths of supermassive stars in pristine atomic cooling halos in the early universe. In this work, we investigate the ability to identify halos likely to form a DCBH based on their properties using a support vector machine (SVM). We implement multiple methods to improve the accuracy of the model, including selecting subsets of critical features and optimizing SVM hyperparameters. We find that our best model requires quantities relevant to star formation, such as the metallicity, incident flux of Lyman-Werner radiation, and halo stellar mass. The SVMs produced from this work can serve as probabilistic and holistic seeding prescriptions for DCBHs in cosmological simulations.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.08706" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.08706" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.08706" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">43. Analytical Description of Baryonic Matter Fluctuations Using Jeans Filtering Functions in Second-Order Cosmological Perturbation Theory
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Diego Fernando Fonseca, Leonardo Castañeda, Luz Ángela García</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> An analytical cosmological perturbation theory framework, derived from the Vlasov equation and incorporating baryonic pressure via Jeans filtering functions, yields novel first- and second-order solutions that quantify how pressure effects shift the filtering scale and impact the evolution of large-scale structure. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Cosmological perturbation theory provides the fundamental framework for describing the evolution of the matter-energy density field in an expanding Universe and serves as the basis for understanding the formation of large-scale structures within the $Λ$CDM paradigm. We present an analytical approach to describe the evolution of fluctuations in a mixed fluid composed of cold dark matter (CDM) and baryonic matter. Assuming that the Universe is governed by General Relativity, we employ the Vlasov equation to derive the general equations of motion for this mixed cosmological fluid, incorporating baryonic effects through the stress tensor by considering only the contributions from baryonic pressure. We introduce the Jeans filtering functions as a biasing tool that allows us to describe baryonic fluctuations with CDM as a tracer, and we obtain an analytical description of the fluctuations -- a novel and uncommon approach compared to the accepted computational advances currently available in this field. First- and second-order solutions are obtained through a single iteration of the equations of motion, with the aim of identifying how the filtering scale behaves in a second-order theory compared to the linear one, as well as some of its impacts on the matter power spectrum without the need to compute it explicitly. For the first time, these kind of solutions are derived entirely through an analytical method. Finally, we obtain analytical expressions for baryonic fluctuations in the density and velocity fields, which can be readily evaluated and provide valuable insights into the role of baryons in the Large-scale structure of the Universe. Consequently, these results reveal how pressure effects shift the filtering scale and how including this component could influence parameters such as the filtering mass and the temperature of the pressure-supported components.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.08820" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.08820" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.08820" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">44. Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Satoshi Suzuki, Shin&#39;ya Yamaguchi, Shoichiro Takeda, Taiga Yamane, Naoki Makishima, Naotaka Kawata, Mana Ihori, Tomohiro Tanaka, Shota Orihashi, Ryo Masumura</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Difference Vector Equalization (DiVE) is introduced as a robust fine-tuning technique for vision-language models that preserves the geometric structure of embeddings using novel average and pairwise vector losses, significantly enhancing generalization across various distribution settings. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Contrastive pre-trained vision-language models, such as CLIP, demonstrate strong generalization abilities in zero-shot classification by leveraging embeddings extracted from image and text encoders. This paper aims to robustly fine-tune these vision-language models on in-distribution (ID) data without compromising their generalization abilities in out-of-distribution (OOD) and zero-shot settings. Current robust fine-tuning methods tackle this challenge by reusing contrastive learning, which was used in pre-training, for fine-tuning. However, we found that these methods distort the geometric structure of the embeddings, which plays a crucial role in the generalization of vision-language models, resulting in limited OOD and zero-shot performance. To address this, we propose Difference Vector Equalization (DiVE), which preserves the geometric structure during fine-tuning. The idea behind DiVE is to constrain difference vectors, each of which is obtained by subtracting the embeddings extracted from the pre-trained and fine-tuning models for the same data sample. By constraining the difference vectors to be equal across various data samples, we effectively preserve the geometric structure. Therefore, we introduce two losses: average vector loss (AVL) and pairwise vector loss (PVL). AVL preserves the geometric structure globally by constraining difference vectors to be equal to their weighted average. PVL preserves the geometric structure locally by ensuring a consistent multimodal alignment. Our experiments demonstrate that DiVE effectively preserves the geometric structure, achieving strong results across ID, OOD, and zero-shot metrics.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.09973" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.09973" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.09973" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.03</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">cs.CV</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling / Generative Models, Representation Learning, Attention</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">45. A Bayesian Perspective on Evidence for Evolving Dark Energy
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Dily Duan Yi Ong, David Yallup, Will Handley</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Bayesian evidence analysis shows that while $\Lambda$CDM is slightly favored over dynamic dark energy ($w_0w_a$CDM) by DESI and Planck data alone, the preference shifts to $w_0w_a$CDM when DES-Y5 data is added, indicating that the dynamic model is primarily preferred because it resolves a significant tension between the DESI and DES-Y5 datasets. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">The DESI collaboration reports a significant preference for a dynamic dark energy model ($w_0w_a$CDM) over the cosmological constant ($Λ$CDM) when their data are combined with other frontier cosmological probes. We present a direct Bayesian model comparison using nested sampling to compute the Bayesian evidence, revealing a contrasting conclusion: for the key combination of the DESI DR2 BAO and the Planck CMB data, we find the Bayesian evidence modestly favours $Λ$CDM (log-Bayes factor $\ln B = -0.57{\scriptstyle\pm0.26}$), in contrast to the collaboration&#39;s 3.1$σ$ frequentist significance in favoring $w_0w_a$CDM. Extending this analysis to also combine with the DES-Y5 supernova catalogue, our Bayesian analysis reaches a significance of $3.07{\scriptstyle\pm0.10}\,σ$ in favour of $w_0w_a$CDM. By performing a comprehensive tension analysis, employing five complementary metrics, we pinpoint the origin: a significant ($\approx 2.95σ$), low-dimensional tension between DESI DR2 and DES-Y5 that is present only within the $Λ$CDM framework. The $w_0w_a$CDM model is preferred precisely because its additional parameters act to resolve this specific dataset conflict. The convergence of our findings with independent geometric analyses suggests that the preference for dynamic dark energy is primarily driven by the resolution of inter-dataset tensions, warranting a cautious interpretation of its statistical significance.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.10631" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.10631" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.10631" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.01</span>
                        <span class="badge bg-primary">Semantic Score: 0.93</span>
                        
                            <span class="badge bg-secondary">astro-ph.CO</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">46. Zero-Order Sharpness-Aware Minimization
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Yao Fu, Yihang Jin, Chunxia Zhang, Junmin Liu, Haishan Ye</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> ZOSA (Zero-Order Sharpness-Aware Minimization) provides an efficient optimization framework for prompt tuning by combining zero-order gradient estimation via Rademacher perturbation with sharpness-aware minimization, successfully targeting flat minima to enhance generalization in resource-limited few-shot learning tasks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Prompt learning has become a key method for adapting large language models to specific tasks with limited data. However, traditional gradient-based optimization methods for tuning prompts are computationally intensive, posing challenges for efficiency. We introduce ZOSA (Zero-Order Sharpness-Aware Minimization), a novel optimization framework that integrates zero-order optimization with sharpness-aware minimization to enhance prompt tuning. ZOSA employs Rademacher perturbation vectors to estimate gradients without requiring backpropagation. By incorporating sharpness-aware principles, it targets flat minima in the loss landscape, improving generalization. An adaptive learning rate, guided by loss variability, further ensures stable convergence. Experiments on few-shot learning tasks, such as text classification and natural language inference, show that ZOSA significantly outperforms existing methods. With its theoretical foundation and computational efficiency, ZOSA offers a practical solution for prompt-based learning in resource-limited settings.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.09156" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.09156" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.09156" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">math.ST</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling / Generative Models, Representation Learning, Attention</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">47. Mock Observations for the CSST Mission: Integral Field Spectrograph--GEHONG: A Package for Generating Ideal Datacubes
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Shuai Feng, Shiyin Shen, Wei Chen, Zhaojun Yan, Renhao Ye, Jianjun Chen, Xuejie Dai, Junqiang Ge, Lei Hao, Ran Li, et al.</span>
                                <span class="author-full" style="display: none;">Shuai Feng, Shiyin Shen, Wei Chen, Zhaojun Yan, Renhao Ye, Jianjun Chen, Xuejie Dai, Junqiang Ge, Lei Hao, Ran Li, Yu Liang, Lin Lin, Fengshan Liu, Jiafeng Lu, Zhengyi Shao, Maochun Wu, Yifei Xiong, Chun Xu, Jun Yin</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> The GEHONG Python package generates realistic, spatially resolved, three-dimensional spectral data cubes for the CSST-IFS, mocking observations of diverse astrophysical targets like galaxies and AGNs by modeling local physical properties and spectra types. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We developed a Python package GEHONG to mock the three-dimensional spectral data cube under the observation of an ideal telescope for the Integral Field Spectrograph of the Chinese Space Station Telescope (CSST-IFS). This package can generate one-dimensional spectra corresponding to local physical properties at specific positions according to a series of two-dimensional distributions of physical parameters of target sources. In this way, it can produce a spatially resolved spectral cube of the target source. Two-dimensional distributions of physical parameters, including surface brightness, stellar population, and line-of-sight velocity, can be modeled using the parametric model or based on real observational data and numerical simulation data. For the generation of one-dimensional spectra, we have considered four types of spectra, including the stellar continuum spectra, ionized gas emission lines, AGN spectra, and stellar spectra. That makes GEHONG able to mock various types of targets, including galaxies, AGNs, star clusters, and HII regions.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.06927" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.06927" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.06927" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.05</span>
                        <span class="badge bg-primary">Semantic Score: 0.92</span>
                        
                            <span class="badge bg-secondary">astro-ph.IM</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">48. Improving Sustainability of Adversarial Examples in Class-Incremental Learning
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Taifeng Liu, Xinjing Liu, Liangqiu Dong, Yang Liu, Yilong Yang, Zhuo Ma</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Sustainable Adversarial Examples (SAE) enhance the longevity of adversarial attacks against Class-Incremental Learning models by employing a Semantic Correction Module, which uses a visual-language model to generalize AE semantics, and a Filtering-and-Augmentation Module to stabilize them against domain drift. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Current adversarial examples (AEs) are typically designed for static models. However, with the wide application of Class-Incremental Learning (CIL), models are no longer static and need to be updated with new data distributed and labeled differently from the old ones. As a result, existing AEs often fail after CIL updates due to significant domain drift. In this paper, we propose SAE to enhance the sustainability of AEs against CIL. The core idea of SAE is to enhance the robustness of AE semantics against domain drift by making them more similar to the target class while distinguishing them from all other classes. Achieving this is challenging, as relying solely on the initial CIL model to optimize AE semantics often leads to overfitting. To resolve the problem, we propose a Semantic Correction Module. This module encourages the AE semantics to be generalized, based on a visual-language model capable of producing universal semantics. Additionally, it incorporates the CIL model to correct the optimization direction of the AE semantics, guiding them closer to the target class. To further reduce fluctuations in AE semantics, we propose a Filtering-and-Augmentation Module, which first identifies non-target examples with target-class semantics in the latent space and then augments them to foster more stable semantics. Comprehensive experiments demonstrate that SAE outperforms baselines by an average of 31.28% when updated with a 9-fold increase in the number of classes.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.09088" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.09088" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.09088" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.CR</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling / Generative Models, Representation Learning, Attention</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">49. Flexible Simulation Based Inference for Galaxy Photometric Fitting with Synthesizer
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <div onclick="toggleAuthors(this)" style="cursor: pointer;">
                            <strong>
                                <span class="author-short">Thomas Harvey, Christopher C. Lovell, Sophie Newman, Christopher J. Conselice, Duncan Austin, Aswin P. Vijayan, Stephen M. Wilkins, Vadim Rusakov, Qiong Li, Nathan Adams, et al.</span>
                                <span class="author-full" style="display: none;">Thomas Harvey, Christopher C. Lovell, Sophie Newman, Christopher J. Conselice, Duncan Austin, Aswin P. Vijayan, Stephen M. Wilkins, Vadim Rusakov, Qiong Li, Nathan Adams, Kai Magdwick, Matthew Ho</span>
                            </strong>
                            <a class="author-toggle text-primary" style="text-decoration: none;"> (show all)</a>
                        </div>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> Synference, a flexible Python framework utilizing simulation-based inference and neural posterior estimation, achieves amortized, high-speed Bayesian inference of galaxy physical parameters and photometric redshifts from multi-band photometry, demonstrating excellent parameter recovery and significant speedup over traditional sampling methods. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">We introduce Synference, a new, flexible Python framework for galaxy SED fitting using simulation-based inference (SBI). Synference leverages the Synthesizer package for flexible forward-modelling of galaxy SEDs and integrates the LtU-ILI package to ensure best practices in model training and validation. In this work we demonstrate Synference by training a neural posterior estimator on $10^6$ simulated galaxies, based on a flexible 8-parameter physical model, to infer galaxy properties from 14-band HST and JWST photometry. We validate this model, demonstrating excellent parameter recovery (e.g. R$^2&gt;$0.99 for M$_\star$) and accurate posterior calibration against nested sampling results. We apply our trained model to 3,088 spectroscopically-confirmed galaxies in the JADES GOODS-South field. The amortized inference is exceptionally fast, having nearly fixed cost per posterior evaluation and processing the entire sample in $\sim$3 minutes on a single CPU (18 galaxies/CPU/sec), a $\sim$1700$\times$ speedup over traditional nested sampling or MCMC techniques. We demonstrate Synference&#39;s ability to simultaneously infer photometric redshifts and physical parameters, and highlight its utility for rapid Bayesian model comparison by demonstrating systematic stellar mass differences between two commonly used stellar population synthesis models. Synference is a powerful, scalable tool poised to maximise the scientific return of next-generation galaxy surveys.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.10640" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.10640" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.10640" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">astro-ph.GA</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
        <div class="arxiv-card card mb-4">
            <div class="arxiv-title card-header">50. Data Descriptions from Large Language Models with Influence Estimation
                
            </div>
            <div class="arxiv-abstract card-body">
                <div class="arxiv-meta mb-3">
                    
                        <strong>Chaeri Kim, Jaeyeon Bae, Taehwan Kim</strong>
                    
                </div>
                <div class="tldr-section mb-3">
                    <p class="fst-italic text-muted"><strong>TLDR:</strong> A novel Explainable AI pipeline generates informative textual descriptions of data using Large Language Models and external knowledge, employing influence estimation and CLIP scores for selection, which effectively enhances model performance in cross-modal transfer classification tasks. (gemini)</p>
                </div>
                <div class="arxiv-abstract-text">Deep learning models have been successful in many areas but understanding their behaviors still remains a black-box. Most prior explainable AI (XAI) approaches have focused on interpreting and explaining how models make predictions. In contrast, we would like to understand how data can be explained with deep learning model training and propose a novel approach to understand the data via one of the most common media - language - so that humans can easily understand. Our approach proposes a pipeline to generate textual descriptions that can explain the data with large language models by incorporating external knowledge bases. However, generated data descriptions may still include irrelevant information, so we introduce to exploit influence estimation to choose the most informative textual descriptions, along with the CLIP score. Furthermore, based on the phenomenon of cross-modal transferability, we propose a novel benchmark task named cross-modal transfer classification to examine the effectiveness of our textual descriptions. In the experiment of zero-shot setting, we show that our textual descriptions are more effective than other baseline descriptions, and furthermore, we successfully boost the performance of the model trained only on images across all nine image classification datasets. These results are further supported by evaluation using GPT-4o. Through our approach, we may gain insights into the inherent interpretability of the decision-making process of the model.</div>
                <div class="mt-3">
                    <div class="arxiv-links mt-2">
                        <a class="btn btn-sm btn-outline-primary" href="https://arxiv.org/abs/2511.07897" target="_blank">arXiv</a>
                        <a class="btn btn-sm btn-outline-success" href="https://arxiv.org/pdf/2511.07897" target="_blank">PDF</a>
                        <a class="btn btn-sm btn-outline-secondary" href="https://ui.adsabs.harvard.edu/#abs/arXiv:2511.07897" target="_blank">ADS</a>
                    </div>
                </div>
                <div class="mt-2">
                    <div class="arxiv-scores">
                        <span class="badge bg-success">Total Score: 9.4</span>
                        <span class="badge bg-info text-dark">Author Score: 0.00</span>
                        <span class="badge bg-primary">Semantic Score: 0.94</span>
                        
                            <span class="badge bg-secondary">cs.AI</span>
                        
                        
                            <span class="badge" style="background-color: #6f42c1; color: white;">Interest: Cosmology, Statistical Inference, Sampling / Generative Models, Representation Learning, Attention</span>
                        
                    </div>
                </div>
            </div>
        </div>
        
    </div>

    <footer class="footer">
        &copy; 2025 arXiv Daily · Powered by <a href="https://arxiv.org/" target="_blank">arXiv</a>
    </footer>
</body>
</html>